"The authors present RDA, the Recurrent Discounted Attention unit, that improves upon RWA, the earlier introduced Recurrent Weighted Average unit, by adding a discount factor.[[INT-NEU,PDI-NEU,RES-NEU], [null], [SMY], [GEN]] While the RWA was an interesting idea[[PDI-POS], [null], [APC], [MAJ]] with bad results (far worse than the standard GRU or LSTM with standard attention except for hand-picked tasks), the RDA brings it more on-par with the standard methods.[[RWK-NEG,MET-NEG,RES-NEG], [CMP-NEG,EMP-NEG], [CRT], [MAJ]]\n\nOn the positive side, the paper is clearly written and adding discount to RWA, while a small change, is original.[[MET-POS,OAL-POS], [CLA-POS,EMP-POS], [APC], [MAJ]] On the negative side, in almost all tasks the RDA is on par or worse than the standard GRU[[RWK-NEG,MET-NEG], [CMP-NEG,EMP-NEG], [CRT], [MAJ]] - except for MultiCopy where it trains faster,[[MET-POS], [EMP-POS], [APC], [MIN]] but not to better results and it looks like the difference is between few and very-few training steps anyway.[[RES-NEG], [EMP-NEG], [CRT], [MIN]] The most interesting result is language modeling on Hutter Prize Wikipedia, where RDA very significantly improves upon RWA - but again, only matches a standard GRU or LSTM.[[RES-NEU,EXT-NEU], [CNT], [DIS], [MIN]] So the results are not strongly convincing, and the paper lacks any mention of newer work on attention.[[RES-NEG], [NOV-NEG], [CRT], [MAJ]] This year strong improvements over state-of-the-art have been achieved using attention for translation (\"Attention is All You Need\") and image classification (e.g., Non-local Neural Networks, but also others in ImageNet competition).[[EXT-NEU], [null], [DIS], [MIN]] To make the evaluation convincing enough for acceptance, RDA should be combined with those models and evaluated more competitively on multiple widely-studied tasks."[[MET-NEU,RES-NEU], [CMP-NEU], [SUG], [MAJ]]