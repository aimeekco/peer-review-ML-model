"Paper summary:\nExisting works on multi-task neural networks typically use hand-tuned weights for weighing losses across different tasks.[[INT-NEU], [null], [SMY], [GEN]] This work proposes a dynamic weight update scheme that updates weights for different task losses during training time by making use of the loss ratios of different tasks.[[MET-NEU], [null], [SMY], [GEN]] Experiments on two different network indicate that the proposed scheme is better than using hand-tuned weights for multi-task neural networks.[[EXP-NEU], [null], [SMY], [GEN]]\n\n\nPaper Strengths:\n- The proposed technique seems simple yet effective for multi-task learning.[[MET-POS], [EMP-POS], [APC], [MAJ]]\n- Experiments on two different network architectures showcasing the generality of the proposed method.[[EXP-POS], [EMP-POS], [APC], [MAJ]]\n\n\nMajor Weaknesses:\n- The main weakness of this work is the unclear exposition of the proposed technique.[[MET-NEG], [EMP-NEG], [DFT], [MAJ]] Entire technique is explained in a short section-3.1 with many important details missing.[[MET-NEU], [SUB-NEG], [DFT], [MAJ]] There is no clear basis for the main equations 1 and 2.[[MET-NEG], [EMP-NEG], [DFT], [MAJ]] How does equation-2 follow from equation-1? Where is the expectation coming from? What exactly does \u2018F\u2019 refer to? There is dependency of \u2018F\u2019 on only one of sides in equations 1 and 2? More importantly, how does the gradient normalization relate to loss weight update?[[MET-NEU], [EMP-NEU], [QSN], [MAJ]] It is very difficult to decipher these details from the short descriptions given in the paper.[[MET-NEG], [SUB-NEG], [DFT], [MAJ]]\n- Also, several details are missing in toy experiments[[EXP-NEG], [EMP-NEG], [DFT], [MAJ]]. What is the task here? What are input and output distributions and what is the relation between input and output? Are they just random noises? If so, is the network learning to overfit to the data as there is no relationship between input and output?[[EXP-NEU], [EMP-NEU], [QSN], [MAJ]] \n\n\nMinor Weaknesses:\n- There are no training time comparisons between the proposed technique and the standard fixed loss learning.[[EXP-NEG], [CMP-NEG], [DFT], [MIN]]\n- Authors claim that they operate directly on the gradients inside the network.[[MET-NEU], [EMP-NEU], [SMY], [GEN]] But, as far as I understood, the authors only update loss weights in this paper.[[EXP-NEG], [EMP-NEG], [DFT], [MIN]] Did authors also experiment with gradient normalization in the intermediate CNN layers?[[EXP-NEU], [null], [QSN], [MIN]]\n- No comparison with state-of-the-art techniques on the experimented tasks and datasets.[[EXP-NEU], [CMP-NEG], [DFT], [MIN]]\n\n\nClarifications:\n- See the above mentioned issues with the exposition of the technique.[[MET-NEU], [null], [DFT], [MIN]]\n- In the experiments, why are the input images downsampled to 320x320?[[EXP-NEU], [null], [QSN], [MIN]]\n- What does it mean by \u2018unofficial dataset\u2019 (page-4)[[DAT-NEU], [null], [QSN], [MIN]]. Any references here[[BIB-NEU], [null], [DFT], [MIN]]?\n- Why is 'task normalized' test-time loss as good measure for comparison between models in the toy example (Section 4)?[[EXP-NEU], [CMP-NEU,EMP-NEU], [QSN], [MIN]] The loss ratios depend on initial loss, which is not important for the final performance of the system.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]]\n\n\nSuggestions:\n- I strongly suggest the authors to clearly explain the proposed technique to get this into a publishable state.[[MET-NEG], [SUB-NEG], [SUG], [MIN]] \n- The term \u2019GradNorm\u2019 seem to be not defined anywhere in the paper.[[CNT], [PNF-NEG], [SUG], [MIN]]\n\n\nReview Summary:\nDespite promising results, the proposed technique is quite unclear from the paper.[[MET-NEG,RES-POS], [EMP-NEG], [CRT], [MAJ]] With its poor exposition of the technique, it is difficult to recommend this paper for publication."[[OAL-NEG], [REC-NEG], [FBK], [MAJ]]