"Summary and significance: The authors prove that for expressing simple multivariate monomials over n variables, networks of depth 1 require exp(n) many neurons, whereas networks of depth n can represent these monomials using only O(n) neurons.[[MET-POS,RES-POS], [IMP-POS], [SMY], [GEN]] \nThe paper provides a simple and clear explanation for the important problem of theoretically explaining the power of deep networks, and quantifying the improvement provided by depth.[[EXP-POS,MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]]\n\n+ves:\nExplaining the power of depth in NNs is fundamental to an understanding of deep learning.[[MET-POS], [EMP-POS], [APC], [MAJ]] The paper is very easy to follow.[[OAL-POS], [PNF-POS], [APC], [MAJ]] and the proofs are clearly written.[[MET-POS], [CLA-POS], [APC], [MAJ]] The theorems provide exponential gaps for very simple polynomial functions.[[MET-POS], [SUB-POS,EMP-POS], [APC], [MAJ]]\n\n-ves:\n1. My main concern with the paper is the novelty of the contribution to the techniques.[[MET-NEG], [NOV-NEG], [CRT], [MAJ]] The results in the paper are more general than that of Lin et al., but the proofs are basically the same, and it's difficult to see the contribution of this paper in terms of the contributing fundamentally new ideas.[[RWK-NEG,MET-NEG,RES-NEG], [NOV-NEG,CMP-NEG], [CRT], [MAJ]] \n2. The second concern is that the results apply only to non-linear activation functions with sufficiently many non-zero derivatives (same requirements as for the results of Lin et al.).[[RWK-NEG,RES-NEG], [CMP-NEG], [CRT], [MAJ]]\n3. Finally, in prop 3.3, reducing from uniform approximations to Taylor approximations, the inequality |E(\u03b4x)| <= \u03b4^(d+1) |N(x) - p(x)| does not follow from the definition of a Taylor approximation.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MIN]]\n\nDespite these criticisms, I contend that the significance of the problem, and the clean and understandable results in the paper make it a decent paper for ICLR."[[RES-POS], [APR-POS,EMP-POS], [APC], [MAJ]]