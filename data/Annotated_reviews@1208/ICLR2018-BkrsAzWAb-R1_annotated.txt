"SUMMARY:\n\nThe authors reinvent a 20 years old technique for adapting a global or component-wise learning rate for gradient descent.[[INT-NEU], [null], [SMY], [GEN]] The technique can be derived as a gradient step for the learning rate hyperparameter, or it can be understood as a simple and efficient adaptation technique.[[MET-NEU], [null], [SMY], [GEN]]\n\n\nGENERAL IMPRESSION:\n\nOne central problem of the paper is missing novelty.[[MET-NEG], [NOV-NEG], [CRT], [MAJ]] The authors are well aware of this. They still manage to provide added value.[[OAL-NEU], [NOV-NEU], [SMY], [MAJ]]\nDespite its limited novelty, this is a very interesting and potentially impactful paper.[[OAL-POS], [NOV-NEU,IMP-POS], [APC], [MAJ]] I like in particular the detailed discussion of related work, which includes some frequently overlooked precursors of modern methods.[[RWK-POS], [SUB-POS], [APC], [MAJ]]\n\n\nCRITICISM:\n\nThe experimental evaluation is rather solid, but not perfect. [[EXP-NEU], [EMP-NEU], [CRT], [MAJ]]It considers three different problems: logistic regression (a convex problem), and dense as well as convolutional networks. That's a solid spectrum.[[EXP-NEG], [EMP-NEG], [SMY], [MAJ]] However, it is not clear why the method is tested only on a single data set: MNIST.[[DAT-NEU,EXP-NEU], [SUB-NEG], [DFT], [MAJ]] Since it is entirely general, I would rather expect a test on a dozen different data sets.[[EXP-NEU], [SUB-NEU], [SUG], [MAJ]] That would also tell us more about a possible sensitivity w.r.t. the hyperparameters \\alpha_0 and \\beta.[[MET-NEU], [EMP-NEU], [SUG], [MAJ]]\n\nThe extensions in section 5 don't seem to be very useful.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] In particular, I cannot get rid of the impression that section 5.1 exists for the sole purpose of introducing a convergence theorem.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Analyzing the actual adaptive algorithm would be very interesting.[[MET-NEU,ANA-POS], [EMP-NEU], [SUG], [MAJ]] In contrast, the present result is trivial and of no interest at all, since it requires knowing a good parameter setting, which defeats a large part of the value of the method.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]]\n\n\nMINOR POINTS:\n\npage 4, bottom: use \\citep for Duchi et al. (2011).[[BIB-NEU], [PNF-NEG], [DFT], [MIN]]\n\nNone of the figures is legible on a grayscale printout of the paper.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]] Please do not use color as the only cue to identify a curve.[[TNF-NEU], [PNF-NEU], [SUG], [MIN]]\n\nIn figure 2, top row, please display the learning rate on a log scale.[[TNF-NEU], [PNF-NEU], [SUG], [MIN]]\n\npage 8, line 7 in section 4.3: \"the the\" (unintended repetition)[[CNT], [PNF-NEG], [SUG], [MIN]]\n\nEnd of section 4: an increase from 0.001 to 0.001002 is hardly worth reporting - or am I missing something?[[RES-NEU], [EMP-NEU], [QSN,CRT], [MIN]]\n"