"This paper looks at the problem of optimizing hyperparameters under the assumption that the unknown function can be approximated by a sparse and low degree polynomial in the Fourier basis.[[INT-NEU,PDI-NEU,MET-NEU], [IMP-NEU,EMP-NEU], [SMY,DIS], [GEN]] The main result is that the approximate minimization can be performed over the boolean hypercube where the number of evaluations is linear in the sparsity parameter.[[DAT-NEU,EXP-NEU,RES-NEU], [IMP-NEU,EMP-NEU], [SUG,DIS], [GEN]] \n\nIn the presented experiments, the new spectral method outperforms the tool based on the Bayesian optimization, technique based on MAB and random search[[EXP-NEU,MET-NEU], [SUB-NEU,EMP-NEU], [SMY], [GEN]]. Their result also has an application in learning decision trees where it significantly improves the sample complexity bound.\[[MET-POS,RES-POS], [IMP-POS], [APC], [MAJ]]n\nThe main theoretical result, i.e., the improvement in the sample complexity when learning decision trees, looks very strong.[[EXP-POS,RES-POS], [SUB-POS], [APC], [MAJ]] However, I find this result to be out of the context with the main theme of the paper.[[INT-NEU,RWK-NEG,RES-NEG], [IMP-NEG], [DFT], [MIN]] \n\nI find it highly unlikely that a person interested in using Harmonica to find the right hyperparamters for her deep network would also be interested in provable learning of decision trees in quasi-polynomial time along with a polynomial sample complexity.[[DAT-NEG,EXP-NEG,MET-NEG,OAL-NEG], [IMP-NEG,EMP-NEG], [DFT], [MIN]] Also the theoretical results are developed for Harmonica-1 while Harmonica-q is the main method used in the experiments[[EXP-NEU,RES-NEU,ANA-NEU], [IMP-NEU,CMP-NEU,EMP-NEU], [SMY,DIS], [GEN]].\n\nWhen it comes to the experiments only one real-world experiment is present[[EXP-NEU], [EMP-NEU], [DIS], [GEN]]. It is hard to conclude which method is better based on a single real-world experiment[[EXP-NEU,MET-NEU,ANA-NEU], [IMP-NEU,CMP-NEU,EMP-NEU], [SUG,DIS], [GEN]]. Moreover, the plots are not very intuitive, i.e., one would expect that Random Search takes the smallest amount of time.[[MET-NEU], [EMP-NEU], [SMY], [GEN]] I guess the authors are plotting the running time that also includes the time needed to evaluate different configurations.[[RWK-NEU,ANA-NEU], [CMP-NEU,EMP-NEU], [DIS], [GEN]] If this is the case, some configurations could easily require more time to evaluate than the others.[[MET-NEU,ANA-NEU], [EMP-NEU], [DIS], [GEN]] It would be useful to plot the total number of function evaluations for each of the methods next to the presented plots.[[MET-NEU,ANA-NEU], [EMP-NEU], [DIS], [GEN]]\n\nIt is not clear what is the stopping criterion for each of the methods used in the experiments[[EXP-NEG,MET-NEG], [EMP-NEG], [DFT], [MIN]]. One weakness of Harmonica is that it has 6 hyperparameters itself to be tuned. [[MET-NEG], [EMP-NEG], [DFT], [MIN]]It would be great to see how Harmonica compares with some of the High-dimensional Bayesian optimization methods.[[DAT-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]] \n\nFew more questions:\n\nWhich problem does Harmonica-q solves that is present in Harmonica-1, and what is the intuition behind the fact that it achieves better empirical results?\n[[MET-NEU,RES-NEU], [CMP-NEU,EMP-NEU], [DIS], [GEN]]\nHow do you find best t minimizers of g_i in line 4 of Algorithm 3?\n[[MET-NEU], [EMP-NEU], [DIS], [GEN]]"