"This paper proposes a learning method (PIB) based on the information bottleneck framework.[[INT-NEU], [null], [SMY], [GEN]]\nPIB pursues the very natural intuition outlined in the information bottleneck literature: hidden layers of deep nets compress the input X while maintaining sufficient information to predict the output Y.[[INT-NEU], [null], [SMY], [GEN]]\nIt should be noted that the limitations of the IB for deep learning are currently under heavy discussion on OpenReview.[[EXT-NEU], [null], [DIS], [GEN]]\nOptimizing the PIB objective is intractable and the authors propose an approximation that applies to binary valued stochastic networks.[[PDI-NEU], [null], [SMY], [GEN]]\nThey use a variational bound to deal with the relevance term, I(Z_l,Y), and  Monte Carlo sampling to deal with the layer-by-layer compression term, I(Z_l,Z_{l+1}).[[MET-NEU], [null], [SMY], [GEN]]\nThey present results on MNIST aiming to demonstrate that using PIBs improves generalization and training speed.[[DAT-NEU,RES-NEU], [null], [SMY], [GEN]]\n\nThis is a timely and interesting topic.[[PDI-POS], [EMP-POS], [APC], [MAJ]] I enjoyed learning about the authors\u2019 proposed approach to a practical learning method based on the information bottleneck.[[MET-POS], [EMP-POS], [APC], [MAJ]] However, the writing made it challenging and the experimental protocol raised some serious questions.[[EXP-NEU], [CLA-NEU], [DIS], [MIN]] In summary, I think the paper needs very careful editing for grammar and language and, more importantly, it needs solid experiments before it\u2019s ready for publication.[[EXP-NEU,OAL-NEG], [CLA-NEG,EMP-NEG], [CRT], [GEN]] When that is done it would make an exciting contribution to the community.[[OAL-NEU], [CLA-NEU], [DIS], [MAJ]] More details follow.\n\n\nComments:\n1. All architectures and objectives (both classic and PIB-based) are trained using a single, fixed learning rate (LR).[[EXP-NEU], [null], [DIS], [GEN]] In my opinion, this is a red flag.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]] The PIB objective is new and different to the other objectives.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Do all objectives happen to yield their best performance under the same LR?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Maybe so, but we won\u2019t know unless the experimental protocol prescribes a sufficient range of LRs for each architecture.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]] In light of this, the fact that SFNN is given extra epochs in Figure 4 does not mean much.[[TNF-NEG,MET-NEG], [EMP-NEG], [CRT], [MIN]]\n2. The batch size for MNIST classification is unusually low (8) .[[DAT-NEU], [null], [DIS], [GEN]] Common batch sizes range from 64 to 1K (typically >= 128).[[DAT-NEU], [null], [DIS], [GEN]]  Why did the authors make this choice?[[DAT-NEU], [SUB-NEU], [QSN], [MIN]]  Is 8 good for architectures A through E?\n3.[[DAT-NEU], [SUB-NEU], [QSN], [MIN]] On a related note, the authors only seem to report results from a single random seed (ie. deterministic architectures are trained exactly once).[[RES-NEU], [EMP-NEU], [DIS], [MIN]] I would like to see results from a few different random seeds.[[RES-NEU], [EMP-NEU], [DIS], [MIN]]  As a result of comments 1,2,3, even though I do believe in the merit of the intuition pursued and the techniques proposed, I am not convinced about the main claim of the paper.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]  In particular, the experiments are not rigorous enough to give serious evidence that PIBs improve generalization and training speed.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]]  \n4. The paper needs some careful editing both for language (cf. following point) but also notation.[[OAL-NEG], [CLA-NEG,PNF-NEG], [CRT], [MIN]]  The authors use notation p_D() in eqn (12) without defining it.[[OAL-NEG], [PNF-NEG], [CRT], [MIN]] My best guess is that it is the same as p_u(), the underlying data distribution, but makes parsing the paper hard.[[OAL-NEG], [PNF-NEG], [CRT], [MIN]] Finally there are a few steps that are not explained: for example, no justification is given for the inequality in eqn (13).\n5.[[MET-NEG,ANA-NEG], [EMP-NEG], [DFT], [MAJ]] Language: the paper needs some careful editing to correct numerous language/grammar issues.[[OAL-NEG], [CLA-NEG], [SUG,CRT], [MIN]] At times it is detrimental to understanding.[[CNT], [CNT], [DIS], [GEN]] For example I had to read the text leading up to eqn (8) a number of times.[[MET-NEG], [CLA-NEG], [CRT], [MIN]]\n6. There is no discussion of computational complexity and wall-clock time comparisons.[[MET-NEG], [CMP-NEG], [DFT], [MIN]] To be clear, I think that even if the proposed approach were to be slower than the state of the art it would still be very interesting.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MIN]] However, there should be some discussion and reporting of that aspect as well.[[RWK-NEU], [CMP-NEU,SUB-NEU], [DIS], [MIN]]\n\n\nMinor comments and questions:\n7. Mutual information is typically typeset using a semicolon instead of a comma, eg. I(X;Z).\n8.[[MET-NEG], [CLA-NEG], [CRT], [MIN]] Why is the mutual information in Figure 3 so low?[[TNF-NEG], [CNT], [QSN], [MIN]] Are you perhaps using natural logarithms to estimate and plot I(Z;Y)?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] If this is base-2 logarithms I would expect a value close to 1. "[[MET-NEU], [EMP-NEU], [DIS], [MIN]]