"This paper introduces MiniMax Curriculum learning, as an approach for adaptively train models by providing it different subsets of data.[[INT-NEU], [null], [SMY], [GEN]] The authors formulate the learning problem as a minimax problem which tries to choose diverse example and \"hard\" examples, where the diversity is captured via a Submodular Loss function and the hardness is captured via the Loss function.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] The authors formulate the problem as an iterative technique which involves solving a minimax objective at every iteration.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] The authors argue the convergence results on the minimax objective subproblem, but do not seem to give results on the general problem.[[EXP-NEU,RES-NEG], [EMP-NEG], [CRT], [MAJ]] The ideas for this paper are built on existing work in Curriculum learning, which attempts to provide the learner easy examples followed by harder examples later on.[[PDI-NEU], [null], [SMY], [GEN]] The belief is that this learning style mimics human learners[[PDI-NEU], [null], [SMY], [GEN]].\n\nPros:\n- The analysis of the minimax objective is novel and the proof technique introduces several interesting ideas.[[PDI-POS], [NOV-POS], [APC], [MAJ]]\n- This is a very interesting application of joint convex and submodular optimization, and uses properties of both to show the final convergence results.[[MET-POS], [EMP-POS], [SMY], [MAJ]]\n- Even through the submodular objective is only approximately solvable, it still translates into a convergence result.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]]\n- The experimental results seem to be complete for the most part.[[EXP-POS,RES-POS], [EMP-POS], [APC], [MAJ]] They argue how the submodular optimization does not really affect the performance and diversity seems to empirically bring improvement on the datasets tried.[[RES-POS,ANA-POS], [EMP-POS], [SMY], [MAJ]]\n\nCons:\n- The main algorithm MCL is only a hueristic.[[MET-NEU], [EMP-NEU], [SMY], [MAJ]] Though the MiniMax subproblem can converge, the authors use this in somewhat of a hueristic manner.[[MET-NEU], [EMP-NEU], [SMY], [MAJ]]\n- It seems somewhat hand wavy in the way the authors describe the hyper parameters of MCL, and it seems unclear when the algorithm converge and how to increase/decrease it over iterations.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n- The objective function also seems somewhat non-intuitive.[[MET-NEU], [EMP-NEU], [SMY], [MAJ]] Though the experimental results seem to indicate that the idea works, I think the paper does not motivate the loss function and the algorithm well.[[EXP-NEU,MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n- It seems to me the authors have experimented with smaller datasets (CIFAR, MNIST, 20NewsGroups).[[DAT-NEU,EXP-NEU], [SUB-NEG], [CRT], [MAJ]] This being mainly an empirical paper, I would have expected results on a few larger datasets (e.g. ImageNet, CelebFaces etc.), particularly to see if the idea also scales to these more real world larger datasets.[[DAT-NEU,EXP-NEU], [SUB-NEU,EMP-NEU], [SUG], [MAJ]]\n\nOverall, I would like to see if the paper could have been stronger empirically.[[OAL-NEU], [EMP-NEU], [SUG], [MAJ]] Nevertheless, I do think there are some interesting ideas theoretically and algorithmically.[[PDI-POS,MET-POS], [EMP-POS], [APC], [MAJ]] For this reason, I vote for a borderline accept.[[OAL-NEU], [REC-POS], [FBK], [MAJ]] "