"The paper analyzes the the effect of increasing the batch size in stochastic gradient descent as an alternative to reducing the learning rate, while keeping the number of training epochs constant.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] This has the advantage that the training process can be better parallelized, allowing for faster training if hundreds of GPUs are available for a short time.[[EXP-POS,ANA-POS], [EMP-POS], [APC], [MAJ]] The theory part of the paper briefly reviews the relationship between learning rate, batch size, momentum coefficient, and the noise scale in stochastic gradient descent.[[MET-POS], [null], [SMY], [GEN]] In the experimental part, it is shown that the loss function and test accuracy depend only on the schedule of the decaying noise scale over training time, and are independent of whether this decaying noise schedule is achieved by a decaying learning rate or an increasing batch size.[[EXP-NEU], [null], [SMY], [GEN]] It is shown that simultaneously increasing the momentum parameter and the batch size also allows for fewer parameters, albeit at the price of some loss in performance.[[EXP-NEU,RES-NEU], [null], [SMY], [GEN]]\n\nCOMMENTS:\n\nThe paper presents a simple observation that seems very relevant especially as computing resources are becoming increasingly available for rent on short time scales.[[RES-POS,FWK-POS], [IMP-POS,EMP-POS], [APC], [MAJ]] The observation is explained well and substantiated by clear experimental evidence.[[EXP-POS,RES-POS], [PNF-POS,EMP-POS], [APC], [MAJ]] The main issue I have is with the part about momentum.[[RES-NEG], [EMP-NEG], [CRT], [MIN]] The paragraph below Eq. 7 provides a possible explanation for the performance drop when $m$ is increased.[[RES-NEU], [null], [DIS], [GEN]] It is stated that at the beginning of the training, or after increasing the batch size, the magnitude of parameter updates is suppressed because $A$ has to accumulate gradient signals over a time scale $B/(N(1-m))$.[[EXP-NEU], [null], [DIS], [GEN]] The conclusion in the paper is that training at high momentum requires additional training epochs before $A$ reaches its equilibrium value. [[EXP-NEU,RES-NEU], [null], [DIS], [GEN]]This effect is well known, but it can easily be remedied.[[RES-NEG], [EMP-NEG], [CRT], [MIN]] For example, the update equations in Adam were specifically designed to correct for this effect.[[RWK-NEU], [CMP-NEU], [DIS], [MIN]] The mechanism is called \"bias-corrected moment estimate\" in the Adam paper, arXiv:1412.6980.[[RWK-NEU], [null], [DIS], [MIN]] The correction requires only two extra multiplications per model parameter and update step.[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]] Couldn't the same or a very similar trick be used to correctly rescale $A$ every time one increases the batch size?.[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]] It would be great to see the equivalent of Figure 7 with correctly rescaled $A$.[[TNF-NEG], [PNF-NEG], [SUG], [MIN]]\n\nMinor issues:\n* The last paragraph of Section 5 refers to a figure 8, which appears to be missing. [[TNF-NEG], [SUB-NEG], [DFT], [MIN]]\n* In Eqs. 4 & 5, the momentum parameter $m$ is not yet defined (it will be defined in Eqs. 6 & 7 below).[[MET-NEG], [SUB-NEG], [DFT], [MIN]]\n* It appears that a minus sign is missing in Eq. 7.[[MET-NEG], [PNF-NEG], [DFT], [MIN]] The update steps describe gradient ascent.[[MET-NEU], [null], [DIS], [MIN]]\n* Figure 3 suggests that most of the time between the first and second change of the noise scale (approx. epochs 60 to 120) are spent on overfitting.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]] This suggests that the number of updates in this segment was chosen unnecessarily large to begin with.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]] It is therefore not surprising that reducing the number of updates does not deteriorate the test set accuracy.[[RES-NEU], [EMP-NEU], [DIS], [MIN]]\n* It would be interesting to see a version of figure 5 where the horizontal axis is the number of epochs.[[TNF-NEU], [PNF-NEU], [SUG], [MIN]] While reducing the number of updates allows for faster training if a large number of parallel hardware instances are available, the total cost of training is still governed by the number of training epochs.[[EXP-NEU], [null], [DIS], [MIN]]\n* It appears like the beginning of the second paragraph in Section 5.2 describes figure 1. Is this correct?"[[TNF-NEU], [CNT], [QSN], [MIN]]