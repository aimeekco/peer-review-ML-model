"This paper presents a method for clustering based on latent representations learned from the classification of transformed data after pseudo-labellisation corresponding to applied transformation. [[INT-NEU,DAT-NEU,MET-NEU], [null], [SMY], [GEN]]Pipeline: -Data are augmented with domain-specific transformations[[RWK-NEU,DAT-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]]. For instance, in the case of MNIST, rotations with different degrees are applied[[EXP-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]]. All data are then labelled as \"original\" or \"transformed by ...(specific transformation)\". -Classification task is performed with a neural network on augmented dataset according to the pseudo-labels.[[RWK-NEU,DAT-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SMY,DIS], [GEN]] -In parallel of the classification, the neural network also learns the latent representation in an unsupervised fashion.[[RWK-NEU], [null], [SMY], [GEN]] -k-means clustering is performed on the representation space observed in the hidden layer preceding the augmented softmax layer.[[PDI-NEU,MET-NEU,ANA-NEU], [EMP-NEU], [SMY,DIS], [GEN]] \n\nDetailed Comments:\n(*) Pros\n-The method outperforms the state-of-art regarding unsupervised methods for handwritten digits clustering on MNIST.[[RWK-NEU,PDI-NEU,MET-NEU], [NOV-NEU,EMP-NEU], [SMY,DIS], [GEN]]\n-Use of ACOL and GAR is interesting, also the idea to make \"labeled\" data from unlabelled ones by using data augmentation.[[PDI-NEU,DAT-NEU,MET-POS], [EMP-POS], [SMY,DIS], [GEN]]\n\n(*) Cons\n-minor: in the title, I find the expression \"unsupervised clustering\" uselessly redundant since clustering is by definition unsupervised.[[RWK-NEG,OAL-NEG], [null], [DIS], [GEN]]\n-Choice of datasets: we already obtained very good accuracy for the classification or clustering of handwritten digits[[DAT-POS,EXP-NEU,MET-NEU,RES-POS], [IMP-POS,EMP-POS], [APC], [MAJ]]. This is not a very challenging task.[[DAT-NEU], [null], [SMY], [GEN]]\nAnd just because something works on MNIST, does not mean it works in general.[[DAT-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] \nWhat are the performances on more challenging datasets like colored images (CIFAR-10, labelMe, ImageNet, etc.)?[[DAT-NEU], [null], [QSN], [GEN]]\n-This is not clear what is novel here since ACOL and GAR already exist.[[RWK-NEG], [CLA-NEG,NOV-NEG], [CRT], [MIN]] The novelty seems to be in the adaptation to GAR from the semi-supervised to the unsupervised setting with labels indicating if data have been transformed or not.[[PDI-NEU,DAT-NEU,EXP-NEU,MET-NEU], [NOV-NEU], [SMY,DIS], [GEN]]\n\n\nMy main problem  was about the lack of novelty. [[PDI-NEG], [NOV-NEG], [DFT], [MIN]]The authors clarified this point, and it turned out that ACOL and GAR have never published elsewhere except in ArXiv[[PDI-NEU,MET-NEU], [null], [SUG,DIS], [GEN]].  The other issue concerned the validation of the approach on databases other than MNIST.[[PDI-NEU,DAT-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SMY,DIS], [GEN]] The author also addressed this point, and I changed my scores accordingly. "[[RWK-NEU,RES-NEU], [IMP-NEU], [SMY,DIS,FBK], [GEN]]