"\nSummary of the paper\n-------------------------------\n\nThis paper proposes to factorize the hidden-to-hidden matrix of RNNs into a Kronecker product of small matrices, thus reducing the number of parameters, without reducing the size of the hidden vector.[[PDI-NEU], [null], [SMY], [GEN]] They also propose to use a soft unitary constraint on those small matrices (which is equivalent to a soft unitary constraint on the Kronecker product of those matrices), that is fast to compute.[[PDI-NEU], [null], [SMY], [GEN]] They evaluate their model on 6 small scale RNN experiments.[[EXP-NEU,MET-NEU], [null], [SMY], [GEN]]\n\nClarity, Significance and Correctness\n--------------------------------------------------\n\nClarity: The main idea is clearly motivated and presented,[[PDI-POS], [CLA-POS], [APC], [MAJ]] but the experiment section failed to convince me (see details below).[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nSignificance: The idea of using factorization for RNNs is not particularly novel.[[PDI-NEG], [NOV-NEG], [CRT], [MAJ]] However, it is really nice to be able to decouple the hidden size and the number of recurrent parameters in a simple way.[[PDI-NEU], [null], [DIS], [GEN]] Also, the combination of Kronecker product and soft unitary constraint is really interesting.[[MET-POS], [EMP-POS], [APC], [MAJ]]\n\nCorrectness: There are minor flaws.[[OAL-NEU], [null], [CRT], [MIN]] Some of the baselines seems to perform poorly, and some comparisons with the baselines seems unfair (see the questions below).[[RWK-NEG], [CMP-NEG], [CRT], [MAJ]]\n\nQuestions\n--------------\n\n1. Section 3: You say that you can vary 'pf' and 'qf' to set the trade-off between computational budget and performances.[[PDI-NEU], [null], [SMY], [GEN]] Have you run some experiments where you vary those parameters?[[EXP-NEU], [SUB-NEU], [QSN], [MIN]]\n2. Section 4: Are you using the soft unitary constraint in your experiments?[[EXP-NEU], [SUB-NEU], [QSN], [MIN]] Do you have an hyper-parameter that sets the amplitude of the constraint?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]] If yes, what is its value?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]] Are you using it also on the vanilla RNN or the LSTM?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]]\n3. Section 4.1: You say that you don't train the recurrent matrix in the KRU version.[[EXP-NEU], [null], [DIS], [MIN]] Do you also not train the recurrent matrix in the other models (RNN, LSTM,...)?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]] If yes, how do you explain the differences?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]] If no, I don't see how those curves compare.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]]\n4. Section 4.3: Why does your LSTM in pMNIST performs so poorly?[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]] There are way better curves reported in the literature (eg in \"Unitary Evolution Recurrent Neural Netwkrs\" or \"Recurrent Batch Normalization\").[[RWK-NEG], [CMP-NEG], [CRT], [MIN]]\n5. General: How does your method compares with other factorization approaches, such as in \"Factorization Tricks for LSTM Networks\"?[[MET-NEU], [CMP-NEU], [QSN], [MAJ]]\n6. Section 4: How does the KRU compares to the other parametrizations, in term of wall-clock time?[[MET-NEU], [CMP-NEU], [QSN], [MAJ]]\n\nRemarks\n------------\n\nThe main claim of the paper is that RNN are over-parametrized and take a long time to train (which I both agree with), but you didn't convinced me that your parametrization solve any of those problems.[[PDI-NEU,MET-NEG], [EMP-NEG], [CRT], [MAJ]] I would suggest to:\n1. Compare more clearly setups where you fix the hidden size.[[EXP-NEU,MET-NEU], [CMP-NEU], [SUG], [MIN]]\n2. Compare more clearly setups where you fix the number of parameters.[[EXP-NEU,MET-NEU], [CMP-NEU], [SUG], [MIN]]\nWith systematic comparisons like that, it would be easier to understand where the gains in performances are coming from.[[EXP-NEU,MET-NEU], [CMP-NEU], [DIS], [MIN]]\n3. Add an experiment where you vary 'pf' and 'qf' (and keep the hidden size fixed) to show how the optimization/generalization performances can be tweaked.[[EXP-NEU,ANA-NEU], [SUB-NEU], [SUG], [MIN]]\n4. Add computation time (wall-clock) for all the experiments, to see how it compares in practice (this could definitively weight in your favor, since you seems to have a nice CUDA implementation).[[EXP-NEU,ANA-NEU], [SUB-NEU,CMP-NEU], [SUG], [MIN]]\n5. Present results on larger-scale applications (Text8, Teaching Machines to Read and Comprehend, 3 layers LSTM speech recognition setup on TIMIT, DRAW, Machine Translation, ...), especially because your method is really easy to plug in any existing code available online.[[MET-NEU,ANA-NEU,RES-NEU], [SUB-NEU,EMP-NEU], [SUG], [MIN]]\n\nTypos / Form\n------------------\n\n1. sct 1, par 3: \"using Householder reflection vectors, it allows a fine-grained\" -> \"using Householder reflection vectors, which allows a fine-grained\"[[CNT], [CLA-NEG], [CRT], [MIN]]\n2. sct 1, par 3: \"This work called as Efficient\" -> \"This work, called Efficient\"[[CNT], [CLA-NEG], [CRT], [MIN]]\n5. sct 1, par 5: \"At the heart of KRU is the use of Kronecker\" -> \"At the heart of KRU, we use Kronecker\"[[CNT], [CLA-NEG], [CRT], [MIN]]\n6. sct 1, par 5: \"Thanks to the properties of Kronecker matrices\" -> \"Thanks to the properties of the Kronecker product\"\n7.[[CNT], [CLA-NEG], [CRT], [MIN]]  sct 1, par 5: \"vanilla real space RNN\" -> \"vanilla RNN\"[[CNT], [CLA-NEG], [CRT], [MIN]]\n8. sct 2, par 1: \"Consider a standard recurrent\" -> \"Consider a standard vanilla recurrent\"[[CNT], [CLA-NEG], [CRT], [MIN]]\n9. sct 2, par 1: \"step t RNN\" -> \"step t, a vanilla RNN\"[[CNT], [CLA-NEG], [CRT], [MIN]]\n11. sct 2.1, par 1: \"U and V, this is efficient using modern BLAS\" -> \"U and V, which can be efficiently computed using modern BLAS\"[[CNT], [CLA-NEG], [CRT], [MIN]]\n12. sct 2.3, par 2: \"matrices have a determinant of 1 or \u22121, i.e., the set of all rotations and reflections respectively\" -> \"matrices, i.e., the set of all rotations and reflections, have a determinant of 1 or \u22121.\"\n13. sct 3, par 1: \"are called as Kronecker\" -> \"are called Kronecker\"[[CNT], [CLA-NEG], [CRT], [MIN]]\n14. sct 3, par 3: \"used it's spectral\" -> \"used their spectral\"[[CNT], [CLA-NEG], [CRT], [MIN]]\n15. sct 3, par 3: \"Kronecker matrices\" -> \"Kronecker products\"\n[[CNT], [CLA-NEG], [CRT], [MIN]]18. sct 4.4, par 3: \"parameters are increased\" -> \"parameters increases\"[[CNT], [CLA-NEG], [CRT], [MIN]]\n19. sct 5: There is some more typos in the conclusion (\"it's\" -> \"its\")[[CNT], [CLA-NEG], [CRT], [MIN]]\n20. Some plots are hard to read / interpret, mostly because of the round \"ticks\" you use on the curves.[[TNF-NEG], [CLA-NEG], [CRT], [MIN]] I suggest you remove them everywhere.[[OAL-NEU], [CLA-NEU], [SUG], [MIN]] Also, in the adding problem, it would be cleaner if you down-sampled a bit the curves (as they are super noisy).[[TNF-NEU], [PNF-NEU], [SUG], [MIN]] In pixel by pixel MNIST, some of the legends might have some typos (FC uRNN), and you should use \"N\" instead of \"n\" to be consistent with the notation of the paper.[[DAT-NEU], [CLA-NEU], [DIS], [MIN]]\n21. Appendix A to E are not necessary, since they are from the literature.[[CNT], [PNF-NEG], [CRT], [MIN]]\n22. sct 3.1, par 2: \"is approximately unitary.\" -> \"is approximately unitary (cf Appendix F).\"[[CNT], [CLA-NEG], [CRT], [MIN]]\n23. sct 4, par 1: \"and backward operations.\" -> \"and backward operations (cf Appendix G and H).\"[[CNT], [CLA-NEG], [CRT], [MIN]]\n\nPros\n------\n\n1. Nice Idea that allows to decouple the hidden size with the number of hidden-to-hidden parameters.[[PDI-POS], [EMP-POS], [APC], [MAJ]]\n2. Cheap soft unitary constraint[[PDI-POS], [EMP-POS], [APC], [MAJ]]\n3. Efficient CUDA implementation (not experimentally verified)[[EXP-POS], [EMP-POS], [APC], [MAJ]]\n\nCons\n-------\n\n1. Some experimental setups are unfair, and some other could be clearer[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]]\n2. Only small scale experiments (although this factorization has huge potential on larger scale experiments)[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]]\n3. No wall-clock time that show the speed of the proposed parametrization."[[ANA-NEG], [SUB-NEG], [DFT], [MAJ]]