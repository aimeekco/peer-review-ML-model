"This paper argues about limitations of RNNs to learn models than exhibit a human-like compositional operation that facilitates generalization to unseen data, ex. zero-shot or one-shot applications.[[PDI-NEU], [null], [SMY], [GEN]] The paper does not present a new method, it only focuses on analyzing learning situations that illustrate their main ideas.[[PDI-NEU,MET-NEU], [NOV-NEU], [DIS], [MAJ]] To do this, they introduce a new dataset that facilitates the analysis of a Seq2Seq learning case.[[DAT-NEU], [NOV-NEU], [DIS], [GEN]] They conduct a complete experimentation, testing different popular RNN architectures, as well as parameter and hyperparameters values.[[EXP-NEU], [null], [DIS], [GEN]] \n\nThe main idea in the paper is that RNNs applied to Seq2Seq case are learning a representation based only on \"memorizing\" a mixture of constructions that have been observed during training, therefore, they can not show the compositional learning abilities exhibit by humans (that authors refer as systematic compositionality).[[PDI-NEU,EXP-NEU], [null], [SMY], [GEN]] Authors present a set of experiments designed to support this observation.[[EXP-NEU], [null], [SMY], [GEN]] \n\nWhile the experiments are compelling,[[EXP-POS], [EMP-POS], [APC], [MAJ]] as I explain below, I believe there is an underlying assumption that is not considered.[[ANA-NEG], [SUB-NEG], [DFT,CRT], [MAJ]] Performance on training set by the best model is close to perfect (99.5%), so the model is really learning the task.[[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]] Authors are then testing the model using test sets that do not follow the same distribution than training data, example,  longer sequences.[[DAT-NEU,EXP-NEU], [null], [DIS], [GEN]] By doing so, they are breaking one of the most fundamental assumptions of inductive machine learning, i.e., the distribution of train and test data should be equal.[[DAT-NEG,EXP-NEG], [EMP-NEG], [CRT], [MAJ]] Accordingly, my main point is the following: the model is indeed learning the task, as measured by performance on training set, so authors are only showing that the solution selected by the RNN does not follow the one that seems to be used by humans.[[DAT-NEU,EXP-NEU,MET-POS], [EMP-POS], [APC], [MAJ]] Importantly, this does not entail that using a better regularization a similar RNN model can indeed learn such a representation.[[MET-NEU], [CMP-NEU], [DIS], [MIN]] In this sense, the paper would really produce a more significant contribution is the authors can include some ideas about the ingredients of a RNN model, a variant of it, or a different type of model, must have to learn the compositional representation suggested by the authors, that I agree present convenient generalization capabilities.[[ANA-NEU,OAL-NEU], [IMP-NEU,SUB-NEU], [SUG], [MAJ]]   \n\n\nAnyway, I believe the paper is interesting and the authors are exposing interesting facts that might be worth to spread in our community, so I rate the paper as slightly over the acceptance threshold."[[OAL-POS], [IMP-POS,REC-POS], [APC,FBK], [MAJ]]