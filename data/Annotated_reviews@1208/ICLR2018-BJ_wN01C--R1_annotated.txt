"In this paper, the authors present an approach to implement deep learning directly on sparsely connected graphs.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] Previous approaches have focused on transferring trained deep networks to a sparse graph for fast or efficient utilization; using this approach, sparse networks can be trained efficiently online, allowing for fast and flexible learning.[[RWK-NEU,MET-NEU,RES-NEU], [NOV-NEU], [SMY], [GEN]] Further investigation is necessary to understand the full implications of the two main conceptual changes introduced here (signed connections that can disappear and random walk in parameter space),[[MET-NEU], [null], [DIS], [GEN]] but the initial results are quite promising.\n\n[[RES-POS], [EMP-POS], [APC], [MAJ]]It would also be interesting to understand more fully how performance scales to larger networks.[[MET-NEU], [SUB-NEU], [DIS], [MIN]] If the target connectivity could be pushed to a very sparse limit, where only a fixed number of connections were added with each additional neuron, then this could significantly shape how these networks are trained at very large scales.[[MET-NEU,RES-NEU], [SUB-NEU], [SUG,DIS], [MIN]] Perhaps the heuristics for initializing the connectivity matrices will be insufficient, but could these be improved in further work?\n\n[[FWK-NEG], [IMP-NEU], [QSN], [MAJ]]As a last minor comment, the authors should specify explicitly what the shaded areas are in Fig. 4b,c."[[TNF-NEG], [PNF-NEG], [DFT,CRT], [MIN]]