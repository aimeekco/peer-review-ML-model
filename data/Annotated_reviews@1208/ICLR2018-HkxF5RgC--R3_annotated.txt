"The paper proposes improving performance of large RNNs by combing techniques of model pruning and persistent kernels.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The authors further propose model-pruning optimizations which are aware of the persistent implementation.[[INT-NEU,MET-NEU], [null], [SMY], [GEN]]\n\nIt's not clear if the paper is relevant to the ICLR audience due to its emphasize on low-level optimization which has little insight in learning representations.[[OAL-NEG], [APR-NEG], [FBK], [MAJ]] The exposition in the paper is also not well-suited for people without a systems background, although I'll admit I'm mostly using myself as a proxy for the average machine learning researcher here.[[OAL-NEG], [APR-NEG], [CRT], [MAJ]] For instance, the authors could do more to explain Lamport Timestamps than a 1974 citation.[[OAL-NEG], [CMP-NEG], [SUG], [MIN]]\n\nModulo problems of relevance and expected audience, the paper is well-written and presents useful improvements in performance of large RNNs, and the work has potential for impact in industrial applications of RNNs.[[OAL-POS], [CLA-POS,IMP-POS], [APC], [MAJ]] The work is clearly novel, and the contributions are clear and well-justified using experiments and ablations."[[EXP-POS,OAL-POS], [NOV-POS,EMP-POS], [APC], [MAJ]]