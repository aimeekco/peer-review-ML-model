"This is an interesting paper, exploring GAN dynamics using ideas from online learning, in particular the pioneering \"sparring\" follow-the-regularized leader analysis of Freund and Schapire (using what is listed here as Lemma 4).[[PDI-POS,MET-POS,ANA-POS,TNF-POS,OAL-POS], [PNF-POS,EMP-POS], [APC], [MAJ]] By restricting the discriminator to be a single layer, the maximum player plays over a concave (parameter) space which stabilizes the full sequence of losses so that Lemma 3 can be proved, allowing proof of the dynamics' convergence to a Nash equilibrium.[[RWK-NEU,EXP-NEU,MET-NEU], [null], [SMY], [GEN]] The analysis suggests a practical (heuristic) algorithm incorporating two features which emerge from the theory: L2 regularization and keeping a history of past models.[[RWK-NEU,ANA-NEU], [EMP-NEU], [SMY], [GEN]] A very simple queue for the latter is shown to do quite competitively in practice.[[RWK-NEU,EXP-NEU], [null], [SMY], [GEN]]\n\nThis paper merits acceptance on theoretical merits alone, because the FTRL analysis for convex-concave games is a very robust tool from theory (see also the more recent sequel [Syrgkanis et al. 2016 \"Fast convergence of regularized learning in games\"]) that is natural to employ to gain insight on the much more brittle GAN case.[[RWK-POS,OAL-POS], [EMP-POS], [APC], [MAJ]] The practical aspects are also interesting, because the incorporation of added randomness into the mixed generation strategy is an area where theoretical justifications do motivate practical performance gains; these ideas could clearly be developed in future work.[[RWK-NEU,EXP-POS,FWK-POS,OAL-POS], [CLA-POS,IMP-POS,EMP-POS], [APC], [MAJ]]"