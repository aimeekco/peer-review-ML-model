"The topic discussed in this paper is interesting.[[PDI-POS], [null], [APC], [MAJ]] Dialogue acts (DAs; or some other semantic relations between utterances) are informative to increase the diversity of response generation.[[MET-POS], [EMP-POS], [SMY], [MAJ]] It is interesting to see how DAs are used for conversational modeling,;[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] however this paper is difficult for me to follow.[[OAL-NEU], [CLA-NEG], [CRT], [MAJ]] For example:\n\n1) the caption of section 3.1 is about supervised learning, however the way of describing the model in this section sounds like reinforcement learning.[[MET-NEU], [EMP-NEG], [CRT], [MAJ]] Not sure whether it is necessary to formulate the problem with a RL framework, since the data have everything that the model needs as for a supervised learning.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]]\n2) the formulation in equation 4 seems to be problematic[[MET-NEG], [CLA-NEG], [CRT], [MIN]]\n3) \"simplify pr(ri|si,ai) as pr(ri|ai,ui\u22121,ui\u22122) since decoding natural language responses from long conversation history is challenging[[MET-NEU], [EMP-NEU], [SUG], [MAJ]]\" to my understanding, the only difference between the original and simplified model is the encoder part not the decoder part. Did I miss something?[[MET-NEU], [EMP-NEU], [QSN], [MAJ]]\n4) about section 3.2, again I didn't get whether the model needs RL for training.[[MET-NEU], [EMP-NEU], [CRT], [MAJ]]\n5) \"We train m(\u00b7, \u00b7) with the 30 million crawled data through negative sampling.\" not sure I understand the connection between training $m(\\cdot, \\cdot)$ and the entire model.[[EXP-NEU,MET-NEU], [EMP-NEG], [CRT], [MAJ]]\n6) the experiments are not convincing.[[EXP-NEG], [null], [CRT], [MAJ]] At least, it should show the generation texts were affected about DAs in a systemic way.[[MET-NEU,ANA-NEU], [EMP-NEU], [SUG], [MAJ]] Only a single example in table 5 is not enough.[[EXP-NEG,TNF-NEU], [SUB-NEG], [CRT], [MAJ]]"