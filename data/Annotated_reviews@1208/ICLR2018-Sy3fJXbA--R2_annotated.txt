"The paper is clear and well written.[[OAL-POS], [CLA-POS], [APC], [MAJ]]\nIt is an incremental modification of prior work (ResNeXt) that performs better on several experiments selected by the author; comparisons are only included relative to ResNeXt.[[RWK-POS,EXP-POS], [CMP-POS], [APC], [MAJ]]\n\nThis paper is not about gating (c.f., gates in LSTMs, mixture of experts, etc) but rather about masking or perhaps a kind of block sparsity, as the \"gates\" of the paper do not depend upon the input: they are just fixed masking matrices (see eq (2)).[[PDI-NEU,MET-NEU], [EMP-NEU], [APC], [MIN]]\n\nThe main contribution appears to be the optimisation procedure for the binary masking tensor g.[[MET-NEU], [null], [SMY], [GEN]] But this procedure is not justified: does each step minimise the loss?[[MET-NEG], [EMP-NEG], [QSN], [MIN]] This seems unlikely due to the sampling.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Can the authors show that the procedure will always converge?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] It would be good to contrast this with other attempts to learn discrete random variables (for example, The Concrete Distribution: Continuous Relaxation of Continuous Random Variables, Maddison et al, ICLR 2017).\n"[[RWK-NEU], [CMP-NEU], [SUG], [MIN]]