"The paper proposes a large experimental analysis with the goal of evaluating the generalization capabilities of CNNs.[[PDI-NEU], [null], [SMY], [GEN]]\nSpecifically the analysis involves three datasets and two visual domains for each dataset: besides the original version\nof each image a new version is created by inverting its colors, i.e. simply rescaling the color channels in [0,1] and then\napplying (1-pixel_value).[[DAT-NEU,ANA-NEU], [null], [SMY], [GEN]]\n\n+ Several possible combinations between datasets and domains are considered to evaluate the network behaviour.[[DAT-NEU], [null], [SMY], [GEN]]\nThe analysis is also performed by varying the network architectures, considering data augmentation and/or fine tuning.[[ANA-NEU], [null], [SMY], [GEN]]\n\n- The text is confused in several points and the final overall conclusions are not fully clear.[[OAL-NEG], [CLA-NEG], [CRT], [MAJ]] Some experimental settings\nare well defined to shed light on few generalization aspects of the networks.[[EXP-POS], [EMP-POS], [APC], [MAJ]] Other do not add a significant novelty\nand their contribution is not clear.[[EXP-NEG], [NOV-NEG,IMP-NEG], [CRT], [MAJ]]\n\nDeatailed comments:\n1) at the end of page 2 \"most conventional data transformation only introduce slight variations...limited in evaluating \nthe generalization capabilities\".[[MET-NEG], [SUB-NEG], [DFT], [MAJ]] Conventional data transformations are introduced for data augmentation without\nsupposing the existance of a significand domain shift between training and test data.[[DAT-NEG,EXP-NEG], [EMP-NEG], [CRT], [MAJ]] If this shift exists (as for the\ncase of negative images) it is possible to refer to the extensive deep domain adaptation literature.[[RWK-NEU], [CMP-NEU], [DIS], [MAJ]] \n\n2) With reference to the previous point, the experiment 1 in Figure 2 provides a standard example of domain shift.[[EXP-NEU,TNF-NEU], [EMP-NEU], [DIS], [MAJ]]\nThe fact that 1-layer softmax and 2-layers MLP perform worse than VGG is not surprising, I do not see it as an \ninteresting contribution.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] It would it make more sense if the comparison was between VGG and ResNet or other\ndifferent deep structures.[[DAT-NEU], [CMP-NEU], [SUG], [MIN]]\n\n3) Also the results of experiments 2 and 3 in figure 2 are not surprising.[[RES-NEG,TNF-NEG], [EMP-NEG], [CRT], [MAJ]] \nIn my understanding, in experiment 2 the network, while learning to recognize the numbers, it also learns to\nbe invariant to color thanks to a tailored data augmentation and the good final results are expected.[[EXP-NEU,MET-POS], [EMP-NEU], [DIS], [MIN]] \nIn experiment 3 instead, the defined setting is supposed to give imporance to colors, as stated at the end of page 4.[[EXP-NEU], [EMP-NEU], [SUG], [MIN]]\nHowever, there is no reason to automatically expect that this will decrease the importance of shape.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]] Every category \nis defined by a specific combination of color and shape that is well recognized at test time.[[EXP-NEU], [null], [DIS], [MIN]]\n\n4) the experiments in figure 3 show that the tailored data augmentation can be done even for a subset of the\nclasses and still work well for all of them.[[EXP-POS,TNF-POS], [EMP-POS], [APC], [MAJ]] It would be interesting to investigate the limits of this statement: what\nwould happen by augmenting only 8 or 7 or 6 or 5... categories instead of 9?[[ANA-NEU], [EMP-NEU], [QSN], [MIN]]\n\n5) the experiment in section 6.1, figure 5 is just slightly different from that in figure 3.[[EXP-NEU,TNF-NEU], [PNF-NEU], [DIS], [MIN]] I would suggest to \nput the two together since they both demonstrate that the network can learn to be invariant to a certain\ndomain aspect as far as data augmentation is used to cover that aspect for at least a part of the observed\ncategories.[[EXP-NEU,TNF-NEU], [PNF-NEU], [SUG], [MIN]]\n\n6) I do not see any novel contribution in the analysis of the batch normalization (end of section 5): bn has been \npreviously used for  domain adaptation\nRevisiting Batch Normalization For Practical Domain Adaptation, arXiv:1603.04779\nAutoDIAL: Automatic DomaIn Alignment Layers, ICCV 2017[[RWK-NEU,MET-NEU,ANA-NEG], [NOV-NEG,CMP-NEU], [CRT], [MAJ]]\n\n7) the experiment in section 6.4 should be presented as an extreme case of that of figure 7.[[EXP-NEU,TNF-NEU], [PNF-NEU], [DIS], [MIN]] \nSo the two can be presented together in the same section.[[EXP-NEU,TNF-NEU], [PNF-NEU], [DIS], [MIN]] Dividing them makes more complicated to \ndraw general conclusions from this particular data augmentation setting.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]]\n\n8) the fine-tuning experiments do not bring significant novelty.[[EXP-NEG], [IMP-NEG], [CRT], [MAJ]] In figure 8, my interpretation of (a) is\nthat the initial model has learned to be invariant to color and this remains true even if the fine-tuning data\ndo not contain any negative data.[[MET-NEU,TNF-NEU], [EMP-NEU], [DIS], [MIN]] Given this conclusion, I would have expected a discussion about \nthe difference between learning with all data at the same time or with fine-tuning in two different \nsteps.[[EXP-NEU,ANA-NEU], [SUB-NEU], [DIS], [MIN]] Morever this fine-tuning experiment needs more details: is it based only on parameter initialization\nor there are some fully frozen network layers?[[EXP-NEU,ANA-NEU], [SUB-NEU], [QSN], [MIN]]\n\nThis work shows few interesting results[[RES-POS], [EMP-POS], [APC], [MAJ]] but the paper is not easy to read, the presentation is sparse \nand the bit and pieces of information do not allow to derive strong final conclusions.\n\n\n"[[OAL-NEG], [CLA-NEG,PNF-NEG], [CRT], [MIN]]