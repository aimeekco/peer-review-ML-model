"The paper introduces Topology Adaptive GCN (TAGCN) to generalize convolutional\nnetworks to graph-structured data.[[PDI-NEU], [null], [SMY], [GEN]]\nI find the paper interesting[[OAL-POS], [null], [APC], [MAJ]] but not very clearly written in some sections,\nfor instance I would better explain what is the main contribution and devote\nsome more text to the motivation.[[PDI-NEG,OAL-NEG], [CLA-NEG], [CRT], [MIN]] Why is the proposed approach better than the\npreviously published ones, and when is that there is an advantage in using it?[[RWK-NEU,MET-NEU], [CMP-NEU], [QSN], [MAJ]]\n\nThe main contribution seems to be the use of the \"graph shift\" operator from\nSandryhaila and Moura (2013), which closely resembles the one from\nShuman et al. (2013).[[RWK-NEU], [CMP-NEU], [DIS], [MAJ]] It is actually not very well explained what is the main\ndifference.[[RWK-NEG], [CMP-NEG], [CRT], [MAJ]]\n\nEquation (2) shows that the learnable filters g are operating on the k-th power\nof the normalized adjacency matrix A, so when K=1 this equals classical GCN\nfrom T. Kipf et al.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MIN]]\nBy using K > 1 the method is able to leverage information at a farther distance\nfrom the reference node.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\nSection 2.2 requires some polishing as I found hard to follow the main story\nthe authors wanted to tell.[[CNT], [CLA-NEU,PNF-NEU], [SUG], [MIN]] The definition of the weight of a path seems\ndisconnected from the main text, ins't A^k kind of a a diffusion operator or\nrandom walk?[[MET-NEG], [CLA-NEG], [QSN,CRT], [MIN]]\nThis makes me wonder what would be the performance of GCN when the k-th power\nof the adjacency is used.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\nI liked Section 3, however while it is true that all methods differ in the way they\ndo the filtering, they also differ in the way the input graph is represented\n(use of the adjacency or not).[[MET-POS], [EMP-POS], [APC], [MAJ]]\n\nExperiments are performed on the usual reference benchmarks for the task and show\nsensible improvements with respect to the state-of-the-art.[[RWK-POS,EXP-POS], [EMP-POS], [APC], [MAJ]] TAGCN with K=2 has\ntwice the number of parameters of GCN, which makes the comparison not entirely\nfair.[[MET-NEG], [CMP-NEG], [CRT], [MAJ]] Did the author experiment with a comparable architecture?[[EXP-NEU], [CMP-NEU], [QSN], [MAJ]]\nAlso, how about using A^2 in GCN or making two GCN and concatenate them in\nfeature space to make the representational power comparable?[[EXP-NEU], [CMP-NEU], [QSN], [MAJ]]\n\nIt is also known that these benchmarks, while being widely used, are small and\nresult in high variance results.[[RES-NEU], [CMP-NEU], [DIS], [MAJ]] The authors should report statistics over\nmultiple runs.[[ANA-NEU,RES-NEU], [SUB-NEU], [SUG], [MAJ]]\nGiven the systematic parameter search, with reference to the actual validation\n(or test?) set I am afraid there could be some overfitting.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] It is quite easy\nto probe the test set to get best performance on these benchmarks.[[DAT-NEU,RES-NEU], [EMP-NEU], [DIS], [MAJ]]\n\nAs a minor remark, please make figures readable also in BW.[[TNF-NEU], [PNF-NEU], [SUG], [MIN]]\n\nOverall I found the paper interesting[[OAL-POS], [null], [APC], [MAJ]] but also not very clear at pointing out\nthe major contribution and the motivation behind it.[[OAL-NEG], [CNT], [CRT], [MAJ]] At risk of being too reductionist:\nit looks as learning a set of filters on different coordinate systems given\nby the various powers of A.[[OAL-NEU], [CNT], [DIS], [MIN]] GCN looks at the nearest neighbors and the paper\nshows that using also the 2-ring improves performance.\n"[[MET-NEU], [CNT], [DIS], [MIN]]