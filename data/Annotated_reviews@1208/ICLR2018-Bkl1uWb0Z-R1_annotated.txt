"This paper adds source side dependency syntax trees to an NMT model without explicit supervision.[[INT-NEU], [null], [SMY], [GEN]] Exploring the use of syntax in neural translation is interesting but I am not convinced that this approach actually works based on the experimental results.[[EXP-NEU,MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nThe paper distinguishes between syntactic and semantic objectives (4th paragraph in section 1), attention, and heads.[[MET-NEU], [null], [SMY], [GEN]] Please define what semantic attention is.[[MET-NEU], [null], [SUG], [MAJ]] You just introduce this concept without any explanation. [[MET-NEU], [EMP-NEG], [CRT], [MAJ]]I believe you mean standard attention, if so, please explain why standard attention is semantic.[[MET-NEU], [EMP-NEU], [SUG], [MAJ]]\n\nClarity. What is shared attention exactly?[[MET-NEU], [null], [QSN], [MIN]] Section 3.2 says that you share attention weights from the decoder with encoder. Please explain this a bit more.[[MET-NEU], [EMP-NEU], [SUG], [MAJ]] Also the example in Figure 3 is not very clear and did not help me in understanding this concept.[[EXP-NEG,TNF-NEU], [EMP-NEG], [CRT], [MAJ]]\n\nResults. A good baseline would be to have two identical attention mechanisms to figure out if improvements come from more capacity or better model structure.[[RWK-POS,RES-NEU], [EMP-POS], [APC], [MAJ]] Flat attention seems to add a self-attention model and is somewhat comparable to two mechanisms.[[RWK-NEU,MET-POS], [CMP-POS], [SMY], [MAJ]] The results show hardly any improvement over the flat attention baseline (at most 0.2 BLEU which is well within the variation of different random initializations).[[RWK-NEU,RES-NEG], [CMP-NEG], [CRT], [MAJ]] It looks as if the improvement comes from adding additional capacity to the model.[[MET-NEU], [EMP-NEU], [SMY], [MAJ]] \n\nEquation 3: please define H.[[MET-NEU], [null], [SUG], [MIN]]"