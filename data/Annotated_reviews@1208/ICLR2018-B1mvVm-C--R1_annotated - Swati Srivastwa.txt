"The authors propose to decompose reinforcement learning into a PATH function that can learn how to solve reusable sub-goals an agent might have in a specific environment and a GOAL function that chooses subgoals in order to solve a specific task in the environment using path segments.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] So I guess it can be thought of as a kind of hierarchical RL.[[MET-NEU], [null], [DIS], [GEN]]\nThe exposition of the model architecture could use some additional detail to clarify some steps and possibly fix some minor errors (see below).[[MET-NEG], [SUB-NEG], [SUG], [MIN]] I would prefer less material but better explained.[[MET-NEU], [EMP-NEU], [SUG], [MIN]] I had to read a lot of sections more than once and use details across sections to fill in gaps.[[PDI-NEU], [null], [DIS], [MIN]] The paper could be more focused around a single scientific question: does the PATH function as formulated help?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nThe authors do provide a novel formulation and demonstrate the gains on a variety of concrete problems taken form the literature.[[MET-POS], [NOV-POS], [APC], [MAJ]] I also like that they try to design experiments to understand the role of specific parts of the proposed architecture.[[EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]]\n\nThe graphs are WAY TOO SMALL to read.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]] Figure #s are missing off several figures.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]]\n\n\nMODEL & ARCHITECTURE\n\nThe PATH function given a current state s and a goal state s', returns a distribution over the best first action to take to get to the goal P(A).[[MET-POS], [EMP-POS], [APC], [MAJ]]  ( If the goal state s\u2019 was just the next state, then this would just be a dynamics model and this would be model-based learning?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] So I assume there are multiple steps between s and s\u2019?).[[MET-NEU], [EMP-NEU], [QSN], [MIN]] \n\nAt the beginning of section 2.1, I think the authors suggest the PATH function could be pre-trained independently by sampling a random state in the state space to be the initial state and a second random state to be the goal state and then using an RL algorithm to find a path.[[MET-NEU], [EMP-NEU], [QSN], [MIN]] \n\nPresumably, once one had found a path ( (s, a0), (s1, a1), (s2, a2), \u2026, (sn-1,an-1),  s\u2019 ) one could then train the PATH policy on the triple (s, s\u2019, a0) ?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]] This seems like a pretty intense process: solving some representative subset of all possible RL problems for a particular environment \u2026 Maybe one choses s and s\u2019 so they are not too far away from each other (the experimental section later confirms this distance is >= 7.[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]] Maybe bring this detail forward)?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nThe expression Trans\u2019( (s,s\u2019), a) = (Trans(s,a), s\u2019) was confusing.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]  I think the idea here is that the expression \nTrans\u2019(  (s,s\u2019) , a ) represents the n-step transition function and \u2018a' represents the first action?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nThe second step is to train the goal function for a specific task.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]] So I gather our policy takes the form of a composed function and the chain rule gives close to their expression in 2.2[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\n    PATH(  s,  Tau( s, th^g ),  a ; th^p )\n\n    d / { d th^g }  PATH(  s,  Tau( s, th^g ),  a ; th^p )[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \n\n         =  {d / d {s\u2019 }  PATH } ( s,  Tau( s, th^g ),  a )    d / {d th^g}  Tau( s, th^g)[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \n\nWhat is confusing is that they define\n\n    A( s, a, th^p, th^g, th^v ) = sum_i   gamma^i  r_{t+1}  +  gamma^k  V(  s_{t+k}  ;  th^v  )   -   V( s_t ;  th^v )[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n\nThe left side contains th^p and th^g, but the right side does not.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Should these parameters be take out of the n-step advantage function A?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nThe second alternative for training the goal function tau seems confusing.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] I get that tau is going to be constrained by whatever representation PATH function was trained on and that this representation might affect the overall performance - performance.[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]]  I didn\u2019t get the contrast with method one.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]  How do we treat the output of Tau as an action?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Are you thinking of the gradient coming back through PATH as a reward signal?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] More detail here would be helpful.[[MET-NEU], [SUB-NEU], [SUG], [MIN]]\n\n\nEXPERIMENTS:\n\nLavaworld: authors show that pretraining the PATH function on longer 7-11 step policies leads to better performance\nwhen given a specific Lava world problem to solve.[[EXP-POS], [EMP-POS], [APC], [MAJ]]  So the PATH function helps and longer paths are better.[[EXP-POS], [EMP-POS], [APC], [MAJ]]  This seems reasonable.[[EXP-POS], [EMP-POS], [APC], [MAJ]]  What is the upper bound on the size of PATH lengths you can train?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]]  \n\nReachability: authors show that different ways of abstracting the state s into a vector encoding affect the performance of the system.[[MET-NEU], [null], [DIS], [MIN]]   From a scientific point of view, this seems orthogonal to the point of the paper, though is relevant if you were trying to build a system.[[EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]]  \n\nTaxi: the authors train the PATH problem on reachability and then show that it works for TAXI.[[EXP-NEU], [null], [DIS], [GEN]]  This isn\u2019t too surprising.[[EXP-NEG], [EMP-NEG], [CRT], [MIN]]  Both picking up the passenger (reachability) and dropping them off somewhere are essentially the same task: moving to a point.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]]  It is interesting that the Task function is able to encode the higher level structure of the TAXI problem\u2019s two phases.[[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]] \n\nAnother task you could try is to learn to perform the same task in two different environments.[[MET-POS], [EMP-POS], [APC], [MAJ]]  Perhaps the TAXI problem, but you have two different taxis that require different actions in order to execute the same path in state space.[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]]  This would require a phi(s) function that is trained in a way that doesn\u2019t depend on the action a.[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\nATARI 2600 games: I am not sure what state restoration is.[[MET-NEU], [null], [DIS], [MIN]]  Is this where you artificially return an agent to a state that would normally be hard to reach?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] The authors show that UA results in gains on several of the games.[[RES-POS], [EMP-POS], [APC], [MAJ]]\n\nThe authors also demonstrate that using multiple agents with different policies can be used to collect training examples for the PATH function that improve its utility over training examples collected by a single agent policy.[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\nRELATED WORK:\n\nGood contrast to hierarchical learning: we don\u2019t have switching regimes here between high-level options[[RWK-POS], [CMP-POS], [APC], [MAJ]]\n\nI don\u2019t understand why the authors say the PATH function can be viewed as an inverse?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Oh - now I get it.\nBecause it takes an extended n-step transition and generates an action.  \n\n\n\n\n\n"[[MET-NEU], [EMP-NEU], [DIS], [MIN]]