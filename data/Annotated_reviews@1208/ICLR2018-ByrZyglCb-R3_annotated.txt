"The paper develops models which attempt to explain the existence of universal perturbations which fool neural networks \u2014 i.e., the existence of a single perturbation which causes a network to misclassify most inputs.[[INT-NEU], [null], [SMY], [GEN]] The paper develops two models for the decision boundary:\n\n(a) A locally flat model in which the decision boundary is modeled with a hyperplane and the normals two the hyperplanes are assumed to lie near a low-dimensional linear subspace.[[MET-NEU], [null], [SMY], [GEN]]\n\n(b) A locally positively curved model, in which there is a positively curved outer bound for the collection of points which are assigned a given label.[[MET-NEU], [null], [SMY], [GEN]] \n\nThe paper works out a probabilistic analysis arguing that when either of these conditions obtains, there exists a fooling perturbation which affects most of the data. [[MET-NEU,ANA-NEU], [null], [SMY], [GEN]]\n\nThe theoretical analysis in the paper is straightforward, in some sense following from the definition.[[MET-NEU,ANA-NEU], [EMP-NEU], [SMY], [MAJ]] The contribution of the paper is to posit these two conditions which can predict the existence of universal fooling perturbations, argue experimentally that they occur in (some) neural networks of practical interest.[[EXP-NEU,MET-NEU], [EMP-NEU], [SMY], [MAJ]] \n\nOne challenge in assessing the experimental claims is that practical neural networks are nonsmooth; the quadratic model developed from the hessian is only valid very locally.[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MAJ]] This can be seen in some of the illustrative examples in Figure 5: there *is* a coarse-scale positive curvature, but this would not necessarily come through in a quadratic model fit using the hessian.[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MAJ]] The best experimental evidence for the authors\u2019 perspective seems to be the fact that random perturbations from S_c misclassify more points than random perturbations constructed with the previous method.[[EXP-POS,MET-POS], [EMP-POS], [SMY], [MAJ]] \n\nI find the topic of universal perturbations interesting, because it potentially tells us something structural (class-independent) about the decision boundaries constructed by artificial neural networks. [[MET-POS], [EMP-POS], [APC], [MAJ]]To my knowledge, the explanation of universal perturbations in terms of positive curvature is novel.[[MET-POS], [NOV-POS], [APC], [MAJ]] The paper would be much stronger if it provided an explanation of *why* there exists this common subspace of universal fooling perturbations, or even what it means geometrically that positive curvature obtains at every data point.[[MET-NEU], [EMP-NEU], [SUG], [MAJ]] \n\nVisually, these perturbations seem to have strong, oriented local high-frequency content \u2014 perhaps they cause very large responses in specific filters in the lower layers of a network, and conventional architectures are not robust to this? [[MET-NEU], [EMP-NEU], [QSN], [MAJ]]\n\nIt would also be nice to see some visual representations of images perturbed with the new perturbations, to confirm that they remain visually similar to the original images.[[TNF-NEU], [CMP-NEU,PNF-NEU], [SUG], [MAJ]] \n"