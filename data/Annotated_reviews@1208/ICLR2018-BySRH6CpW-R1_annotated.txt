"This paper proposes training binary and ternary weight distribution networks through the local reparametrization trick and continuous optimization.[[INT-NEU], [null], [SMY], [GEN]] The argument is that due to the central limit theorem (CLT) the distribution on the neuron pre-activations is approximately Gaussian, with a mean given by the inner product between the input and the mean of the weight distribution and a variance given by the inner product between the squared input and the variance of the weight distribution.[[RWK-NEU,MET-NEU], [null], [SMY], [GEN]] As a result, the parameters of the underlying discrete distribution can be optimized via backpropagation by sampling the neuron pre-activations with the reparametrization trick.[[MET-NEU,RES-NEU], [null], [SMY], [GEN]] The authors further propose appropriate initialisation schemes and regularization techniques to either prevent the violation of the CLT or to prevent underfitting.[[MET-NEU], [null], [SMY], [GEN]] The method is evaluated on multiple experiments.[[EXP-NEU], [null], [SMY], [GEN]]\n\nThis paper proposed a relatively simple idea for training networks with discrete weights that seems to work in practice.[[PDI-NEU,MET-POS], [EMP-POS], [SMY], [MAJ]] My main issue is that while the authors argue about novelty, the first application of CLT for sampling neuron pre-activations at neural networks with discrete r.v.s is performed at [1].[[RWK-NEU,MET-NEG], [NOV-NEG], [CRT], [MAJ]] While [1] was only interested in faster convergence and not on optimization of the parameters of the underlying distribution, the extension was very straightforward.[[RWK-NEU,MET-NEU], [NOV-NEU], [CRT], [MAJ]] I would thus suggest that the authors update the paper accordingly.[[RWK-NEU,MET-NEU], [EMP-NEU], [SUG], [MAJ]] \n\nOther than that, I have some other comments[[OAL-NEU], [null], [DIS], [GEN]]:\n- The L2 regularization on the distribution parameters for the ternary weights is a bit ad-hoc; why not penalise according to the entropy of the distribution which is exactly what you are trying to achieve?[[MET-NEU], [EMP-NEU], [QSN], [MAJ]] \n- For the binary setting you mentioned that you had to reduce the entropy thus added a \u201cbeta density regulariser\u201d. Did you add R(p) or log R(p) to the objective function?[[MET-NEU], [EMP-NEU], [QSN], [MAJ]] Also, with alpha, beta = 2 the beta density is unimodal with a peak at p=0.5; essentially this will force the probabilities to be close to 0.5, i.e. exactly what you are trying to avoid.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] To force the probability near the endpoints you have to use alpha, beta < 1 which results into a \u201cbowl\u201d shaped Beta distribution.[[MET-NEU], [EMP-NEU], [SUG], [MAJ]] I thus wonder whether any gains you observed from this regulariser are just an artifact of optimization. [[MET-NEU,RES-NEU], [EMP-NEU], [CRT], [MAJ]] \n- I think that a baseline (at least for the binary case) where you learn the weights with a continuous relaxation, such as the concrete distribution, and not via CLT would be helpful. [[RWK-NEU,MET-NEU], [EMP-NEU], [SUG], [MAJ]]Maybe for the network to properly converge the entropy for some of the weights needs to become small (hence break the CLT).[[MET-NEU], [EMP-NEU], [SUG], [MAJ]] \n\n[1] Wang & Manning, Fast Dropout Training.[[RWK-NEU,BIB-NEU], [null], [SUG], [MIN]]\n\nEdit: After the authors rebuttal I have increased the rating of the paper:[[OAL-POS], [REC-POS], [APC], [MAJ]] \n- I still believe that the connection to [1] is stronger than what the authors allude to; eg. the first two paragraphs of sec. 3.2 could easily be attributed to [1].[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MAJ]]\n- The argument for the entropy was to include a term (- lambda * H(p)) in the objective function with H(p) being the entropy of the distribution p.[[MET-NEU], [null], [SMY], [GEN]] The lambda term would then serve as an indicator to how much entropy is necessary.[[MET-NEU], [EMP-NEU], [SMY], [GEN]]\n- There indeed was a misunderstanding with the usage of the R(p) regularizer at the objective function (which is now resolved).[[MET-NEU], [EMP-NEU], [DIS], [MAJ]]\n- The authors showed benefits compared to a continuous relaxation baseline.[[RWK-NEU,RES-POS], [CMP-POS], [APC], [MAJ]]"