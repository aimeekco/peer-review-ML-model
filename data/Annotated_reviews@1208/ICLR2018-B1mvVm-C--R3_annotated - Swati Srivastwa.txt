"Thank you for the submission. [[EXT-NEU], [null], [APC], [MIN]] It was an interesting read.[[OAL-POS], [null], [APC], [MAJ]]  Here are a few comments: \n\nI think when talking about modelling the dynamics of the world, it is natural to discuss world models and model based RL, which also tries to explicitly take advantage of the separation between the dynamics of the world and the reward scheme.[[PDI-NEU], [null], [DIS], [MIN]]  Granted, most world model also try to predict the reward.[[MET-NEU], [null], [DIS], [GEN]]  I\u2019m not sure there is something specific I\u2019m proposing here, I do understand the value of the formulation given in the work, I just find it strange that model based RL is not mention at all in the paper.[[MET-NEG], [EMP-NEG], [DFT], [MIN]]  \n\nI think reading the paper, it should be much clearer how the embedding is computed for Atari, and how this choice was made.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MIN]]  Going through the paper I\u2019m not sure I know how this latent space is constructed.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]  This however should be quite important.[[FWK-NEU], [IMP-NEU], [CRT], [MIN]]  The goal function tries to predict states in this latent space.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]  So the simpler the structure of this latent space, the easier it should be to train a goal function, and hence quickly adapt to the current reward scheme. [[MET-NEG], [EMP-NEG], [CRT], [MIN]]  \n\nIn complex environments learning the PATH network is far from easy.[[CNT], [null], [DIS], [MIN]]  I.e. random walks will not expose the model to most states of the environment (and dynamics).[[CNT], [null], [DIS], [MIN]]  Curiosity-driven RL can be quite inefficient at exploring the space. [[MET-NEU], [null], [DIS], [MIN]] If the focus is transfer, one could argue that another way of training the PATH net could be by training jointly the PATH net and goal net, with the intend of then transferring to another reward scheme.[[MET-NEU], [null], [DIS], [MIN]] \n\nA3C is known to be quite high variance.[[MET-NEU], [null], [DIS], [MIN]]  I think there are a lot of little details that don\u2019t seem that explicit to me.[[CNT], [null], [CRT], [MIN]]  How many seeds are run for each curve (are the results an average over multiple seeds).[[EXP-NEU,RES-NEU], [CNT], [QSN], [MIN]]  What hyper-parameters are used.[[EXP-NEU], [EMP-NEU], [QSN], [MIN]]  What is the variance between the seeds.[[EXP-NEU], [EMP-NEU], [QSN], [MIN]]  I feel that while the proposed solution is very intuitive, and probably works as described,[[RES-POS], [EMP-POS], [APC], [MAJ]]  the paper does not do a great job at properly comparing with baseline and make sure the results are solid. [[RWK-NEG,RES-NEG], [CMP-NEG], [CRT], [MIN]] In particular looking at Riverraid-new is the advantage you have there significant?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]]  How does the game do on the original task?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]  \n\nThe plots could also use a bit of help.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]]  Lines should be thicker.[[CNT], [PNF-NEU], [SUG], [MIN]]  Even when zooming, distinguishing between colors is not easy.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]] Because there are more than two lines in some plots, it can also hurt people that can\u2019t distinguish colors easily. \n\n"[[TNF-NEG], [PNF-NEG], [CRT], [MIN]]