"In this paper, the authors look to improve Neural Architecture Search (NAS), which has been successfully applied to discovering successful neural network architectures, albeit requiring many computational resources. [[INT-NEU], [null], [SMY], [GEN]]The authors propose a new approach they call Efficient Neural Architecture Search (ENAS), whose key insight is parameter sharing.[[MET-NEU], [null], [SMY], [GEN]] In NAS, the practitioners have to retrain for every new architecture in the search process, but in ENAS this problem is avoided by sharing parameters and using discrete masks.[[RWK-NEU,MET-NEU], [CMP-POS], [SMY], [GEN]] In both approaches, reinforcement learning is used to  learn a policy that maximizes the expected reward of some validation set metric.[[RWK-NEU,MET-NEU], [CMP-NEU], [SMY], [GEN]] Since we can encode a neural network as a sequence, the policy can be parameterized as an RNN where every step of the sequence corresponds to an architectural choice.[[MET-POS], [EMP-POS], [SMY], [GEN]] In their experiments, ENAS achieves test set metrics that are almost as good as NAS, yet require significantly less computational resources and time.[[RWK-NEU,EXP-POS], [CMP-POS], [SMY], [GEN]]\n\nThe authors present two ENAS models: one for CNNs, and another for RNNs.[[MET-NEU], [null], [SMY], [GEN]] Initially it seems like the controller can choose any of B operations in a fixed number of layers along with choosing to turn on or off ay pair of skip connections.[[MET-NEU], [EMP-NEU], [SMY], [MAJ]] However, in practice we see that the search space for modeling both skip connections and choosing convolutional sizes is too large, so the authors use only one restriction to reduce the size of the state space.[[MET-NEU], [EMP-NEU], [SMY], [MAJ]] This is a limitation, as the model space is not as flexible as one would desire in a discovery task.[[MET-NEU], [EMP-NEU], [DFT], [MAJ]] Moreover, their best results (and those they choose to report in the abstract) are due to fixing 4 parallel branches at every layer combined with a 1 x 1 convolution, and using ENAS to learn the skip connections.[[MET-NEU,RES-POS], [EMP-NEU], [APC], [MAJ]] Thus, they are essentially learning the skip connections while using a human-selected model.[[MET-NEU], [EMP-NEU], [SMY], [MAJ]] \n\nENAS for RNNs is similar: while NAS searches for a new architecture, the authors use a recurrent highway network for each cell and use ENAS to find the skip connections.[[MET-NEU], [EMP-NEU], [SMY], [GEN]] Thus, it seems like the term Efficient Neural Architecture Search promises too much since in both tasks they are essentially only using the controller to find skip connections.[[MET-NEU], [EMP-NEU], [CRT], [MAJ]] Although finding an appropriate architecture for skip connections is an important task, finding an efficient method to structure RNN cells seems like a significantly more important goal.[[RWK-NEU,MET-NEU], [EMP-NEU], [SMY], [MAJ]]\n\nOverall, the paper is well-written, and it brings up an important idea: that parameter sharing is important for discovery tasks so we can avoid re-training for every new architecture in the search process.[[PDI-POS], [CLA-POS], [APC], [MAJ]] Moreover, using binary masks to control network path (essentially corresponding to training different models) is a neat idea.[[PDI-POS], [EMP-POS], [APC], [MAJ]] It is also impressive how much faster their model performs on tasks without sacrificing much performance.[[MET-POS], [EMP-POS], [APC], [MAJ]] The main limitation is that the best architectures as currently described are less about discovery and more about human input;[[MET-NEU], [EMP-NEU], [DFT], [MAJ]] -- finding a more efficient search path would be an important next step.[[MET-NEU,FWK-NEU], [EMP-NEU], [SUG], [MAJ]]"