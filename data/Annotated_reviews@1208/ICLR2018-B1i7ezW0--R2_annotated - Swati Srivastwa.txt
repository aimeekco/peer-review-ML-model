"In summary, the paper is based on a recent work Balestriero & Baraniuk 2017 to do semi-supervised learning.[[PDI-NEU,RWK-NEU], [null], [SMY], [GEN]] In Balestriero & Baraniuk, it is shown that any DNN can be approximated via a linear spline and hence can be inverted to produce the \"reconstruction\" of the input, which can be naturally used to do unsupervised or semi-supervised learning. [[EXT-NEU], [null], [SMY], [GEN]]This paper proposes to use automatic differentiation to compute the inverse function efficiently.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The idea seems interesting.[[PDI-POS], [EMP-POS], [APC], [MAJ]] However, I think there are several main drawbacks, detailed as follows:\n\n1. The paper lacks a coherent and complete review of the semi-supervised deep learning.[[MET-NEG], [SUB-NEG], [DFT], [MIN]] Herewith some important missing papers, which are the previous or current state-of-the-art.[[RWK-NEG], [SUB-NEG,CMP-NEG], [DFT], [MIN]]\n\n[1] Laine S, Aila T. Temporal Ensembling for Semi-Supervised Learning[J]. arXiv preprint arXiv:1610.02242, ICLR 2016.[[RWK-NEG], [SUB-NEG,CMP-NEG], [DFT], [MIN]]\n[2] Li C, Xu K, Zhu J, et al. Triple Generative Adversarial Nets[J]. arXiv preprint arXiv:1703.02291, NIPS 2017.[[RWK-NEG], [SUB-NEG,CMP-NEG], [DFT], [MIN]]\n[3] Dai Z, Yang Z, Yang F, et al. Good Semi-supervised Learning that Requires a Bad GAN[J]. arXiv preprint arXiv:1705.09783, NIPS 2017.[[RWK-NEG], [SUB-NEG,CMP-NEG], [DFT], [MIN]]\n\nBesides, some papers should be mentioned in the related work such as Kingma et. al. 2014.[[RWK-NEG], [SUB-NEG,CMP-NEG], [SUG], [MIN]] I'm not an expert of the network inversion and not sure whether the related work of this part is sufficient or not.[[RWK-NEG], [SUB-NEG,CMP-NEG], [DFT], [MIN]]\n\n2. The motivation is not sufficient and not well supported.[[PDI-NEG], [SUB-NEG], [CRT], [MIN]] \n\nAs stated in the introduction, the authors think there are several drawbacks of existing methods including \"training instability, lack of topology generalization and computational complexity.[[INT-NEU], [CMP-NEU], [DIS], [MIN]] \" Based on my knowledge, there are two main families of semi-supervised deep learning methods, classified by depending on deep generative models or not. [[MET-NEU], [null], [DIS], [MIN]]  The generative approaches based on VAEs and GANs are time consuming, but according to my experience, the training of VAE-based methods are stable and the topology generalization ability of such methods are good.[[EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]]  Besides, the feed-forward approaches including [1] mentioned above are efficient and not too sensitive with respect to the network architectures.[[MET-POS], [EMP-POS], [APC], [MAJ]]   Overall, I think the drawbacks mentioned in the paper are not common in existing methods and I do not see clear benefits of the proposed method.[[RWK-NEG,MET-NEG], [CMP-NEG], [CRT], [MIN]]  Again, I strongly suggest the authors to provide a complete review of the literature.[[RWK-NEG], [SUB-NEG], [DFT], [MIN]] \n\nFurther, please explicitly support your claim via experiments.[[EXP-NEG,ANA-NEG], [SUB-NEG], [DFT], [MIN]]  For instance, the proposed method should be compared  with the discriminative approaches including VAT and [1] in terms of the training efficiency.[[EXP-NEU], [CMP-NEU], [SUG], [MIN]]  It's not fair to say GAN-based methods require more training time because these methods can do generation and style-class disentanglement while the proposed method cannot.[[MET-NEG], [CMP-NEG], [CRT], [MIN]] \n\n3. The experimental results are not so convincing.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]]  \n\nFirst, please systematically compare your methods with existing methods on the widely adopted benchmarks including MNIST with 20, 100 labels and SVHN with 500, 1000 labels and CIFAR10 with 4000 labels.[[DAT-NEU,MET-NEG], [CMP-NEG], [CRT], [MIN]]  It is not safe to say the proposed method is the state-of-the-art by only showing the results in one setting.[[RWK-NEG,MET-NEG], [CMP-NEG], [CRT], [MAJ]] \n\nSecond, please report the results of the proposed method with comparable architectures used in previous methods and state clearly the number of parameters in each model.[[MET-NEG,RES-NEG], [CMP-NEG], [CRT], [MAJ]]  Resnet is powerful but previous methods did not use that.[[RWK-NEG,MET-NEG], [CMP-NEG], [CRT], [MIN]] \n\nLast, show the sensitive results of the proposed method by tuning alpha and beta.[[EXP-NEU,MET-NEU,RES-NEU], [EMP-NEU], [DIS], [MIN]]  For instance, please show what is the actual contribution of the proposed reconstruction loss to the classification accuracy with the other losses existing or not?[[EXP-NEU,MET-NEU,RES-NEU], [EMP-NEU], [QSN], [MIN]]\n\nI think the quality of the paper should be further improved by addressing these problems and currently it should be rejected."[[OAL-NEG], [CLA-NEG,PNF-NEG,REC-NEG], [CRT,FBK], [MAJ]]