"This paper presents an analysis of LSTMS showing that they have a from where the memory cell contents at each step is a weighted combination of the \u201ccontent update\u201d values computed at each time step.[[PDI-NEU], [null], [SMY], [GEN]] The weightings are defined in terms of an exponential decay on each dimension at each time step (given by the forget gate), which lets the cell be computed sequentially in linear time rather than in the exhaustive quadratic time that would apparently be necessary for this definition.[[PDI-NEU], [null], [SMY], [GEN]] Second, the paper offers a simplification of LSTMs that compute the value by which the memory cell at each time step in terms of a deterministic function of the input rather than a function of the input and the current context.[[PDI-NEU], [null], [SMY], [GEN]] This reduced form of the LSTM is shown to perform comparably to \u201cfull\u201d LSTMs.[[PDI-NEU], [null], [SMY], [GEN]]\n\nThe decomposition of the LSTM in terms of these weights is useful, and suggests new strategies for comparing existing quadratic time attention-based extensions to RNNs.[[EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]] The proposed model variations (which replaces the \u201ccontent update\u201d that has a recurrent network in terms of context-independent update) and their evaluations seem rather more arbitrary.[[EXP-NEU,MET-NEU], [EMP-POS], [APC], [MAJ]]  First, there are two RNNs present in the LSTM- one controls the gates, one controls the content update.[[EXP-NEU,MET-NEU], [null], [DIS], [GEN]]  You get rid of one, not the other.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MIN]]  You can make an argument for why the one that was ablated was \u201cmore interesting\u201d, but really this is an obvious empirical question that should be addressed.[[ANA-NEU], [SUB-NEU], [SUG], [MAJ]]  The second problem of what tasks to evaluate on is a general problem with comparing RNNs.[[PDI-NEU], [null], [DIS], [GEN]]  One non-language task (e.g., some RL agent with an LSTM, or learning to execute or something) and one synthetic task (copying or something) might be sensible.[[PDI-NEU], [null], [SUG], [MIN]]  Although I don\u2019t think this is the responsibility of this paper (although something that should be considered).[[EXT-NEU], [null], [DIS], [GEN]] \n\nFinally, there are many further simplifications of LSTMs that could have been explored in the literature: coupled input-forget gates (Greff et al, 2015), diagonal matrices for gates, GRUs.[[EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]]  When proposing yet another simplification, some sense for how these different reductions is useful, so I would recommend comparison to those.[[EXP-NEU], [CMP-NEU], [SUG], [MIN]] \n\nNotes on clarity:\nBefore Eq 1 it\u2019s hard to know what the antecedent of \u201cwhich\u201d is without reading ahead.[[MET-NEG], [CLA-NEG], [CRT], [MIN]] \n\nFor componentwise multiplication, you have been using \\circ, but then for the iterated component wise product, \\prod is used.[[MET-NEG], [CLA-NEG], [CRT], [MIN]] To be consistent, notation like \\odot and \\bigodot might be a bit clearer.[[MET-NEU], [CLA-NEU], [DIS], [MIN]]\n\nThe discussion of dynamic programming: the dynamic program is also only available because the attention pattern is limited in a way that self attention is not.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] This might be worth mentioning.[[MET-NEU], [SUB-NEU], [SUG], [MIN]]\n\nWhen presenting Eq 11, the definition of w_j^t elides a lot of complexity.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Indeed, w_j^t is only ever implicitly defined in Eq 8, whereas things like the input and forget gates are defined multiple times in the text..[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Since w_j^t can be defined iteratively and recursively (as a dynamic program), it\u2019s probably worth writing both out, for expository clarity.[[MET-NEU], [CLA-NEU], [SUG], [MIN]]\n\nEq 11 might be clearer if you show that Eq 8 can also be rewritten in the same wheat, provided, you make h_{t-1} an argument to output and content.[[MET-NEU], [CLA-NEU], [SUG], [MIN]]\n\nTable 4 is unclear.[[TNF-NEG], [CLA-NEG], [CRT], [MIN]] In a language model, the figure looks like it is attending to the word that is being generated, which is clearly not what you want to convey since language models don\u2019t condition on the word they are predicting.[[TNF-NEG], [CLA-NEG], [CRT], [MIN]] Presumably the strong diagonal attention is attending to the previous word when computing the representation to generate the subsequent word?[[TNF-NEU], [CLA-NEU], [QSN], [MIN]] In any case, this figure should be corrected to reflect this.[[TNF-NEU], [PNF-NEU], [SUG], [MIN]] This objection also concerns the right hand figure, and the semantics of the meaning of the upper vs lower triangles should be clarified in the caption (rather than just in the text)."[[TNF-NEU], [CLA-NEU], [SUG], [MIN]]