"This paper proposes to re-formulate the GAN saddle point objective (for a logistic regression discriminator) as a minimization problem by dualizing the maximum likelihood objective for regularized logistic regression (where the dual function can be obtained in closed form when the discriminator is linear).[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]]  They motivate their approach by repeating the previously made claim that the naive gradient approach is non-convergent for generic saddle point problems (Figure 1); while a gradient approach often works well for a minimization formulation.[[PDI-POS,MET-POS], [EMP-POS], [APC], [MAJ]] \n\nThe dual problem of regularized logistic regression is an entropy-regularized concave quadratic objective problem where the Hessian is y_i y_j <x_i, x_j>, thus highlighting the pairwise similarities between the points x_i & x_j; here the labels represent whether the point x comes from the samples A from the target distribution or B from the proposal distribution.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MIN]]  This paper then compare this objective with the MMD distance between the samples A & B.[[PDI-NEU], [null], [SMY], [GEN]]  It points out that the adversarial logistic distance can be viewed as an iteratively reweighted empirical estimator of the MMD distance, an interesting analogy (but also showing the limited power of the adversarial logistic distance for getting good generating distributions, given e.g. that the MMD has been observed in the past to perform poorly for face generation [Dziugaite et al. UAI 2015]). [[RWK-NEU,MET-NEU], [null], [DIS], [MIN]] From this analogy, one could expect the method to improves over MMD, but not necessarily significantly in comparison to an approach which would use more powerful discriminators.[[MET-NEG], [CMP-NEG], [CRT], [MIN]] \n\nThis paper then investigates the behavior of this adversarial logistic distance in the context of aligning distributions for domain adaptation, with experiments on a visual adaptation task.[[INT-NEU,EXP-NEU], [null], [DIS], [MIN]]  They observe better performance for their approach in comparison to ADDA, improved WGAN and MMD, when restricting the discriminators to be a linear classifier.[[MET-POS], [EMP-POS], [APC], [MAJ]]\n\n== Evaluation \n\nI found this paper quite clear to read and enjoyed reading it.[[OAL-POS], [CLA-POS], [APC], [MAJ]] The observations are interesting, despite being on the toyish side.[[RES-POS], [CLA-POS], [APC], [MAJ]]  I am not an expert on GANs for domain adaptation, and thus I can not judge of the quality of the experimental comparison for this application, but it seemed reasonable, apart for the restriction to the linear discriminators (which is required by the framework of this paper).[[EXP-POS], [CMP-POS], [APC], [MAJ]] \n\nOne concern about the paper (but this is an unfortunate common feature of most GAN papers) is that it ignores the vast knowledge on saddle point optimization coming from the optimization community.[[OAL-NEG], [SUB-NEG], [CRT], [MIN]]  The instability of a gradient method on non-strongly convex-concave saddle point problems (like the bilinear form of Figure 1) is a well-known property, and many alternative *stable* gradient based algorithms have been proposed to solve saddle point problems which do not require transforming them to a minimization problem as suggested in this paper.[[MET-POS], [CLA-POS], [APC], [MAJ]]  Moreover, the transformation to the minimization form crucially required the closed form computation of the dual function (with w* just defined above equation (2)), and this is limited to linear discriminators,  thus ruling out the use of the proposed approach to more powerful discriminators like deep neural nets.[[MET-NEG], [CLA-POS], [APC], [MAJ]]  Thus the significance appears a bit limited to me.[[OAL-NEG], [IMP-NEG], [CRT], [MAJ]] \n\n== Other comments\n\n1) Note that d(A, B'_theta) is *equal* to min_alpha max_w  (...)  above equation (2) (it is not just an upper bound).[[MET-NEU], [null], [CRT], [GEN]]  This is a standard result coming from the fact that the Fenchel dual problem to regularized maximum likelihood is the maximum entropy problem with a quadratic objective as (2).[[RES-POS], [EMP-POS], [APC], [MAJ]]   See e.g. Section 2.2 of [Collins et al. JMLR 2008] (this is for the more general multiclass logistic regression problem, but (2) is just the binary special case of equation (4) in the [Collins ... ] reference).[[RWK-NEU], [CMP-NEU], [DIS], [MIN]]  And note that the \"w(u)\" defined in this reference is the lambda*w*(alpha) optimal relationship defined in this paper (but without the 1/lambda factor because of just slightly different writing; the point though is that strong duality holds there and thus one really has equality).[[RWK-NEU], [CMP-NEU], [DIS], [MIN]]\n\n\n[Collins et al. JMLR 2008] Michael Collins, Amir Globerson, Terry Koo , Xavier Carreras, Peter L. Bartlett, Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks, , JMLR 2008.\n\n [Dziugaite et al. UAI 2015] Gintare Karolina Dziugaite, Daniel M. Roy, and Zoubin Ghahramani. Training generative neural networks via maximum mean discrepancy optimization. In UAI, 2015"[[BIB-NEU], [null], [DIS], [GEN]]