"This paper aims to synthesize programs in a Java-like language from a task description (X) that includes some names and types of the components that should be used in the program. [[INT-NEU,EXP-NEU], [EMP-NEU], [SMY], [GEN]]The paper argues that it is too difficult to map directly from the description to a full program, so it instead formulates the synthesis in two parts.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] First, the description is mapped to a \"sketch\" (Y) containing high level program structure but no concrete details about,[[RWK-NEG], [SUB-NEG,EMP-NEG], [DFT], [MIN]] e.g., variable names. Afterwards, the sketch is converted into a full program (Prog) by stochastically filling in the abstract parts of the sketch with concrete instantiations.[[RWK-NEU,EXP-NEU], [EMP-NEU], [SMY], [GEN]]\n\nThe paper presents an abstraction method for converting a program into a sketch, a stochastic encoder-decoder model for converting descriptions to trees, and rejection sampling-like approach for converting sketches to programs.[[INT-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] Experimentally, it is shown that using sketches as an intermediate abstraction outperforms directly mapping to the program AST.[[INT-NEU,EXP-NEU], [EMP-NEU], [SMY], [GEN]] The data is derived from an online repository of ~1500 Android apps,[[RWK-NEU,DAT-NEU], [null], [SMY], [GEN]] and from that were extracted ~150k methods, which makes the data very respectable in terms of realisticness and scale.[[RWK-NEU,DAT-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] This is one of the strongest points of the paper.[[OAL-NEG], [NOV-POS,IMP-POS], [APC], [MAJ]]\n\nOne point I found confusing is how exactly the Combinatorial Concretization step works.[[RWK-NEG,EXP-NEG], [EMP-NEG], [DFT], [MIN]] Am I correct in understanding that this step depends only on Y, and that given Y,[[RWK-NEU,EXP-NEU], [null], [QSN], [GEN]] Prog is conditionally independent of X?[[RWK-NEU,EXP-NEU], [null], [QSN], [GEN]] If this is correct, how many Progs are consistent with a typical Y?[[RWK-NEU,EXP-NEU], [null], [QSN], [GEN]] Some additional discussion of why no learning is required for the P(Prog | Y) step would be appreciated.[[RWK-POS], [null], [FBK], [MAJ]]\n\nI'm also curious whether using a stochastic latent variable (Z) is necessary[[RWK-NEU,EXP-NEU], [null], [FBK], [GEN]]. Would the approach work as well using a more standard encoder-decoder model with determinstic Z?[[PDI-NEU,MET-NEU], [null], [QSN], [GEN]]\n\nSome discussion of Grammar Variational Autoencoder (Kusner et al) would probably be appropriate.[[RWK-POS,BIB-POS], [APR-POS], [APC], [MAJ]]\n\nOverall, I really like the fact that this paper is aiming to do program synthesis on programs that are more like those found \"in the wild\".[[EXP-POS,OAL-POS], [EMP-POS], [APC], [MAJ]] While the general pattern of mapping a specification to abstraction with a neural net and then mapping the abstraction to a full program with a combinatorial technique is not necessarily novel,[[PDI-NEG,EXP-NEG,MET-NEG], [NOV-NEG,EMP-NEG], [DFT], [MIN]] I think this paper adds an interesting new take on the pattern (it has a very different abstraction than say, DeepCoder), and this paper is one of the more interesting recent papers on program synthesis using machine learning techniques, in my opinion.[[PDI-POS,OAL-POS], [NOV-POS,EMP-POS], [APC], [MAJ]]\n"