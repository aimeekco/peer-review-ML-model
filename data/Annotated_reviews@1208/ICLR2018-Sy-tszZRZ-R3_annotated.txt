"This is quite an interesting paper. Thank you.[[OAL-POS], [null], [APC], [MAJ]] Here are a few comments:\n\nI think this style of writing theoretical papers is pretty good, where the main text aims of preserving a coherent story while the technicalities of the proofs are sent to the appendix.[[OAL-POS], [CLA-POS,PNF-POS], [APC], [MAJ]] \nHowever I would have appreciated a little bit more details about the proofs in the main text (maybe more details about the construct that is involved).[[MET-NEG], [SUB-NEG], [SUG], [MIN]] I can appreciate though that this a fine line to walk.[[MET-POS], [EMP-POS], [APC], [MIN]] Also in the appendix, please restate the lemma that is being proven.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] Otherwise one will have to scroll up and down all the time to understand the proof.[[MET-NEG], [PNF-NEG], [CRT], [MIN]] \n\nI think the paper could also discuss a bit more in detail the results provided.[[RES-NEU], [SUB-NEU], [SUG], [MIN]] For example a discussion of how practical is the algorithm proposed for exact counting of linear regions would be nice.[[MET-NEU], [SUB-NEU], [SUG], [MIN]] Though regardless, I think the findings speak for themselves and this seems an important step forward in understanding neural nets.[[RES-POS,OAL-POS], [IMP-POS], [APC], [MAJ]] \n\n****************\nI had reduced my score based on the observation made by Reviewer 1 regarding the talk Montufar at SampTA.[[EXT-NEG,OAL-NEU], [REC-NEG], [FBK], [MAJ]] Could the authors prioritize clarification to that point ![[RWK-NEU], [SUB-NEU], [QSN], [MIN]] \n - Thanks for the clarification and adding this citation. "[[BIB-POS,OAL-POS], [SUB-POS], [APC], [MAJ]]