"In this paper, the authors consider symmetric (3rd order) CP decomposition of a PPMI tensor M (from neighboring triplets), which they call CP-S.[[PDI-NEU], [null], [SMY], [GEN]] Additionally, they propose an extension JCP-S, for n-order tensor decompositions.[[MET-NEU], [null], [SMY], [GEN]] This is then compared with random, word2vec, and NNSE, the latter of two which are matrix factorization based (or interpretable) methods.[[RWK-NEU,MET-NEU], [null], [SMY], [GEN]] The method is shown to be superior in tasks of 3-way outlier detection, supervised analogy recovery, and sentiment analysis.[[MET-NEU,ANA-NEU], [null], [SMY], [GEN]] Additionally, it is evaluated over the MEN and Mturk datasets.[[DAT-NEU,MET-NEU], [null], [SMY], [GEN]]\n\n\nFor the JCP-S model, the loss function is unclear to me.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] L is defined for 3rd order tensors only;  how is the extended to n > 3?[[MET-NEG], [EMP-NEG], [QSN], [MIN]] Intuitively it seems that L is redefined, and for, say, n = 4, the model is M(i,j,k,n) = \\sum_1^R u_ir u_jr u_kr u_nr.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] However, the statement \"since we are using at most third order tensors in this work\" I am further confused.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Is it just that JCP-S also incorporates 2nd order embeddings?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] I believe this requires clarification in the manuscript itself.[[MET-NEG], [EMP-NEG], [SUG], [MIN]]\n\nFor the evaluations, there are no other tensor-based methods evaluated, although there exist several well-known tensor-based word embedding models existing:\n\nPengfei Liu, Xipeng Qiu\u2217 and Xuanjing Huang, Learning Context-Sensitive Word Embeddings with Neural Tensor Skip-Gram Model,  IJCAI 2015\n\nJingwei Zhang and Jeremy Salwen, Michael Glass and Alfio Gliozzo.[[RWK-NEU,BIB-NEU], [null], [DIS], [MIN]] Word Semantic Representations using Bayesian Probabilistic Tensor Factorization, EMNLP 2014\n\nMo Yu, Mark Dredze, Raman Arora, Matthew R. Gormley, Embedding Lexical Features via Low-Rank Tensors\n\nto name a few via quick googling.[[RWK-NEU,BIB-NEU], [null], [DIS], [MIN]]\n\nAdditionally, since it seems the main benefit of using a tensor-based method is that you can use 3rd order cooccurance information, multisense embedding methods should also be evaluated.[[RWK-NEU,MET-NEU], [EMP-NEU], [SUG], [MIN]] There are many such methods, see for example \n\nJiwei Li, Dan Jurafsky, Do Multi-Sense Embeddings Improve Natural Language Understanding?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nand citations within, plus quick googling for more recent works.[[RWK-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\nI am not saying that these works are equivalent to what the authors are doing, or that there is no novelty, but the evaluations seem extremely unfair to only compare against matrix factorization techniques, when in fact many higher order extensions have been proposed and evaluated, and especially so on the tasks proposed (in particular the 3-way outlier detection).[[RWK-NEG], [CMP-NEU], [DIS], [MIN]] \n\nObserve also that in table 2, NNSE gets the highest performance in both MEN and MTurk.[[TNF-NEG], [EMP-NEU], [QSN], [MIN]] Frankly this is not very surprising; matrix factorization is very powerful, and these simple word similarity tasks are well-suited for matrix factorization.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] So, statements like \"as we can see, our embeddings very clearly outperform the random embedding at this task\" is  an unnecessary inflation of a result that 1) is not good[[RES-NEG], [EMP-NEG], [CRT], [MIN]] and 2) is reasonable to not be good.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] \n\nOverall, I think for a more sincere evaluation, the authors need to better pick tasks that clearly exploit 3-way information and compare against other methods proposed to do the same.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n\nThe multiplicative relation analysis is interesting,[[ANA-POS], [EMP-POS], [APC], [MAJ]] but at this point it is not clear to me why multiplicative is better than additive in either performance or in giving meaningful interpretations of the model.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] \n\nIn conclusion, because the novelty is also not that big (CP decomposition for word embeddings is a very natural idea) I believe the evaluation and analysis must be significantly strengthened for acceptance. "[[MET-NEU], [NOV-NEG], [CRT], [MIN]]