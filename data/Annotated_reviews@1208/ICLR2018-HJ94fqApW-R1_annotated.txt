"In this paper, the authors propose a data-dependent channel pruning approach to simplify CNNs with batch-normalizations.[[INT-NEU,PDI-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] The authors view CNNs as a network flow of information and applies sparsity regularization on the batch-normalization scaling parameter \\gamma which is seen as a \u201cgate\u201d to the information flow[[RWK-NEU,PDI-NEU,EXP-NEU,MET-NEU,ANA-NEU], [EMP-NEU], [SMY], [GEN]]. Specifically, the approach uses iterative soft-thresholding algorithm step to induce sparsity in \\gamma during the overall training phase of the CNN (with additional rescaling to improve efficiency[[RWK-NEU,EXP-NEU,MET-NEU,RES-POS], [EMP-NEU], [APC], [GEN]]. In the experiments section, the authors apply their pruning approach on a few representative problems and networks. [[PDI-NEU,EXP-NEU], [EMP-NEU], [SMY,DIS], [GEN]]\n\nThe concept of applying sparsity on \\gamma to prune channels is an interesting one, compared to the usual approaches of sparsity on weights[[PDI-POS,EXP-POS,MET-POS,ANA-POS], [CMP-POS,EMP-POS], [APC], [MAJ]]. However, the ISTA, which is equivalent to L1 penalty on \\gamma is in spirit same as \u201csmaller-norm-less-informative\u201d assumption. [[RWK-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]]Hence, the title seems a bit misleading.[[OAL-NEG], [null], [DFT,FBK], [MIN]] \n\nThe quality and clarity of the paper can be improved in some sections[[INT-POS], [CLA-POS], [APC], [MAJ]]. Some specific comments by section:\n\n3. Rethinking Assumptions:\n-\tWhile both issues outlined here are true in general, the specific examples are either artificial or can be resolved fairly easily[[RWK-NEG,PDI-NEG,OAL-NEG], [IMP-NEG], [DFT], [MIN]]. For example: L-1 norm penalties only applied on alternate layers is artificial and applying the penalties on all Ws would fix the issue in this case. [[RWK-NEG,MET-NEG,RES-NEG], [IMP-NEG,EMP-NEG], [DFT], [MIN]]Also, the scaling issue of W can be resolved by setting the norm of W to 1, as shown in He et. al., 2017[[RWK-NEG,BIB-NEG], [IMP-NEG], [DFT], [MIN]]. Can the authors provide better examples here?[[RWK-NEU], [null], [QSN], [GEN]]\n-\tCan the authors add specific citations of the existing works which claim to use Lasso, group Lasso, thresholding to enforce parameter sparsity?[[RWK-NEU,BIB-NEU], [null], [QSN], [GEN]]\n\n4. Channel Pruning\n-\tThe notation can be improved by defining or replacing \u201csum_reduced\u201d\n-\tISTA \u2013 is only an algorithm, the basic assumption is still L1 -> sparsity or smaller-norm-less-informative. [[RWK-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [GEN]]Can the authors address the earlier comment about \u201ca theoretical gap questioning existing sparsity inducing formulation and actual computational algorithms\u201d[[RWK-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [GEN]]?\n-\tCan the authors address the earlier comment on \u201chow to set thresholds for weights across different layers\u201d, by providing motivation for choice of penalty for each layer?[[RWK-NEU,EXP-NEU,MET-NEU,ANA-NEG], [EMP-NEU], [DFT,DIS], [MIN]] \n-\tCan the authors address the earlier comment on how their approach provides \u201cguarantees for preserving neural net functionality approximately\u201d?\n\n[[PDI-NEU,EXP-NEU], [EMP-NEU], [DIS,QSN], [GEN]]5. Experiments\n-\tCIFAR-10: Since there is loss of accuracy with channel pruning, it would be useful to compare accuracy of a pruned model with other simpler models with similar param.size? [[RWK-NEU,EXP-NEU,MET-NEU,RES-NEU], [EMP-NEU], [SMY,DIS,QSN], [GEN]](like pruned-resnet-101 vs. resnet-50 in ISLVRC subsection)\n-\tISLVRC: The comparisons between similar param-size models is exteremely useful in highlighting the contribution of this. [[MET-NEU], [CMP-NEU,EMP-NEU], [DIS], [GEN]]However, resnet-34/50/101 top-1 error rates from Table 3/4 in (He et.al. 2016) seem to be lower than reported in table 3 here. Can the authors clarify?\[[RWK-NEU,EXP-NEU,TNF-NEU,BIB-NEU], [CLA-NEU,PNF-NEU], [SMY,DIS,QSN], [GEN]]n-\tFore/Background: Can the authors add citations for datasets, metrics for this problem?[[RWK-NEU,PDI-NEU,DAT-NEU,MET-NEU], [EMP-NEU], [DIS,QSN], [GEN]]\n\n\nOverall, the channel pruning with sparse \\gammas is an interesting concept and the numerical results seem promising[[PDI-POS,RES-POS,OAL-POS], [IMP-POS], [APC], [MAJ]]. The authors have started with right motivation and the initial section asks the right questions, however,[[RWK-POS], [null], [QSN], [GEN]] some of those questions are left unanswered in the subsequent work as detailed above.[[RWK-NEG], [CLA-NEG], [DFT,QSN], [MIN,GEN]]