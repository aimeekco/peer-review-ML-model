"This manuscript introduce a scheme for learning the recurrent parameter matrix in a neural network that uses the Cayley transform and a scaling weight matrix.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] This scheme leads to good performance on sequential data tasks and requires fewer parameters than other techniques[[RWK-POS,DAT-POS,RES-POS], [CMP-POS,EMP-POS], [APC], [MAJ]]\n\nComments:\n-- It\u2019s not clear to me how D is determined for each test.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Given the definition in Theorem 3.1 it seems like you would have to have some knowledge of how many eigenvalues in W you expect to be close to -1.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]  \n-- For the copying and adding problem test cases, it might be useful to clarify or cite something clarifying that the failure mode RNNs run into with temporal ordering problems is an exploding gradient, rather than any other pathological training condition, just to make it clear why these experiments are relevant.[[EXP-NEG,MET-NEG], [SUB-NEU], [SUG], [MIN]] \n-- The ylabel in Figure 1 is \u201cTest Loss\u201d which I didn\u2019t see defined.[[TNF-NEG], [PNF-NEG], [DFT], [MIN]] Is this test loss the cross entropy?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] If so, I think it would be more effective to label the plot with that.[[TNF-NEG], [PNF-NEG], [SUG], [MIN]]\n-- The plots in figure 1 and 2 have different colors to represent the same set of techniques.[[TNF-NEG], [PNF-NEG], [SUG], [MIN]] I would suggest keeping a  consistent color scheme[[TNF-NEG], [PNF-NEG], [SUG], [MIN]]\n-- It looks like in Figure 1 the scoRNN is outperformed by the uRNN in the long run in spite of the scoRNN convergence being smoother, which should be clarified.[[TNF-POS,MET-NEU], [CMP-POS,PNF-NEU], [SUG], [MIN]]\n-- It looks like in Figure 2 the scoRNN is outperformed by the LSTM across the board, which should be clarified.[[TNF-POS,MET-NEU], [CMP-POS,PNF-NEU], [SUG], [MIN]]\n-- How is test set accuracy defined in section 5.3?[[MET-NEU,RES-POS], [EMP-NEU], [QSN], [MIN]] Classifying digits? Recreating digits?[[MET-NEU,RES-POS], [EMP-NEU], [QSN], [MIN]] \n-- When discussing table 1, the manuscript mentions scoRNN and Restricted-capacity uRNN have similar performance for 16k parameters and then state that scoRNN has the best test accuracy at 96.2%.[[MET-NEU,RES-NEU,TNF-NEU], [null], [SMY,DIS], [GEN]] However, there is no example for restricted-capacity uRNN with 69k parameters to show that the performance of restricted-capacity uRNN doesn't also increase similarly with more parameters.[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]]\n-- Overall it\u2019s unclear to me how to completely determine the benefit of this technique over the others because, for each of the tests, different techniques may have superior performance.[[EXP-NEG,MET-NEG], [CMP-NEG,EMP-NEG], [CRT], [MIN]] For instance, LSTM performs best in 5.2 and in 5.3 for the MNIST test accuracy.[[MET-NEU], [CMP-NEU], [DIS], [MIN]] scoRNN and Restricted-capacity uRNN perform similarly for permuted MNIST Test Accuracy in 5.3.[[MET-NEU], [CMP-NEU], [DIS], [MIN]] Finally, scoRNN seems to far outperform the other techniques in table 2 on the TIMIT speech dataset.[[DAT-POS,MET-POS], [CMP-NEU,EMP-POS], [APC], [MAJ]] I don\u2019t understand the significance of each test and why the relative performance of the techniques vary from one to the other.[[RES-NEG], [IMP-NEG], [DIS], [MIN]]\n-- For example, the manuscript seems to be making the case that the scoRNN gradients are more stable than those of a uRNN, but all of the results are presented in terms of network accuracy and not gradient stability.[[MET-NEU,RES-NEU], [EMP-NEU], [DIS], [MIN]] You can sort of see that generally the convergence is more gradual for the scoRNN than the uRNN from the training graphs but it'd be nice if there was an actual comparison of the stability of the gradients during training (as in Figure 4 of the Arjovsky 2016 paper being compared to for instance) just to make it really clear."[[RWK-NEU,EXP-NEU,MET-NEU,TNF-NEU], [CMP-NEU], [SUG,DIS], [MIN]]