"This paper proposes a learning-to-learn approach to training inference networks in VAEs that make explicit use of the gradient of the log-likelihood with respect to the latent variables to iteratively optimize the variational distribution.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The basic approach follows Andrychowicz et al. (2016), but there are some extra considerations in the context of learning an inference algorithm.[[RWK-NEU,MET-NEU], [null], [SMY], [GEN]] \n\nThis approach can significantly reduce the amount of slack in the variational bound due to a too-weak inference network (above and beyond the limitations imposed by the variational family).[[MET-POS], [EMP-POS], [APC], [MAJ]]  This source of error is often ignored in the literature,[[MET-NEG], [EMP-NEG], [CRT], [MIN]] although there are some exceptions that may be worth mentioning:\n* Hjelm et al. (2015; https://arxiv.org/pdf/1511.06382.pdf) observe it for directed belief networks (admittedly a different model class).[[RWK-POS,MET-POS], [EMP-POS], [APC], [MAJ]]\n* The ladder VAE paper by Sonderby et al. (2016, https://arxiv.org/pdf/1602.02282.pdf) uses an architecture that reduces the work that the encoder network needs to do, without increasing the expressiveness of the variational approximation.[[RWK-NEU], [CMP-NEU], [DIS], [GEN]]\n* The structured VAE paper by Johnson et al. (2016, https://arxiv.org/abs/1603.06277) also proposes an architecture that reduces the load on the inference network.\n* A very recent paper by Krishnan et al. (https://arxiv.org/pdf/1710.06085.pdf, posted to arXiv days before the ICLR deadline) is probably closest; it also examines using iterative optimization (but no learning-to-learn) to improve training of VAEs.[[RWK-NEU], [null], [DIS], [GEN]] They remark that the benefits on binarized MNIST are pretty minimal compared to the benefits on sparse, high-dimensional data like text and recommendations; this suggests that the learning-to-learn approach in this paper may shine more if applied to non-image datasets and larger numbers of latent variables.[[DAT-NEG,MET-NEG], [CMP-NEU,EMP-NEG], [CRT], [MAJ]]\n\nI think this is good and potentially important work,[[OAL-POS], [CNT], [APC], [MAJ]] although I do have some questions/concerns about the results in Table 1 (see below).[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] \n\n\nSome more specific comments:\n\nFigure 2: I think this might be clearer if you unrolled a couple of iterations in (a) and (c).[[TNF-NEG], [PNF-NEG], [CRT], [MIN]]\n\n(Dempster et al. 1977) is not the best reference for this section; that paper only considers the case where the E and M steps can be done in closed form on the whole dataset.[[DAT-NEG,RWK-NEG], [CMP-NEG], [CRT], [MIN]] A more relevant reference would be Stochastic Variational Inference by Hoffman et al. (2013), which proposes using iterative optimization of variational parameters in the inner loop of a stochastic optimization algorithm.[[MET-NEU,RWK-NEU], [CMP-NEU], [SUG], [MIN]]\n\nSection 4: The statement p(z)=N(z;mu_p,Sigma_p) doesn\u2019t quite match the formulation of Rezende&Mohamed (2014).[[MET-POS], [EMP-POS], [APC], [MAJ]] First, in the case where there is only one layer of latent variables, there is almost never any reason to use anything but a normal(0, I) prior, since the first weight matrix of the decoder can reproduce the effects of any mean or covariance.[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]] Second, in the case where there are two or more layers, the joint distribution of all z need not be Gaussian (or even unimodal) since the means and variances at layer n can depend nonlinearly on the value of z at layer n+1.[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]] An added bonus of eliminating the mu_p, Sigma_p: you could get rid of one subscript in mu_q and sigma_q, which would reduce notational clutter.[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\nWhy not have mu_{q,t+1} depend on sigma_{q,t} as well as mu_{q,t}?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nTable 1: These results are strange in a few ways:\n* The gap between the standard and iterative inference network seems very small (0.3 nats at most).[[RES-NEU], [EMP-NEG], [CRT], [MIN]] This is much smaller than the gap in Figure 5(a).[[RES-NEG,TNF-NEG], [EMP-NEG], [CRT], [MIN]]\n* The MNIST results are suspiciously good overall, given that it\u2019s ultimately a Gaussian approximation and simple fully connected architecture.[[RES-POS], [EMP-POS], [APC], [MAJ]] I\u2019ve read a lot of papers evaluating that sort of model/variational distribution as a baseline, and I don\u2019t think I\u2019ve ever seen a number better than ~87 nats."[[EXT-NEU], [null], [DIS], [GEN]]