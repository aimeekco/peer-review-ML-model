"The paper studies convolutional neural networks where the stride is smaller than the convolutional filter size;[[RWK-NEU], [null], [SMY], [GEN]] the so called overlapping convolutional architectures[[RWK-NEU,MET-NEU], [null], [SMY], [GEN]]. The main object of study is to quantify the benefits of overlap in convolutional architectures.[[RWK-NEU,MET-POS], [EMP-POS], [APC], [MAJ]]\n\nThe main claim of the paper is Theorem 1, which is that overlapping convolutional architectures are efficient with respect to non-overlapping architectures, i.e.,[[RWK-NEU,MET-NEU], [CMP-NEU], [SMY,DIS], [GEN]] there exists functions in the overlapping architecture which require an exponential increase in size to be represented in the non-overlapping architecture;[[RWK-NEU,DAT-POS,EXP-NEU,MET-NEU], [IMP-POS,EMP-NEU], [SMY], [GEN]] whereas overlapping architecture can capture within a linear size the functions represented by the non-overlapping architectures.[[RWK-NEU,DAT-POS], [null], [SMY], [GEN]] The main workhorse behind the paper is the notion of rank of matricized grid tensors following a paper of Cohen and Shashua which capture the relationship between the inputs and the outputs, the function implemented by the neural network.[[PDI-NEU,DAT-NEU,EXP-NEU,RES-NEU,ANA-NEU,BIB-NEU], [IMP-NEU,EMP-NEU], [SMY], [GEN]] \n\n(1) The results of the paper hold only for product pooling and linear activation function except for the representation layer, which allows general functions.[[RWK-NEG,RES-NEG], [IMP-NEG], [DFT], [MIN]] It is unclear why the generalized convolutional networks are stated with such generality when the results apply only to this special case[[PDI-NEG,RES-NEG], [CLA-NEG,IMP-NEG], [DFT], [MIN]]. That this is the case should be made clear in the title and abstract[[ABS-NEU,RWK-NEU], [CLA-NEU], [QSN], [GEN]]. The paper makes a point that generalized tensor decompositions can be potentially applied to solve the more general case[[RWK-NEU,MET-NEU], [IMP-NEU], [SMY,DIS], [GEN]], but since it is left as future work, the paper should make it clear throughout.[[RWK-NEG,FWK-NEG], [CLA-NEG,REC-NEG], [DFT], [MIN]]\n\n(2) The experiment is minimal and even the given experiment is not described well.[[EXP-POS,RES-POS], [EMP-POS], [APC], [MAJ]] What data augmentation was used for the CIFAR-10 dataset?[[DAT-NEU], [null], [QSN], [GEN]] It is only mentioned that the data is augmented with translations and horizontal flips.[[RWK-NEU,DAT-NEU], [null], [SMY], [GEN]]What is the factor of augmentation[[RWK-NEU,EXP-NEU], [null], [QSN], [GEN]]? How much translation[[EXP-NEU], [null], [QSN], [GEN]]? These are important because there maybe a much simpler explanation to the benefit of overlap: it is able to detect these translated patterns easily.[[RWK-NEU,EXP-NEU,RES-NEU,TNF-NEU], [IMP-NEU], [SMY], [GEN]] Indeed, this simple intuition seems to be why the authors chose to make the problem by introducing translations and flips.[[RWK-NEG,PDI-NEG,EXP-NEG,MET-NEG], [null], [DFT,QSN], [MIN]] \n\n(3) It is unclear if the paper resolves the mystery that they set out to solve, which is a reconciliation of the following two observations[[PDI-NEU,EXP-NEU,ANA-NEU], [CLA-NEG], [SUG,QSN], [GEN]] (a) why are non-overlapping architectures so common?[[RWK-NEU,MET-NEU], [null], [QSN], [GEN]] (b) why only slight overlap is used in practice?[[RWK-NEU,EXP-NEU], [null], [QSN], [GEN]]  The paper seems to claim that since overlapping architectures have higher expressivity that answers (a)[[RWK-NEU,EXP-POS,RES-POS], [EMP-POS], [APC], [MAJ]]. It appears that the paper does not answer (b) well: it points out that since there is exponential increase, there is no reason to increase it beyond a particular point.[[RWK-NEG,EXP-NEU,RES-NEG], [IMP-NEU,EMP-NEU], [DFT,DIS], [MAJ]] It seems the right resolution will be to show that after the overlap is set to a certain small value[[RWK-NEU], [SUB-NEG,IMP-NEG], [SMY,DIS], [MIN]], there will be *only* linear increase with increasing overlap; i.e., the paper should show that small overlap networks are efficient with respect to *large* overlap networks; a comparison that does not seem to be made in the paper.[[RWK-NEG,DAT-NEU,EXP-NEU,ANA-NEU], [CMP-NEG,EMP-NEU], [DFT,DIS], [GEN]] \n\n(4) Small typo: the dimensions seem to be wrong in the line below the equation in page 3[[RWK-NEG,EXP-NEG], [EMP-NEG], [DFT], [MIN]]. \n\nThe paper makes important progress on a highly relevant problem using a new methodology (borrowed from a previous paper)[[RWK-POS,MET-POS,BIB-POS], [SUB-POS,EMP-POS], [APC], [MAJ]]. However, the writing is hurried and the high-level conclusions are not fully supported by theory and experiments. [[RWK-NEG,EXP-NEG,ANA-NEG], [CLA-NEG,EMP-NEG], [DFT], [MIN]]"