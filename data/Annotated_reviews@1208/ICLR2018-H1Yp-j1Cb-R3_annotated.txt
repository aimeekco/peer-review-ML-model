"The paper applies tools from online learning to GANs. [[PDI-NEU], [null], [SMY], [GEN]]In the case of a shallow discriminator, the authors proved some results on the convergence of their proposed algorithm (an adaptation of FTRL) in GAN games, by leveraging the fact that when D update is small, the problem setup meets the ideal conditions for no-regret algorithms.[[PDI-NEU,EXP-NEU,MET-NEU,RES-NEU], [null], [SMY], [GEN]] The paper then takes the intuition from the semi-shallow case and propose a heuristic training procedure for deep GAN game.[[RWK-NEU,PDI-NEU], [null], [SMY], [GEN]] \n\nOverall the paper is very well written.[[OAL-POS], [CLA-POS], [SMY], [MAJ]] The theory is significant to the GAN literature, probably less so to the online learning community.[[RWK-NEU], [IMP-NEU], [SMY,DFT], [GEN]] In practice, with deep D, trained by single gradient update steps for G and D, instead of the \"argmin\" in Algo 1., the assumptions of the theory break.[[RWK-NEU,EXP-NEU], [EMP-NEU], [SMY], [GEN]] This is OK as long as sufficient experiment results verify that the intuitions suggested by the theory still qualitatively hold true. [[RWK-NEU,PDI-NEU,EXP-NEU], [IMP-NEU,EMP-NEU], [SMY,SUG], [GEN]]However, this is where I have issues with the work:\n\n1) In all quantitative results, Chekhov GAN do not significantly beat unrolled GAN.[[RWK-NEG,MET-NEG,RES-NEG], [IMP-NEG,EMP-NEG], [DFT,CRT], [MIN]] Unrolled GAN looks at historical D's through unrolled optimization, but not the history of G.[[RWK-NEU,MET-NEG], [IMP-NEG], [DFT], [MIN]] So this lack of significant difference in results raise the question of whether any improvement of Chekhov GAN is coming from the online learning perspective for D and G, or simply due to the fact that it considers historical D models (which could be motivated by sth other than the online learning theory).[[RWK-NEG,EXP-NEG,MET-NEG,RES-NEG], [IMP-NEG,EMP-NEG], [DFT], [MIN]]\n\n2) The mixture GAN approach suggested in Arora et al. (2017) is very related to this work, as acknowledged in Sec. 2.1, but no in-depth analysis is carried out. [[RWK-NEU], [null], [SMY], [GEN]]I suggest the authors to either discuss why Chekhov GAN is obviously superior and hence no experiments are needed, or compare them experimentally.[[RWK-NEG,EXP-NEG,MET-NEG], [CLA-NEG,IMP-NEG,EMP-NEG], [SUG,DFT,CRT], [MIN]] \n\n3) In the current state, it is hard to place the quantitative results in context with other common methods in the recent literature such as WGAN with gradient penalty.[[RWK-NEG,MET-NEU,RES-NEU], [NOV-NEU,EMP-NEG], [SMY,DFT], [MIN]] I suggest the authors to either report some results in terms of inception scores on cifar10 with similar architectures used in other methods for comparison.[[RWK-NEG,MET-NEG,RES-NEG], [SUB-NEG,CMP-NEG,EMP-NEG], [DFT], [MIN]] Alternatively please show WGAN-GP and/or other method results in at least one or two experiments using the evaluation methods in the paper.[[RWK-NEG,EXP-NEG,MET-NEG,RES-NEG], [SUB-NEG], [DFT], [MIN]] \n\nIn summary, almost all the experiments in the paper are trying to establish improvement over basic GAN, which would be OK if the gap between theory and practice is small.[[RWK-NEU], [null], [SUG], [GEN]] But in this case, it is not.[[RWK-NEG], [null], [CRT], [MIN]] So it is not entirely convincing that the practical Algo 2 works better for the reason suggested by the theory, nor it drastically improves practical results that it could become the standard technique in the literature.[[RWK-NEG,EXP-NEG,MET-NEG,RES-NEG,OAL-NEG], [IMP-NEG,EMP-NEG], [DFT,CRT], [MIN]] "