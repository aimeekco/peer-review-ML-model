"This work proposes an approach to meta-learning in which temporal convolutions and attention are used to synthesize labeled examples (for few-shot classification) or action-reward pairs (for reinforcement learning) in order to take the appropriate action.[[INT-NEU,PDI-NEU,MET-NEU], [null], [SMY], [GEN]] The resulting model is general-purpose and experiments demonstrate efficacy on few-shot image classification and a range of reinforcement learning tasks.[[EXP-POS,RES-POS], [EMP-POS], [APC], [MAJ]]\n\nStrengths\n\n- The proposed model is a generic meta-learning useful for both classification and reinforcement learning.[[MET-POS], [EMP-POS], [APC], [MAJ]]\n- A wide range of experiments are conducted to demonstrate performance of the proposed method.[[EXP-POS], [SUB-POS], [APC], [MAJ]]\n\nWeaknesses\n\n- Design choices made for the reinforcement learning setup (e.g. temporal convolutions) are not necessarily applicable to few-shot classification.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n- Discussion of results relative to baselines is somewhat lacking.[[RWK-NEG,RES-NEG], [CMP-NEG], [DFT], [MIN]]\n\nThe proposed approach is novel to my knowledge and overcomes specificity of previous approaches while remaining efficient.[[RWK-POS,MET-POS], [NOV-POS], [APC], [MAJ]]\n\nThe depth of the TC block is determined by the sequence length.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] In few-shot classification, the sequence length can be known a prior.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] How is the sequence length determined for reinforcement learning tasks?[[MET-NEU], [EMP-NEU], [QSN], [GEN]] In addition, what is done at test-time if the sequence length differs from the sequence length at training time?[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [GEN]]\n\nThe causality assumption does not seem to apply to the few-shot classification case.[[MET-NEU], [EMP-NEU], [CRT], [MIN]] Have the authors considered lifting this restriction for classification and if so does performance improve?[[MET-NEU,RES-NEU], [EMP-NEU], [QSN], [MIN]]\n\nThe Prototypical Networks results in Tables 1 and 2 do not appear to match the performance reported in Snell et al. (2017).[[RWK-NEG,RES-NEG,TNF-NEG], [CMP-NEG], [CRT], [MAJ]]\n\nThe paper is well-written overall.[[OAL-POS], [CLA-POS], [APC], [MAJ]] Some additional discussion of the results would be appreciated (for example, explaining why the proposed method achieves similar performance to the LSTM/OPSRL baselines).[[RWK-NEU,RES-NEU], [SUB-NEU], [SUG], [MIN]]\n\nI am not following the assertion in 5.2.3 that MAML adaption curves can be seen as an upper bound on the performance of gradient-based methods.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] I am wondering if the authors can clarify this point.[[MET-NEG], [CLA-NEG], [CRT], [MIN]]\n\nOverall, the proposed approach is novel and achieves good results on a range of tasks.[[MET-POS,MET-POS], [EMP-POS], [APC], [MAJ]]\n\nEDIT: I have read the author's comments and am satisfied with their response. I believe the paper is suitable for publication in ICLR."[[OAL-POS], [APR-POS], [APC], [MAJ]]