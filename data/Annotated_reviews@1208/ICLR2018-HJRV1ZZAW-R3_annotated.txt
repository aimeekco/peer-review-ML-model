"This paper proposes a convnet-based neural network architecture for reading comprehension and demonstrates reasonably good performance on SQuAD and TriviaQA with a great speed-up.[[DAT-NEU,PDI-NEU,RES-POS], [EMP-POS], [SMY,APC], [MAJ]]\n\nThe proposed architecture combines a few recent DL techniques: residual networks, dilated convolutions and gated linear units.[[MET-NEU], [null], [SMY], [GEN]]\n\nI understand the motivation that ConvNet has a great advantage of easing parallelization and thus is worth exploring.[[MET-NEU], [null], [DIS], [GEN]] However, I think the proposed architecture in this paper is less motivated.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Why is GLU chosen? Why is dilation used?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] According to Table 4, dilation is really not worth that much and GLU seems to be significantly better than ReLU, but why?[[MET-NEU], [CMP-NEU], [QSN], [MAJ]]\n\nThe architecture search (Table 3 and Figure 4) seems to quite arbitrary.[[MET-NEG,TNF-NEG], [EMP-NEG], [CRT], [MAJ]] I  would like to see more careful architecture search and ablation studies.[[MET-NEU,ANA-NEU], [SUB-NEU], [DIS], [MIN]] Also, why is Conv DrQA significantly worse than DrQA while Conv BiDAF can be comparable to BiDAF?[[MET-NEU], [CMP-NEU], [QSN], [MIN]]\n\nI would like to see more explanations of Figure 4.[[TNF-NEU], [SUB-NEU], [DIS], [MIN]] How important is # of layers and residual connections?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nMinor:\n- It\u2019d be helpful to add the formulation of gated linear units and residual layers.[[MET-NEU], [EMP-NEU], [QSN], [MIN]] \n- It is necessary to put Table 5 in the main paper instead of Appendix.[[TNF-NEU], [PNF-NEU], [SUG], [MIN]] These are still the main results of the paper."[[RES-NEU,TNF-NEU], [null], [DIS], [MIN]]