"The authors are motivated by two problems: Inputting non-Euclidean data (such as graphs) into deep CNNs, and analyzing optimization properties of deep networks.[[PDI-NEU], [null], [SMY], [GEN]]  In particular, they look at the problem of maze testing, where, given a grid of black and white pixels, the goal is to answer whether there is a path from a designated starting point to an ending point.[[PDI-NEU], [null], [SMY], [GEN]] \n\nThey choose to analyze mazes because they have many nice statistical properties from percolation theory.[[ANA-NEU], [null], [SMY], [GEN]] For one, the problem is solvable with breadth first search in O(L^2) time, for an L x L maze.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] They show that a CNN can essentially encode a BFS, so theoretically a CNN should be able to solve the problem.[[MET-NEU], [null], [SMY], [GEN]] Their architecture is a deep feedforward network where each layer takes as input two images: one corresponding to the original maze (a skip connection), and the output of the previous layer.[[MET-NEU], [null], [SMY], [GEN]] Layers alternate between convolutional and sigmoidal.[[MET-NEU], [null], [DIS], [MIN]] The authors discuss how this architecture can solve the problem exactly.[[MET-NEU], [null], [SMY], [GEN]] The pictorial explanation for how the CNN can mimic BFS is interesting[[MET-POS,TNF-POS], [EMP-POS], [APC], [MAJ]] but I got a little lost in the 3 cases on page 4.[[CNT], [null], [CRT], [GEN]] For example, what is r?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] And what is the relation of the black/white and orange squares?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] I thought this could use a little more clarity. [[MET-NEG], [CLA-NEG], [SUG], [MIN]]\n\nThough experiments, they show that there are two kinds of minima, depending on whether we allow negative initializations in the convolution kernels.[[EXP-NEU], [null], [SMY], [GEN]] When positive initializations are enforced, the network can more or less mimic the BFS behavior, but never when initializations can be negative.[[EXP-NEU], [null], [DIS], [GEN]] They offer a rigorous analysis into the behavior of optimization in each of these cases, concluding that there is an essential singularity in the cost function around the exact solution, yet learning succumbs to poor optima due to poor initial predictions in training.[[EXP-NEG,ANA-NEG], [EMP-NEG], [CRT], [MIN]] \n\nI thought this was an impressive paper that looked at theoretical properties of CNNs.[[MET-NEU,OAL-NEU], [null], [DIS], [MAJ]] The problem was very well-motivated, and the analysis was sharp and offered interesting insights into the problem of maze solving.[[PDI-POS,ANA-POS], [EMP-POS], [APC], [MAJ]] What I thought was especially interesting is how their analysis can be extended to other graph problems; while their analysis was specific to the problem of maze solving, they offer an approach -- e.g. that of finding \"bugs\" when dealing with graph objects -- that can extend to other problems.[[MET-NEU,ANA-NEU], [SUB-NEU], [DIS], [MIN]] I would be excited to see similar analysis of other toy problems involving graphs.[[ANA-NEU,TNF-NEU], [SUB-NEU,EMP-NEU], [SUG], [MIN]]\n\nOne complaint I had was inconsistent clarity: while a lot was well-motivated and straightforward to understand, I got lost in some of the details (as an example, the figure on page 4 did not initially make much sense to me).[[TNF-NEG], [CLA-nEG,PNF-NEG], [CRT], [MIN]] Also, in the experiments, the authors mention multiple attempt with the same settings -- are these experiments differentiated only by their initialization?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]] Finally, there were various typos throughout (one example is \"neglect minimua\" on page 2 should be \"neglect minima\").[[OAL-NEG], [CLA-NEG], [CRT], [MIN]]\n\nPros: Rigorous analysis, well motivated problem, generalizable results to deep learning theory[[RES-POS,ANA-POS], [EMP-POS], [APC], [MAJ]]\nCons: Clarity "[[OAL-NEG], [CLA-NEG], [CRT], [MIN]]