"# Summary of paper\nThe authors propose a parallel algorithm for training deep neural networks. Unlike other parallel variants of SGD, this parallelizes across the layers and not across samples.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]]\n\n# Summary of review\nThe idea of model-parallelism (as opposed to data parallelism) is appealing and an important open problem.[[PDI-POS], [EMP-POS], [APC], [MAJ]] However, this contribution is far from correctly addressing the problem.[[OAL-NEG], [EMP-NEG], [CRT], [MAJ]] The Algorithm is poorly described and crucial parts of the algorithm are very confusing.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Mathematical rigor in the proof and discussion is lacking. Proof has mathematical errors.[[MET-NEG], [SUB-NEG], [DFT], [MAJ]] \n\n# Detailed comments\n* Key definitions are scattered across the paper, making it very difficult to understand and forcing the reader to continuously go back and forth looking for the definition of  a variable.[[OAL-NEG], [PNF-NEG], [CRT], [MIN]] To make things worse, some variables are simply not defined.[[OAL-NEG], [PNF-NEG], [CRT], [MAJ]] For example, I can't find the definition of D.[[CNT], [PNF-NEG], [CRT], [MIN]] From the context it seems to be the number of layers in the network (I shouldn't need to guess).[[MET-NEU], [null], [DIS], [GEN]] \n\n* From Algorithm 1, the bracket notation is used for both indexing and specifying the size of the variables?[[MET-NEG], [PNF-NEG], [QSN], [MIN]] This is nonstandard and confusing.[[MET-NEG], [PNF-NEG], [CRT], [MIN]]\n\n* Again, from Algorithm 1, it is not clear which parts can be performed asynchronously.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] It is even not clear to me if the algorithm can be run asynchronously (as some of the other reviewers seem to imply) or if its a synchronous algorithm but analyzed asynchronously to accomodate for delay in the information coming from their \"continuous-propagation\" factorization?[[MET-NEG], [EMP-NEG], [QSN,CRT], [MAJ]]  \n\n* Eq. (3) and (4): I doubt this is true without some assumptions on the distribution of the data generating process.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]  \n\n* The proof, despite being a trivial application of existing work, has obvious flaws.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]  After equation (17) it is stated that \"the left-hand side is independent of x_{k, m, l}\" which is not true since Theta_{k+1} is computed **precisely** using x_{k, m, l} and so is not independent (this is actually done correctly in Lian 2015, where the expectation is correctly carried on that term). [[MET-NEG], [EMP-NEG], [CRT], [MIN]] \n\n* The proof relies on an inequality (16) in which key quantities are not defined (what is L? is L = L_d?) and which is impossible to verify in practice (T is not known).[[MET-NEG], [EMP-NEG], [CRT], [MIN]]  This crucial detail is only mentioned in the appendix, giving the impression in the main text that the algorithm is always convergent.[[CNT], [PNF-NEG], [CRT], [MIN]]  It should clearly be stated in the main text that convergence depends on a step-size that needs to be defined from unknown quantities.[[CNT], [PNF-NEG], [CRT], [MIN]]\n\n* As mentioned in the other reviews, key references are lacking, e.g., for ODE interpretation, Eq. (3) and (4).[[BIB-NEG], [SUB-NEG], [CRT], [MIN]]\n\nIn appendix:\n\n * Assumption 3, 4: Why is upper superindex d?[[MET-NEG], [EMP-NEG], [QSN], [MIN]] In any case, be consistent, most of the time these are used but then its stated \"for all Theta\" (whithout superindex)\n * Proposition: what is L? is L = L_d? \n\n\nOther\n\n  * Assumption 5: decay -> delay?\n\n"[[MET-NEG], [EMP-NEG], [QSN], [MIN]]