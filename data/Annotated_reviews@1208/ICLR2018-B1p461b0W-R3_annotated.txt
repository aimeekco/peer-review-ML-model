"The authors study the effect of label noise on classification tasks.[[PDI-NEU], [null], [SMY], [GEN]] They perform experiments of label noise in a uniform setting, structured setting as well provide some heuristics to mitigate the effect of label noise such as changing learning rate or batch size. [[PDI-NEU], [null], [SMY], [GEN]]\n\nAlthough, the observations are interesting, especially the one on MNIST where the network performs well even with correct labels slightly above chance, the overall contributions are incremental.[[DAT-POS,RES-POS], [EMP-POS], [APC], [MAJ]] Most of the observations of label noise such as training with structured noise, importance of larger datasets have already been archived in prior work such as in Sukhbataar et.al. (2014) and Van Horn et. al (2015).[[RWK-NEU], [null], [SMY], [GEN]] Agreed that the authors do a more detailed study on simple MNIST classification, but these insights are not transferable to more challenging domains.[[DAT-NEG,RES-NEG], [EMP-NEG], [CRT], [MIN]] \n\nThe main limitation of the paper is proposing a principled way to mitigate noise as done in Sukhbataar et.al. (2014), or an actionable trade-off between data acquisition and training schedules.[[RWK-NEG,MET-NEG], [CMP-NEG,EMP-NEG], [CRT], [MAJ]] \n\nThe authors contend that the way they deal with noise (keeping number of training samples constant) is different from previous setting which use label flips.[[RWK-NEU,MET-NEU], [EMP-NEU], [DIS], [MAJ]] However, the previous settings can be reinterpreted in the authors setting.[[RWK-NEU], [EMP-NEU], [DIS], [MIN]] I found the formulation of the \\alpha to be non-intuitive and confusing at times.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] The graphs plot number of noisy labels per clean label so a alpha of 100 would imply 1 right label and 100 noisy labels for total 101 labels.[[TNF-NEU], [CNT], [DIS], [MIN]] In fact, this depends on the task at hand (for MNIST it is 11 clean labels for 101 labels).[[DAT-NEU], [null], [DIS], [GEN]] This can be improved to help readers understand better.[[MET-NEU], [EMP-NEU], [SUG], [MIN]] \n\nThere are several unanswered questions as to how this observation transfers to a semi-supervised or unsupervised setting, and also devise architectures depending on the level of expected noise in the labels.[[MET-NEG], [SUB-NEG], [DFT], [MIN]] \n\nOverall, I feel the paper is not up to mark and suggest the authors devote using these insights in a more actionable setting.[[OAL-NEG], [APR-NEG], [CRT], [MAJ]] \nMissing citation: \"Training Deep Neural Networks on Noisy Labels with Bootstrapping\", Reed et al.\n"[[BIB-NEG], [SUB-NEG], [DFT], [MIN]]