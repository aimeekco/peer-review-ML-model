"The authors proposed a graph neural network based architecture for learning generative models of graphs.[[INT-NEU], [null], [SMY], [GEN]] Compared with traditional learners such as LSTM, the model is better at capturing graph structures and provides a flexible solution for training with arbitrary graph data.[[MET-POS,TNF-POS], [EMP-POS], [APC], [MAJ]] The representation is clear with detailed empirical studies.[[OAL-POS], [SUB-POS,PNF-POS], [APC], [MAJ]] I support its acceptance.[[OAL-POS], [REC-POS], [FBK], [MAJ]]\n\nThe draft does need some improvements and here is my suggestions.[[OAL-NEU], [null], [SUG], [GEN]]\n1. Figure 1 could be improved using a concrete example like in Figure 6.[[TNF-NEU], [PNF-NEU], [SUG], [MIN]] If space allowed, an example of different ordering leads to the same graph will also help.[[TNF-NEU], [PNF-NEU], [SUG], [MIN]]\n\n2. More details on how node embedding vectors are initialized.[[EXP-NEU,ANA-NEU], [SUB-NEU], [SUG], [MIN]] How does different initializations affect results?[[MET-NEU,RES-NEU], [EMP-NEU], [QSN], [MIN]] Why is nodes at different stages with the same initialization problematic?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\n3. More details of how conditioning information is used, especially for the attention mechanism used later in parse tree generation.[[MET-NEU,ANA-NEU], [EMP-NEU], [SUG], [MIN]]\n\n4. The sequence ordering is important.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] While the draft avoids the issue theoretically, it does has interesting results in molecule generation experiment.[[EXP-POS,RES-POS], [EMP-POS], [APC], [MAJ]] I suggest the authors at least discuss the empirical over-fitting problem with respect to ordering.[[OAL-POS], [SUB-POS,EMP-POS], [APC], [MAJ]]\n\n5. In Section 4.1, the choice of ER random graph as a baseline is too simplistic.[[RWK-NEG], [CMP-NEG], [CRT], [MIN]] It does not provide a meaningful comparison.[[RWK-NEG], [EMP-NEG], [CRT], [MAJ]] A better generative model for cycles and trees could help.[[MET-NEU], [EMP-NEU], [SUG], [MIN]]\n\n6. When comparing training curves with LSTM, it might be helpful to also include the complexity comparison of each iteration."[[MET-NEU], [SUB-NEU], [SUG], [MIN]]