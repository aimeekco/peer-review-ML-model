"This paper introduces the LR-Net, which uses the reparametrization trick inspired by a similar component in VAE.[[INT-NEU], [null], [SMY], [GEN]] Although the idea of reparametrization itself is not new, applying that for the purpose of training a binary or ternary network, and sample the pre-activations instead of weights is novel.[[PDI-NEG,MET-NEU], [NOV-POS], [SMY], [MAJ]]  From the experiments, we can see that the proposed method is effective. [[EXP-NEU,MET-POS], [EMP-NEU], [APC], [MAJ]]\n\nIt seems that there could be more things to show in the experiments part.[[EXP-NEU], [SUB-NEG], [DIS], [MAJ]] For example, since it is using a multinomial distribution for weights, it makes sense to see the entropy w.r.t. training epochs.[[EXP-NEU], [EMP-NEU], [SUG], [MAJ]] Also, since the reparametrization is based on the Lyapunov Central Limit Theorem, which assumes statistical independence, a visualization of at least the correlation between the pre-activation of each layer would be more informative than showing the histogram.[[RWK-NEU,EXP-NEU,TNF-NEU], [PNF-NEU], [SUG], [MAJ]] \n\nAlso, in the literature of low precision networks, people are concerning both training time and test time computation demand.[[RWK-NEU], [CMP-NEU], [SMY], [MAJ]] Since you are sampling the pre-activations instead of weights, I guess this approach is also able to reduce training time complexity by an order.[[RWK-NEU,MET-POS], [CMP-POS], [SUG], [MAJ]] Thus a calculation of train/test time computation could highlight the advantage of this approach more boldly.[[EXP-NEU,MET-NEU], [EMP-NEU], [SUG], [MAJ]] "