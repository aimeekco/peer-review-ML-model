"This paper proposes to use reinforcement learning instead of pre-defined heuristics to determine the structure of the compressed model in the knowledge distillation process.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]]\n\nThe draft is well-written, and the method is clearly explained.[[MET-POS,OAL-POS], [CLA-POS,EMP-POS], [APC], [MAJ]] However, I have the following concerns for this draft:\n\n1. The technical contribution is not enough.[[EXP-NEG,MET-NEG], [SUB-NEG], [DFT], [MAJ]] First, the use of reinforcement learning is quite straightforward.[[MET-NEG], [SUB-NEG,EMP-NEG], [CRT], [MIN]] Second, the proposed method seems not significantly different from the architecture search method in [1][2] \u2013 their major difference seems to be the use of \u201cremove\u201d instead of \u201cadd\u201d when manipulating the parameters.[[MET-NEG,RWK-NEG], [CMP-NEG], [CRT], [MAJ]] It is unclear whether this difference is substantial, and whether the proposed method is better than the architecture search method.[[MET-NEG,RWK-NEG], [CMP-NEG], [CRT], [MAJ]]\n\n2. I also have concern with the time efficiency of the proposed method.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Reinforcement learning involves multiple rounds of knowledge distillation, and each knowledge distillation is an independent training process that requires many rounds of forward and backward propagations.[[MET-NEU,EXT-NEU], [CNT], [DIS], [GEN]] Therefore, the whole reinforcement learning process seems very time-consuming and difficult to be generalized to big models and large datasets (such as ImageNet).[[DAT-NEU,MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] It would be necessary for the authors to make direct discussions on this issue, in order to convince others that their proposed method has practical value.[[MET-NEU], [SUB-NEU], [SUG,DIS], [MAJ]]\n\n[1] Zoph, Barret, and Quoc V. Le. \"Neural architecture search with reinforcement learning.\" ICLR (2017).\n[2] Baker, Bowen, et al. \"Designing Neural Network Architectures using Reinforcement Learning.\" ICLR (2017).\n"[[RWK-NEU,BIB-NEU], [CMP-NEU], [SUG,DIS], [MIN]]