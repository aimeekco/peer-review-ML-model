"This paper proposes a new way of sampling data for updates in deep-Q networks.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The basic principle is to update Q values starting from the end of the episode in order to facility quick propagation of rewards back along the episode.[[PDI-NEU], [null], [SMY], [GEN]]\n\nThe paper is interesting, but it lacks the proper comparisons to previously published techniques.[[RWK-NEG,OAL-NEG], [SUB-NEG,CMP-NEG], [CRT], [MIN]]\n\nThe results presented by this paper shows improvement over the baseline.[[RES-POS], [CMP-POS], [APC], [MAJ]] But the Atari results is still significantly worse than the current SOTA.[[RWK-NEG,RES-NEG], [CMP-NEG], [CRT], [MAJ]]\n\nIn the non-tabular case, the authors have actually moved away from Q learning and defined an objective that is both on and off-policy.[[MET-NEU], [null], [DIS], [MIN]] Some (theoretical) analysis would be nice.[[ANA-NEG], [SUB-NEG], [SUG], [MIN]] It is hard to judge whether the objective defined in the non-tabular defines a contraction operator at all in the tabular case.[[TNF-NEG], [EMP-NEG], [CRT], [MIN]]\n\nThere has been a number of highly relevant papers.[[RWK-NEU], [null], [DIS], [MIN]] Prioritized replay, for example, could have a very similar effect to proposed approach in the tabular case.[[MET-NEU,TNF-NEU], [EMP-NEU], [DIS], [MIN]]\n\nIn the non-tabular case, the Retrace algorithm, tree backup, Watkin's Q learning all bear significant resemblance to the proposed method.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MIN]] Although the proposed algorithm is different from all 3, the authors should still have compared to at least one of them as a baseline.[[RWK-NEG,MET-NEG], [CMP-NEG], [CRT], [MIN]] The Retrace algorithm specifically has also been shown to help significantly in the Atari case, and it defines a convergent update rule."[[MET-NEU], [null], [DIS], [MIN]]