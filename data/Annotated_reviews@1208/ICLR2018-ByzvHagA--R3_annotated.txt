"This paper presents a regularization mechanism which penalizes covariance between all dimensions in the latent representation of a neural network.[[INT-NEU], [null], [SMY], [GEN]] This penalty is meant to disentangle the latent representation by removing shared covariance between each dimension.[[MET-NEU], [null], [SMY], [GEN]] \n\nWhile the proposed penalty is described as a novel contribution, there are multiple instances of previous work which use the same type of penalty (Cheung et. al. 2014, Cogswell et. al. 2016)[[RWK-NEU,MET-NEG,BIB-NEU], [NOV-NEG,CMP-NEG], [CRT], [MAJ]]. Like this work, Cheung et. al. 2014 propose the XCov penalty which penalizes cross-covariance to disentangle subsets of dimensions in the latent representation of autoencoder models.[[RWK-NEU,BIB-NEU], [CMP-NEU], [SUG], [MAJ]] Cogswell et. al. 2016 also proposes a similar penalty (DeCov) to this work for reducing overfitting in supervised learning.[[RWK-NEU,BIB-NEU], [CMP-NEU], [SMY], [MAJ]]\n\nThe novel contribution of the regularizer proposed in this work is that it also penalizes the variance of individual dimensions along with the cross-covariance.[[MET-POS], [NOV-POS], [SMY], [MAJ]] Intuitively, this should lead to dimensionality reduction as the model will discard variance in dimensions which are unnecessary for reconstruction.[[MET-NEU], [EMP-NEU], [SMY], [MAJ]] But given the similarity to previous work, the authors need to quantitatively evaluate the value in additionally penalizing variance of each dimension as compared with earlier work.[[EXP-NEU], [EMP-NEU], [SUG], [MAJ]] Cogswell et. al. 2016 explicitly remove these terms from their regularizer to prevent the dynamic range of the activations from being unnecessarily rescaled.[[RWK-NEU,BIB-NEU], [CMP-NEU], [SMY], [MAJ]]  It would be helpful to understand how this approach avoids this issues;[[MET-NEU], [EMP-NEU], [SUG], [MAJ]] - i.e.,  if you penalize all the variance terms then you could just be arbitrarily rescaling the activities, so what prevents this trivial solution?[[MET-NEU], [EMP-NEU], [QSN], [MAJ]]\n\nThere doesn't appear to be a definition of the L1 penalty this paper compares against and it's unclear why this is a reasonable baseline.[[RWK-NEG], [CMP-NEG], [CRT], [MAJ]] The evaluation metrics this work uses (MAPC, CVR, TdV, UD) need to be justified more in the absence of their use in previous work.[[RWK-NEU,MET-NEU], [EMP-NEU], [DFT], [MAJ]] While they evaluate their method on non-toy dataset such as CIFAR, they do not show what the actual utility of their proposed regularizer serves for such a dataset beyond having no-regularization at all.[[MET-NEU], [EMP-NEG], [CRT], [MAJ]] Again, the utility of the evaluation metrics proposed in this work is unclear.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nThe toy examples are kind of interesting but it would be more compelling if the dimensionality reduction aspect extended to real datasets.[[DAT-NEU,EXP-NEU], [SUB-NEU], [SUG], [MAJ]]\n\n> Our method has no penalty on the performance on tasks evaluated in the experiments, while it does disentangle the data[[DAT-NEU,EXP-NEU], [EMP-NEU], [SMY], [MAJ]]\n\nThis needs to be expanded in the results as all the results presented appear to show Mean Squared Error increasing when increasing the weight of the regularization penalty.[[RES-NEU,ANA-NEU], [EMP-NEU], [SUG], [MAJ]]\n"