"This article tackles the extraction of sentiments at a fine-grained level.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]]  Thus, the authors insist on context modeling to obtain a relevant analysis of a word's meaning.[[PDI-NEU], [null], [SMY], [MIN]]  The authors propose a first modeling called ASC.[[MET-NEU], [null], [SMY], [GEN]]  Identifying some weakness in the formulation, the authors propose 5 solutions.[[RES-NEU], [null], [DIS], [MIN]] \nThe authors apply their different models on a small dataset (semeval 2014), they compare basic memory network implementations with their approaches.[[DAT-NEU,MET-NEU], [null], [SMY], [GEN]] \n\n==\n\nThe authors do not put into perspective their approach.[[MET-NEU], [null], [DIS], [MIN]]  Given the literature in topics / sentiment modeling, it is a real weakness of this article.[[RWK-NEG,MET-NEG], [CMP-NEG,EMP-NEG], [CRT], [MIN]] \n\nTo improve readability, the authors should propose a diagram of the network, summarizing all notations[[TNF-NEU], [PNF-NEU], [SUG], [MIN]] \n\nDimension of c_i / o ?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]  d\n\nDefinition of u (eq. 3) => v?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nAt the beginning of section 3, the discussion about the independence of the terms in the decomposition (6) is not completely relevant: alpha terms embedded the relation between the target and the context i[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nAmong the different solutions, IT and CI are very redundant.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] A kind of matching between the context and the target is already considered in the definition of alpha -with metric learning in M- : why not using those terms instead of ai <di,dt>?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\nWe see that the authors build models that become more and more complex, but their motivation in combining attention and IT/CI is not clear: they learn the relation between context and target twice without any factorization.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nIn section 5, the authors mention briefly some works from the RNN domain & the classical use case of memory networks.[[RWK-NEU], [null], [DIS], [MIN]] They claim that:\n\"The above studies cannot be directly applied to the ASC task as they are not capable of mining finer-grained aspect dependent sentiments.[[MET-NEG,ANA-NEG], [EMP-NEG], [CRT], [MIN]]\"\nI do not agree with them: latent representations from RNN are fine-grained & context dependent; latent representations from Socher et al. are also fine-grained & target dependent: the position in the latent space -modeling context- has an impact on the estimated sentiment.[[RWK-NEG,ANA-NEG], [CMP-NEG], [CRT], [MIN]]\n\nIn the experimental section, there is no discussion about the ration between the dataset size and the number of parameters to estimate.[[DAT-NEG,EXP-NEG], [EMP-NEG], [CRT], [MIN]] However: there is a strong risk of overfitting in the current situation and we will always wonder if the given figures correspond to \"lucky trial\".[[TNF-NEU], [CNT], [DIS], [GEN]]\nIt is true that the authors use strong regularization techniques (drop out, external knowledge of words embedding...).[[MET-POS], [EMP-POS], [APC], [MAJ]] However, the validation procedure is not clear in the article.[[MET-POS], [EMP-POS], [CRT], [MAJ]]\n\nThe main weakness of the experimental section resides in the lack of comparison with classical approach in sentiment analysis: none of the state-of-the-art approaches are implemented here (RNN, basic models on W2V aggregations, ...).[[RWK-NEG,MET-NEG,ANA-NEG], [EMP-NEG], [CRT], [MAJ]] That makes  the contribution very difficult to evaluate.[[OAL-NEG], [REC-NEG], [CRT], [MAJ]]\n\nThe analysis of the results are interesting, both from the quantitative & qualitative point of view."[[RES-POS,ANA-POS], [EMP-POS], [APC], [MAJ]]