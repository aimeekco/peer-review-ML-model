"In this paper a neural-network based method for multi-frame video prediction is proposed.[[PDI-NEU], [null], [SMY], [GEN]] It builds on the previous work of [Finn et al. 2016] that uses a neural network to predict transformation parameters of an affine image transformation for future frame prediction, an idea akin to the Spatial Transformer Network paper of [Jaderberg et al., 2015].[[RWK-NEU,PDI-NEU], [null], [SMY], [GEN]] What is new compared to [Finn et al. 2016] is that the authors managed to train the network in combination with an adversarial loss, which allows for the generation of more realistic images.[[PDI-NEU], [null], [SMY], [GEN]] Time series modelling is performed via convolutional LSTMs.[[PDI-NEU], [null], [SMY], [GEN]] The authors evaluate their method based on a mechanical turk survey, where humans are asked to judge the realism of the generated images; additionally, they propose to measure prediction quality by the distance between the manually annotated positions of objects within ground truth and predicted frames.[[MET-NEU], [null], [SMY], [GEN]]\n\nMy main concerns with this paper are novelty, reproducibility and evaluation.[[MET-NEU,OAL-NEU], [NOV-NEU], [DIS], [MAJ]]\n\n* Novelty. The network design builds heavily on the work of [Finn et al., 2106].[[RWK-NEU], [NOV-NEU], [DIS], [MAJ]] A number of design decisions (such as instance normalization) seem to help yield better results,[[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]] but are minor contributions.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] A major contribution is certainly the combination with an adversarial loss, which is a non-trivial task.[[MET-POS], [EMP-POS], [APC], [MAJ]] However, the authors claim that their method is the first to combine multi-frame video prediction with an adversarial loss, which is not true.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] A recent work, presented at CVPR this year also does multi-frame prediction featuring an adversarial loss and explicitly models and captures the full dense optical flow (though in the latent space) that allows non-trivial motion extrapolation to future frames.[[MET-NEU], [null], [DIS], [MIN]]  This work is neither mentioned in the related work nor compared to.[[RWK-NEG], [CMP-NEG,SUB-NEG], [CRT], [MAJ]]  \n \nLu et al. , Flexible Spatio-Temporal Networks for Video Prediction, CVPR 2017\n\nThis recent work builds on another highly relevant work, that is also not mentioned in the paper:\n\nPatraucean et al.[[RWK-NEG], [CMP-NEG,SUB-NEG], [CRT], [MAJ]] Spatio-temporal video autoencoder with differentiable memory, arxiv 2017\n\nSince this is prior state-of-the-art and directly applicable to the problem, a comparison is a must. [[RWK-NEU], [CMP-NEU], [CRT], [MAJ]] \n\n* Reproducibility and evaluation\nThe description of the network is quite superficial.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Even if the authors released their code used for training (which is not mentioned), I think the authors should aim for a more self-contained exposition.[[EXP-NEU], [EMP-NEU], [SUG], [MIN]]  I doubt that a PhD student would be able to reimplement the method and achieve comparable results given the paper at hand only.[[EXT-NEU], [null], [DIS], [GEN]]  It is also not mentioned whether the other methods that the authors compare to are re-trained on their newly proposed training dataset.[[DAT-NEU,EXP-NEG], [SUB-NEG], [DFT], [MAJ]]  Hence, it remains unclear to what extend the achieved improvements are due to the proposed network design changes or the particular dataset they use for training.[[DAT-NEU,EXP-NEU], [EMP-NEG], [CRT], [MAJ]]  The authors also don't show any results on previous datasets, which would allow for a more objective comparison to existing state of the art.[[DAT-NEU,RWK-NEG,RES-NEG], [SUB-NEG,EMP-NEG], [DFT,CRT], [MAJ]]  Another point of criticism is the way the Amazon Mechanical Turk evaluation was performed.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]]  Since only individual images were shown, the evaluation mainly measures the quality of the generated images.[[EXP-NEU,MET-NEU], [EMP-NEU], [CRT], [MAJ]]  Since the authors combine their method with a GAN, it is not surprising that the generated images look more realistic.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]]  However, since the task is *video* prediction, it seems more natural to show small video snippets rather than individual images, which would also evaluate temporal consistency.[[RES-NEU], [EMP-NEU], [CRT], [MAJ]] \n\n* Further comments:\nThe paper contains a number of broken sentences, typos and requires a considerable amount of polishing prior to publication.\n"[[OAL-NEG], [EMP-NEG], [CRT], [MAJ]] 