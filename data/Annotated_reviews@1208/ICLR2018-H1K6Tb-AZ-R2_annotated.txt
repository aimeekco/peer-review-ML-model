"This paper presents a modification of a numeric solution: Incomplete Dot Product (IDP), that allows a trained network to be used under different hardware constraints.[[INT-NEU], [null], [SMY], [GEN]] The IDP method works by incorporating a 'coefficient' to each layer (fully connected or convolution), which can be learned as the weights of the model are being optimized.[[MET-NEU], [null], [SMY], [GEN]] These coefficients can be used to prune subsets of the nodes or filters, when hardware has limited computational capacity. [[DAT-POS,MET-POS], [EMP-POS], [SMY], [MAJ]]\n\nThe original IDP method (cited in the paper) is based on iteratively training for higher hardware capacities.[[RWK-NEU], [NULL], [SMY], [GEN]] This paper improves upon the limitation of the original IDP by allowing the weights of the network be trained concurrently with these coefficients, and authors present a loss function that is linear combination of loss function under original or constrained network setting. [[MET-POS], [CMP-POS], [APC], [MAJ]]They also present results for a 'harmonic' combination which was not explained in the paper at all.[[MET-NEG,RES-NEU], [SUB-NEG], [DFT], [MIN]]\n\nOverall the paper has very good motivation and significance.[[OAL-POS], [IMP-POS], [APC], [MAJ]] \nHowever the writing is not very clear and the paper is not self-contained at all.[[OAL-NEG], [CLA-NEG], [DFT], [MAJ]] I was not able to understand the significance of early stopping and how this connects with loss aggregation, and how the learning process differs from the original IDP paper, if they also have a scheduled learning setting.[[RWK-NEU,MET-NEU], [CLA-NEG,CMP-NEG], [DFT], [MIN]] \n\nAdditionally, there were several terms that were unexplained in this paper such as 'harmonic' method highlighted in Figure  3.[[TNF-NEG], [CLA-NEG], [DFT], [MIN]] As is, while results are promising,[[RES-POS], [null], [APC], [MAJ]] I can't fully assess that the paper has major contributions.  "[[OAL-NEG], [IMP-NEG], [DFT], [MAJ]]