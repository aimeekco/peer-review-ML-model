"EDIT: The rating has been changed. See thread below for explanation / further comments.[[EXT-NEU], [null], [DIS], [GEN]]\n\nORIGINAL REVIEW: In this paper, the authors present a new training strategy, VAN, for training very deep feed-forward networks without skip connections (henceforth called VDFFNWSC) by introducing skip connections early in training and then gradually removing them.[[PDI-NEU], [null], [SMY], [GEN]] \n\nI think the fact that the authors demonstrate the viability of training VDFFNWSCs that could have, in principle, arbitrary nonlinearities and normalization layers, is somewhat valuable and as such I would generally be inclined towards acceptance,[[EXP-POS,MET-POS], [REC-POS], [APC,FBK], [MAJ]] even though the potential impact of this paper is limited because the training strategy proposed is (by deep learning standards) relatively complicated, requires tuning two additional hyperparameters in the initial value of \\lambda as well as the step size for updating \\lambda, and seems to have no significant advantage over just using skip connections throughout training.[[EXP-NEG], [IMP-NEG], [CRT], [MAJ]] So my rating based on the message of the paper would be 6/10.[[OAL-NEU], [REC-NEU], [FBK], [MAJ]]\n\nHowever, there appear to be a range of issues.[[OAL-NEG], [null], [DIS], [GEN]] As long as those issues remain unresolved, my rating is at is but if those issues were resolved it could go up to a 6.[[OAL-NEU], [REC-NEU], [FBK], [MAJ]]\n\n+++ Section 3.1 problems +++\n\n- I think the toy example presented in section 3.1 is more confusing than it is helpful because the skip connection you introduce in the toy example is different from the skip connection you introduce in VANs.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] In the toy example, you add (1 - \\alpha)wx whereas in the VANs you add (1 - \\alpha)x.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Therefore, the type of vanishing gradient that is observed when tanh saturates, which you combat in the toy model, is not actually combated at all in the VAN model.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] While it is true that skip connections combat vanishing gradients in certain situations, your example does not capture how this is achieved in VANs.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n- The toy example seems to be an example where Lagrangian relaxation fails, not where it succeeds.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Looking at figure 1, it appears that you start out with some alpha < 1 but then immediately alpha converges to 1, i.e. the skip connection is eliminated early in training, because wx is further away from y than tanh(wx).[[EXP-NEG,TNF-NEG], [EMP-NEG], [CRT], [MIN]] Most of the training takes place without the skip connection.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]] In fact, after 10^4 iterations, training with and without skip connection seem to achieve the same error.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] It appears that introducing the skip connection was next to useless and the model failed to recognize the usefulness of the skip connection early in training.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n- Regarding the optimization algorithm involving \\alpha^* at the end of section 3: It looks to me like a hacky, unprincipled method with no guarantees that just happened to work in the particular example you studied.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] You motivate the choice of \\alpha^* by wanting to maximize the reduction in the local linear approximation to \\mathcal{C} induced by the update on w. [[MET-NEU], [EMP-NEU], [DIS], [MIN]]However, this reduction grows to infinity the larger the update is.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Does that mean that larger updates are always better?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Clearly not.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] If we wanted to reduce the size of the objective according to the local linear approximation, why wouldn't we choose infinitely large step sizes?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Hence, the motivation for the algorithm you present is invalid.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Here is an example where this algorithm fails: consider the point (x,y,w,\\alpha,\\lambda) = (100, \\sigma(100), 1.0001, 1, 1).[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Here, w has almost converged to its optimum w* = 1.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Correspondingly, the derivative of C is a small negative value.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] However, \\alpha* is actually 0, and this choice would catapult w far away from w*.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\nIf I haven't made a mistake in my criticisms above, I strongly suggest removing section 3.1 entirely or replacing it with a completely new example that does not suffer from the above issues.[[CNT], [EMP-NEG], [CRT], [MIN]]\n\n+++ ResNet scaling +++\n\nThere is a crucial difference between VANs and ResNets.[[DAT-NEU], [null], [DIS], [MIN]] In the VAN initial state (alpha = 0.5), both the residual path and the skip path are multiplied by 0.5 whereas for ResNet, neither is multiplied by 0.5.[[DAT-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] Because of this, the experimental results between the two architectures are incomparable.[[RES-NEG], [CMP-NEG], [CRT], [MIN]]\n\nIn a question I posed earlier, you claimed that this scaling makes no difference when batch normalization is used.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] I disagree. Let's look at an example. Consider ResNet first.[[DAT-NEU], [null], [DIS], [MIN]] It can be written as x + r_1 + r_2 + .. + r_B, where r_b is the value computed by residual block b.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Now let's assume we insert a scaling constant after each residual block, say c = 0.5.[[MET-NEU], [null], [DIS], [MIN]] Then the result is c^{B}x + c^{B-1}r_1 + c^{B-2}r_2 + .. + r_B.[[RES-NEU], [null], [DIS], [MIN]] Therefore, contributions of lower blocks vanish exponentially.[[RES-NEU], [null], [DIS], [MIN]] This effect is not combated by batch normalization.[[MET-NEU], [null], [DIS], [MIN]]\n\nSo the learning dynamics for VAN and ResNet are very different because of this scaling. [[MET-NEU], [null], [DIS], [MIN]]Therefore, there is an open question: are the differences in results between VAN and ResNet in your experiments caused by the removal of skip connections during training or by this scaling?[[EXP-NEU,RES-NEU], [EMP-NEU], [DIS], [MIN]] Without this information, the experiments have limited value.[[EXP-NEG], [EMP-NEG], [CRT], [MIN]] In fact, I suspect that the vanishing of the contribution of lower blocks bears more responsibility for the declining performance of VAN at higher depths than the removal of skip connections.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\nIf my assessment of the situation is correct, I would like to ask you to repeat your experiments with the following two settings: \n\n- ResNet where after each block you multiply the result of the addition by 0.5, i.e. x_{l+1} = 0.5\\mathcal{F}(x_l) + 0.5x_l[[MET-NEU,RES-NEU], [EMP-NEU], [DIS], [MIN]]\n- VAN with the following altered equation: x_{l+1} = \\mathcal{F}(x_l) + (1-\\alpha)x_l, i.e. please remove the alpha in front of \\mathcal{F}.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Also, initialize \\alpha to zero.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] This ensures that VAN starts out as a regular ResNet.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\n+++ writing issues +++\n\nTitle:\n\n- \"VARIABLE ACTIVATION NETWORKS: A SIMPLE METHOD TO TRAIN DEEP FEED-FORWARD NETWORKS WITHOUT SKIP-CONNECTIONS\" This title can be read in two different ways.[[INT-NEG], [CLA-NEG], [CRT], [MIN]] (A) [Train] [deep feed-forward networks] [without skip-connections] and (B) [Train] [deep feed-forward networks without skip connections]. In (A), the `without skip-connections' modifies the `train' and suggests that training took place without skip connections.[[INT-NEG], [CLA-NEG], [CRT], [MIN]] In (B), the `without skip-connections' modifies `deep feed-forward networks' and suggests that the network trained has no skip connections. You must mean (B), because (A) is false.[[INT-NEG], [CLA-NEG], [CRT], [MIN]] Since it is not clear from reading the title whether (A) or (B) is true, please reword it.[[INT-NEG], [CLA-NEG], [CRT], [MIN]]\n\nAbstract:\n\n- \"Part of the success of ResNets has been attributed to improvements in the conditioning of the optimization problem (e.g., avoiding vanishing and shattered gradients).[[ABS-NEG], [CLA-NEG], [CRT], [MIN]] In this work we propose a simple method to extend these benefits to the context of deep networks without skip-connections.[[MET-NEU], [null], [DIS], [MIN]]\" Again, this is ambiguous.[[CNT], [null], [DIS], [MIN]] To me, this sentence implies that you extend the benefit of avoiding vanishing and exploding gradients to fully-connected networks without skip connections.[[CNT], [null], [DIS], [MIN]] However, nowhere in your paper do you show that trained VANs have less exploding / vanishing gradients than fully-connected networks trained the old-fashioned way. Again, please reword or include evidence.\n- \"where the proposed method is shown to outperform many architectures without skip-connections\" Again, this sentence makes no sense to me.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] It seems to imply that VAN has skip connections.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] But in the abstract you defined VAN as an architecture without skip connections.[[ABS-NEG], [EMP-NEG], [CRT], [MIN]]  Please make this more clear.[[ABS-NEG], [EMP-NEG], [CRT], [MIN]] \n\nIntroduction:\n- \"Indeed, Zagoruyko & Komodakis (2016) demonstrate that it is better to increase the width of ResNets than the depth, suggesting that perhaps only a few layers are learning useful representations.\"[[INT-NEU,RWK-NEU], [null], [DIS], [GEN]]  Just because increasing width may be better than increasing depth does not mean that deep layers don't learn useful representations.[[INT-NEU,RWK-NEU], [null], [DIS], [GEN]] In fact, the claim that deep layers don't learn useful representations is directly contradicted by the paper.[[INT-NEG], [CNT], [CRT], [MIN]]\n\nsection 3.1:\n- replace \"to to\" by \"to\" in the second line[[CNT], [CLA-NEG], [SUG], [MIN]]\n\nsection 4:\n- \"This may be a result of the ensemble nature of ResNets (Veit et al., 2016), which does not play a significant role until the depth of the network increases.[[CNT], [CLA-NEG], [CRT], [MIN]]\" The ensemble nature of ResNet is a drawback, not an advantage, because it causes a lack of high-order co-adaptataion of layers.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Therefore, it cannot contribute positively to the performance or ResNet.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n\nAs mentioned in earlier comments, please reword / clarify your use of \"activation function\".[[MET-NEU], [CLA-NEU], [DIS], [MIN]] It is generally used a synonym for \"nonlinearity\", so please use it in this way.[[MET-NEU], [CLA-NEU], [SUG], [MIN]] Change your claim that VAN is equivalent to PReLU.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Please include your description of how your method can be extended to networks which do allow for skip connections.[[MET-NEU,ANA-NEU], [SUB-NEU], [DIS], [MIN]]\n\n+++ Hyperparameters +++\n\nSince the initial values of \\lambda and \\eta' are new hyperparameters, include the values you chose for them, explain how you arrived at those values and plot the curve of how \\lambda evolves for at least some of the experiments."[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MAJ]]