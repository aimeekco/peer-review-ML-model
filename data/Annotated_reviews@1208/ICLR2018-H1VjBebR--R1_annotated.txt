"The paper addresses the problem of learning mappings between different domains without any supervision.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] It belongs to the recent family of papers based on GANs[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]].\nThe paper states three conjectures (predictions in the paper):[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]]\n1. GAN are sufficient to learn \u00ab\u00a0semantic mappings\u00a0\u00bb in an unsupervised way, if the considered networks are small enough\n2. Controlling the complexity of the network, i.e. the number of the layers, is crucial to come up with what is called \u00ab\u00a0semantic\u00a0\u00bb mappings when learning in an unsupervised way.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]]\nMore precisely there is tradeoff to achieve between the complexity of the model and its simplicity.[[RWK-NEU,MET-NEU], [null], [SMY], [GEN]] A rich model is required in order to minimize the discrepancy between the distributions of the domains, while a  not too complex model is necessary to avoid mappings that are not \u00ab\u00a0meaningful\u00a0\u00bb.\n To this aim, the authors  introduce a new notion of function complexity which can be seen as a proxy of Kolmogorov complexity.[[RWK-NEU,MET-NEU], [null], [SMY], [GEN]]The introduced notion is very simple and intuitive and is defined as  the depth of a network  which is necessary to  implement the considered function.[[RWK-NEU,MET-NEU], [null], [SMY], [GEN]] \nBased on this definition, and assuming identifiability (i.e. uniqueness up to invariants), and for networks with Leaky ReLU activations,  the authors prove that if the number of mappings which preserve a degree of discrepancy (density preserving in the text) is small, then the  set of \u00ab\u00a0minimal\u00a0\u00bb mappings  of complexity C   that achieve the same degree of  discrepancy is also small. [[RWK-NEU,ANA-NEU],  [null], [DIS], [GEN]]\nThis result is related to the third conjecture of the paper that is :\n3. the number of the number of mappings which preserve a degree of discrepancy  is small.[[RES-NEU], [SUB-NEG], [CRT], [MIN]]\n\nThe authors also prove a byproduct result stating that identifiability holds for Leaky ReLU networks with one hidden layer.[[RWK-NEU,ANA-NEU], [EMP-NEG], [CRT], [MIN]]\n\nThe paper  comes with a series of experiments to empirically \u00ab\u00a0demonstrate\u00a0\u00bb the conjectures. [[EXP-NEU],  [null], [SMY], [MIN]]\n\nThe paper is well written. [[OAL-POS], [CLA-POS], [APC], [MAJ]] The different ideas are clearly stated and discussed, and hence open interesting questions and debates[[PDI-POS], [EMP-POS], [APC], [MAJ]]\n\nSome of these questions that need to be addressed IMHO:\n\n- A critical general question: if the addressed problem is the alignment between e.g. images and not image generation, why not formalizing the problem as a similarity search one (using e.g. EMD or any other transport metric).[[PDI-POS], [EMP-POS], [APC], [MAJ]]The alignment task  hence reduces to computing a ranking from this similarity.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] I have the impression that we use a jackhammer to break a small brick here (no offence). But maybe that I\u2019m missing something here.\n- Several works consider the size and the depth of the network as hyper-parameters to optimize, and this is not new.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] What is the actual contribution of the paper w.r.t. to this body of work?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n- It is considered that the GAN are trained without any problem, and therefore work in an optimal regime[[RWK-NEU,ANA-NEU], [CMP-NEU], [DIS], [GEN]]. But the training of the GAN is in itself a problem.[[RWK-NEU,ANA-NEU], [EMP-NEU], [DIS], [MIN]] How does this affect the paper statements and results?\n-[[RES-NEU], [EMP-NEU], [QSN], [MIN]] Are the results still valid for another measure of discrepancy based for instance on another measure, e.g. Wasserstein?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\n\nSome minor remarks :\n- p3: the following sentence is not clear  \u00ab\u00a0 Our hypothesis is that the lowest complexity small discrepancy mapping approximates the alignment of the target semantic function.[[MET-NEG], [CLA-NEG], [CRT], [MIN]]\u00a0\u00bb\n- p6: $C^{\\epsilon_0}_{A,B}$ is used (after Def. 2) before being defined. \n- p7: build->built\n\nSection II :\nA diagram explaining  the different mappings (h_A, h_B, h_AB, etc.) and their spaces (D_A, D_B, D_Z) would greatly help the understanding.[[MET-NEU], [CLA-NEG], [QSN], [MIN]]\n\nPapers 's pros :\n- clarity\n- technical results[[RES-POS], [CLA-POS,EMP-POS], [APC], [MAJ]]\n\ncons:\n- doubts about the interest and originality\n\n\[[OAL-NEG], [NOV-NEG], [CRT], [MIN]]nThe authors provided detailed and convincing answers to my questions.[[EXT-NEU], [null], [DIS], [MIN]] I thank them for that.[[EXT-NEU], [null], [DIS], [MIN]]  My scores were changed accrodingly.\n"[[OAL-NEU], [REC-NEU], [FBK], [MAJ]]