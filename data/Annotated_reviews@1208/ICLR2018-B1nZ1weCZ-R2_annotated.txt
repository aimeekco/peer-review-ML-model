"\nThe authors show empirically that formulating multitask RL itself as an active learning and ultimately as an RL problem can be very fruitful.[[PDI-NEU], [null], [SMY], [GEN]]  They design and explore several approaches  to the active learning (or active sampling) problem, from a basic \nchange to the distribution to UCB to feature-based neural-network based RL.[[PDI-NEU], [null], [SMY], [GEN]]  The domain is video games.[[PDI-NEU], [null], [SMY], [GEN]]    All proposed approaches beat the uniform sampling baselines and the more sophisticated approaches do better in the scenarios with more tasks (one multitask  problem had 21 tasks).[[RWK-POS,MET-POS], [CMP-POS,EMP-POS], [APC], [MAJ]] \n\n\nPros:\n\n- very promising results with an interesting active learning approach to multitask RL[[RES-POS], [EMP-POS], [APC], [MAJ]]\n\n- a number of approaches developed for the basic idea[[MET-POS], [CMP-POS,EMP-POS], [APC], [MAJ]]\n\n- a variety of experiments, on challenging multiple task problems (up to 21 tasks/games)[[EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]]\n\n- paper is overall well written/clear[[OAL-POS], [CLA-POS], [APC], [MAJ]]\n\nCons:\n\n- Comparison only to a very basic baseline (i.e. uniform sampling)[[RWK-NEG], [CMP-NEG], [CRT], [MIN]]\nCouldn't comparisons be made, in some way, to other multitask work?[[RWK-NEU], [CMP-NEU], [QSN], [MIN]]\n\n\n\nAdditional  comments:\n\n- The assumption of the availability of a target score goes against\nthe motivation that one need not learn individual networks[[MET-NEG], [CMP-NEG], [CRT], [MIN]] ..  authors\nsay instead one can use 'published' scores, but that only assumes\nsomeone else has done the work (and furthermore, published it!).[[MET-NEU], [EMP-NEU], [SUG], [MIN]]\n\nThe authors do have a section on eliminating the need by doubling an\nestimate for each task) which makes this work more acceptable (shown\nfor 6 tasks or MT1, compared to baseline uniform sampling).[[MET-POS], [EMP-POS], [APC], [MAJ]]\n\nClearly there is more to be done here for a future direction (could be\nmentioned in future work section).[[FWK-NEG], [IMP-NEG], [CRT], [MAJ]]\n\n- The averaging metrics (geometric, harmonic vs arithmetic, whether\n  or not to clip max score achieved) are somewhat interesting, but in\n  the main paper, I think they are only used in section 6 (seems like\n  a waste of space).[[MET-POS], [EMP-POS], [APC], [MAJ]] Consider moving some of the results, on showing\n  drawbacks of arithmetic mean with no clipping (table 5 in appendix E), from the appendix to\n  the main paper.[[RES-NEG], [EMP-NEG], [CRT], [MIN]]\n\n\n- The can be several benefits to multitask learning, in particular\n  time and/or space savings in learning new tasks via learning more\n  general features.[[EXT-NEU], [null], [DIS], [GEN]] Sections 7.2 and 7.3 on specificity/generality of\n  features were interesting.[[CNT], [EMP-POS], [APC], [MAJ]]\n\n\n\n--> Can the authors show that a trained network (via their multitask\n    approached) learns significantly faster on a brand new game    (that's similar to games already trained on), compared to learning from\n    scratch?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]]\n\n--> How does the performance improve/degrade (or the variance), on the\n    same set of tasks, if the different multitask instances (MT_i)\n    formed a supersets hierarchy, ie if MT_2 contained all the\n    tasks/games in MT_1, could training on MT_2 help average\n    performance on the games in MT_1 ? [[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]]Could go either way since the network\n   has to allocate resources to learn other games too.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]  But is there a pattern?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\n\n\n- 'Figure 7.2' in section 7.2 refers to Figure 5.[[TNF-NEU], [PNF-NEU], [DIS], [MIN]]\n\n\n- Can you motivate/discuss better why not providing the identity of a\n  game as an input is an advantage?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Why not explore both\n  possibilities?[[CNT], [EMP-NEU], [QSN], [MIN]] what are the pros/cons? (section 3)\n\n\n\n\n"[[CNT], [null], [QSN], [MIN]]