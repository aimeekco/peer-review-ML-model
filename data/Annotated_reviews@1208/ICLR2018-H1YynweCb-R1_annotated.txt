"The paper presents a method to parametrize unitary matrices in an RNN as a Kronecker product of smaller matrices.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] Given N inputs and output, this method allows one to specify a linear transformation with O(log(N)) parameters, and perform a forward and backward pass in O(Nlog(N)) time.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]]  \nIn addition a relaxation is performed allowing each constituent to deviate a bit from unitarity (\u201csoft unitary constraint\u201d).[[PDI-NEU,ANA-NEU], [null], [SMY], [GEN]]\nThe paper shows nice results on a number of small tasks.[[RES-POS], [EMP-POS], [APC], [MAJ]] \n\nThe idea is original to the best of my knowledge and is presented clearly.[[PDI-POS], [CLA-POS,NOV-POS], [APC], [MAJ]]\nI especially like the idea of \u201csoft unitary constraint\u201d which can be applied very efficiently in this factorized setup.[[MET-POS], [EMP-POS], [APC], [MAJ]] I think this is the main contribution of this work.[[MET-POS], [null], [APC], [MAJ]]\n\nHowever the paper in its current form has a number of problems:\n\n- The authors state that a major shortcoming of previous (efficient) unitary RNN methods is the lack of ability to span the entire space of unitary matrices.[[RWK-NEU,MET-NEU], [null], [SMY], [GEN]] This method presents a family that can span the entire space, but the efficient parts of this family (which give the promised speedup) only span a tiny fraction of it, as they require only O(log(N)) params to specify an O(N^2) unitary matrix.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] Indeed in the experimental section only those members are tested.[[EXP-NEG], [SUB-NEG], [DFT], [MIN]]\n\n- Another claim that is made is that complex numbers are key, and again the argument is the need to span the entire space of unitary matrices, but the same comment still hold - that is not the space this work is really dealing with, and no experimental evidence is provided that using complex numbers was really needed.[[EXP-NEG], [SUB-NEG], [DFT], [MIN]]\n\n- In the experimental section an emphasis is made as to how small the number of recurrent params are, but at the same time the input/output projections are very large, leaving the reader wondering if the workload simply shifted from the RNN to the projections.[[EXP-NEG], [EMP-NEG], [CRT], [MIN]] This needs to be addressed.[[EXP-NEU], [SUB-NEU], [SUG], [MIN]]\n\n- Another aspect of the previous points is that it\u2019s not clear if stacking KRU layers will work well.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]] This is important as stacking LSTMs is a common practice.[[EXP-NEU], [null], [DIS], [MIN]] Efficient KRU span a restricted subspace whose elements might not compose into structures that are expressive enough.[[MET-NEU], [null], [DIS], [MIN]]  One way to overcome this potential problem is to add projection matrices between layers that will do some mixing, but this will blow the number of parameters.[[ANA-NEU], [null], [SUG], [MIN]] This needs to be explored.[[MET-NEU], [SUB-NEU], [DIS], [MIN]]\n\n- The authors claim that the soft unitary constraint was key for the success of the network, yet no details are provided as to how this constraint was applied, and no analysis was made for its significance. \n"[[MET-NEG,ANA-NEG], [IMP-NEG,SUB-NEG], [DFT], [MAJ]]