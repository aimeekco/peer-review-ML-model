"This paper proposes to fine-tune the last layer while keeping the others fixed, after initial end-to-end training, viewing the last layer learning under the light of kernel theory (well actually it's just a linear model).[[INT-NEU], [null], [SMY], [GEN]]\n\nSummary of evaluation\n\nThere is not much novelty in this idea (of optimizing carefully only the last layer as a post-training stage or treating the last layer as kernel machine in a post-processing step), which dates back at least a decade,[[PDI-NEG], [NOV-NEG], [CRT], [MAJ]] so the only real contribution would be in the experiments.[[EXP-POS], [null], [SMY], [MAJ]] However the experimental setup is questionable as it does not look like the same care has been given to control overfitting with the 'regular training' method.[[EXP-NEG], [EMP-NEG], [DFT], [MAJ]]\n\nMore details\n\nPrevious work on the same idea: at least a decade old, e.g., Huang and LeCun 2006. See a review of such work in 'Deep Learning using Linear Support Vector Machines' more recently.[[RWK-NEG], [CMP-NEG], [DFT], [MAJ]]\n\nExperiments\n\nYou should also have a weight norm penalty in the end-to-end ('regular training') case and make sure it is appropriately and separately tuned (not necessarily the same value as for the post-training).[[EXP-NEU], [EMP-NEG], [SUG], [MIN]] Otherwise, the 'improvements' may simply be due to better regularization in one case vs the other, and the experimental curves suggest that interpretation is correct.\n"[[EXP-NEU,RES-NEU], [EMP-NEU], [SUG], [MIN]]