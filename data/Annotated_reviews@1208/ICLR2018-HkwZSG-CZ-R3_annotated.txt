"Language models are important components to many NLP tasks.[[EXT-NEU], [null], [SMY], [GEN]] The current state-of-the-art language models are based on recurrent neural networks which compute the probability of a word given all previous words using a softmax function over a linear function of the RNN's hidden state.[[EXT-NEU], [null], [SMY], [GEN]] This paper argues the softmax is not expressive enough and proposes to use a more flexible mixture of softmaxes.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The use of a mixture of softmaxes is motivated from a theoretical point of view by translating language modeling into matrix factorization.[[PDI-NEU], [null], [SMY], [GEN]]\n\nPros:\n--The paper is very well written and easy to follow.[[OAL-POS], [CLA-POS], [APC], [MAJ]] The ideas build up on each other in an intuitive way.[[PDI-POS,MET-POS], [EMP-POS], [APC], [MAJ]]\n--The idea behind the paper is novel: translating language modeling into a matrix factorization problem is new as far as I know.[[PDI-POS,OAL-POS], [NOV-POS], [APC], [MAJ]]\n--The maths is very rigorous.[[MET-POS], [EMP-POS], [APC], [MAJ]]\n--The experiment section is thorough.[[EXP-POS], [EMP-POS], [APC], [MAJ]]\n\nCons:\n--To claim SOTA all models need to be given the same capacity (same number of parameters).[[RWK-NEU], [CMP-NEU], [SUG,DIS], [MIN]] In Table 2 the baselines have a lower capacity.[[RWK-NEG,EXP-NEG,TNF-NEG], [CMP-NEG], [CRT], [MAJ]] This is an unfair comparison[[RWK-NEG,EXP-NEG], [CMP-NEG], [CRT], [MAJ]]\n--I suspect the proposed approach is slower than the baselines.[[RWK-NEG,MET-NEG], [CMP-NEG], [CRT], [MAJ]] There is no mention of computational cost.[[ANA-NEG], [SUB-NEG], [DFT,CRT], [MIN]] Reporting that would help interpret the numbers.[[ANA-NEU], [SUB-NEU], [SUG], [MIN]] \n\nThe SOTA claim might not hold if baselines are given the same capacity.[[RWK-NEG,EXP-NEG], [EMP-NEG], [CRT], [MIN]] But regardless of this, the paper has very strong contributions and deserves acceptance at ICLR."[[OAL-POS], [REC-POS], [FBK], [MAJ]]