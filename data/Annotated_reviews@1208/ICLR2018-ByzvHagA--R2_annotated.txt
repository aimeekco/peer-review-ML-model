"\nI think the first intuition is interesting.[[MET-NEU], [EMP-POS], [SMY], [MAJ]] However I think the benefits are not clear enough.[[MET-NEG], [EMP-NEG], [DIS], [MAJ]] Maybe finding better examples where the benefits of the proposed regularization are stressed could help.[[EXP-NEU], [EMP-NEU], [SUG], [MAJ]] \n\nThere is a huge amount of literature about ICA, unmixing, PCA, infomax... based on this principle that go beyond of the proposal.[[RWK-NEU], [CMP-NEU], [DIS], [MAJ]] I do not see a clear novelty in the proposal.[[MET-NEU], [NOV-NEG], [CRT], [MAJ]]  \n\nFor instance the proposed regularization can be achieved by just adding a linear combination at the layer which based on PCA.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] As shown in [Szegedy et al 2014, \"Intriguing properties of neural networks\"] adding an extra linear transformation does not change the expressive power of the representation. [[RWK-NEU,MET-NEU,BIB-NEU], [EMP-NEU], [DIS], [MAJ]]   \n\n\n- \"Inspired by this, we consider a simpler objective: a representation disentangles the data well when its components do not correlate...[[MET-NEU], [EMP-NEU], [SMY], [MAJ]]\"\n\nThe first paragraph is confusing since jumps from total correlation to correlation without making clear the differences.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\nAlthough correlation is a second oder approach to total correlation are not the same. This is extremely important since the whole proposal is based on that.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]]\n\n- Sec 2.1. What prevents the regularization to enforce the weights in the linear layers to be very small and thus minimize the covariance.[[MET-NEU], [EMP-NEU], [QSN], [MAJ]] I think the definition needs to enforce the out-diagonal terms in C to be small with respect to the terms in the diagonal.[[MET-NEU], [EMP-NEU], [SUG], [MAJ]]   \n\n- All the evaluation measures are based on linear relations, some of them should take into account non-linear relations (i.e. total correlation, mutual information...) in order to show that the method gets something interesting.[[MET-NEU], [EMP-NEU], [SUG], [MAJ]]\n\n- The first experiment (dim red) is not clear to me.[[EXP-NEG], [null], [CRT], [MAJ]] The original dimensionality of the data is 4, and only a linear relation is introduced. I do not understand the dimensionality reduction if the dimensionality of the transformed space is 10.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Also the data problem is extremely simple, and it is not clear the didactic benefit of using it.[[DAT-NEU,MET-NEU], [EMP-NEG], [CRT], [MAJ]] I think a much more complicated data would be more interesting.[[DAT-NEU], [SUB-NEU], [SUG], [MAJ]] Besides L_1 is not well defined.[[MET-NEU], [EMP-NEG], [CRT], [MAJ]] If it is L_1 norm on the output coefficients the comparison is misleading.[[MET-NEU], [CMP-NEG], [CRT], [MAJ]] \n\n- Sec 3.3. As in general the model needs to be compared with other regularization techniques to stress its benefits[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MAJ]].\n\n- Sec 3.4. Here the comparison makes clear that not a real benefit is obtained with the proposal.[[MET-NEG], [CMP-NEG], [CRT], [MAJ]] The idea behind regularization is to help the model to avoid overfitting and thus improving the quality of the prediction in future samples.[[PDI-NEU,MET-NEU], [EMP-NEU], [SMY], [MAJ]] However the MSE obtained when not using regularization is the same (or even smaller) than when using it.[[EXP-NEU,MET-NEU], [EMP-NEG], [CRT], [MAJ]]   \n"