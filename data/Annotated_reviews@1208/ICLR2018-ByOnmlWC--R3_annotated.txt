"This paper proposes a genetic algorithm inspired policy optimization method, which mimics the mutation and the crossover operators over policy networks.[[INT-NEU,MET-NEU], [null], [SMY], [GEN]]\n\nThe title and the motivation about the genetic algorithm are missing leading and improper.[[INT-NEG,MET-NEU], [null], [CRT], [MAJ]] The genetic algorithm is a black-box optimization method, however, the proposed method has nothing to do with black-box optimization.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] \n\nThe mutation is a method to sample individual independence of the objective function, which is very different with the gradient step.[[MET-NEU], [null], [SMY], [MAJ]] Mimicking the mutation by a gradient step is very unreasonable.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] \n\nThe crossover operator is the policy mixing method employed in game context (e.g., Deep Reinforcement Learning from Self-Play in Imperfect-Information Games, https://arxiv.org/abs/1603.01121 ).[[RWK-NEU,MET-NEU,BIB-NEU], [CMP-NEU], [DIS], [MAJ]] It is straightforward if two policies are to be mixed. Although the mixing method is more reasonable than the genetic crossover operator, it is strange to compare with that operator in a method far away from the genetic algorithm.[[RWK-NEU,MET-NEU], [CMP-NEG], [CRT], [MAJ]]\n\nIt is highly suggested that the method is called as population-based method as a set of networks is maintained, instead of as \"genetic\" method.[[MET-NEU], [EMP-NEU], [SUG], [MAJ]]\n\nAnother drawback, perhaps resulted from the \"genetic algorithm\" motivation is that the proposed method has not been well explained.[[MET-NEU], [EMP-NEG], [CRT], [MAJ]] The only explanation is that this method mimics the genetic algorithm. However, this explanation reveals nothing about why the method could work well -- a random exploration could also waste a lot of samples with a very high probability.[[MET-NEU], [EMP-NEG], [CRT], [MAJ]]\n\nThe baseline methods result in rewards much lower than those in previous experimental papers.[[RWK-NEU,RES-NEU], [CMP-NEG], [CRT], [MAJ]] It is problemistic that if the baselines have bad parameters.[[RWK-NEG,MET-NEG], [EMP-NEG], [SUG], [MAJ]]\n1. Benchmarking Deep Reinforcement Learning for Continuous Control\n2. Deep Reinforcement Learning that Matters[[MET-NEU], [EMP-NEU], [SUG], [MAJ]]"