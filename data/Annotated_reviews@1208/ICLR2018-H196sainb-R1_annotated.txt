"This paper presents a new method for obtaining a bilingual dictionary, without requiring any parallel data between the source and target languages.[[INT-POS,DAT-NEU,MET-POS], [IMP-POS,EMP-POS], [APC], [MAJ]] The method consists of an adversarial approach for aligning two monolingual word embedding spaces, followed by a refinement step using frequent aligned words (according to the adversarial mapping).[[RWK-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] The approach is evaluated on single word translation, cross-lingual word similarity, and sentence translation retrieval tasks.\[[PDI-NEU,MET-NEU,ANA-NEU], [EMP-NEU], [SMY], [GEN]]n\nThe paper presents an interesting approach which achieves good performance.[[INT-POS,PDI-POS,RES-POS], [IMP-POS], [APC], [MAJ]]The work is presented clearly, the approach is well-motivated and related to previous studies, and a thorough evaluation is performed.\[[RWK-POS,PDI-POS,ANA-POS], [CLA-POS,IMP-POS,EMP-POS], [APC], [MAJ]]n\nMy one concern is that the supervised approach that the paper compares to is limited:[[INT-NEG], [CMP-NEG], [DFT], [MIN]] it is trained on a small fixed number of anchor points, while the unsupervised method uses significantly more words[[RWK-NEG,DAT-NEG,MET-NEG], [CMP-NEG,EMP-NEG], [DFT], [MIN]]. I think the paper's comparisons are valid[[RWK-POS], [CMP-POS], [APC], [MAJ]], but the abstract and introduction make very strong claims about outperforming \"state-of-the-art supervised approaches\"[[ABS-NEU,INT-NEU,RES-NEU], [NOV-NEU,EMP-NEU], [SUG,FBK], [GEN]]. I think either a stronger supervised baseline should be included (trained on comparable data as the unsupervised approach), or the language/claims in the paper should be softened. [[RWK-NEG,PDI-NEG,DAT-NEG,MET-NEG], [IMP-NEG,EMP-NEG], [DFT], [MIN]]The same holds for statements like \"... our method is a first step ...\", which is very hard to justify. [[RWK-NEU,MET-NEU], [CMP-NEU], [SMY,SUG], [GEN]]I also do not think it is necessary to over-sell, given the solid work in the paper.[[INT-NEG,OAL-NEG], [NOV-NEG], [DFT], [MIN]]\n\nFurther comments, questions and suggestions:\n- It might be useful to add more details of your actual approach in the Abstract, not just what it achieves[[ABS-NEG,PDI-NEG], [null], [DFT], [MIN]].\n- Given you use trained word embeddings, it is not a given that the monolingual word embedding spaces would be alignable in a linear way. [[RWK-NEG,PDI-NEG], [null], [DFT], [MIN]]The actual word embedding method, therefore, has a big influence on performance (as you show).[[MET-NEU,RES-NEU], [EMP-NEU], [DIS], [GEN]] Could you comment on how crucial it would be to train monolingual embedding spaces on similar domains[[RWK-NEU,EXP-NEU], [null], [QSN], [GEN]]/data with similar co-occurrence statistics, in order for your method to be appropriate?[[DAT-NEU,MET-NEU,RES-NEU], [null], [QSN], [GEN]]\n- Would it be possible to add weights to the terms in eq. (6), or is this done implicitly[[PDI-NEU,DAT-NEU], [null], [SMY], [GEN]]?\n- How were the 5k source words for Procrustes supervised baseline selected[[RWK-NEU,MET-NEU], [null], [QSN], [GEN]]?\n- Have you considered non-linear mappings, or jointly training the monolingual word embeddings while attempting the linear mapping between embedding spaces[[RWK-NEU,DAT-NEU,EXP-NEU,MET-NEU,ANA-NEU], [EMP-NEU], [SMY,QSN], [GEN]]?\n- Do you think your approach would benefit from having a few parallel training points?[[PDI-NEU,DAT-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SMY,QSN], [GEN]]\n\nSome minor grammatical mistakes/typos (nitpicking):\n- \"gives a good performance\" -> [[RWK-NEG], [PNF-NEG], [DFT], [MIN]]\"gives good performance\"\n- \"Recent works\", \"several works\", \"most works\", etc.-> [[RWK-POS,PDI-POS,RES-POS], [SUB-POS], [APC], [MAJ]]\"recent studies\", \"several studies\", etc.\n- \"i.e, the improvements\" -> \"i.e., the improvements\"\n\[[RWK-POS,PDI-POS], [IMP-POS], [APC], [MAJ]]nThe paper is well-written, relevant and interesting[[INT-POS], [CLA-POS,IMP-POS], [APC], [MAJ]]. I therefore recommend that the paper be accepted.\n\n"[[INT-POS], [REC-POS], [APC], [MAJ]]