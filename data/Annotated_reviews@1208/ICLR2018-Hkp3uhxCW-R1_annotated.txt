"*Summary*\n\nThe paper applies variational inference (VI) with the 'reparameterisation' trick for Bayesian recurrent neural networks (BRNNs).[[INT-NEU,MET-NEU], [null], [SMY], [GEN]] The paper first considers the \"Bayes by Backprop\" approach of Blundell et al. (2015) and then modifies the BRNN model with a hierarchical prior over the network parameters, which then requires a hierarchical variational approximation with a simple linear recognition model.[[RWK-NEU,PDI-NEU,EXP-NEU,MET-NEU,BIB-NEU], [null], [SMY], [GEN]] Several experiments demonstrate the quality of the prediction and the uncertainty over dropout. [[RWK-NEU,EXP-POS,ANA-NEG], [EMP-NEG], [SMY,APC], [GEN]] \n\n*Originality + significance*\n\nTo my knowledge, there is no other previous work on VI with the reparameterisation trick for BRNNs.[[RWK-POS,PDI-POS,MET-POS], [NOV-POS,IMP-POS,EMP-POS], [APC], [MAJ]] However, one could say that this paper is, on careful examination, an application of reparameterisation gradient VI for a specific application. [[RWK-POS,EXP-POS], [EMP-POS], [APC], [MAJ]]\n\nNevertheless, the parameterisation of the conditional variational distribution q(\\theta | \\phi, (x, y)) using recognition model is interesting and could be useful in other models.[[PDI-POS,EXP-POS,MET-NEU], [IMP-POS,EMP-POS], [APC], [MAJ]] However, this has not been tested or concretely shown in this paper.[[RWK-NEG,OAL-NEG], [SUB-NEG], [DFT], [MIN]] The idea of modifying the model by introducing variables to obtain a looser bound which can accommodate a richer variational family is also not new, see: hierarchical variational model (Ranganath et al., 2016) for example. [[PDI-NEU,MET-NEU,BIB-NEU], [null], [SMY], [GEN]]\n\n*Clarity*\n\nThe paper is, in general, well-written. However, the presentation in 4 is hard to follow.[[OAL-NEU], [CLA-POS,PNF-NEG], [APC,CRT], [GEN]] I would prefer if appendix A3 was moved up front -- in this case, it would make it clear that the model is modified to contain \\phi, a variational approximation over both \\theta and \\phi is needed, and a q that couples \\theta, \\phi and and the gradient of the log likelihood term wrt \\phi is chosen.[[RWK-NEG,BIB-NEG], [PNF-NEG], [CRT], [MIN]] \n\nAdditional comments:\n\nWhy is the variational approximation called \"sharpened\"?\n\nAt test time, normal VI just uses the fixed q(\\theta) after training.[[EXP-NEU], [null], [QSN], [GEN]] It's not clear to me how prediction is done when using 'posterior sharpening' -- how is q(\\theta | \\phi, x) in eqs. 19-20 parameterised?[[PDI-NEG,EXP-NEG], [CLA-NEG], [CRT], [MIN]] The first paragraph of page 5 uses q(\\theta | \\phi, (x, y)), but y is not known at test time.[[PDI-NEG,EXP-NEG], [SUB-NEG,IMP-NEG], [DFT], [MIN]]\n\nWhat is C in eq. 9?[[RWK-NEU], [null], [QSN], [GEN]]\n\nThis comment \"variational typically underestimate the uncertainty in the posterior...whereas expectation propagation methods are mode averaging and so tend to overestimate uncertainty...\" is not precise.[[PDI-NEG,MET-NEG], [IMP-NEG,EMP-NEG], [DFT,CRT], [MIN]] EP can do mode averaging as well as mode seeking, depending on the underlying and approximate factor graphs.[[RWK-NEU], [null], [SMY], [GEN]] In the Bayesian neural network setting when the likelihood is factorised point-wise and there is one factor for each likelihood, EP is just as mode-seeking as variational.[[EXP-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] On the other hand, variational methods can avoid modes too, see the mixture of Gaussians example in the \"Two problems with variational EM... \" paper by Turner and Sahani (2010).[[RWK-NEU,PDI-NEU,MET-NEU,BIB-NEU], [null], [SMY], [MAJ]]\n\nThere are also many hyperparameters that need to be chosen -- what would happen if these are optimised using the free-energy?[[PDI-NEU,EXP-NEU], [null], [QSN], [GEN]] Was there any KL reweighting scheduling as done in the original BBB paper?[[RWK-NEU,EXP-NEU], [null], [QSN], [GEN]] \n\nWhat is the significance of the difference between BBB and BBB with sharpening in the language modelling task?[[MET-NEU], [CMP-NEU], [QSN], [GEN]] Was sharpening used in the image caption generation task?[[RWK-NEU], [null], [QSN], [GEN]]\n\nWhat is the computational complexity of BBB with posterior sharpening?[[PDI-NEU,EXP-NEU], [null], [QSN], [GEN]] Twice that BBB? If this is the case, would BBB get to the same performance if we optimise it for longer?[[RWK-NEU,PDI-NEU,MET-NEU], [null], [QSN], [GEN]] Would be interesting to see the time/accuracy frontier."[[EXT-NEU], [null], [FBK], [GEN]]