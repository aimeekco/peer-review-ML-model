"The paper presents a method that leverages demonstrations from experts provided in the shape of sequences of states (actually, state transitions are enough, they don't need to come in sequences) to faster learn reinforcement learning tasks.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The authors propose to learn subgoals (actually local rewards) to encourage the agent to go towards the same direction as the expert when encountering similar states.[[PDI-NEU], [null], [SMY], [GEN]] The main claimed advantage is that it doesn't require the knowledge of the actions taken by the expert, only observations of states.[[PDI-NEU], [null], [SMY], [GEN]] \n\nTo me, there is a major flaw in the approach.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Ho and Ermon 2016 extensively study the fact that imitation is not possible in stochastic environment without the knowledge of the actions.[[RWK-NEU], [null], [DIS], [GEN]] As the author say, learning the actions from state transitions in a standard stochastic MDP would require to learn the model.[[MET-NEU], [null], [DIS], [MIN]] Yet, the authors demonstrate their approach in environments where the controlable dynamics is mainly deterministic (if one decides to turn right, the agents indeed turns right).[[MET-NEU], [null], [DIS], [GEN]] So by subtracting features from successive states, the method mainly encodes the action as it almost encodes the one step dynamics in one shot.[[MET-NEU], [null], [DIS], [MIN]] \n\nAlso the main assumption is that there is an easy way to compute similarity between states.[[MET-NEU], [null], [DIS], [MIN]] This assumption is not met in the HealthGathering environment as several different states may generate very similar vision features.[[MET-NEU], [null], [DIS], [MIN]] This causes the method not to work.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] This brings us back to the fact that features encoding the actual dynamics, potentially on many consecutive states (e.g. feature expectations used in IRL or occupancy probability used in Ho and Ermon 2016), are mandatory.[[RWK-NEU], [CMP-NEU], [DIS], [MIN]] \n\nThe method is also very close to the simplest IRL method possible which consists in placing positive rewards on every state the expert visited.[[RWK-NEU], [CMP-NEU], [DIS], [MIN]]  So I would have liked a comparison to that simple method (using similar regression technique to generalize over states with similar features).[[MET-NEU], [CMP-NEU], [DIS], [MIN]]  \n\nFinally, I also think that using expert data generated by a pre-trained network makes the experimental section very weak.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]]  Indeed, it is unlikely that this kind of data can be obtained and training on this type of data is just a kind of distillation of the optimal network making the weights of the network close to the right optimum.[[DAT-NEG,EXP-NEG], [EMP-NEG], [CRT], [MAJ]]  With real data, acquired from humans, the training is likely to end up in a very different minima.[[DAT-NEU,EXP-NEU], [null], [DIS], [MIN]]  \n\nConcerning the related work, the authors didn't mention the Universal Value Function Approximation (Schaul et al, @ICML 2015) which precisely extends V and Q functions to generalize over goals.[[RWK-NEG], [CMP-NEG], [DFT,CRT], [MIN]]  This very much relates to the method used to generalize over subgoals in the paper.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]  Also, the state if the art in IRL and learning from demonstration is lacking a lot of references.[[RWK-NEG], [SUB-NEG], [CRT], [MIN]]  For instance, learning via RL + demonstrations was already studied into papers by Farahmand et al (APID, @NIPS 2013), Piot et al (RLED, @ ECML 2014) or Chemali & Lazaric (DPID, @IJCAI 2015) before Hester et al (DQfD @AAAI 2018).[[RWK-NEU,BIB-NEU], [null], [DIS], [GEN]]  Some work is cited in the wrong context.[[RWK-NEG], [null], [CRT], [MIN]]  For instance, Borsa et al 2017 doesn't do inverse RL (as said in the related work section) but learn to perform a task only from the extrinsic reward provided by the environment (as said in the introduction).[[RWK-NEU], [null], [DIS], [GEN]]  BTW, I would suggest to refer to published papers if they exist instead of their Arxiv version (e.g. Hester et al, DQfD). "[[OAL-POS], [REC-POS], [APC], [MAJ]] 