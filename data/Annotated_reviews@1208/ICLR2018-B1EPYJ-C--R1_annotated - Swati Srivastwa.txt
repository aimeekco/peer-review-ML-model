"This paper proposes several client-server neural network gradient update strategies aimed at reducing uplink usage while maintaining prediction performance.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]]  The main approaches fall into two categories: structured, where low-rank/sparse updates are learned, and sketched, where full updates are either sub-sampled or compressed before being sent to the central server.[[MET-NEU], [null], [SMY], [GEN]]  Experiments are based on the federated averaging algorithm. [[EXP-NEU], [null], [SMY], [GEN]] The work is valuable,[[OAL-POS], [null], [APC], [MAJ]] but has room for improvement.[[OAL-NEG], [null], [CRT], [MIN]]\n\nThe paper is mainly an empirical comparison of several approaches, rather than from theoretically motivated algorithms. [[OAL-NEU], [null], [DIS], [GEN]] This is not a criticism, however, it is difficult to see the reason for including the structured low-rank experiments in the paper (itAs a reader, I found it difficult to understand the actual procedures used. [[EXP-NEG], [EMP-NEG], [CRT], [MAJ]] For example, what is the difference between the random mask update and the subsampling update (why are there no random mask experiments after figure 1, even though they performed very well)?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]]  How is the structured update \"learned\"?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]  It would be very helpful to include algorithms.[[MET-NEU], [SUB-NEU], [SUG], [MIN]]\n\nIt seems like a good strategy is to subsample, perform Hadamard rotation, then quantise.[[MET-NEU], [EMP-NEU], [SUG], [MIN]]    For quantization, it appears that the HD rotation is essential for CIFAR, but less important for the reddit data. [[DAT-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]] It would be interesting to understand when HD works and why,  and perhaps make the paper more focused on this winning strategy, rather than including the low-rank algo.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]  \n\nIf convenient, could the authors comment on a similarly motivated paper under review at iclr 2018:\nVARIANCE-BASED GRADIENT COMPRESSION FOR EFFICIENT DISTRIBUTED DEEP LEARNING[[RWK-NEU], [CMP-NEU], [QSN], [MIN]]\n\npros:\n\n- good use of intuition to guide algorithm choices[[MET-POS], [EMP-POS], [APC], [MAJ]]\n- good compression with little loss of accuracy on best strategy\[[RES-POS], [EMP-POS], [APC], [MAJ]]n- good problem for FA algorithm / well motivated[[RES-POS], [EMP-POS], [APC], [MAJ]]\n- \n\ncons:\n\n- some experiment choices do not appear well motivated / inclusion is not best choice[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]]\n- explanations of algos / lack of 'algorithms' adds to confusion[[MET-NEG], [SUB-NEG], [CRT], [MAJ]]\n\na useful reference:\n\nStrom, Nikko. \"Scalable distributed dnn training using commodity gpu cloud computing.\" Sixteenth Annual Conference of the International Speech Communication Association. 2015.\n\n"[[BIB-NEU], [null], [DIS], [GEN]]