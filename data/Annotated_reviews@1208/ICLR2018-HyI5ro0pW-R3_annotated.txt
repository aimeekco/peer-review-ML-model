"This paper proposes replacing fully connected layers with block-diagonal fully connected layers and proposes two methods for doing so. [[INT-NEU,MET-NEU], [null], [SMY], [GEN]] It also make some connections to random matrix theory.[[RWK-NEU], [null], [SMY], [GEN]]\n\nThe parameter pruning angle in this paper is fairly weak.[[OAL-NEG], [CNT], [CRT], [MAJ]]  The networks it is demonstrated on are not particularly large (largeness usually being the motivation for pruning) and the need for making them smaller is not well motivated.[[PDI-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]]  Additionally MNIST is a uniquely bad dataset for evaluating pruning methods, since they tend to work uncharacteristically well on MNIST (This can be seen in some of the references the paper cites).[[DAT-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nThe random matrix theory part of this paper is intriguing, but left me wondering \"and then what?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\"  It is presented as a collection of observations with no synthesis or context for why they are important. [[RES-NEU], [EMP-NEU], [DIS], [MIN]] I'm usually quite happy to see connections being made to other fields,[[MET-NEU], [EMP-NEU], [DIS], [MIN]] but it is not clear at all how this particular connection is more than a curiosity.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]]  This paper would be much stronger if it offered some way to exploit this connection.[[OAL-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nThere are two half-papers here, one on parameter pruning and one on applying insights from random matrix theory to neural networks, but I don't see a strong connection between them. [[MET-NEG], [EMP-NEG], [CRT], [MAJ]]Moreover, they are both missing their other half where the technique or insight they propose is exploited to achieve something. \n"[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]