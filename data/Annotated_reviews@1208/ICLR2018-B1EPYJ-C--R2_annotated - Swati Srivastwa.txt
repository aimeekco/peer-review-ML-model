"\nThe authors examine several techniques that lead to low communication updates during distributed training in the context of Federated learning (FL).[[MET-NEU], [null], [DIS], [GEN]] Under the setup of FL, it is assumed that training takes place over edge-device like compute nodes that have access to subsets of data (potentially of different size), and each node can potentially be of different computational power.[[MET-NEU], [null], [DIS], [GEN]] Most importantly, in the FL setup, communication is the bottleneck.[[MET-NEU], [null], [DIS], [GEN]] Eg a global model is to be trained by local updates that occur on mobile phones, and communication cost is high due to slow up-link.[[EXP-NEU,MET-NEU], [null], [DIS], [GEN]]\n\nThe authors present techniques that are of similar flavor to quantized+sparsified updates.[[MET-NEU], [null], [DIS], [GEN]] They distinguish theirs approaches into 1) structured updates and 2) sketched updates.[[MET-NEU], [null], [DIS], [GEN]] For 1) they examine a low-rank version of distributed SGD where instead of communicating full-rank model updates, the updates are factored into two low rank components, and only one of them is optimized at each iteration, while the other can be randomly sampled.[[EXP-NEU], [null], [DIS], [GEN]]\nThey also examine random masking, eg a sparsification of the updates, that retains a random subset of the entries of the gradient update (eg by zero-ing out a random subset of elements).[[EXP-NEU], [null], [DIS], [GEN]] This latter technique is similar to randomized coordinate descent.[[MET-NEU], [null], [DIS], [GEN]]\n\nUnder the theme of sketched updates, they examine quantized and sparsified updates with the property that in expectation they are identical to the true updates.[[EXP-NEU,MET-NEU], [null], [DIS], [GEN]] The authors specifically examine random subsampling (which is the same as random masking, with different weights) and probabilistic quantization, where each element of a gradient update is randomly quantized to b bits.[[EXP-NEU,MET-NEU], [null], [DIS], [GEN]] \n\nThe major contribution of this paper is their experimental section, where the authors show the effects of training with structured, or sketched updates, in terms of reduced communication cost, and the effect on the training accuracy.[[EXP-POS,ANA-POS], [EMP-POS], [APC], [MAJ]] They present experiments on several data sets, and observe that among all the techniques, random quantization can have a significant reduction of up to 32x in communication with minimal loss in accuracy.[[DAT-POS,EXP-POS,RES-POS], [EMP-POS], [APC], [MAJ]]\n\nMy main concern about this paper is that although the presented techniques work well in practice,[[MET-POS], [EMP-POS], [APC], [MAJ]] some of the algorithms tested are similar algorithms that have already been proven to work well in practice.[[MET-NEU], [CMP-NEU], [DIS], [MIN]] For example, it is unclear how the performance of the presented quantization algorithms compares to say  QSGD [1] and Terngrad [2].[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Although the authors cite QSGD, they do not directly compare against it in experiments.[[EXP-NEG], [CMP-NEG], [DFT], [MIN]]\n\nAs a matter of fact, one of the issues of the presented quantized techniques (the fact that random rotations might be needed when the dynamic range of elements is large, or when the updates are nearly sparse) is easily resolved by algorithms like QSGD and Terngrad that respect (and promote) sparsity in the updates.[[MET-NEU], [CMP-NEU], [DIS], [MIN]] \n\nA more minor comment is that it is unclear that averaging is the right way to combine locally trained models for nonconvex problems.[[EXP-NEG], [EMP-NEG], [CRT], [MIN]] Recently, it has been shown that averaging can be suboptimal for nonconvex problems, eg a better averaging scheme can be used in place [3].[[MET-NEU], [null], [DIS], [GEN]] However, I would not worry too much about that issue, as the same techniques presented in this paper apply to any weighted linear averaging algorithm.[[MET-NEU], [null], [DIS], [GEN]]\n\nAnother minor comment: The legends in the figures are tiny, and really hard to read.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]]\n\nOverall this paper examines interesting structured and randomized low communication updates for distributed FL,[[RES-POS], [EMp-POS], [APC], [MAJ]] but lacks some important experimental comparisons.[[EXP-NEG], [CMP-NEG], [DFT], [MAJ]]\n\n\n[1] QSGD: Communication-Optimal Stochastic Gradient Descent, with Applications to Training Neural Networks https://arxiv.org/abs/1610.02132[[BIB-NEU], [null], [DIS], [GEN]]\n[2] TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning\nhttps://arxiv.org/abs/1705.07878\n[3] Parallel SGD: When does averaging help? \nhttps://arxiv.org/abs/1606.07365\n\n"[[BIB-NEU], [null], [DIS], [GEN]]