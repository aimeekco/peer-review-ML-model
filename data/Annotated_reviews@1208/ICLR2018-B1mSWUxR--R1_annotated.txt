"This paper dives deeper into understand reward augmented maximum likelihood training.[[PDI-NEU,EXP-NEU], [null], [SMY], [GEN]] Overall, I feel that the paper is hard to understand and that it would benefit from more clarity, e.g., section 3.3 states that decoding from the softmax q-distribution is similar to the Bayes decision rule.[[MET-NEG,OAL-NEG], [CLA-NEG,PNF-NEG], [CRT], [MAJ]] Please elaborate on this.[[OAL-NEU], [SUB-NEU], [DIS], [MIN]]\n\nDid you compare to minimum bayes risk decoding which chooses the output with the lowest expected risk amongst a set of candidates?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nSection 4.2.2 says that Ranzato et al. and Bahdanau et al. require sampling from the model distribution.[[RWK-NEU], [null], [SMY,DIS], [GEN]] However, the methods analyzed in this paper also require sampling (cf. Appendix D.2.4 where you mention a sample size of 10),[[MET-NEU,ANA-NEU], [SUB-NEU,EMP-NEU], [DIS], [MIN]] Please explain the difference."[[MET-NEU], [CMP-NEU], [DIS], [MIN]]