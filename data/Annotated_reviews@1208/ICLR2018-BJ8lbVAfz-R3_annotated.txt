"This paper proposes a model using hidden neurons with self-organising activation function, whose outputs feed to classifier with softmax output function.[[INT-NEU], [null], [SMY], [GEN]] It is trained with supervised learning, by minimising the cross-entropy error between labels and the softmax output.\n\nThe paper's claim of combining unsupervised (self-organising) with supervised training is misleading and confusing.[[PDI-NEU], [null], [SMY], [GEN]] In this model, self-organising is a property of the hidden neurons' activation (eq. 1-3), and the training procedure is entirely supervised.[[MET-NEU], [null], [SMY], [GEN]] It is misleading to claim any unsupervised or semi-supervised learning based on the *self-organising part* of, for example, eq. 14, which is merely a result of applying chain rule through the hidden neurons' activation.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n\nWhile this model is proposed as an extension of Kohonen's self-organising map (SOM), the paper fails to mention, or compare with, several historically important extension of SOM, which should perhaps at least include the generative topographic mapping (GTM, Bishop et al. 1998), an important probabilistic generalisation of SOM.[[RWK-NEG], [SUB-NEG,CMP-NEG], [DFT,CRT], [MAJ]]\n\nFinally, the evaluation of the model in comparison with other models is questionable.[[MET-NEG], [CMP-NEG], [CRT], [MAJ]] For example, while the configuration the paper's baseline models are not given, the baseline accuracy of MNIST classification using MLP is 16.2%.[[DAT-NEU,RWK-NEG], [CMP-NEG], [DFT], [MAJ]] This is much worse than the baseline of 12% in LeCun et al. (1998), using simple linear classifier without any preprocessing.[[RWK-NEG], [CMP-NEG], [DFT], [MAJ]]  The 7% accuracy from the proposed model is not in the range of modern deep learning models (The state-of-art accuracy is <0.3%).[[RWK-NEG,RES-NEG], [CMP-NEG], [CRT], [MAJ]]  Similar problem also exist in results from other datasets.[[DAT-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]]  They are therefore unable to support the paper's claim on robust performance[[RES-NEG], [EMP-NEG], [CRT], [MIN]] \n\nPros:\nThe question of internal representation is interesting.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] \nCombining self-organising with classification.[[CNT], [null], [DIS], [GEN]] \nComparing learned representations from different models.[[MET-NEU], [CMP-NEU], [DIS], [MIN]] \n\nCons:\nNot clearly written.[[DAT-NEG], [CLA-NEG], [CRT], [MIN]] \nMixing the concept of unsupervised/semi-supervised learning is confusing.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] \nModel evaluation is questionable.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\nDoes not compare existing extensions of SOM."[[MET-NEG], [CMP-NEG], [CRT], [MIN]]