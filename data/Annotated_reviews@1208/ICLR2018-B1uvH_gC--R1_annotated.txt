"The paper describes a manifold learning method that adapts the old ideas of multidimensional scaling, with geodesic distances in particular, to neural networks.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The goal is to switch from a non-parametric to a parametric method and hence to have a straightforward out-of-sample extension.[[PDI-NEU], [null], [SMY], [GEN]]\n\nThe paper has several major shortcomings:\n* Any paper dealing with MDS and geodesic distances should test the proposed method on the Swiss roll, which has been the most emblematic benchmark since the Isomap paper in 2000.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Not showing the Swiss roll would possibly let the reader think that the method does not perform well on that example.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] In particular, DR is one of the last fields where deep learning cannot outperform older methods like t-SNE.[[MET-NEU], [null], [DIS], [GEN]] Please add the Swiss roll example.[[MET-NEU], [SUB-NEU], [DIS], [MIN]]\n* Distance preservation appears more and more like a dated DR paradigm.[[MET-NEU], [SUB-NEU], [DIS], [MIN]] Simple example from 3D to 2D are easily handled but beyond the curse of dimensionality makes things more complicated, in particular due to norm computation.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Computation accuracy of the geodesic distances in high-dimensional spaces can be poor.[[RES-NEG], [EMP-NEG], [QSN], [MIN]] This could be discussed and some experiments on very HD data should be reported.[[DAT-NEG], [SUB-NEG], [DFT], [MIN]]\n* Some key historical references are overlooked, like the SAMMANN.[[BIB-NEG], [null], [CRT], [MIN]] There is also an over-emphasis on spectral methods, with the necessity to compute large matrices and to factorize them, probably owing to the popularity of spectral DR metods a decade ago.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Other methods might be computationally less expensive, like those relying on space-partitioning trees and fast multipole methods (subquadratic complexity).[[MET-NEG], [CMP-NEG], [CRT], [MAJ]] Finally, auto-encoders could be mentioned as well; they have the advantage of providing the parametric inverse of the mapping too.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]]\n* As a tool for unsupervised learning or exploratory data visualization, DR can hardly benefit from a parametric approach.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] The motivation in the end of page 3 seems to be computational only.[[MET-NEG], [SUB-NEG], [CRT], [MIN]]\n* Section 3 should be further detailed (step 2 in particular).[[MET-NEG], [SUB-NEG], [CRT], [MIN]]\n* The experiments are rather limited, with only a few artifcial data sets and hardly any quantitative assessment except for some monitoring of the stress.[[DAT-NEG,EXP-NEG], [EMP-NEG], [CRT], [MIN]] The running times are not in favor of the proposed method.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] The data sets sizes are, however, quite limited, with N<10000 for point cloud data and N<2000 for the image manifold.[[DAT-NEG], [SUB-NEG], [DFT], [MIN]]\n* The conclusion sounds a bit vague and pompous ('by allowing a limited infusion of axiomatic computation...').[[RES-NEG], [SUB-NEG], [CRT], [MIN]] What is the take-home message of the paper?"[[OAL-NEU], [IMP-NEU], [QSN], [MAJ]]