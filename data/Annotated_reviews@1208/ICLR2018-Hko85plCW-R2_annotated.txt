"The paper proposes an extension to a previous monotonic attention model (Raffel et al 2017) to attend to a fixed-sized window up to the alignment position.[[PDI-NEU,BIB-NEU], [EMP-NEU], [SMY], [GEN]] Both the soft attention approximation used for training the monotonic attention model, and the online decoding algorithm is extended to the chunkwise model.[[RWK-NEU,EXP-NEU,MET-NEU], [null], [SMY], [GEN]] In terms of the model this is a relatively small extention of Raffel et al 2017.[[RWK-NEU,BIB-NEU], [null], [DIS], [GEN]]\n\nResults show that for online speech recognition the model matches the performance of an offline soft attention baseline, doing significantly better than the monotonic attention model. [[EXP-NEU,MET-NEU,RES-NEU], [IMP-POS], [SMY,DIS], [GEN]]Is the offline attention baseline unidirectional or bidirectional?[[RWK-NEU], [null], [SMY,QSN], [GEN]] In case it is unidirectional it cannot really be claimed that the proposed model's performance is competitive with an offline model.[[RWK-NEU,PDI-NEU,RES-NEU], [null], [QSN], [MIN]]\n\nMy concern with the statement that all hyper-parameters are kept the same as the monotonic model is that the improvement might partly be due to the increase in total number of parameters in the model.[[RWK-NEU,PDI-NEU], [EMP-NEU], [SMY,SUG], [GEN]] Especially given that w=2 works best for speech recognition, it not clear that the model extension is actually helping.[[RWK-NEG,MET-NEG], [CLA-NEG], [DFT,CRT], [MIN]] My other concern is that in speech recognition the time-scale of the encoding is somewhat arbitrary, so possibly a similar effect could be obtained by doubling the time frame through the convolutional layer.[[RWK-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] While the empirical result is strong it is not clear that the proposed model is the best way to obtain the improvement.[[PDI-POS,MET-NEG], [CLA-NEG,EMP-NEG], [APC], [MAJ]]\n\nFor document summarization the paper presents a strong result for an online model, but the fact that it is still less accurate than the soft attention baseline makes it hard to see the real significance of this.[[RWK-POS,RES-POS,ANA-NEG], [IMP-NEG,EMP-POS], [DFT,CRT], [MIN]] If the contribution is in terms of speed (as shown with the synthetic benchmark in appendix B) more emphesis should be placed on this in the paper.[[RWK-NEU], [null], [SMY,DIS], [GEN]] \nSentence summarization tasks do exhibit mostly monotonic alignment, and most previous models with monotonic structure were evaluated on that, so why not test that here?[[RWK-NEU,EXP-NEU], [null], [QSN], [GEN]]\n\nI like the fact that the model is truely online, but that contribution was made by Raffel et al 2017, and this paper at best proposes a slightly better way to train and apply that model.[[RWK-NEU,PDI-POS,EXP-NEU,MET-NEU,BIB-NEU], [EMP-POS], [SMY], [GEN]]\n\n---\n The additional experiments in the new version gives stronger support in favour of the proposed model architecture (vs the effect of hyperparameter choices).[[PDI-NEU,EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]] While I'm still on the fence on whether this paper is strong enough to be accepted for ICLR, this version is certainly improves the quality of the paper.[[OAL-POS], [IMP-POS,REC-POS], [APC], [MAJ]] \n"