"This is an intriguing paper on running regressions on probability distributions: i.e. a target distribution is expressed as a function of input distributions. [[INT-NEU], [null], [SMY], [GEN]]A well-written manuscript,;[[OAL-POS], [CLA-POS], [APC], [MAJ]] though the introduction could have motivated the problem a little better (i.e. why would we want to do this).[[INT-NEU], [null], [SUG], [MAJ]] The novelty in the paper is implementing such a regression in a layered network.[[MET-NEU], [NOV-NEU], [SMY], [MAJ]] The paper shows how the densities at each nodes are computed (and normalised).[[MET-NEU], [null], [SMY], [GEN]] Optimisation by back propagation and discretization of the densities to carry out numerical integration are well explained and easy to follow.[[MET-POS], [EMP-POS], [APC], [MAJ]] The paper uses three problems to illustrate the idea -- a synthetic dataset, a mean reverting stochastic process and a prediction problem on stock indices. [[PDI-NEU,DAT-NEU,MET-NEU], [null], [SMY], [GEN]] \nMy only two reservations of this paper is the illustration on the stock index data -- it seems to me, returns on individual constituent stocks of an index are used as samples of the return on the index itself.[[DAT-NEU,EXP-NEU], [EMP-NEU], [SMY], [MAJ]]  But this cannot be true when the index is a weighted sum of the constituent assets.  [[MET-NEU], [EMP-NEG], [CRT], [MAJ]]Secondly, it is not clear to me why one would force a kernel density estimate on the asset returns and then bin the density into 100 bins for numerical reasons;[[MET-NEU], [EMP-NEU], [CRT], [MAJ]] -- does the smoothing that results from this give any advantage over a histogram of the returns in 100 bins?[[MET-NEU,RES-NEU], [EMP-NEU], [QSN], [MAJ]]\n "