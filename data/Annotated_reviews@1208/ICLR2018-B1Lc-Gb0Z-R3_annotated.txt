"The paper discusses the problem of optimizing neural networks with hard threshold and proposes a novel solution to it.[[PDI-POS,RES-POS], [NOV-POS], [APC], [MAJ]] The problem is of significance because in many applications one requires deep networks which uses reduced computation and limited energy.[[PDI-POS,RES-POS], [IMP-POS], [APC], [MAJ]] The authors frame the problem of optimizing such networks to fit the training data as a convex combinatorial problems.[[PDI-POS,DAT-POS,EXP-POS], [EMP-POS], [APC], [MAJ]] However since the complexity of such a problem is exponential, the authors propose a collection of heuristics/approximations to solve the problem.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] These include, a heuristic for setting the targets at each layer, using a soft hinge loss, mini-batch training and such.[[EXP-NEU], [EMP-NEU], [DIS], [GEN]] Using these modifications the authors propose an algorithm (Algorithm 2 in appendix) to train such models efficiently.[[EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]] They compare the performance of a bunch of models trained by their algorithm against the ones trained using straight-through-estimator (SSTE) on a couple of datasets, namely, CIFAR-10 and ImageNet.[[DAT-NEU,MET-NEU], [null], [DIS], [GEN]] They show superiority of their algorithm over SSTE.[[MET-NEU], [null], [DIS], [GEN]] \n\nI thought the paper is very well written and provides a really nice exposition of the problem of training deep networks with hard thresholds.[[EXP-POS,OAL-POS], [CLA-POS], [APC], [MAJ]] The authors formulation of the problem as one of combinatorial optimization and proposing Algorithm 1 is also quite interesting.[[MET-POS], [EMP-POS], [APC], [MAJ]] The results are moderately convincing in favor of the proposed approach.[[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]] Though a disclaimer here is that I'm not 100% sure that SSTE is the state of the art for this problem.[[RWK-NEG,MET-NEG], [CMP-NEG], [CRT], [MIN]] Overall i like the originality of the paper and feel that it has a potential of reasonable impact within the research community.[[FWK-POS,OAL-POS], [IMP-POS], [APC], [MAJ]] \n\nThere are a few flaws/weaknesses in the paper though, making it somewhat lose.[[OAL-NEG], [CNT], [CRT], [MIN]] \n- The authors start of by posing the problem as a clean combinatorial optimization problem and propose Algorithm 1.[[PDI-POS,MET-POS], [EMP-POS], [APC], [MAJ]] Realizing the limitations of the proposed algorithm, given the assumptions under which it was conceived in, the authors relax those assumptions in the couple of paragraphs before section 3.1 and pretty much throw away all the nice guarantees, such as checks for feasibility, discussed earlier.[[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]] \n- The result of this is another algorithm (I guess the main result of the paper), which is strangely presented in the appendix as opposed to the main text, which has no such guarantees.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MIN]]  \n- There is no theoretical proof that the heuristic for setting the target is a good one, other than a rough intuition[[RES-NEG], [EMP-NEG], [CRT], [MAJ]]\n- The authors do not discuss at all the impact on generalization ability of the model trained using the proposed approach.[[MET-NEG,RES-NEG], [SUB-NEG], [CRT], [MAJ]] The entire discussion revolves around fitting the training set and somehow magically everything seem to generalize and not overfit. \n"[[DAT-NEG,EXP-NEG], [EMP-NEG], [CRT], [MIN]]