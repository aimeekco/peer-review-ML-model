"This paper investigates the complexity of neural networks with piecewise linear activations by studying the number of linear regions of the representable functions.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] It builds on previous works Montufar et al. (2014) and Raghu et al. (2017) and presents improved bounds on the maximum number of linear regions.[[RWK-POS,RES-POS], [EMP-POS], [APC], [MAJ]] It also evaluates the number of regions of small networks during training.[[MET-NEU], [null], [SMY], [GEN]] \n\nThe improved upper bound given in Theorem 1 appeared in SampTA 2017 - Mathematics of deep learning ``Notes on the number of linear regions of deep neural networks'' by Montufar.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [GEN]] \n\nThe improved lower bound given in Theorem 6 is very modest but neat.[[MET-POS], [EMP-POS], [APC], [MIN]] Theorem 5 follows easily from this.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \n\nThe improved upper bound for maxout networks follows a similar intuition but appears to be novel.[[MET-POS], [NOV-POS], [APC], [MAJ]] \n\nThe paper also discusses the exact computation of the number of linear regions in small trained networks.[[PDI-NEU], [CNT], [DIS], [GEN]] It presents experiments during training and with varying network sizes.[[EXP-NEU], [null], [DIS], [GEN]] These give an interesting picture, consistent with the theoretical bounds, and showing the behaviour during training.[[EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]] \n\nHere it would be interesting to run more experiments to see how the number of regions might relate to the quality of the trained hypotheses. \n\n\n\n"[[EXP-NEU], [SUB-NEU], [DIS], [MIN]]