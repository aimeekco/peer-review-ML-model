"This paper proposes to extend the Prototypical Network (NIPS17) to the semi-supervised setting with three possible \nstrategies.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] One consists in self-labeling the unlabeled data and then updating the prototypes on the basis of the \nassigned pseudo-labels.[[RWK-NEU,DAT-NEU], [null], [SMY], [GEN]] Another is able to deal with the case of distractors i.e.  unlabeled samples not beloning to\nany of the known categories.[[RWK-NEU,PDI-NEU,DAT-NEU], [null], [SMY], [GEN]] In practice this second solution is analogous to the first, but a general 'distractor' class\nis added.[[RWK-NEU,EXP-NEU], [null], [SMY], [GEN]] Finally the third technique learns to weight the samples according to their distance to the original prototypes.[[RWK-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]]\n\nThese strategies are evaluated in a particular semi-supervised transfer learning setting:  the models are first trained \non some source categories with few labeled data and large unlabeled samples (this setting is derived by subselecting\nmultiple times a large dataset), then they are used on a final target task with again few labeled data and large \nunlabeled samples but beloning to a different set of categories.[[RWK-NEU,PDI-NEU,DAT-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]]\n\n+ the paper is well written, well organized and overall easy to read[[OAL-NEG], [CLA-POS,IMP-POS,PNF-POS], [APC], [MAJ]]\n+/-  this work builds largely on previous work.[[RWK-NEG], [SUB-NEG,EMP-NEG], [DFT], [MIN]] It introduces only some small technical novelty inspired by soft-k-means\nclustering that anyway seems to be effective.[[RWK-NEU,MET-NEU], [NOV-NEU,EMP-NEU], [SMY,DIS], [GEN]]\n+ different aspect of the problem are analyzed by varying the number of disctractors and varying the level of\nsemantic relatedness between the source and the target sets\n\nFew notes and questions[[RWK-NEU,EXP-NEU,ANA-NEU], [EMP-NEU], [SMY], [GEN]]\n1) why for the omniglot experiment the table reports the error results? [[RWK-NEU,RES-NEU], [null], [QSN], [GEN]]It would be better to present accuracy as for the other tables/experiments[[EXP-NEG,RES-NEG,ANA-NEG,TNF-NEG], [IMP-NEG,EMP-NEG], [SUG,CRT], [MIN]]\n2) I would suggest to use source and target instead of train and test -- these two last terms are confusing because\nactually there is a training phase also at test time.[[RWK-NEG,EXP-NEG], [CLA-NEG,EMP-NEG], [CRT], [MIN]]\n3) although the paper indicate that there are different other few-shot methods that could be applicable here[[RWK-NEG,MET-NEU], [SUB-NEG,EMP-NEG], [DFT], [MIN]], \nno other approach is considered besides the prothotipical network and its variants. [[RWK-NEU], [null], [SUG], [GEN]]An further external reference \ncould be used to give an idea of what would be the experimental result at least in the supervised case.[[EXP-NEU,RES-NEU,FWK-NEU], [null], [SMY], [GEN]]\n\n\n\n\n"