"Detecting out of distribution examples is important since it lets you know when neural network predictions might be garbage.[[PDI-NEU,EXP-NEU,MET-NEU], [null], [SMY], [GEN]] The paper addresses this problem with a method inspired by adversarial training, and shows significant improvement over best known method, previously published in ICLR 2017.[[INT-NEU,PDI-NEU,MET-POS,ANA-POS,BIB-NEU], [IMP-POS,CMP-POS,EMP-POS], [APC], [MAJ]]\n\nPrevious method used at the distribution of softmax scores as the measure.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] Highly peaked -> confidence, spread out -> out of distribution.[[RWK-NEU], [null], [SMY], [GEN]] The authors notice that in-distribution examples are also examples where it's easy to drive the confidence up with a small step.[[PDI-NEU,EXP-NEU], [null], [SMY], [GEN]] The small step is in the direction of gradient when top class activation is taken as the objective.[[PDI-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] This is also the gradient used to determine influence of predictors, and it's the gradient term used for adversarial training \"fast gradient sign\" method.[[PDI-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]]\n\nTheir experiments show improvement across the board using DenseNet on collection of small size dataset (tiny imagenet, cifar, lsun).[[PDI-NEU,DAT-NEU,ANA-POS], [IMP-POS,EMP-NEU], [SMY,APC], [GEN]] For instance at 95% threshold (detect 95% of out of distribution examples), their error rate goes down from 34.7% for the best known method, to 4.3% which is significant enough to prefer their method to the previous work.[[RWK-NEU,EXP-NEU,ANA-NEU], [IMP-NEU,EMP-NEU], [SMY,DIS], [GEN]]\n\n"