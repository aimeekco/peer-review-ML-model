"Summary of the paper\n---------------------------\nThe paper addresses the issue of online optimization of hyper-parameters customary involved in deep architectures learning. [[INT-NEU], [null], [SMY], [GEN]] The covered framework is limited to regularization parameters.[[MET-NEU], [null], [SMY], [GEN]] These hyper-parameters, noted $\\lambda$, are updated along the training of model parameters $\\theta$ by relying on the generalization performance (validation error).[[MET-NEU], [null], [SMY], [GEN]] The paper proposes a dynamical system including the dynamical update of $\\theta$ and the update of the gradient $y$, derivative of $\\theta$ w.r.t. to the hyper-parameters.[[MET-NEU], [null], [SMY], [GEN]] The main contribution of the paper is to propose a way to re-initialize $y$ at each update of $\\lambda$ and a clipping procedure of $y$ in order to maintain the stability of the dynamical system.[[MET-NEU], [null], [SMY], [GEN]] Experimental evaluations on synthetic or real datasets are conducted to show the effectiveness of the approach.[[EXP-NEU], [null], [SMY], [GEN]]\n\nComments\n-------------\n- The materials of the paper sometimes may be quite not easy to follow.[[OAL-NEU], [CLA-NEU], [CRT], [MAJ]] Nevertheless the paper is quite well written.[[OAL-POS], [CLA-POS], [APC], [MAJ]]\n- The main contributions of the paper can be seen as an incremental version of (Franceschi et al, 2017) based on the proposal in (Luketina et al., 2016)[[RWK-NEU,MET-NEU,BIB-NEU], [CMP-POS], [APC], [MAJ]]. As such the impact of the contributions appears rather limited even though the experimental results show a better stability of the method compared to competitors.[[RWK-NEU,EXP-POS,MET-NEU], [CMP-POS], [APC], [MAJ]]\n- One motivation of the approach is to fix the slow convergence of the method in (Franceschi et al, 2017).[[RWK-NEU,MET-NEU,BIB-NEU], [EMP-NEU], [SMY], [MAJ]] The paper will gain in quality if a theoretical analysis of the speed-up brought by the proposed approach is discussed.[[MET-NEU], [EMP-NEU], [SUG], [MAJ]]\n- The goal of the paper is to address automatically the learning of regularization parameters.[[MET-NEU], [EMP-NEU], [SMY], [MAJ]] Unfortunately, Algorithm 1 involves several other hyper-parameters (namely clipping factor $r$, constant $c$ or $\\eta$) which choices are not clearly discussed.[[MET-NEU], [EMP-NEG], [CRT], [MAJ]] It turns that the paper trades a set of hyper-parameters for another one which tuning may be tedious.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] This fact weakens the scope of the online hyper-parameter optimization approach.[[MET-NEU], [EMP-NEU], [CRT], [MAJ]]\n- It may be helpful to indicate the standard deviations of the experimental results.[[RES-NEU], [EMP-NEU], [SUG], [MAJ]]"