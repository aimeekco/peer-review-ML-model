"- Good work on developing VAEs for few-shot learning.[[OAL-POS], [CNT], [APC], [MAJ]]\n- Most of the results are qualitative[[RES-POS], [EMP-POS], [APC], [MAJ]] and I reckon the paper was written in haste.[[OAL-NEG], [CLA-NEG], [CRT], [MIN]]\n- The rest of the comments are below:\n\n- 3.1: I got a bit confused over what X actually is:\n -- \"We would like to learn a generative model for **sets X** of the form\".[[MET-NEG], [CLA-NEG], [CRT], [MIN]]\n --\"... to refer to the **class X_i** ...\".[[MET-NEG], [CLA-NEG], [CRT], [MIN]]\n -- \"we can lower bound the log-likelihood of each **dataset X** ...\[[MET-NEG], [CLA-NEG], [CRT], [MIN]]"\n\n- 3.2: \"In general, if we wish to learn a model for X in which each latent variable ci affects some arbitrary subset Xi of the data (**where the Xi may overlap**), ...\": Which is just like learning a Z for a labeled X but learning it in an unsupervised manner, i.e. the normal VAE, isn't it?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] If not, could you please elaborate on what is different (in the case of 3.2 only, I mean)?[[MET-NEU], [CMP-NEU], [QSN], [MIN]] i.e. Could you please elaborate on what's different (in terms of learning) between 3.2 and a normal latent Z that is definitely allowed to affect different classes of the data without knowing the classes?[[MET-NEU], [CMP-NEU], [QSN], [MIN]] \n\n- Figure 1 is helpful to clarify the main idea of a VHE.[[TNF-POS], [PNF-POS], [APC], [MAJ]] \n\n- \"In a VHE, this recognition network takes only small subsets of a class as input, which additionally ...\": And that also clearly leads to loss of information that could have been used in learning.[[DAT-NEU,MET-NEU], [EMP-NEU], [DIS], [MAJ]]  So there is a possibility for potential regularization but there is definitely a big loss in estimation power.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]  This is obviously possible with any regularization technique, but I think it is more of an issue here since parts of the data are not even used in learning.[[DAT-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] \n\n- \"Table 4.1 compares these log likelihoods, with VHE achieving state-of-the-art.[[RWK-NEU,TNF-NEU], [CMP-NEU], [DIS], [MIN]]  To\": Where is Table 4.1??[[TNF-NEU], [PNF-NEU], [QSN], [MIN]] \n\n- This is a minor point and did not have any impact on the evaluation but VAE --> VHE, reparameterization trick --> resampling trick.[[MET-NEU], [null], [DIS], [MIN]]  Maybe providing rather original headings is better?[[CNT], [PNF-NEU], [QSN], [MIN]]  It's a style issue that is up to tastes anyway so, again, it is minor.[[CNT], [PNF-NEU], [DIS], [MIN]]\n\n- \"However, sharing latent variables across an entire class reduces the encoding cost per element is significantly\": typo.[[CNT], [CLA-NEG], [CRT], [MIN]]\n\n- \"Figure ?? illustrates\".\n"[[TNF-NEU], [CLA-NEU], [QSN], [MIN]]