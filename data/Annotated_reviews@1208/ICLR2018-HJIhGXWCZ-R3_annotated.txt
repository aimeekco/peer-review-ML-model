"Summary: \n\nI like the general idea of learning \"output stochastic\" noise models in the paper,[[MET-POS], [EMP-POS], [APC], [MAJ]] but the idea is not fully explored (in terms of reasonable variations and their comparative performance).[[PDI-NEG], [CNT], [CRT], [MAJ]]  I don't fully understand the rationale for the experiments: I cannot speak to the reasons for the GAN's failure (GANs are not easy to train and this seems to be reflected in the results);[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]] the newly proposed model seems to improve with samples simply because the evaluation seems to reward the best sample.[[MET-POS], [EMP-POS], [APC], [MAJ]]  I.e., with enough throws, I can always hit the bullseye with a dart even when blindfolded.[[EXT-NEU], [null], [DIS], [GEN]]\n\nComments:\n\nThe model proposes to learn a conditional stochastic deep model by training an output noise model on the input x_i and the residual y_i - g(x_i).[[MET-NEU], [null], [SMY], [GEN]]  The trained residual function can be used to predict a residual z_i for x_i.[[EXP-NEU], [null], [DIS], [MIN]]  Then for out-of-sample prediction for x*, the paper appears to propose sampling a z uniformly from the training data {z_i}_i (it is not clear from the description on page 3 that this uniformly sampled z* = z_i depends on the actual x* -- as far as I can tell it does not).[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MIN]]  The paper does suggest learning a p(z|x) but does not provide implementation details nor experiment with this approach.[[EXP-NEG,MET-NEG], [SUB-NEG], [DFT], [MAJ]]\n\nI like the idea of learning an \"output stochastic\" model -- it is much simpler to train than an \"input stochastic\" model that is more standard in the literature (VAE, GAN) and there are many cases where I think it could be quite reasonable. [[MET-POS], [EMP-POS], [APC], [MAJ]] However, I don't think the authors explore the idea well enough -- they simply appear to propose a non-parametric way of learning the stochastic model (sampling from the training data z_i's) and do not compare to reasonable alternative approaches. [[EXP-NEG,MET-NEG], [CMP-NEG], [DFT,CRT], [MAJ]] To start, why not plot the empirical histogram of p(z|x) (for some fixed x's) to get a sense of how well-behaved it is as a distribution.[[EXP-NEU,MET-NEU], [EMP-NEU], [SUG], [MIN]]  Second, why not simply propose learning exponential family models where the parameters of these models are (deep nets) conditioned on the input?[[EXP-NEU,MET-NEU], [EMP-NEU], [SUG,QSN], [MIN]]  One could even start with a simple Gaussian and linear parameterization of the mean and variance in terms of x. [[EXP-NEU,MET-NEU], [EMP-NEU], [SUG], [MIN]] If the contribution of the paper is the \"output stochastic\" noise model, I think it is worth experimenting with the design options one has with such a model.[[EXP-NEU], [SUB-NEU], [SUG], [MIN]]\n\nThe experiments range over 4 video datasets. [[DAT-NEU,EXP-NEU], [null], [DIS], [GEN]] PSNR is evaluated on predicted frames -- PSNR does not appear to be explicitly defined but I am taking it to be the metric defined in the 2nd paragraph from the bottom on page 7. [[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]] The new model \"EEN\" is compared to a deterministic model and conditional GAN.[[MET-NEU], [CMP-NEU], [DIS], [MIN]]  The GAN never seems to perform well -- the authors claim mode collapse, but I wonder if the GAN was simply hard to train in the first place and this is the key reason?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]]  Unsurprisingly (since the EEN noise does not seem to be conditioned on the input), the baseline deterministic model performs quite well. [[RWK-POS,MET-POS], [CMP-POS,EMP-POS], [APC], [MAJ]] If I understand what is being evaluated correctly (i.e., best random guess) then I am not surprised the EEN can perform better with enough random samples.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]]  Have we learned anything?\n"[[CNT], [null], [QSN], [MIN]]