"This paper mainly focuses on the square loss function of linear networks.[[INT-NEU,MET-NEU], [null], [SMY], [GEN]] It provides the sufficient and necessary characterization for the forms of critical points of one-hidden-layer linear networks.[[MET-POS], [SUB-POS], [APC], [MAJ]] Based on this characterization, the authors are able to discuss different types of non-global-optimal critical points and show that every local minimum is a global minimum for one-hidden-layer linear networks.[[DAT-POS,RES-POS], [EMP-POS], [SMY,APC], [MAJ]] As an extension, the manuscript also characterizes the analytical forms for the critical points of deep linear networks and deep ReLU networks, although only a subset of non-global-optimal critical points are discussed.[[DAT-NEU,ANA-NEU], [null], [SMY], [GEN]] In general, this manuscript is well written.[[OAL-POS], [CLA-POS], [APC], [MAJ]]   \n\nPros:\n1. This manuscript provides the sufficient and necessary characterization of critical points for deep networks.[[DAT-POS,RES-POS], [SUB-POS], [APC], [MAJ]] \n2. Compared to previous work, the current analysis for one-hidden-layer linear networks doesn\u2019t require assumptions on parameter dimensions and data matrices.[[RWK-POS,ANA-POS], [CMP-POS], [APC], [MAJ]] The novel analyses, especially the technique to characterize critical points and the proof of item 2 in Proposition 3, will probably be interesting to the community.[[DAT-POS,MET-POS,ANA-POS,FWK-POS], [NOV-POS,IMP-POS], [APC], [MAJ]]\n3. It provides an example when a local minimum is not global for a one-hidden-layer neural network with ReLU activation.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\nCons:\n1. I'm concerned that the contribution of this manuscript is a little incremental.[[OAL-NEU], [SUB-NEU], [CRT], [MIN]] The equivalence of global minima and local minima for linear networks is not surprising based on existing works e.g. Hardt & Ma (2017) and Kawaguchi (2016).[[RWK-NEG,DAT-NEG], [CMP-NEG], [CRT], [MIN]]  \n2. Unlike one-hidden-layer linear networks, the characterizations of critical points for deep linear networks and deep ReLU networks seem to be hard to be interpreted.[[DAT-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] This manuscript doesn't show that every local minimum of these two types of deep networks is a global minimum, which actually has been shown by existing works like Kawaguchi (2016) with some assumptions.[[RWK-NEG,DAT-NEG], [CMP-NEG], [CRT], [MIN]] The behaviors of linear networks and practical (deep and nonlinear) networks are very different.[[MET-NEG], [CMP-NEG], [CRT], [MIN]] Under such circumstance, the results about one-hidden-layer linear networks are less interesting to the deep learning community.[[RES-NEG], [IMP-NEG,EMP-NEG], [CRT], [MAJ]]\n\nMinors:\nThere are some mixed-up notations: tilde{A_i} => A_i , and rank(A_2) => rank(A)_2 in Proposition 3."[[MET-NEG], [PNF-NEG], [CRT], [MIN]]