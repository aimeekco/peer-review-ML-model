"The paper develops an interesting approach for solving multi-class classification with softmax loss.[[INT-POS], [null], [SMY], [GEN]]\n\nThe key idea is to reformulate the problem as a convex minimization of a \"double-sum\" structure via a simple conjugation trick.[[PDI-NEU], [null], [SMY], [GEN]]  SGD is applied to the reformulation: in each step samples a subset of the training samples and labels, which appear both in the double sum.[[MET-NEU], [null], [SMY], [GEN]]  The main contributions of this paper are: \"U-max\" idea (for numerical stability reasons) and an \"\"proposing an \"implicit SGD\" idea.\n\nUnlike the first review, I see what the term \"exact\" in the title is supposed to mean. I believe this was explained in the paper.[[PDI-POS], [EMP-NEU], [SMY], [MIN]] I agree with the second reviewer that the approach is interesting.[[EXT-POS], [EMP-POS], [FBK], [MIN]] However, I also agree with the criticism (double sum formulations exist in the literature; comments about experiments); and will not repeat it here.[[EXP-NEU,MET-NEU], [EMP-NEU], [CRT], [MAJ]] I will stress though that the statement about Newton in the paper is not justified. Newton method does not converge globally with linear rate. Cubic regularisation is needed for global convergence. Local rate is quadratic.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]  \n\nI believe the paper could warrant acceptance if all criticism raised by reviewer 2 is addressed.[[EXT-NEU], [REC-NEU], [SUG], [MAJ]]\n\nI apologise for short and late review: I got access to the paper only after the original review deadline."[[EXT-NEU], [null], [DIS], [GEN]]