"This paper revisits the idea of exponentially weighted lambda-returns at the heart of TD algorithms.[[INT-NEU], [null], [SMY], [GEN]] The basic idea is that instead of geometrically weighting the n-step returns we should instead weight them according to the agent's own estimate of its confidence in it's learned value function.[[PDI-NEU], [null], [SMY], [GEN]] The paper empirically evaluates this idea on Atari games with deep non-linear state representations, compared to state-of-the-art baselines.[[RWK-NEU], [null], [SMY], [GEN]]\n\nThis paper is below the threshold because there are issues with the : 1) motivation, 2) the technical details, and (3) the empirical results.[[EXP-NEG,MET-NEG,RES-NEG], [CMP-NEG,EMP-NEG], [CRT], [MAJ]]\n\nThe paper begins by stating that the exponential weighting of lambda returns is ad-hoc and unjustified.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] I would say the idea is well justified in several ways.[[PDI-POS], [EMP-POS], [APC], [MAJ]] First the lambda return definition lends itself to online approximations that achieve a fully incremental online form with linear computation and nearly as good performance of the off-line version.[[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]] Second, decades of empirical results illustrating good performance of TD compared with MC methods. [[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]] And an extensive literature of theoretical results.[[RWK-POS,RES-POS], [EMP-POS], [APC], [MAJ]]  The paper claims that the exponential has been noted to be ad-hoc, please provide a reference for this.[[RWK-NEU,MET-NEU], [null], [DIS], [MIN]] \n\nThere have been several works that have noted that lambda can and perhaps should be changed as a function of state (Sutton and Barto, White and White [1], TD-Gammon).[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MIN]]  In fact, such works even not that lambda should be related to confidence.[[MET-NEU], [null], [DIS], [MIN]]  The paper should work harder to motivate why adapting lambda as a function of state---which has been studied---is not sufficient.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n\nI don't completely understand the objective.[[PDI-NEG], [EMP-NEG], [CRT], [MIN]] Returns with higher confidence should be weighted higher, according to the confidence estimate around the value function estimate as a function of state? [[MET-NEU], [EMP-NEU], [QSN], [MIN]]With longer returns, n>>1, the role of the value function in the target is down-weighted by gamma^n---meaning its accuracy is of little relevance to the target.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] How does your formalism take this into account?[[MET-NEU], [null], [QSN], [MIN]] The basic idea of the lambda return assumes TD targets are better than MC targets due to variance, which place more weight on shorter returns.[[MET-NEU], [CMP-NEU], [DIS], [MIN]]\n\nI addition I don't understand how learning confidence of the value function has a realizable target.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] We do not get supervised targets of the confidence of our value estimates.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] What is your network updating toward?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nThe work of Konidaris et al [1] is a more appropriate reference for this work (rather than the Thomas reference provided).[[RWK-NEU], [CMP-NEU], [DIS], [GEN]] Your paper does not very clearly different itself from Konidaris's work here.[[RWK-NEG], [CMP-NEG], [CRT], [MAJ]] Please expand on this.[[ANA-NEU], [SUB-NEU], [DIS], [GEN]]\n\nThe experiments have some issues.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]] One issue is that basic baselines could more clearly illustrate what is going on.[[RWK-NEU], [CMP-NEU], [DIS], [GEN]] There are two such baselines: random fixed weightings of the n-step returns, and persisting with the usual weighting but changing lambda on each time step (either randomly or according to some decay schedule).[[RWK-NEU], [CMP-NEU], [DIS], [GEN]] The first baseline is a sanity check to ensure that you are not observing some random effect.[[RWK-NEU], [CMP-NEU], [DIS], [GEN]] The second checks to see if your alternative weighting is simply approximating the benefits of changing lambda with time or state.[[RWK-NEU], [CMP-NEU], [DIS], [GEN]]\n\nI would say the current results indicate the conventional approach to TD is working well if not better than the new one.[[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]] Looking at fig 3, its clear the kangaroo is skewing the results, and that overall the new method is performing worse.[[MET-NEG,RES-NEG,TNF-NEG], [EMP-NEG], [CRT], [MIN]] This is further conflated by fig7 which attempts to illustrate the quality of the learned value functions.[[TNF-NEU], [null], [DIS], [GEN]] In Kangaroo, the domain where your method does best, the l2 error is worse.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] On the other hand in sea quest and space invaders, where your method does worse, the l2 error is better.[[RES-POS], [EMP-POS], [APC], [MAJ]] These results seem conflicting, or at least raise more questions than they answer.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] \n\n[1] A Greedy Approach to Adapting the Trace Parameter for Temporal Difference Learning .[[BIB-NEU], [null], [DIS], [GEN]]  Adam White and Martha White.[[BIB-NEU], [null], [DIS], [GEN]]  Autonomous Agents and Multi-agent Systems (AAMAS), 2016[[BIB-NEU], [null], [DIS], [GEN]] \n[2] G. D. Konidaris, S. Niekum, and P. S. Thomas. TD\u03b3: Re-evaluating complex backups in temporal difference learning.[[BIB-NEU], [null], [DIS], [GEN]]  In Advances in Neural Information Processing Systems 24, pages 2402\u20132410. 2011. "[[BIB-NEU], [null], [DIS], [GEN]] 