"This paper proposes an activation function, called displaced ReLU (DReLU), to improve the performance of CNNs that use batch normalization.[[INT-NEU], [null], [SMY], [GEN]] Compared to ReLU, DReLU cut the identity function at a negative value rather than the zero.[[MET-NEU], [null], [SMY], [GEN]] As a result, the activations outputted by DReLU can have a mean closer to 0 and a variance closer to 1 than the standard ReLU. [[MET-NEU,RES-NEU], [EMP-NEU], [DIS], [GEN]]The DReLU is supposed to remedy the problem of covariate shift better.[[MET-NEU], [null], [DIS], [GEN]] \n\nThe presentation of the paper is clear.[[OAL-POS], [PNF-POS], [APC], [GEN]] The proposed method shows encouraging results in a controlled setting (i.e., all other units, like dropout, are removed).[[MET-POS], [null], [APC], [GEN]] Statistical tests are performed for many of the experimental results, which is solid.[[EXP-POS], [EMP-POS], [DIS], [GEN]]\n\nHowever, I have some concerns. \n1) As DReLU(x) = max{-\\delta, x}, what is the optimal strategy to determine \\delta? [[MET-NEU], [EMP-NEU], [QSN], [MIN]]If it is done by hyperparameter tuning with cross-validation, the training cost may be too high.[[MET-NEU], [null], [DIS], [GEN]]\n2) I believe the control experiments are encouraging,[[EXP-POS], [EMP-POS], [DIS], [GEN]] but I do not agree that other techniques like Dropouts are not useful.[[MET-NEG], [EMP-NEG], [DIS], [GEN]] Using DReLU to improve the state-of-art neural network in an uncontrolled setting is important.[[MET-POS], [EMP-POS], [DIS], [GEN]] The arguments for skipping this experiments are respectful,[[MET-POS], [EMP-POS], [DIS], [GEN]] but not convincing enough.[[MET-NEU], [EMP-NEU], [DIS], [GEN]]  \n3) Batch normalization is popular, especially for the convolutional neural networks.[[MET-NEU], [null], [DIS], [GEN]] However, its application is not universal, which can limit the use of the proposed DReLU. It is a minor concern, anyway.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \n\n\n"