"This paper proposes two contributions: first, applying CNNs+self-attention modules instead of LSTMs, which could result in significant speedup and good RC performance; second, enhancing the RC model training with passage paraphrases generated by a neural paraphrasing model, which could improve the RC performance marginally.[[INT-NEU,PDI-NEU,DAT-NEU,RES-NEU], [null], [SMY], [GEN]]\n\nFirstly, I suggest the authors rewrite the end of the introduction.[[INT-NEG], [CLA-NEG], [SUG], [MIN]] The current version tends to mix everything together and makes the misleading claim.[[INT-NEG, OAL-NEG], [NOV-NEG], [CRT], [MAJ]] When I read the paper, I thought the speeding up mechanism could give both speed up and performance boost, and lead to the 82.2 F1.[[MET-NEU,RES-NEU], [EMP-NEU], [DIS], [MIN]] But it turns out that the above improvements are achieved with at least three different ideas: (1) the CNN+self-attention module; (2) the entire model architecture design; and (3) the data augmentation method.[[MET-NEU,RES-NEU], [EMP-NEU], [DIS], [MIN]] \n\nSecondly, none of the above three ideas are well evaluated in terms of both speedup and RC performance, and I will comment in details as follows:[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]]\n\n(1) The CNN+self-attention was mainly borrowing the idea from (Vaswani et al., 2017a) from NMT to RC.[[RWK-NEU,DAT-NEU], [NOV-NEU], [DIS], [MIN]] The novelty is limited[[PDI-NEG,OAL-NEG], [NOV-NEG], [CRT], [MAJ]] but it is a good idea to speed up the RC models.[[RES-POS], [EMP-POS], [APC], [MAJ]] However, as the authors hoped to claim that this module could contribute to both speedup and RC performance, it will be necessary to show the RC performance of the same model architecture, but replacing the CNNs with LSTMs.[[DAT-NEU,RES-NEU,ANA-NEU], [SUB-NEU,EMP-NEU], [DIS], [MIN]] Only if the proposed architecture still gives better results, the claims in the introduction can be considered correct.[[INT-NEU,RES-NEU], [EMP-NEU], [DIS], [MAJ]]\n\n(2) I feel that the model design is the main reason for the good overall RC performance.[[MET-NEU], [EMP-NEU], [SUG,DIS], [MIN]] However, in the paper there is no motivation about why the architecture was designed like this.[[MET-NEG], [SUB-NEG,EMP-NEG], [DFT,CRT], [MAJ]] Moreover, the whole model architecture is only evaluated on the SQuAD dataset.[[DAT-NEG], [SUB-NEG], [CRT], [MIN]] As a result, it is not convincing that the system design has good generalization.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] If in (1) it is observed that using LSTMs in the model instead of CNNs could give on par or better results, it will be necessary to test the proposed model architecture on multiple datasets, as well as conducting more ablation tests about the model architecture itself.[[DAT-NEG,EXP-NEG,MET-NEU,ANA-NEU], [EMP-NEG], [SUG], [MAJ]]\n\n(3) I like the idea of data augmentation with paraphrasing.[[MET-POS], [EMP-POS], [APC], [MAJ]] Currently, the improvement is only marginal,[[RES-NEU], [EMP-NEU], [DIS], [MIN]] but there seems many other things to play with.[[FWK-POS], [IMP-POS], [APC], [GEN]] For example, training NMT models with larger parallel corpora; training NMT models with different language pairs with English as the pivot; and better strategies to select the generated passages for data augmentation.\[[DAT-NEU,MET-POS,FWK-POS], [IMP-POS], [APC], [MAJ]]n\nI am looking forward to the test performance of this work on SQuAD."[[DAT-NEU,FWK-NEU], [IMP-NEU], [DIS], [MIN]]