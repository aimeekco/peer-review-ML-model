"The main concern of this submission is the novelty.[[OAL-NEU], [NOV-NEU], [SMY], [GEN]] Proposed method to visualize the loss function sounds too incremental from existing works.[[RWK-POS,PDI-POS,EXP-POS], [SUB-POS,EMP-POS], [APC], [MAJ]] One of the main distinctions is using filter-wise normalization, but it is somehow trivial.[[RWK-NEU], [EMP-NEU], [SMY], [GEN]] In experiments, no comparisons against existing works is performed (at least on toy/controlled environments).[[RWK-NEG], [CMP-NEG], [DFT,CRT], [MIN]] Some findings in this submission indeed look interesting,[[RWK-POS], [IMP-POS], [APC], [MAJ]] but it is not clear if those results are something difficult to find with other existing standard ways,[[RWK-NEU,RES-NEG], [CLA-NEG,IMP-NEG], [DFT], [MIN]] or even how reliable they are since the effectiveness has not been evaluated.[[RWK-NEU,EXP-NEG], [EMP-NEG], [DFT,CRT], [MIN]] \n\nMinor comments: \nIn introduction, parameter with zero training error doesn't mean it's a global minimizer\nIn section 2, it is not clear that visualizing loss function is helpful in see the reasons of generalization given minima.[[RWK-NEU,EXP-NEG,MET-NEG], [CLA-NEG,EMP-NEG], [DFT,CRT], [MIN]] \nIn figure 2, why do we have solutions at 0 for small batch size and 1 for large batch size case?[[RWK-NEG,TNF-NEG], [IMP-NEG], [DFT,CRT], [MIN]] (why should they be different?)[[RWK-NEU], [null], [QSN], [GEN]]\n"