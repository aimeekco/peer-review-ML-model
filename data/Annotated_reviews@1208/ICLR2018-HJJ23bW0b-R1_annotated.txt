"I was very confused by some parts of the paper that are simple copy-past from the paper of Downey et al.[[RWK-NEG], [CNT], [CRT], [MAJ]]  which has been accepted for publication in NIPS.[[EXT-NEU], [null], [SMY], [GEN]] In particular, in section 3, several sentences are taken as they are from the Downey et al.\u2019s paper.[[RWK-NEU], [null], [SMY], [GEN]] Some examples :\n\n\u00ab\u00a0provide a compact representation of a dynamical system\nby representing state as a set of predictions of features of future observations.[[RWK-NEU], [null], [SMY], [GEN]]\u00a0\u00bb \n\n\u00ab\u00a0a predictive state is defined as\u2026 , where\u2026  is a vector of features of future observations and ...  is a vector of\nfeatures of historical observations.[[RWK-NEU], [null], [DIS], [GEN]] The features are selected such that ...  determines the distribution\nof future observations \u2026 Filtering is the process of mapping a predictive state\u2026\u00a0\u00bb[[RWK-NEU], [null], [DIS], [GEN]]\nEven the footnote has been copied & pasted: \u00ab\u00a0For convenience we assume that the system is k-observable: that is, the distribution of all future observations\nis determined by the distribution of the next k observations. (Note: not by the next k observations\nthemselves.)[[RWK-NEU], [null], [DIS], [MIN]] At the cost of additional notation, this restriction could easily be lifted.[[MET-NEU], [null], [DIS], [MIN]]\u00a0\u00bb\n\u00ab\u00a0 This approach is fast, statistically consistent, and reduces to simple\nlinear algebra operations.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\u00a0\u00bb \n\nNormally, I should have stopped reviewing, but I decided to continue  since those parts only concerned the preliminaries part.[[EXT-NEU], [null], [DIS], [MIN]]\n\nA key element in PSRNN is to used as an initialization a kernel ridge regression.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] The main result here, is to show that using orthogonal random features approximates well the original kernel comparing to random fourrier features as considered in PSRNN.[[RES-NEU], [null], [DIS], [MAJ]] This result is formally stated and proved in the paper.[[RES-NEU], [EMP-NEU], [DIS], [MAJ]]\n\nThe paper comes with some experiments in order to empirically demonstrate the superiority  orthogonal random features over RFF.[[EXP-NEU], [null], [SMY], [GEN]] Three data sets are considered (Swimmer,  Mocap and  Handwriting).[[DAT-NEU], [null], [SMY], [GEN]]  \n\nI found it that the contribution of the paper is very limited.[[OAL-NEG], [SUB-NEG], [CRT], [MAJ]]  The connexion to PSRNN is very tenuous since the main results are about the regression part.[[RES-NEU], [EMP-NEU], [DIS], [MAJ]]  in Theorems 2 and 3 there are no mention to PSRNN.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] \n\nAlso the experiment is not very convincing.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]]  The datasets are too small with observations in low dimensions, and I found it not very fair to consider LSTM in such settings.[[DAT-NEG,MET-NEG], [SUB-NEG,EMP-NEG], [DFT,CRT], [MAJ]] \n\nSome minor remarks:\n\n- p3: We use RFs-> RFFs\n- p5: ||X||, you mean |X| the size of the dataset\n- p12: Eq (9).[[DAT-NEG], [SUB-NEG], [DFT], [MIN]]  You need to add \u00ab\u00a0with probability $1-\\rho$ as in Avron\u2019s paper.[[CNT], [EMP-NEU], [DIS], [MIN]] \n- p12: the derivation of Eq (10) from Eq (9) needs to be detailed.[[MET-NEU], [EMP-NEU], [SUG], [MIN]]    \n\n\nI thank the author for their detailed answers.[[EXT-NEU], [null], [DIS], [GEN]]  Some points have been clarified but other still raise issues.[[EXT-NEU], [null], [DIS], [MIN]] In particular, I continue thinking that the contribution is limited.[[OAL-NEG], [SUB-NEG], [CRT], [MAJ]] Accordingly, I did not change my scores."[[OAL-NEU], [REC-NEU], [FBK], [MAJ]]