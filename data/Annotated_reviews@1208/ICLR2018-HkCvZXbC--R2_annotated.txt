"[Overview]\n\nThis paper proposed a new generative adversarial network, called 3C-GAN for generating images in a composite manner.[[RWK-NEU,PDI-NEU,MET-NEU], [null], [SMY], [GEN]] In 3C-GAN, the authors exploited two generators, one (G1) is for generating context images, and the other one (G2) is for generating semantic contents.[[RWK-NEU,EXP-NEU,MET-NEU], [null], [SMY], [GEN]] To generate the semantic contents, the authors introduced a conditional GAN scheme, to force the generated images to match the annotations.[[RWK-NEU,PDI-NEU,MET-NEU], [null], [SMY], [GEN]] After generating both parts in parallel, they are combined using alpha blending to compose the final image.[[RWK-NEU], [null], [SMY], [GEN]] This generated image is then sent to the discriminator.[[RWK-NEU], [null], [SMY], [GEN]] The experiments were conducted on three datasets, MNIST, SVHN and MS-CelebA.[[RWK-NEU,DAT-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] The authors showed qualitative results on all three datasets, demonstrating that AC-GAN could disentangle the context part from the semantic part in an image, and generate them separately.[[RWK-NEU,PDI-NEU,DAT-NEU,EXP-NEU,MET-NEU], [null], [SMY], [GEN]]\n\n[Strenghts]\n\nThis paper introduced a layered-wise image generation, which decomposed the image into two separate parts: context part, and semantic part.[[INT-NEU,MET-NEU], [null], [SMY], [GEN]] Corresponding to these two parts are two generators.[[RWK-NEU], [null], [SMY], [GEN]] To ensure this, the authors introduced three strategies:\n\n1. Adding semantic labels: the authors used image semantic labels as the input and then exploited a conditional GAN to enforce one of the generators to generate semantic parts of images.[[RWK-NEU,PDI-NEU,EXP-NEU,MET-NEU], [null], [SMY], [GEN]] As usual, the label information was added as the input of generator and discriminator as well.[[RWK-NEU,ANA-POS], [CMP-POS,EMP-POS], [APC], [MAJ]]\n\n2. Adding label difference cost: the intuition behind this loss is that changing the label condition should merely affect the output of G2.[[RWK-NEU,PDI-NEU], [null], [SMY], [GEN]] Based on this, outputs of Gc should not change much when flipping the input labels.[[RWK-NEG,RES-NEG], [IMP-NEG], [CRT], [MIN]]\n\n3. Adding exclusive prior: the prior is that the masks of context part (m1) and semantic part (m2) should be exclusive to each other.[[RWK-NEU], [null], [SMY], [GEN]] Therefore, the authors added another loss to reduce the sum of component-wise multiplication between m1 and m2.[[PDI-NEU], [EMP-NEU], [SMY], [GEN]]\n\nDecomposing the semantic part from the context part in an image based on a generative model is an interesting problem.[[PDI-POS], [EMP-POS], [APC,DIS], [MAJ]] However, to my opinion, completing it without any supervision is challenging and meaningless.[[OAL-NEG], [SUB-NEG], [DFT], [MIN]] In this paper, the authors proposed a conditional way to generate images compositionally.[[RWK-NEU,PDI-NEU,MET-NEU], [null], [SMY], [GEN]] It is an interesting extension of previous works, such as Kwak & Zhang (2016) and Yang (2017).[[RWK-POS,BIB-POS], [IMP-POS], [APC], [MAJ]]\n\n[Weaknesses]\n\nThis paper proposed an interesting and intuitive image generation model.[[PDI-POS,MET-POS], [EMP-POS], [APC], [MAJ]] However, there are several weaknesses existed:\n\n1.[[OAL-NEG], [SUB-NEG], [DFT], [MIN]] There is no quantitative evaluation and comparisons.[[ANA-NEG,OAL-NEG], [CMP-NEG], [DFT,CRT], [MIN]] From the limited qualitative results shown in Fig.2-10, we can hardly get a comprehensive sense about the model performance.[[RWK-NEG,RES-NEG,TNF-NEG], [IMP-NEG], [DFT], [MIN]] The authors should present some quantitative evaluations in the paper, which are more persuasive than a number of examples.[[RWK-NEG,OAL-NEG], [SUB-NEG], [DFT], [MIN]] To do that, I suggest the authors exploited evaluation metrics, such as Inception Score to evaluate the overall generation performance.[[RWK-NEU], [null], [SUG], [GEN]] Also, in Yang (2017) the authors proposed adversarial divergence, which is suitable for evaluating the conditional generation.[[PDI-NEU,ANA-NEU,BIB-NEU], [null], [SMY], [GEN]] Hence, I suggest the authors use a similar way to evaluate the classification performance of classification model trained on the generated images.[[RWK-NEU,EXP-NEU,MET-NEU], [null], [SUG], [GEN]] This should be a good indicator to show whether the proposed 3C-GAN could generate more realistic images which facilitate the training of a classifier.[[RWK-NEU,PDI-NEU], [null], [SMY], [GEN]]\n\n2. The authors should try more complicated datasets, like CIFAR-10.[[RWK-NEU,DAT-NEU], [null], [SUG], [GEN]] Recently, CIFAR-10 has become a popular dataset as a testbed for evaluating various GANs.[[RWK-NEU,DAT-NEU,EXP-NEU], [null], [SMY], [GEN]] It is easy to train since its low resolution, but also means a lot since it a relative complicated scene.[[RWK-NEU,EXP-NEU], [null], [SMY], [GEN]] I would suggest the authors also run the experiments on CIFAR-10.[[RWK-NEU,EXP-NEU], [null], [SUG], [GEN]]\n\n3. The authors did not perform any ablation study.[[RWK-NEG,OAL-NEG], [SUB-NEG], [DFT], [MIN]] Apart from several generation results based on 3C-GAN, iIcould not found any generation results from ablated models.[[RWK-NEU,EXP-NEU,MET-NEU], [null], [SMY], [GEN]] As such, I can hardly get a sense of the effects of different losses and know about the relative performance in the whole GAN spectrum. [[RWK-NEG,EXP-NEG,MET-NEG], [SUB-NEG,EMP-NEG], [DFT], [MIN]]I strongly suggest the authors add some ablation studies.[[RWK-NEG], [SUB-NEG], [DFT], [MIN]] The authors should at least compare with one-layer conditional GAN. [[RWK-NEU,MET-NEU], [CMP-NEU], [SUG], [GEN]]\n\n4. The proposed model merely showed two-layer generation results.[[RWK-NEG,RES-NEG], [SUB-NEG,EMP-NEG], [DFT], [MIN]] There might be two reasons: one is that it is hard to extend it to more layer generation as I know, and the other one reason is the inflexible formulation to compose an image in 2.2.1 and formula (6).[[EXT-NEU], [null], [DIS], [GEN]] The authors should try some datasets like MNIST-TWO in Yang (2017) for demonstration.[[RWK-NEU,DAT-NEU], [null], [SUG], [GEN]]\n\n5. Please show f1, m1, f2, m2 separately, instead of showing the blending results in Fig3, Fig4, Fig6, Fig7, Fig9, and Fig10.[[RWK-NEG,TNF-NEG], [SUB-NEG,PNF-NEG], [DFT], [MIN]] I would like to see what kind of context image and foreground image 3C-GAN has generated so that I can compare it with previous works like Kwak & Zhang (2016) and Yang (2017).[[RWK-NEU], [null], [SMY], [GEN]]\n\n6. I did not understand very well the label difference loss in (5).[[RWK-NEG,OAL-NEG], [IMP-NEG], [DFT], [MIN]] Reducing the different between G_c(z_u, z_v, z_l) and G_c(z_u, z_v, z_l^f) seems not be able to force G1 and G2 to generate different parts of an image.[[RWK-NEU,EXP-NEU], [null], [SMY], [GEN]] G2 takes all the duty  can still obtain a lower L_ld.[[RWK-NEU], [null], [SMY], [GEN]] From my point of view, the loss should be added to G1 to make G1 less prone to the variation of label information.[[RWK-NEU,EXP-NEU,ANA-NEU], [null], [SMY], [GEN]]\n\n7. Minor typos and textual errors. In Fig.1, should the right generator be G2 rather than G1? In 2.1.3 and 2.2.1, please add numbers to the equations.[[RWK-NEG,OAL-NEG], [PNF-NEG], [QSN,CRT], [GEN]]\n\n[Summary]\n\nThis paper proposed an interesting way of generating images, called 3C-GAN.[[PDI-POS,OAL-POS], [IMP-POS,EMP-POS], [APC], [MAJ]] It generates images in a layer-wise manner.[[RWK-NEU], [null], [SMY], [GEN]] To separate the context and semantic part in an image, the authors introduced several new techniques to enforce the generators in the model undertake different duties. [[RWK-NEU], [EMP-NEU], [SMY], [GEN]]In the experiments, the authors showed qualitative results on three datasets, MNIST, SVHN and CelebA.[[RWK-NEU,DAT-NEU,EXP-NEU,RES-NEU], [null], [SMY], [GEN]] However, as I pointed out above, the paper missed quantitative evaluation and comparison, and ablation study.[[RWK-NEG], [CMP-NEG], [DFT], [MIN]] Taking all these into account, I think this paper still needs more works to make it solid and comprehensive before being accepted.[[OAL-NEG], [SUB-NEG], [DFT], [MIN]]"
