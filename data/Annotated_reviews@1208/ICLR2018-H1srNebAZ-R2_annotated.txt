"The paper proposes to study the behavior of activations during training and testing to shed more light onto the inner workings of neural networks.[[INT-NEU], [null], [SMY], [GEN]] This is an important area and findings in this paper are interesting![[PDI-POS], [null], [APC], [MAJ]]\n\nHowever, I believe the results are preliminary and the paper lacks an adequate explanation/hypothesis for the observed phenomenon either via a theoretical work or empirical experiments.[[MET-NEU,RES-NEU], [EMP-NEG], [DFT], [MAJ]]\n- Could we look at the two distributions of inputs that each neuron tries to separate?[[MET-NEU], [null], [QSN], [MIN]] \n- Could we perform more extensive empirical study to substantiate the phenomenon here? Under which conditions do neurons behave like binary classifiers? (How are network width/depth, activation functions affect the results).[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nAlso, a binarization experiment (and finding) similar to the one in this paper has been done here:\n[1] Argawal et al. Analyzing the Performance of Multilayer Neural Networks for Object Recognition. [[RWK-NEU,EXP-NEU,BIB-NEU], [CMP-NEU], [DIS], [MIN]]2014\n\n+ Clarity: The paper is easy to read.[[OAL-POS], [CLA-POS], [APC], [MAJ]] A few minor presentation issues:\n- ReLu --> ReLU\n\n+[[OAL-NEU], [PNF-NEG], [DFT], [MIN]] Originality: \nThe paper is incremental work upon previous research (Tishby et al. 2017; Argawal et al 2014).[[RWK-NEU,BIB-NEU], [CMP-POS], [DIS], [MIN]]\n\n+ Significance:\nWhile the results are interesting,[[RES-POS], [null], [APC], [MIN]] the contribution is not significant as the paper misses an important explanation for the phenomenon.[[MET-NEU], [IMP-NEG], [DFT], [MIN]] I'm not sure what key insights can be taken away from this.[[OAL-NEG], [IMP-NEG], [DFT], [MAJ]]\n\n\n"