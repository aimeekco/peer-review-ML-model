"This paper introduces sparse persistent RNNs, a mechanism to add pruning to the existing work of stashing RNN weights on a chip.[[INT-NEU], [null], [SMY], [GEN]] The paper describes the use additional mechanisms for synchronization and memory loading.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] \n\nThe evaluation in the main paper is largely on synthetic workloads (i.e. large layers with artificial sparsity).[[MET-NEU], [null], [SMY], [GEN]]  With evaluation largely over layers instead of applications, I was left wondering whether there is an actual benefit on real workloads.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Furthermore, the benefit over dense persistent RNNs for OpenNMT application (of absolute 0.3-0.5s over dense persistent rnns?) did not appear significant unless you can convince me otherwise.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] \n\nStoring weights persistent on chip should give a sharp benefit when all weights fit on the chip.[[MET-NEU,RES-NEU], [EMP-NEU], [DIS], [MIN]] One suggestion I have to strengthen the paper is to claim that due to pruning, now you can support a larger number of methods or method configurations and to provide examples of those.[[MET-NEU], [EMP-NEU], [SUG], [MIN]]\n\nTo summarize, the paper adds the ability to support pruning over persistent RNNs.[[MET-POS,OAL-POS], [EMP-POS], [APC], [MAJ]] However, Narang et. al., 2017 already explore this idea, although briefly.[[RWK-NEG], [CMP-NEU], [DIS], [MIN]] Furthermore, the gains from the sparsity appear rather limited over real applications.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] I would encourage the authors to put the NMT evaluation in the main paper (and perhaps add other workloads).[[MET-POS], [PNF-NEU], [SUG], [MAJ]] Furthermore, a host of techniques are discussed (Lamport timestamps, memory layouts) and implementing them on GPUs is not trivial.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] However, these are well known and the novelty or even the experience of implementing these on GPUs should be emphasized."[[MET-NEU], [SUB-NEU], [DIS], [MIN]]