"The authors introduce the concept of angle bias (angle between a weight vector w and input vector x)  by which the resultant pre-activation (wx) is biased if ||x|| is non-zero or ||w|| is non-zero (theorm 2 from the article).[[INT-NEU,PDI-NEU,MET-NEU], [null], [SMY], [GEN]] The angle bias results in almost constant activation independent of input sample resulting in no weight updates for error reduction.[[MET-NEU,RES-NEU], [null], [SMY], [GEN]]   Authors chose to add an additional optimization constraint LCW (|w|=0) to achieve zero-mean pre-activation while, as mentioned in the article, other methods like batch normalization BN tend to push for |x|=0 and unit std to do the same.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \n\nClearly, because of lack of scaling factor incase of LCW, like that in BN, it doesnot perform well when used with ReLU.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] When using with sigmoid the activation being bouded (0,1) seems to compensate for the lack of scaling in input.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MIN]] While BN explicitly makes the activation zero-mean LCW seems to achieve it through constraint on the weight features.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Though it is shown to be computationally less expensive LCW seems to work in only specific cases unlike BN."[[MET-NEU], [CMP-NEU,EMP-NEU], [CRT], [MIN]]