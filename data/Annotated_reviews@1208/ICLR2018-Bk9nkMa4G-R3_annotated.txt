"This paper presents a method based on a Bayesian classifier that improves classification of rare classes in datasets with long tail class distributions.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The method is based on balance the class-priors to generalize well for rare classes.[[MET-NEU], [null], [SMY], [GEN]] By using a Gaussian Mixture Model (GMM), authors are able to obtain a factorization of class-likelihoods and class-priors leading to a closed-form maximum likelihood estimation that can be integrated to differente classification models, such as current deep learning classifiers.[[MET-NEU,RES-NEU], [null], [SMY], [GEN]] Authors, also propose an evaluation approach that addresses the bias towards the head and intra-class-variation of classes in the tail.[[MET-NEU], [null], [SMY], [GEN]]\n\nThey face class imbalance problems, particular long tail distributions, by fixing: i) The covariance matrices of all the classes to be the identity, and ii) The priors over each class to be uniform.[[EXP-NEU], [null], [SMY], [GEN]] So all classes, popular and rare, have equal weight for Bayesian classification.[[MET-NEU], [null], [SMY], [GEN]] To me, this is not a fundamental way to solve the long-tail problem, in the sense that by fixing isotropic likelihoods and flat priors, authors are also ignoring information that can be relevant in some classification problems, where a good prior can be useful to disambiguate confusing situations.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] On other hand, using a unimodal function to model each class is an over-simplification that ignores intra-class complexity.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] \n\nThe datasets used by the authors are balanced, so they artificially transform them into long-tailed,[[DAT-POS], [EMP-POS], [APC], [MIN]] it will be good to test directly on real long-tailed datasets.[[DAT-NEU], [SUB-NEU], [SUG], [MIN]] The experimentation is only performed using small to medium datasets (< 80K instances), it will be good to show if the benefits of the proposed approach can also be present in the case of large datasets.[[DAT-NEG,EXP-NEU,MET-NEU], [SUB-NEG], [DFT], [MIN]] In this sense, I agree with the authors that the evaluation protocol for long-tailed datasets can't be just based on average accuracy, however, the protocol proposed requires to train the model several times, therefore, it does not scale properly to large datasets that are the common rule in the deep learning world.[[DAT-NEG,MET-NEG], [SUB-NEG,EMP-NEG], [DFT,CRT], [MIN]] Results respect to similar state-of-the-art techniques shows a reasonable improvement (depends of the dataset, approx. 1-3%)."[[DAT-NEU,RES-POS], [CMP-POS], [APC], [MAJ]]