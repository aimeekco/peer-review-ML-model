"The paper consider a method for \"weight normalization\" of layers of a neural network. [[INT-NEU,MET-NEU], [null], [SMY], [GEN]] The weight matrix is maintained normalized, which helps accuracy.[[MET-POS,RES-NEU], [EMP-POS], [APC], [MIN]]  However, the simplest way to normalize on a fully connected layer is quadratic (adding squares of weights and taking square root).[[MET-NEU], [CNT], [DIS], [GEN]]\n\n The paper proposes \"FastNorm\", which is a way to implicitly maintain the normalized weight matrix using much less computation.[[MET-NEU], [null], [SMY], [GEN]]  Essentially, a normalization vector is maintained an updated separately.[[MET-NEU], [null], [SMY], [GEN]] \n\n  Pros:   Natural method to do weight normalization efficeintly\n\n[[MET-POS], [EMP-POS], [APC], [MAJ]]  Cons:   A very natural and simple solution that is fairly obvious.[[RES-NEG], [CNT], [CRT], [MIN]]\n\n          Limited experiments \n\n"[[EXP-NEG], [SUB-NEG], [CRT], [MIN]]