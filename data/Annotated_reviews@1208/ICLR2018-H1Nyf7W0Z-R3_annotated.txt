"Summary of the paper: \n\nThis paper presents a method, called \\alpha-DM (the authors used this name because they are using \\alpha-Divergence to measure the distance between two distributions), [[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] that addresses three important problems simultaneously: \n(a) Objective score discrepancy: i.e., in ML we minimize a cost function but we measure performance using something else, e.g., minimizing cross entropy and then measuring performance using BLEU score in Machine Translation (MT).[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] \n(b) Sampling distribution discrepancy: The model is trained using samples from true distribution but evaluated using samples from the learned distribution\n[[DAT-NEU], [null], [SMY], [GEN]](c) Sample inefficiency: The RL model might rarely draw samples with high rewards which makes it difficult to compute gradients accurately for objective function\u2019s optimization[[DAT-NEU,MET-NEU], [null], [SMY], [GEN]] \n\nThen the authors present the results for machine translation task and also analysis of their proposed method.[[MET-NEU,RES-NEU,ANA-NEU], [null], [SMY], [GEN]]\n\nMy comments / feedback: \n\nThe paper is well written and the problem addressed by the paper is an important one.[[PDI-POS,OAL-POS], [CLA-POS], [APC], [MAJ]] My main concerns about this work are have two aspects: \n(a)\tNovelty\n1.\tThe idea is a good one and is great incremental research building on the top of previous ideas.[[PDI-POS], [NOV-POS], [APC], [MAJ]] I do not agree with statements like \u201cWe demonstrate that the proposed objective function generalizes ML and RL objective functions \u2026\u201d that authors have made in the abstract. There is not enough evidence in the paper to validate this statement.[[ABS-NEG,MET-NEU], [EMP-NEG], [CRT], [MAJ]]\n(b)\tExperimental Results\n2.\tThe performance of the proposed method is not significantly better than other models in MT task.[[EXP-NEU,RES-NEU], [EMP-NEU], [DFT], [MAJ]] I am also wondering why authors have not tried their method on at least one more task?[[EXP-NEU], [null], [QSN], [MAJ]] E.g., in CNN+LSTM based image captioning, the perplexity is minimized as cost function but the performance is measured by BLEU etc.[[MET-NEU], [EMP-NEU], [DFT], [MAJ]]  \n\nSome minor comments: \n\n1.\tIn page 2, 6th line after eq (1), \u201c\u2026 these two problems\u201d --> \u201c\u2026 these three problems\u201d \n2.[[MET-NEU], [CLA-NEG], [CRT], [MIN]]\tIn page 2, the line before the last line, \u201c\u2026 resolbing problem\u201d --> \u201c\u2026 resolving problem\u201d\n"[[MET-NEU], [CLA-NEG], [CRT], [MIN]]