"The paper proposes a technique for training quantized neural networks, where the precision (number of bits) varies per layer and is learned in an end-to-end fashion.[[PDI-NEU], [null], [SMY], [GEN]] The idea is to add two terms to the loss, one representing quantization error, and the other representing the number of discrete values the quantization can support (or alternatively the number of bits used).[[PDI-NEU], [null], [SMY], [GEN]] Updates are made to the parameter representing the # of bits via the sign of its gradient.[[PDI-NEU], [null], [SMY], [GEN]] Experiments are conducted using a LeNet-inspired architecture on MNIST and CIFAR10.[[DAT-NEU,EXP-NEU], [null], [SMY], [GEN]]\n\nOverall, the idea is interesting, as providing an end-to-end trainable technique for distributing the precision across layers of a network would indeed be quite useful.[[PDI-POS,MET-POS], [EMP-POS], [APC], [MAJ]] I have a few concerns: First, I find the discussion around the training methodology insufficient.[[EXP-NEG,MET-NEG], [SUB-NEG], [DFT], [MAJ]] Inherently, the objective is discontinuous since # of bits is a discrete parameter.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] This is worked around by updating the parameter using the sign of its gradient.[[EXP-NEU,MET-NEU], [null], [DIS], [MIN]] This is assuming the local linear approximation given by the derivative is accurate enough one integer away; this may or may not be true, but it's not clear and there is little discussion of whether this is reasonable to assume.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MIN]]\n\nIt's also difficult for me to understand how this interacts with the other terms in the objective (quantization error and loss).[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MIN]] We'd like the number of bits parameter to trade off between accuracy (at least in terms of quantization error, and ideally overall loss as well) and precision.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]] But it's not at all clear that the gradient of either the loss or the quantization error w.r.t. the number of bits will in general suggest increasing the number of bit (thus requiring the bit regularization term).[[EXP-NEG], [EMP-NEG], [CRT], [MIN]] This will clearly not be the case when the continuous weights coincide with the quantized values for the current bit setting.[[MET-NEU], [null], [DIS], [MIN]] More generally, the direction of the gradient will be highly dependent on the specific setting of the current weights.[[MET-NEU], [null], [DIS], [MAJ]] It's unclear to me how effectively accuracy and precision are balanced by this training strategy, and there isn't any discussion of this point either.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nI would be less concerned about the above points if I found the experiments compelling.[[EXP-NEU], [EMP-NEU], [DIS], [MAJ]] Unfortunately, although I am quite sympathetic to the argument that state of the art results or architectures aren't necessary for a paper of this kind, the results on MNIST and CIFAR10 are so poor that they give me some concern about how the training was performed and whether the results are meaningful.[[EXP-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] Performance on MNIST in the 7-11% test error range is comparable to a simple linear logistic regression model; for a CNN that is extremely bad.[[DAT-NEG,MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] Similarly, 40% error on CIFAR10 is worse than what some very simple fully connected models can achieve.[[DAT-NEG,MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nOverall, while I like the and think the goal is good,[[MET-POS], [EMP-POS], [APC], [MAJ]] I think the motivation and discussion for the training methodology is insufficient,[[EXP-NEG,MET-NEG], [SUB-NEG], [DFT], [MAJ]] and the empirical work is concerning.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] I can't recommend acceptance. "[[OAL-NEG], [REC-NEG], [FBK], [MAJ]]