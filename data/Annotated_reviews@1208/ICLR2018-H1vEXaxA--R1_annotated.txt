"Summary: \n\nThis paper proposes a multi-agent communication task where the agents learn to translate as a side-product to solving the communication task. [[INT-NEU,RWK-NEU,PDI-NEU], [null], [SMY], [GEN]]Authors use the image modality as a bridge between two different languages and the agents learn to ground different languages to same image based on the similarity[[PDI-NEU,ANA-NEU], [null], [SMY], [GEN]]. This is achieved by learning to play the game in both directions. Authors show results in a word-level translation task and also a sentence-level translation task. They also show that having more languages help the agent to learn better[[PDI-NEU,RES-POS], [null], [APC], [MAJ]].\n\nMy comments:\n\nThe paper is well-written and I really enjoyed reading this paper.[[OAL-POS], [CLA-POS], [APC], [MAJ]] While the idea of pivot based common representation learning for language pairs with no parallel data is not new, adding the communication aspect as an additional supervision is novel.[[PDI-NEU,DAT-NEU,ANA-NEU], [null], [DIS], [GEN]] However I would encourage authors to rephrase their claim of emergent translation (the title is misleading) as the authors pose this as a supervised problem and the setting has enough constraints to learn a common representation for both languages (bridged by the image) and hence there is no autonomous emergence of translation out of need.[[INT-NEU,PDI-NEG,ANA-NEG], [EMP-NEG], [CRT], [MAJ]] I see this work as adding communication to improve the translation learning.[[EXP-NEU,ANA-NEU], [null], [DIS], [MIN]]\n\nIs your equation 1 correct? I understand that your logits are reciprocal of mean squared error.[[EXP-NEU,MET-NEU,ANA-NEU], [EMP-NEU], [DIS,QSN], [MIN]] But don\u2019t you need a softmax before applying the NLL loss mentioned in equation 1? In current form of equation 1, I think you are not including the distractor images into account while computing the loss? Please clarify.[[EXP-NEG,ANA-NEG], [EMP-NEU], [DIS,QSN], [MIN]]\n\nWhat is the size of the vocabulary used in all the experiments?[[EXP-NEG], [SUB-NEU], [QSN], [MIN]] Because Gumbel Softmax doesn\u2019t scale well to larger vocabulary sizes and it would be worth mentioning the size of your vocabulary in all the experiments.[[RWK-NEU,EXP-NEG], [SUB-NEU], [DIS], [MIN]]\n\nAre you willing to release the code for reproducing the results?[[EXP-NEG], [EMP-NEU], [QSN], [MIN]]\n\nMinor comments:\n\nIn appendix C, Table 4 caption: you say target sentence is \u201cTrg\u201d but it is \u201cRef\u201d in the table.[[TNF-NEU], [EMP-NEU], [DIS], [MIN]] Also is the reference sentence for skateboard example typo-free?\[[OAL-NEU], [CLA-NEU], [QSN], [MIN]]