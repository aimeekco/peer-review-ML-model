"This paper proposes a fast way to learn convolutional features that later can be used with any classifier.[[INT-POS,PDI-POS], [null], [SMY], [GEN]] The acceleration of the training comes from a reduced number of training epocs and a specific schedule decay of the learning rate.[[PDI-NEU], [null], [SMY], [GEN]] \nIn the evaluation the features are used with support vector machines (SVN) and extreme learning machines on MNIST and CIFAR10/100 datasets.[[DAT-NEU,MET-NEU], [null], [SMY], [GEN]]\n\nPros:\nThe paper compares different classifiers on three datasets.[[DAT-POS,MET-POS], [EMP-POS], [APC], [MAJ]]\n\nCons:\n- Considering an adaptive schedule of the learning decay is common practice in modern machine learning.[[MET-NEG], [NOV-NEG], [CRT], [MIN]] Showing that by varying the learning rate the authors can reduce the number of training epocs and still obtain good performance is not a contribution and it is actually implemented in most of the recent deep learning libraries, like Keras or Pytorch.[[RWK-NEG,MET-NEG,RES-NEG], [NOV-NEG,CMP-NEG], [CRT], [MIN]]\n- It is not clear why, once a CNN has been trained, one should want to change the last layer and use a SVN or other classifiers.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n- There are many spelling errors[[OAL-NEG], [CLA-NEG], [CRT], [MIN]]\n- Comparing CNN based methods with hand-crafted features as in Fig. 1 and Tab.3 is not interesting anymore.[[MET-NEG,TNF-NEG], [EMP-NEG], [CRT], [MAJ]] It is well known that CNN features are much better if enough data is available.\n "[[MET-NEU], [SUB-NEU], [DIS], [GEN]]