"This work proposes to densely connected layers to RNNs by concatenating previously constructed layers together as an input to the current layer. [[PDI-NEU,MET-NEU], [null], [SMY], [GEN]]In addition, attention context is computed for each layer, then, combined together as a single context.[[RWK-NEU], [null], [SMY], [GEN]] Experimental results on English-French and English-German translation tasks and text summarization show comparable performance to a conventional non-densely connected layers with few number of parameters.[[EXP-NEU,MET-NEU,ANA-NEU], [CMP-NEU,EMP-NEU], [SMY], [GEN]]\n\nMotivation is clear in that it applies the densely connected networks in vision to texts and the gains achieved by smaller number of parameters look reasonable.[[PDI-POS,EXP-NEU,MET-NEU], [CLA-POS,EMP-NEU], [SMY,APC], [GEN]] However I have some concerns to this paper.[[OAL-NEU], [IMP-NEU], [APC], [GEN]]\n\n- It is a combination of two techniques, dense connections and multiple attention and it is not clear where the actual gain come from.[[RWK-NEU,EXP-NEG,MET-NEG], [CLA-NEG,EMP-NEG], [DFT,CRT], [MIN]] I'd expect more ablation studies by isolating the effects of dense connection and the use of multiple attention mechanisms.[[EXT-NEU], [CNT], [CNT], [CNT]]\n\n- It is not clear why the experiments for dense sticked to a particular hidden size, e.g., 256 for machine translation, and varies only the number of layers.[[EXP-NEG,MET-NEG], [CLA-NEG,EMP-NEG], [DFT], [MIN]] Do you have experiments by fixing the number of layers and varying the hidden size?[[EXP-NEU,MET-NEU], [null], [QSN], [GEN]]\n\nOther comment:\n\n- Section 3: sequence-to=sequence -> sequence-to-sequence\n\n- It is not clear why the concatenation of all layers is not experimented which is mentioned in section 3.2. Memory problem?[[RWK-NEU,PDI-NEG,EXP-NEG,MET-NEG], [CLA-NEG,EMP-NEG], [DFT], [MIN]] "