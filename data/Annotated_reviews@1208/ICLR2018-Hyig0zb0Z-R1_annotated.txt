"This paper applies gated convolutional neural networks [1] to speech recognition, using the training criterion ASG [2].[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] It is fair to say that this paper contains almost no novelty.[[OAL-NEG], [NOV-NEG], [CRT], [MAJ]]\n\nThis paper starts by bashing the complexity of conventional HMM systems, and states the benefits of their approach.[[MET-NEU], [null], [SMY], [GEN]] However, all of the other grapheme-based end-to-end systems enjoy the same benefit as CTC and ASG.[[MET-NEU], [null], [SMY], [GEN]] Prior work along this line includes [3, 4, 5, 6, 7].[[RWK-NEU], [null], [SMY], [GEN]]\n\nUsing MFSC, or more commonly known as log mel filter bank outputs, has been pretty common since [8].[[MET-NEU], [null], [SMY], [GEN]] Having a separate subsection (2.1) discussing this seems unnecessary.[[MET-NEG], [SUB-NEG], [CRT], [MIN]]\n\nArguments in section 2.3 are weak because, again, all other grapheme-based end-to-end systems have the same benefit as CTC and ASG.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] It is unclear why discriminative training, such as MMI, sMBR, and lattice-free MMI, is mentioned in section 2.3.[[EXP-NEG], [EMP-NEG], [CRT], [MIN]]  Discriminative training is not invented to overcome the lack of manual segmentations, and is equally applicable to the case where we have manual segmentations.[[EXP-NEU], [null], [DIS], [MIN]] \n\nThe authors argue that ASG is better than CTC in section 2.3.1 because it does not use the blank symbol and can be faster during decoding.[[MET-NEU], [CMP-NEU], [DIS], [MIN]]  However, once the transition scores are introduced in ASG, the search space becomes quadratic in the number of characters, while CTC is still linear in the number characters.[[MET-NEU], [CMP-NEU], [DIS], [MIN]]  In addition, ASG requires additional forward-backward computation for computing the partition function (second term in eq 3).[[MET-NEU], [null], [DIS], [MIN]] There is no reason to believe that ASG can be faster than CTC in both training and decoding.[[MET-NEU], [CMP-NEU], [DIS], [MIN]]\n\nThe connection between ASG, CTC, and marginal log loss has been addressed in [9], and it does make sense to train ASG with the partition function. [[EXP-POS,MET-NEU], [EMP-POS], [APC], [MAJ]]Otherwise, the objective won't be a proper probability distribution.[[RES-NEU], [EMP-NEU], [DIS], [MIN]]\n\nThe citation style in section 2.4 seems off.[[CNT], [PNF-NEG], [CRT], [MIN]] Also see [4] for a great description of how beam search is done in CTC.[[MET-NEU], [null], [DIS], [MIN]]\n\nDetails about training, such as the optimizer, step size, and batch size, are missing.[[EXP-NEG,ANA-NEG], [SUB-NEG], [CRT], [MAJ]] Does no batching (in section 3.2) means a batch size of one utterance?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]]\n\nIn the last paragraph of section 3.2, why is there a huge difference in real-time factors between the clean and other set?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Something is wrong unless the authors are using different beam widths in the two settings.[[EXP-NEU], [EMP-NEU], [QSN], [MIN]]\n\nThe paper can be significantly improved if the authors compare the performance and decoding speed against CTC with the same gated convnet.[[RES-NEU], [null], [QSN], [MIN]] It would be even better to compare CTC and ASG to seq2seq-based models with the same gated convnet.[[OAL-NEU], [null], [SUG], [MIN]] Similar experiments should be conducted on switchboard and wsj because librespeech is several times larger than switchboard and wsj.[[EXP-NEU,MET-NEU], [SUB-NEU], [SUG], [MIN]] None of the comparison in table 4 is really meaningful, because none of the other systems have parameters as many as 19 layers of convolution.[[TNF-NEG], [EMP-NEG], [CRT], [MAJ]] Why does CTC fail when trained without the blanks?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Is there a way to fix it besides using ASG?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] It is also unclear why speaker-adaptive training is not needed.[[ANA-NEG], [EMP-NEG], [CRT], [MIN]] At which layer do the features become speaker invariant?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Can the system improve further if speaker-adaptive features are used instead of log mels? [[MET-NEU], [EMP-NEU], [QSN], [MIN]]This paper would be much stronger if the authors can include these experiments and analyses.[[EXP-NEU,ANA-NEU], [SUB-NEU], [SUG], [MAJ]]\n\n[1] R Collobert, C Puhrsch, G Synnaeve, Wav2letter: an end-to-end convnet-based speech recognition system, 2016\n\n[2] Y Dauphin, A Fan, M Auli, D Grangier, Language modeling with gated convolutional nets, 2017\n\n[3] A Graves and N Jaitly, Towards End-to-End Speech Recognition with Recurrent Neural Networks, 2014[[BIB-NEU], [null], [DIS], [MIN]]\n\n[4] A Maas, Z Xie, D Jurafsky, A Ng, Lexicon-Free Conversational Speech Recognition with Neural Networks, 2015[[BIB-NEU], [null], [DIS], [MIN]]\n\n[5] Y Miao, M Gowayyed, F Metze, EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding, 2015\n\n[6] D Bahdanau, J Chorowski, D Serdyuk, P Brakel, Y Bengio, End-to-end attention-based large vocabulary speech recognition, 2016[[BIB-NEU], [null], [DIS], [MIN]]\n\n[7] W Chan, N Jaitly, Q Le, O Vinyals, Listen, attend and spell, 2015[[BIB-NEU], [null], [DIS], [MIN]]\n\n[8] A Graves, A Mohamed, G Hinton, Speech recognition with deep recurrent neural networks, 2013[[BIB-NEU], [null], [DIS], [MIN]]\n\n[9] H Tang, L Lu, L Kong, K Gimpel, K Livescu, C Dyer, N Smith, S Renals, End-to-End Neural Segmental Models for Speech Recognition, 2017"[[BIB-NEU], [null], [DIS], [MIN]]