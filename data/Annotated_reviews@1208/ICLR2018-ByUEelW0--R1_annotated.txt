"The paper proposes to add a rotation operation in long short-term memory (LSTM) cells.[[INT-NEU], [null], [SMY], [GEN]] It performs experiments on bAbI tasks and showed that the results are better than the simple baselines with original LSTM cells.[[RWK-NEU,EXP-NEU,RES-NEU], [CMP-POS], [SMY], [GEN]] There are a few problems with the paper.\n\nFirstly, the title and abstract discuss \"modifying memories\", but the content is only about a rotation operation.[[ABS-NEU,MET-NEG], [EMP-NEG], [CRT], [MAJ]] Perhaps the title should be \"Rotation Operation in Long Short-Term Memory\"?[[ABS-NEU], [null], [QSN], [MAJ]]\n\nSecondly, the motivation of adding the rotation operation is not properly justified.[[MET-NEU], [EMP-NEG], [SMY], [MAJ]] What does it do that a usual LSTM cell could not learn?[[MET-NEU], [EMP-NEU], [QSN], [MAJ]] Does it reduce the excess representational power compared to the LSTM cell that could result in better models?[[MET-NEU], [EMP-NEU], [QSN], [MAJ]]  Or does it increase its representational capacity so that some pattern is modeled in the new cell structure that was not possible before?[[MET-NEU], [EMP-NEU], [QSN], [MAJ]] This is not clear at all after reading the paper.[[MET-NEU], [EMP-NEU], [DFT], [MAJ]] Besides, the idea of using a rotation operation in recurrent networks has been explored before [3].[[RWK-NEU], [NOV-NEG], [CRT], [MAJ]]\n\nFinally, the task (bAbI) and baseline models (LSTM from a Keras tutorial) are too weak.[[RWK-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] There have been recent works that nearly solved the bAbI tasks to perfection (e.g., [1][2][4][5], and many others).[[RWK-NEU], [EMP-NEG], [CRT], [MAJ]] The paper presented a solution that is weak compared to these recent results.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nIn a summary, the main idea of adding rotation to LSTM cells is not properly justified in the paper, and the results presented are quite weak for publication in ICLR 2018.[[PDI-NEG], [APR-NEG,EMP-NEG], [CRT], [MAJ]]\n\n[1] Sainbayar Sukhbaatar, Jason Weston, Rob Fergus. End-to-end memory networks, NIPS 2015\n[2] Caiming Xiong, Stephen Merity, Richard Socher. Dynamic Memory Networks for Visual and Textual Question Answering, ICML 2016\n[3] Mikael Henaff, Arthur Szlam, Yann LeCun, Recurrent Orthogonal Networks and Long-Memory Tasks, ICML 2016 \n[4] Caglar Gulcehre, Sarath Chandar, Kyunghyun Cho, Yoshua Bengio, Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes, ICLR 2017\n[5] Mikael Henaff, Jason Weston, Arthur Szlam, Antoine Bordes, Yann LeCun, Tracking the World State with Recurrent Entity Networks, ICLR 2017\n"[[BIB-NEU], [null], [SUG], [MIN]]