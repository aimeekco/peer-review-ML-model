"This paper suggests a reparametrization of the transition matrix.[[INT-NEU,MET-NEU], [null], [SMY], [GEN]] The proposed reparametrization which is based on Singular Value Decomposition can be used for both recurrent and feedforward networks.[[MET-NEU], [null], [SMY], [GEN]]\n\nThe paper is well-written and authors explain related work adequately.[[OAL-POS], [CLA-POS], [APC], [MAJ]] The paper is a follow up on Unitary RNNs which suggest a reparametrization that forces the transition matrix to be unitary.[[MET-NEU], [EMP-NEU], [SMY], [GEN]] The problem of vanishing and exploding gradient in deep network is very challenging and any work that shed lights on this problem can have a significant impact.[[PDI-NEU,FWK-NEU], [IMP-NEU], [APC], [MIN]] \n\nI have two comments on the experiment section:\n\n- Choice of experiments.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]] Authors have chosen UCR datasets and MNIST for the experiments while other experiments are more common.[[DAT-NEU,EXP-NEU], [CMP-NEU,EMP-NEU], [DIS], [MIN]] For example, the adding problem, the copying problem and the permuted MNIST problem and language modeling are the common experiments in the context of RNNs.[[DAT-NEU,EXP-NEU], [CMP-NEU,EMP-NEU], [DIS], [MIN]] For feedforward settings, classification on CIFAR10 and CIFAR100 is often reported.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]]\n\n- Stopping condition.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]]\ The plots suggest that the optimization has stopped earlier for some models.[[RES-NEG], [EMP-NEG], [CRT], [MIN]] Is this because of some stopping condition or because of gradient explosion?[[RES-NEU], [EMP-NEU], [QSN], [MIN]] Is there a way to avoid this?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\n- Quality of figures.[[TNF-NEU], [PNF-NEU], [DIS], [MIN]] Figures are very hard to read because of small font.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]] Also, the captions need to describe more details about the figures."[[TNF-NEG], [SUB-NEG, PNF-NEG], [DFT], [MIN]]