"This paper considers the problem of autonomous lane changing for self-driving cars in multi-lane multi-agent slot car setting.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The authors propose a new learning strategy called Q-masking which couples well a defined low level controller with a high level tactical decision making policy.[[PDI-NEU], [null], [SMY], [GEN]]\n\nThe authors rightly say that one of the skills an autonomous car must have is the ability to change lanes, however this task is not one of the most difficult for autonomous vehicles to achieve and this ability has already been implemented in real vehicles.[[PDI-NEU], [null], [SMY], [GEN]]  Real vehicles also decouple wayfinding with local vehicle control, similar to the strategy employed here. [[PDI-NEU], [null], [SMY], [GEN]] To make a stronger case for this research being relevant to the real autonomous driving problem, the authors would need to compare their algorithm to a real algorithm and prove that it is more \u201cdata efficient.[[PDI-NEU,MET-NEU], [CMP-NEU], [SUG], [MIN]]\u201d  This is a difficult comparison since the sensing strategies employed by real vehicles \u2013 LIDAR, computer vision, recorded, labeled real maps are vastly different from the slot car model proposed by the authors.[[MET-NEG], [CMP-NEG], [CRT], [MAJ]]  In term of impact, this is a theoretical paper looking at optimizing a sandbox problem where the results may be one day applicable to the real autonomous driving case.[[RES-NEU,FWK-NEU], [IMP-NEU], [DIS], [MIN]]\nIn this paper the authors investigate \u201cthe use and place\u201d of deep reinforcement learning in solving the autonomous lane change problem they propose a framework that uses Q-learning to learn \u201chigh level tactical decisions\u201d and introduce \u201cQ-masking\u201d a way of limiting the problem that the agent has to learn to force it to learn in a subspace of the Q-values.[[MET-NEU], [null], [SMY], [GEN]]\nThe authors claim that \u201cBy relying on a controller for low-level decisions we are also able to completely eliminate collisions during training or testing, which makes it a possibility to perform training directly on real systems.[[MET-NEU], [null], [SMY], [GEN]]\u201d  I am not sure what is meant by this since in this paper the authors never test their algorithm on real systems and in real systems it is not possible to completely eliminate collisions.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]]  If it were, this would be a much sought breakthrough.[[MET-NEU], [null], [SUG], [GEN]] Additionally for their experiment authors use the SUMO top view driving simulator. [[EXP-NEU], [null], [SMY], [GEN]] This choice makes their algorithm not currently relevant to most autonomous vehicles that use ego-centric sensing. [[MET-NEG], [EMP-NEG], [CRT], [MAJ]] \nThis paper presents a learning algorithm that can \u201coutperform a greedy baseline in terms of efficiency\u201d and \u201chumans driving the simulator in terms of safety and success\u201d within their top view driving game.[[RWK-POS,MET-POS], [EMP-POS], [APC], [MAJ]]  The game can be programmed to have an \u201cn\u201d lane highway, where n could reasonable go up to five to represent larger highways.[[MET-NEU], [null], [SMY], [MIN]]  The authors limit the problem by specifying that all simulated cars must operate between a preset minimum and maximum and follow a target (random) speed within these limits.[[PDI-NEU,MET-NEU], [null], [DIS], [MIN]]  Cars follow a fixed model of behavior, do not collide with each other and cannot switch lanes.[[MET-NEU], [null], [DIS], [MIN]]  It is unclear if the simulator extends beyond a single straight section of highway, as shown in Figure 1. [[EXP-NEG,TNF-NEG], [EMP-NEG], [CRT], [MIN]] The agent is tasked with driving the ego-car down the n-lane highway and stopping at \u201cthe exit\u201d in the right hand lane D km from the start position.[[MET-NEU], [null], [DIS], [MIN]] \nThe authors use deep Q learning from Mnih et al 2015 to learn their optimal policy.[[RWK-NEU,MET-NEU], [null], [DIS], [MIN]]  They use a sparse reward function of +10 for reaching the goal and -10x(lane difference from desired lane) as a penalty for failure.  This simple reward function is possible because the authors do not require the ego car to obey speed limits or avoid collisions.[[MET-NEU], [null], [DIS], [GEN]]    \n\nThe authors limit what the car is able to do \u2013 for example it is not allowed to take actions that would get it off the highway.[[MET-NEU], [null], [DIS], [GEN]]  This makes the high level learning strategy more efficient because it does not have to explore these possibilities (Q-masking).[[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]]  The authors claim that this limitation of the simulation is made valid by the ability of the low level controller to incorporate prior knowledge and perfectly limit these actions.[[RES-POS], [EMP-POS], [APC], [MAJ]]  In the real world, however, it is unlikely that any low level controller would be able to do this perfectly.[[EXT-NEU], [null], [DIS], [GEN]]\nIn terms of evaluation, the authors do not compare their result against any other method.[[MET-NEG,RES-NEG], [CMP-NEG], [DIS], [MIN]]  Instead, using only one set of test parameters, the authors compare their algorithm to a \u201cgreedy baseline\u201d policy that is specified a \u201calways try to change lanes to the right until the lane is correct\u201d then it tries to go as fast as possible while obeying the speed limit and not colliding with any car in front.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MIN]]   It seems that baseline is additionally constrained vs the ego car due to the speed limit and the collision avoidance criteria and is not a fair comparison.[[RWK-NEG,MET-NEG], [CMP-NEG], [CRT], [MAJ]]  So given a fixed policy and these constraints it is not surprising that it underperforms the Q-masked Q-learning algorithm.[[RWK-NEG,MET-NEG], [CMP-NEG], [CRT], [MAJ]]  \nWith respect to the comparison vs. human operators of the car simulation, the human operators were not experts.[[RWK-NEG,RES-NEG], [CMP-NEG], [CRT], [MAJ]]  They were only given \u201ca few trials\u201d to learn how to operate the controls before the test.[[EXP-NEG,MET-NEG], [CMP-NEG], [CRT], [MAJ]]  It was reported that the human participants \u201cdid not feel comfortable\u201d with the low level controller on, possibly indicating that the user experience of controlling the car was less than ideal.[[EXP-NEG,MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]]  With the low level controller off, collisions became possible.[[RES-NEU], [null], [DIS], [MIN]]  It is possibly not a fair claim to say that human drivers were \u201cless safe\u201d but rather that it was difficult to play the game or control the car with the safety module on.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]]  This could be seen as a game design issue.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]  It was not clear from this presentation how the human participants were rewarded for their performance.[[RES-NEU], [EMP-NEU], [DIS], [MIN]]  In more typical HCI experiments the gender distribution and ages ranges of participants are specified as well as how participants were recruited and how the game was motivated, including compensation (reward) are specified.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]]  \nOverall, this paper presents an overly simplified game simulation with a weak experimental result.\n"[[EXP-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]]