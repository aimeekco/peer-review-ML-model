"The paper seems to claims that\n1) certain ConvNet architectures, particularly AlexNet and VGG, have too many parameters,[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]]\n 2) the sensible solution is leave the trunk of the ConvNet unchanged, and to randomly sparsify the top-most weight matrices.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]]\nI have two problems with these claims:\n1) Modern ConvNet architectures (Inception, ResNeXt, SqueezeNet, BottleNeck-DenseNets and ShuffleNets) don't have large fully connected layers.[[PDI-NEG], [EMP-NEG], [CRT], [MIN]]\n2) The authors reject the technique of 'Deep compression' as being impractical.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] I suspect it is actually much easier to use in practice as you don't have to a-priori know the correct level of sparsity for every level of the network.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\np3. What does 'normalized' mean?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Batch-norm?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\np3. Are you using an L2 weight penalty?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] If not, your fully-connected baseline may be unnecessarily overfitting the training data.[[DAT-NEG,EXP-NEG], [EMP-NEG], [CRT], [MIN]]\np3. Table 1. Where do the choice of CL Junction densities come from?[[EXP-NEU,TNF-NEU], [EMP-NEU], [QSN], [MIN]] Did you do a grid search to find the optimal level of sparsity at each level?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\np7-8. I had trouble following the left/right & front/back notation.[[TNF-NEU], [EMP-NEU], [QSN], [MIN]]\np8. Figure 7. How did you decide which data points to include in the plots?"[[DAT-NEU,TNF-NEU], [EMP-NEU], [QSN], [MIN]]