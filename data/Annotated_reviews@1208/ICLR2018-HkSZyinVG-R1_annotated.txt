"The authors propose using piecewise linear activation functions with contraints to make it continous.[[INT-NEU,MET-NEU], [null], [SMY], [GEN]] They report that, during training, tuning piecewise versions of the multiple activation functions such as ReLU, ELU, LReLU converge to shifted ELU termed ShELU in this article.[[EXP-NEU,MET-NEU,RES-NEU], [null], [SMY], [GEN]] Authors claim to achive better performance when using ShELU  while learning an individual bias shift for each neuron.[[EXP-NEU,MET-NEU,RES-NEU], [null], [SMY,DIS], [GEN]]\n\nGiven a PReLU (learnable alpha) or ELU is applied on pre-activation wx+b at each neuron, one can achieve the same shift as that reported in ShELU if required.[[EXP-NEU,MET-NEU,RES-NEU], [null], [SMY,DIS], [GEN]] Authors present no clear explanation on why the shift should result in improved performance."[[RES-NEG,ANA-NEG], [EMP-NEG], [CRT], [MAJ]]