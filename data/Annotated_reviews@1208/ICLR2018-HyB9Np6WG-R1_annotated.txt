"This paper provides a new method for learning representations of prepositions.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The basic idea is to count word pairs which co-occur with a preposition, rather than single words which co-occur, as in standard word vector models such as word2vec.[[PDI-NEU], [null], [SMY], [GEN]] This seems to work quite well, and I speculate that it is because prepositions often function to indicate grammatical relations between different arguments, rather than being content-bearing words themselves.[[PDI-POS], [EMP-POS], [APC], [MAJ]] The paper counts up these word-pair co-occurrences in a tensor, then applies a tensor decomposition and low-rank approximation method to produce word and preposition representations.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] Experiments show that the method helps to find paraphrases of phrasal verbs, as well as improve downstream performance on preposition selection and preposition attachment disambiguation.[[EXP-POS,MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]]\n\nThis paper was quite interesting and clearly written for the most part.[[OAL-POS], [EMP-POS], [APC], [MAJ]] I enjoyed reading it and the various evaluations that it described that target both the use of prepositions inside phrasal verbs as well as in its role in indicating grammatical relationships between different elements in a sentence.[[MET-POS], [CLA-POS,EMP-POS], [APC], [MAJ]]  I think that this work could be quite useful to the field, but that a number of frustrating weaknesses prevent me from recommending it without qualifications.[[OAL-NEG], [REC-NEG], [FBK], [MIN]]\n\nThe main weaknesses of the paper are in the soundness of some of its qualitative analyses and claims.[[ANA-POS,MET-POS], [EMP-POS], [APC], [MAJ]] First, I found the cosine similarity scores in Table 1 largely uninterpretable.[[RES-NEG,TNF-NEG], [EMP-NEG], [CRT], [MAJ]] The claim is that different prepositions should have representations that are sufficiently distinct from each other.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] Even if we accept this premise (and why should we? They are of the same syntactic category after all), using the similarity scores to make this argument is not reasonable, as there is no absolute interpretation or calibration of the similarity scores that can be applied across models.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] Is 0.22 in similarity high or low?[[RES-NEU], [EMP-NEU], [QSN], [MIN]]\n\nThe other evaluation decision that is confusing is the paraphrase evaluation of the phrasal verbs.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] This was not done systematically, but a broad general claim that the tensor multiplication models does the best cannot be verified.[[RES-NEG], [EMP-NEG], [QSN], [MAJ]] To me, the word2vec addition paraphrases also look quite good.[[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]] It seems to me that a human subject experiment to somehow compare the two methods is required.[[EXP-NEU,MET-NEU], [SUB-NEU], [SUG], [MIN]]\n\nI wonder whether this approach could be generalized to other classes of words or morphemes.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] For example, you could imagine that in a morphologically rich language, this method would work well to learn the representation of certain morphemes such as case endings or verbal conjugation."[[MET-NEU], [EMP-NEU], [DIS], [GEN]]