"This paper proposes to combine reinforcement learning with supervised learning to speed up learning.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] Unlike their claim in the paper, the idea of combining supervised and RL is not new.[[PDI-NEG,RWK-NEG], [NOV-NEG,CMP-NEG], [CRT], [MAJ]] A good example of this is a supervised actor-critic by Barto (2004).[[PDI-NEU,EXT-NEU], [CMP-NEU], [DIS], [GEN]] I think even alphaGo uses some form of supervision.[[PDI-NEU,EXT-NEU], [CMP-NEU], [DIS], [GEN]] However, if I understand correctly, it seems that combining supervision of RL at a later fine-tuning phase by considering supervision as a regularization term is an interesting idea that seems novel.[[PDI-POS,EXP-POS,MET-POS], [NOV-POS], [APC], [MAJ]]\n\nHaving the luxury of some supervised episodes is of course useful.[[MET-POS], [EMP-POS], [APC], [MAJ]] The first step of building a supervised initial model looks straight forward.[[MET-POS], [EMP-POS], [APC], [MAJ]] The next step of the algorithm is less easy to follow, and presentation of the ideas could be much better.[[MET-NEG], [PNF-NEG,EMP-NEG], [CRT], [MIN]] This part of the paper leaves me already with many questions such as why is it essential to consider only a deterministic case and also to consider greedy optimization? Doesn\u2019t this prevent exploration? What are the network parameters (e.g. size of layers) etc.[[MET-NEG], [EMP-NEU], [QSN], [MIN]] I am not sure I could redo the work from the provided information.\n\n[[MET-NEG], [EMP-NEG], [CRT], [MIN]]Overall, it is unclear to me what the advantage of the algorithm is over pure supervised learning, and I don\u2019t think a compelling case has been made.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] Since the influence of the supervision is increased by increasing alpha, it can be expected that results should be better for increasing alpha.[[RES-NEG], [EMP-NEG], [DIS], [MIN]] The results seem to indicate that an intermediate level of alpha is best, though I would even question the statistical significance by looking at the curves in Figure 3.[[MET-NEG,RES-NEU,TNF-NEG], [EMP-NEG], [DIS,QSN], [MIN]] Also, what is the epoch number, and why is this 1 for alpha=0?[[MET-NEU], [PNF-NEU], [QSN], [MIN]] If the combination of supervised learning with RL is better, than this should be clearly stated.[[MET-NEU], [PNF-NEG], [DFT], [MAJ]] Some argument is made that pure supervision is overfitting, but would one then not simply add some other regularizer?[[MET-NEU], [SUB-NEG], [QSN], [MIN]] \n\nThe presentation could also be improved with some language edits.[[OAL-NEU], [PNF-NEG], [SUG], [MIN]] Several articles are wrongly placed and even some meaning is unclear.[[OAL-NEU], [PNF-NEG], [CRT], [MIN]] For example, the phrase \u201ccontinuous input sequence\u201d does not make sense; maybe you mean \u201cinput sequence of real valued quantities\u201d.\n\n[[OAL-NEG], [CLA-NEG,PNF-NEG], [SUG,CRT], [MIN]]In summary, while the paper contains some good ideas, I certainly think it needs more work to make a clear case for this method. \n"[[MET-NEG], [PNF-NEG,EMP-NEG], [SMY,SUG], [MAJ]]