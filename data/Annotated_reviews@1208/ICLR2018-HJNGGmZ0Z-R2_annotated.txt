"The paper claims that image captioning systems work so well, while most recent state of the art papers show that they produce 50% errors, so far from perfect.[[PDI-NEU], [null], [SMY], [GEN]]\n\nThe paper lacks novelty, just reports some results without proper analysis or insights.[[ANA-NEG,OAL-NEG], [NOV-NEG], [CRT], [MAJ]]\n\nMain weakness of the paper:\n - Missing many IC systems citations and comparisons (see https://competitions.codalab.org/competitions/3221#results)[[RWK-NEG], [CMP-NEG,SUB-NEG], [DFT,CRT], [MAJ]]\n - According to \"SPICE: Semantic Propositional Image Caption Evaluation\" current metrics used in image captioning don't correlate with human judgement.[[EXT-NEU], [null], [DIS], [GEN]]\n- Most Image Caption papers which use a pre-trained CNN model, do fine-tune the image feature extractor to improve the results (see Vinyals et al. 2016).[[EXT-NEU], [null], [DIS], [GEN]] Therefore correlation of the image features with the captions is weaker that it could be[[EXT-NEU], [null], [DIS], [GEN]].\n- The experiments reported in Table1 are way below state-of-the-art results, there a tons of previous work with much better results, see https://competitions.codalab.org/competitions/3221#results[[RWK-NEG,RES-NEG], [CMP-NEG], [CRT], [MAJ]]\n - To provide a fair comparison authors, should compare their results with other paper results.[[RWK-NEU,RES-NEU], [CMP-NEU], [CRT], [MAJ]]\n - Tables 2 and 3 are missing the original baselines.[[TNF-NEG], [CMP-NEG], [CRT], [MAJ]]\nThe evaluation used in the paper don't correlate well with human ratings see (SPICE paper), therefore trying to improve them marginally doesn't make a difference.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n- Getting better performance by switching from VGG19 to ResNet152 is expected, however they obtain worse results than Vinyals et al. 2016 with inception_v3.[[RES-NEG], [CMP-NEG], [CRT], [MAJ]] \n- The claim \"The bag of objects model clusters these group the best\" is not supported by any evidence or metric.[[ANA-NEG], [SUB-NEG], [DFT], [MAJ]]\n\nOne interesting experiment but missing in section 4.4 would be how the image features change after fine-tuning for the captioning task.[[EXP-NEG], [SUB-NEG], [DFT], [MIN]]\n\n\nTypos:\n - synsest-level -> synsets-level"[[CNT], [CLA-NEG], [CRT], [MIN]]