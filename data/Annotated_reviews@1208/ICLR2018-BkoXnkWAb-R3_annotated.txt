"Summary:\nThis paper proposes a simple recipe to preserve proximity to zero mean for activations in deep neural networks.[[INT-NEU], [null], [SMY], [GEN]] The proposal is to replace the non-linearity in half of the units in each layer with its \"bipolar\" version -- one that is obtained by flipping the function on both axes.[[MET-NEU], [null], [SMY], [GEN]]\nThe technique is tested on deep stacks of recurrent layers, and on convolutional networks with depth of 28, showing that improved results over the baseline networks are obtained.[[RWK-NEU,EXP-NEU,RES-POS], [CMP-POS], [SMY], [GEN]] \n\nClarity:\nThe paper is easy to read.[[OAL-POS], [CLA-POS], [APC], [MAJ]] The plots in Fig. 2 and the appendix are quite helpful in improving presentation.[[TNF-POS], [PNF-POS], [APC], [MIN]] The experimental setups are explained in detail.[[EXP-POS], [EMP-POS], [APC], [MAJ]] \n\nQuality and significance:\nThe main idea from this paper is simple and intuitive.[[PDI-NEU], [null], [APC], [MAJ]] However, the experiments to support the idea do not seem to match the motivation of the paper.[[PDI-NEU,EXP-NEG], [EMP-NEG], [CRT], [MAJ]] As stated in the beginning of the paper, the motivation behind having close to zero mean activations is that this is expected to speed up training using gradient descent.[[INT-NEU,MET-NEU], [EMP-POS], [APC], [MAJ]] However, the presented results focus on the performance on held-out data instead of improvements in training speed.[[DAT-NEU,RES-NEU], [EMP-NEG], [CRT], [MAJ]] This is especially the case for the RNN experiments.[[EXP-NEU], [null], [CRT], [MAJ]]\n\nFor the CIFAR-10 experiment, the training loss curves do show faster initial progress in learning.[[EXP-NEU,TNF-POS], [EMP-POS], [SMY], [MAJ]] However, it is unclear that overall training time can be reduced with the help of this technique.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] To evaluate this speed up effect, the dependence on the choice of learning rate and other hyperparameters should also be considered.[[MET-NEU], [EMP-NEU], [SUG], [MAJ]]\n\nNevertheless, it is interesting to note the result that the proposed approach converts a deep network that does not train into one which does in many cases.[[MET-POS], [EMP-POS], [APC], [MAJ]] The method appears to improve the training for moderately deep convolutional networks without batch normalization (although this is tested on a single dataset),;[[MET-POS], [EMP-POS], [APC], [MAJ]] but is not practically useful yet since the regularization benefits of Batch Normalization are also taken away.[[MET-NEU], [EMP-NEG], [CRT], [MAJ]]\n"