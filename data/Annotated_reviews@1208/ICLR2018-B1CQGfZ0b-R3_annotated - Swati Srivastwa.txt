"General-purpose program synthesizers are powerful but often slow, so work that investigates means to speed them up is very much welcome\u2014this paper included.[[PDI-NEU], [null], [SMY], [GEN]] The idea proposed (learning a selection strategy for choosing a subset of synthesis examples) is good.[[PDI-POS], [EMP-POS], [APC], [MAJ]] For the most paper, the paper is clearly-written, with each design decision justified and rigorously specified.[[OAL-POS], [CLA-POS], [APC], [MAJ]] The experiments show that the proposed algorithm allows a synthesizer to do a better job of reliably finding a solution in a short amount of time (though the effect is somewhat small).[[EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]]\n\nI do have some serious questions/concerns about this method:[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\nPart of the motivation for this paper is the goal of scaling to very large sets of examples.[[CNT], [null], [DIS], [GEN]] The proposed neural net setup is an autoencoder whose input/output size is proportional to the size of the program input domain.[[EXP-NEU], [null], [DIS], [GEN]] How large can this be expected to scale (a few thousand)?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]] \n\nThe paper did not specify how often the neural net must be trained.[[EXP-NEG], [SUB-NEG], [DFT], [MIN]] Must it be trained for each new synthesis problem?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]]  If so, the training time becomes extremely important (and should be included in the \u201cNN Phase\u201d time measurements in Figure 4).[[EXP-NEU,TNF-NEU], [EMP-NEU], [QSN], [MIN]]  If this takes longer than synthesis, it defeats the purpose of using this method in the first place.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MIN]] \nAlternatively, can the network be trained once for a domain, and then used for every synthesis problem in that domain (i.e. in your experiments, training one net for all possible binary-image-drawing problems)?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]]  If so, the training time amortizes to some extent\u2014can you quantify this?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]] \nThese are all points that require discussion which is currently missing from the paper.[[EXP-NEG,ANA-NEG], [SUB-NEG], [DFT], [MIN]] \n\nI also think that this method really ought to be evaluated on some other domain(s) in addition to binary image drawing.[[MET-NEU], [EMP-NEU], [SUG], [MIN]]  The paper is not an application paper about inferring drawing programs from images; rather, it proposes a general-purpose method for program synthesis example selection.[[OAL-NEU], [APR-NEU], [DIS], [MAJ]]  As such, it ought to be evaluated on other types of problems to demonstrate this generality.[[MET-NEU], [APR-NEU], [DIS], [MIN]]  Nothing about the proposed method (e.g. the neural net setup) is specific to images, so this seems quite readily doable.[[MET-POS], [EMP-POS], [APC], [MAJ]] \n\nOverall: I like the idea this paper proposes,[[PDI-POS,RES-POS], [EMP-POS], [APC], [MAJ]]  but I have some misgivings about accepting it in its current state.[[RES-NEG,OAL-NEG], [REC-NEG], [FBK], [MAJ]] \n\n\n\n\nWhat follows are comments on specific parts of the paper:[[OAL-NEU], [null], [DIS], [GEN]] \n\n\nIn a couple of places early in the paper, you mention that the neural net computes \u201cthe probability\u201d of examples.[[MET-NEU], [null], [DIS], [GEN]]  The probability of what?[[MET-NEU], [null], [DIS], [GEN]]  This was totally unclear until fairly deep into Section 3.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \n - Page 2: \u201cthe neural network computes the probability for other examples not in the subset\u201d[[CNT], [null], [QSN], [MIN]] \n - Page 3: \u201cthe probability of all the examples conditioned on[[MET-NEU], [null], [DIS], [MIN]] \u2026\u201d\n\nOn a related note, I don\u2019t like the term \u201cSelection Probability\u201d for the quantity it describes.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]  This quantity is \u2018the probability of an input being assigned the correct output.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \u2019 That happens to be (as you\u2019ve proven) a good measure by which to select examples for the synthesizer.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]  The first property (correctness) is a more essential property of this quantity, rather than the second (appropriateness as an example selection measure).[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \n\nPage 5: \u201cFigure blah shows our neural network architecture\u201d - missing reference to Figure 3.[[TNF-NEG], [PNF-NEG], [DFT], [MIN]] \n\nPage 5: \u201cnote that we do not suggest a specific neural network architecture for the middle layers, one should select whichever architecture that is appropriate for the domain at hand\u201d - such as?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]]  What are some architectures that might be appropriate for different domains?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]  What architecture did you use in your experiments?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]] \n\nThe description of the neural net in Section 3.3 (bottom of page 5) is hard to follow on first read-through.[[MET-NEG], [CLA-NEG,EMP-NEG], [CRT], [MIN]]  It would be better to lead with some high-level intuition about what the network is supposed to do before diving into the details of how it\u2019s set up.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]  The first sentence on page 6 gives this intuition; this should come much earlier.[[CNT], [PNF-NEG], [CRT], [MIN]] \n\nPage 5: \u201ca feed-forward auto-encoder with N input neurons\u2026\u201d Previously, N was defined as the size of the input domain.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]]  Does this mean that the network can only be trained when a complete set of input-output examples is available (i.e. outputs for all possible inputs in the domain)?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]]  Or is it fine to have an incomplete example set?\n\n"[[DAT-NEU], [SUB-NEU], [QSN], [MIN]] 