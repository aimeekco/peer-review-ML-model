"The problem of interest is to train deep neural network models with few labelled training samples.[[PDI-NEU], [null], [SMY], [GEN]] The specific assumption is there is a large pool of unlabelled data, and a heuristic function that can provide label annotations, possibly with varying levels of noises, to those unlabelled data.[[DAT-NEU,PDI-NEU], [null], [SMY], [GEN]] The adopted learning model is of a student/teacher framework as in privileged learning/knowledge distillation/model compression, and also machine teaching.[[MET-NEU], [null], [SMY], [GEN]] The student (deep neural network) model will learn from both labelled and unlabelled training data with the labels provided by the teacher (Gaussian process) model.[[DAT-NEU,MET-NEU], [null], [SMY], [GEN]] The teacher also supplies an uncertainty estimate to each predicted label.[[CNT], [null], [SMY], [GEN]] How about the heuristic function?[[MET-NEU], [null], [QSN], [MIN]] This is used for learning initial feature representation of the student model.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Crucially, the teacher model will also rely on these learned features.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Labelled data and unlabelled data are therefore lie in the same dimensional space.[[DAT-NEU], [EMP-NEU], [DIS], [MIN]] \n\nSpecific questions to be addressed:\n1)\tClustering of strongly-labelled data points.[[DAT-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]] Thinking about the statement \u201ceach an expert on this specific region of data space\u201d, if this is the case, I am expecting a clustering for both strongly-labelled data points and weakly-labelled data points.[[DAT-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]]  Each teacher model is trained on a portion of strongly-labelled data, and will only predict similar weakly-labelled data.[[DAT-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]]  On a related remark, the nice side-effect is not right as it was emphasized that data points with a high-quality label will be limited.[[DAT-NEG,MET-NEG], [EMP-NEG], [CRT], [MIN]]  As well, GP models, are quite scalable nowadays (experiments with millions to billions of data points are available in recent NIPS/ICML papers, though, they are all rely on low dimensionality of the feature space for optimizing the inducing point locations).[[EXT-NEU], [null], [DIS], [GEN]]  It will be informative to provide results with a single GP model.[[RES-NEU], [SUB-NEU], [SUG], [MIN]]  \n2)\tFrom modifying learning rates to weighting samples.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]  Rather than using uncertainty in label annotation as a multiplicative factor in the learning rate, it is more \u201cintuitive\u201d to use it to modify the sampling procedure of mini-batches (akin to baseline #4); sample with higher probability data points with higher certainty.[[DAT-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]]  Here, experimental comparison with, for example, an SVM model that takes into account instance weighting will be informative, and a student model trained with logits (as in knowledge distillation/model compression). \n"[[EXP-NEU], [SUB-NEU], [SUG], [MIN]] 