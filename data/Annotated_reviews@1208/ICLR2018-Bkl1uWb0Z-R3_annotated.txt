"This paper induces latent dependency syntax in the source side for NMT.[[INT-NEU], [null], [SMY], [GEN]] Experiments are made in En-De and En-Ru.[[EXP-NEU], [null], [SMY], [GEN]]\n\nThe idea of imposing a non-projective dependency tree structure was proposed previously by Liu and Lapata (2017) and the structured attention model by Kim and Rush (2017).[[RWK-NEU,BIB-NEU], [CMP-NEU], [DIS], [MAJ]] In light of this, I see very little novelty in this paper.[[PDI-NEG], [NOV-NEG], [CRT], [MAJ]] The only novelty seems to be the gate that controls the amount of syntax needed for generating each target word.[[PDI-POS], [NOV-POS], [DIS], [MAJ]] Seems thin for a ICLR paper.[[OAL-NEG], [APR-NEG], [FBK], [MAJ]]\n\nCaption of Fig 1: \"subject/object\" are syntactic functions, not semantic roles.[[TNF-NEU], [CLA-NEG], [SUG], [MIN]]\n\nI don't see how the German verb \"orders\" inflects with gender...[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Can you post the gold German sentence?[[MET-NEU], [null], [QSN], [MAJ]]\n\nSec 2 is poorly explained. What is z_t? Do you mean u_t instead? This is confusing.[[MET-NEU], [EMP-NEU], [QSN], [MAJ]]\n \nExpressions (12) to (15) are essentially the same as in Liu and Lapata (2017), not original contributions of this paper.[[RWK-NEU], [NOV-NEG,CMP-NEG], [CRT], [MAJ]]\n\nWhy is hard attention (sec 3.3) necessary?[[MET-NEU], [EMP-NEU], [QSN], [MAJ]] It's not differentiable and requires sampling for training.[[EXP-NEU,MET-NEU], [null], [SMY], [MAJ]] This basically spoils the main advantage of structured attention mechanisms as proposed by Kim and Rush (2017).[[RWK-NEU,BIB-NEU], [EMP-NEU], [DIS], [MAJ]]\n\nExperimentally, the gains are quite small compared to flat attention, which is disappiointing.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nIn table 3, it would be very helpful to display the English source.[[MET-NEU,TNF-NEU], [PNF-POS], [SUG], [GEN]]\n\nTable 4 is confusing.[[TNF-NEU], [CLA-NEG], [CRT], [GEN]] The DA numbers (rightmost three columns) are for the 2016 or 2017 dataset?[[DAT-NEG], [null], [QSN], [MIN]]\n\nComparison with predicted parses by Spacy are by no means \"gold\" parses...[[RWK-NEU,EXP-NEU], [CMP-NEU], [DIS], [MIN]]\n\nMinor comments:\n- Sec 1: \"... optimization techniques like Adam, Attention, ...\" -> Attention is not an optimization technique, but part of a model;[[MET-NEU], [CLA-NEU], [DFT], [MIN]]\n- Sec 1: \"abilities not its representation\" -> comma before \"not\".[[MET-NEU], [CLA-NEU], [DFT], [MIN]]\n"