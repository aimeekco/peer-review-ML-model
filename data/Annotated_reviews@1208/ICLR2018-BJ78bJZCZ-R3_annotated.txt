"Summary:\nThis paper proposes an extension to the RWA model by introducing the discount gates to computed discounted averages instead of the undiscounted attention.[[INT-NEU,PDI-NEU,MET-NEU], [null], [SMY], [GEN]] The problem with the RWA is that the averaging mechanism can be numerically unstable due to the accumulation operations when computing d_t.[[MET-NEU], [EMP-NEU], [DIS], [GEN]]\n\nPros:\n- Addresses an issue of RWAs.[[MET-POS], [EMP-POS], [APC], [MAJ]]\n\nCons:\n-The paper addresses a problem with an issue with RWAs. [[MET-NEU], [EMP-NEU], [SMY], [MIN]]But it is not clear to me why would that be an important contribution.[[MET-NEG], [NOV-NEG], [CRT], [MIN]]\n-The writing needs more work.[[OAL-NEG], [CLA-NEG], [CRT], [MIN]]\n-The experiments are lacking and the results are not good enough.[[EXP-NEG,RES-NEG], [SUB-NEG,EMP-NEG], [CRT], [MAJ]]\n\nGeneral Comments:\n\nThis paper addresses an issue regarding to RWA which is not really widely adopted and well-known architecture, because it seems to have some have some issues that this paper is trying to address.[[PDI-NEG], [NOV-NEG], [CRT], [GEN]] I would still like to have a better justification on why should we care about RWA and fixing that model.[[MET-NEG], [CNT], [DIS], [GEN]] \n\nThe writing of this paper seriously needs more work.[[OAL-NEG], [CLA-NEG], [CRT], [MIN]]  The Lemma 1 doesn't make sense to me, I think it has a typo in it, it should have been (-1)^t c instead of -1^t c.[[MET-NEG], [CLA-NEG], [CRT], [MIN]]\n\nThe experiments are only on toyish and small scale tasks.[[DAT-NEG,EXP-NEG], [SUB-NEG], [CRT], [MAJ]] According to the results the model doesn't really do better than a simple LSTM or GRU."[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]]