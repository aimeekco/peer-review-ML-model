"\nThis work proposes to study the generalization of learning neural networks via the Fourier-based method.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] It first gives a Fourier-based generalization bound, showing that Rademacher complexity of functions with small bandwidth and Fourier l_1 norm will be small.[[RWK-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] This leads to generalization for 2-layer networks with appropriate bounded size.[[RWK-NEU], [null], [SMY], [GEN]] For 2-layer networks with sine activation functions, assuming that the data distribution has nice spectral property (ie bounded bandwidth), it shows that the local minimum of the population risk (if with isolated component condition) will have small size, and also shows that the gradient of the empirical risk is close to that of the population risk.[[RWK-NEU,DAT-NEU,EXP-NEU,MET-NEU], [null], [SMY], [GEN]] Empirical results show that the size of the networks learned on random labels are larger than those learned on true labels, and shows that a regularizer implied by their Fourier-based generalization bound can effectively reduce the generalization gap on random labels. [[RWK-NEU,EXP-NEU], [null], [SMY], [GEN]]\n\nThe idea of applying the Fourier-based method to generalization is interesting.[[PDI-POS,EXP-POS,MET-POS], [IMP-POS,EMP-POS], [APC], [MAJ]] However, the theoretical results are not very satisfactory.[[RWK-NEU,RES-NEG], [IMP-NEG], [DFT], [MIN]] \n-- How do the bounds here compared to those obtained by directly applying Rademacher complexity to the neural network functions?[[RWK-NEU,MET-NEU], [null], [QSN], [GEN]] \n-- How to interpret the isolated components condition in Theorem 4?[[RWK-NEU,EXP-NEU,MET-NEU], [null], [QSN], [GEN]] Basically, it means that B(P_X) should be a small constant.[[RWK-NEU,EXP-NEG], [EMP-NEG], [DFT], [MIN]] What type of distributions of X will be a good example?[[RWK-NEU,EXP-NEU], [null], [QSN], [GEN]] \n-- It is not easy to put together the conclusions in Section 6.1 and 6.2.[[EXT-NEU], [CNT], [CNT], [CNT]] Suppose SGD leads to a local minimum of the empirical loss.[[EXP-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] One can claim that this is an approximate local minimum (ie, small gradient) by Corollary 3.[[RWK-NEU,EXP-NEU,MET-NEU], [null], [SMY], [GEN]] But to apply Theorem 4, one will need a version of Theorem 4 for approximate local minima.[[RWK-NEU,EXP-NEU], [null], [SMY], [GEN]] Also, one needs to argue that the local minimum obtained by SGD will satisfy the isolated component condition.[[RWK-NEU,EXP-NEU,MET-NEU], [null], [SMY,DIS,QSN], [GEN]] The argument in Section 8.6 is not convincing, ie, there is potentially a large approximation error in (41) and one cannot claim that Lemma 1 and Theorem 4 are still valid without the isolated component condition. [[RWK-NEG,PDI-NEG,EXP-NEG], [IMP-NEG,EMP-NEG], [CRT], [MIN]]\n"