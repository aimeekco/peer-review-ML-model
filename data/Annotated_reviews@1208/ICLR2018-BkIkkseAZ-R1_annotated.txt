"The paper studies the theoretical properties of the two-layer neural networks.[[INT-NEU], [NULL], [SMY], [GEN]] \n\nTo summarize the result, let's use the theta to denote the layer closer to the label, and W to denote the layer closer to the data.[[RES-NEU], [NULL], [DIS], [GEN]] \n\nThe paper shows that \na) if W is fixed, then with respect to the randomness of the data, with prob. 1, the Jacobian matrix of the model is full rank[[RES-NEU], [SUB-NEU], [DIS], [GEN]]\nb) suppose that we run an algorithm with fresh samples, then with respect to the randomness of the k-th sample, we have that with prob. 1, W_k is full rank, and the Jacobian of the model is full rank. [[RES-NEU], [SUB-NEU], [DIS], [GEN]]\n\nIt's know (essentially from the proof of Carmon and Soudry) that if the Jacobian of the model is full rank for any matrix W w.r.t the randomness of the data, then all stationary points are global.[[RWK-NEU], [SUB-NEU], [DIS], [GEN]] But the paper cannot establish such a result. [[RES-NEG], [APR-NEG], [CRT], [MAJ]]\n\nThe paper is not very clear, and after figuring out what it's doing, I don't feel it really provides many new things beyond C-S and Xie et al.[[ANA-NEG], [APR-NEG], [CRT], [MAJ]]\n\nThe paper argues that it works for activation beyond relu but result a) is much much weaker than the one with for all quantifier for W.[[ANA-NEG], [APR-NEG], [CRT], [MAJ]] result b) is very sensitive to the exactness of the events (such as W is exactly full rank) --- the events that the paper talks just naturally never happen as long as the density of the random variables doesn't degenerate.[[DAT-NEG], [null], [CRT], [MAJ]]  \n\nAs the author admitted, the results don't provide any formal guarantees for the convergence to a global minimum.[[RES-NEG], [APR-NEG], [DFT], [MAJ]] It's also a bit hard for me to find the techniques here provide new ideas that would potentially lead to resolving this question. [[MET-NEU], [NOV-NEU], [DFT], [MIN]]\n\n--------------------\n\nadditional review after seeing the author's response: \n\nThe author's response pointed out some of the limitation of Soudry and Carmon, and Xie et al's which I agree.[[RWK-POS], [SUB-POS], [DIS], [GEN]] However, none of this limitation is addressed by this paper (or addressed in a misleading way to some extent.)[[ANA-NEG], [null], [CRT], [MAJ]]  The key technical limitation is the dependency of the local minima on the weight parameters[[EXP-NEU], [SUB-NEU], [DFT], [MAJ]]. Soudry and Carmon addresses this in a partial way by using the random dropout, which is a super cool idea.[[RWK-POS], [NOV-POS], [DIS], [GEN]] Xie et al couldn't address this globally but show that the Jacobian is well conditioned for a class of weights.[[RWK-NEU], [NOV-NEU], [DIS], [GEN]] The paper here doesn't have either and only shows that for a single fixed weight matrix, the Jacobian is well-conditioned.[[MET-NEG], [NOV-NEG], [CRT], [MAJ]] \n\nI don't also see the value of extension to other activation function[[MET-NEG], [SUB-NEG], [DFT], [GEN]]. To some extent this is not consistent with the empirical observation that relu is very important for deep learning.[[ANA-NEG], [SUB-NEG], [DFT], [MAJ]] \n\nRegarding the effect of randomness, since the paper only shows the convergence to a first-order optimal solution, I don't see why randomness is necessary.[[MET-NEG], [SUB-NEG], [DFT], [MIN]] Gradient descent can converge to a first order optimal solution. [[MET-NEU], [SUB-NEU], [DIS], [GEN]](Indeed I have a typo in my previous review regarding \"w.r.t. k-th sample\", which should be \"w.r.t. k-th update\". )[[ANA-NEU], [CLA-NEU], [DIS], [GEN]] Moreover, to justify the effect of the randomness, the paper should have empirical experiments.[[EXP-NEU], [null], [SUG], [MIN]] \n\nI think the writing of the paper is also misleading in several places. \n"[[OAL-NEG], [CLA-NEG], [CRT], [MAJ]]