"The authors describe a method for performing query completion with error correction using a neural network that can achieve real-time performance.[[INT-NEU,MET-NEU], [null], [SMY], [GEN]] The method described uses a character-level LSTM, and modifies the beam search procedure with a an edit distance-based probability to handle cases where the prefix may contain errors.[[MET-NEU], [null], [SMY], [GEN]] Details are also given on how the authors are able to achieve realtime completion[[MET-NEU], [null], [SMY], [GEN]].\n\nOverall, it\u2019s nice a nice study of the query completion application.[[MET-POS], [null], [APC], [MAJ]] The paper is well explained, and it\u2019s also nice that the runtime is shown for each of the algorithm blocks.[[MET-POS], [CLA-POS,EMP-POS], [APC], [MAJ]] Could imagine this work giving nice guidelines for others who also want to run query completion using neural networks.[[EXT-POS], [null], [APC], [MAJ]] The final dataset is also a good size (36M search queries).[[DAT-POS], [SUB-POS], [APC], [MAJ]]\n\nMy major concerns are perhaps the fit of the paper for ICLR as well as the thoroughness of the final experiments.[[OAL-NEU], [APR-NEU], [DIS], [MAJ]] Much of the paper provides background on LSTMs and edit distance, which granted, are helpful for explaining the ideas.[[RWK-POS], [EMP-POS], [APC], [MAJ]] But much of the realtime completion section is also standard practice, e.g. maintaining previous hidden states and grouping together the different gates.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] So the paper feels directed to an audience with less background in neural net LMs.[[RWK-NEU], [SUB-NEG], [CRT], [MAJ]]\n\nSecondly, the experiments could have more thorough/stronger baselines.[[RWK-NEU,EXP-NEU], [EMP-NEU], [SUG], [MAJ]] I don\u2019t really see why we would try stochastic search. And expected to see more analysis of how performance was impacted as the number of errors increased, even if errors were introduced artificially, and expected analysis of how different systems scale with varying amounts of data.[[DAT-NEU,MET-NEG,ANA-NEU], [EMP-NEG], [CRT], [MAJ]] The fact that 256 hidden dimension worked best while 512 overfit was also surprising, as character language models on datasets such as Penn Treebank with only 1 million words use hidden states far larger than that for 2 layers.[[DAT-NEU,MET-NEU], [EMP-NEU], [DIS], [MAJ]] More regularization required?[[MET-NEU], [null], [QSN], [MIN]]"