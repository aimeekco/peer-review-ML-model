"\nThe authors describe how to use Bayesian neural networks with Thompson sampling\nfor efficient exploration in q-learning.[[MET-NEU], [null], [SMY], [GEN]] The Bayesian neural networks are only\nBayesian in the last layer. [[MET-NEU], [null], [SMY], [GEN]] That is, the authors learn all the previous layers\nby finding point estimates.[[MET-NEU], [null], [SMY], [GEN]] The Bayesian learning of the last layer is then\ntractable since it consists of a linear Gaussian model.[[MET-NEU], [null], [SMY], [GEN]] The resulting method is\ncalled BDQL.[[MET-NEU], [null], [SMY], [GEN]] The experiments performed show that the proposed approach, after\nhyper-parameter tuning, significantly outperforms the epsilon-greedy\nexploration approaches such as DDQN.[[EXP-POS,MET-NEU], [CMP-POS], [APC], [MAJ]]\n\nQuality:\n\nI am very concern about the authors stating on page 1 \"we sample from the\nposterior on the set of Q-functions\".[[MET-NEU], [EMP-NEU], [CRT], [MIN]] I believe this statement is not correct.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\nThe Bayesian posterior distribution is obtained by combining an assumed\ngenerative model for the data, data sampled from that model and some prior\nassumptions.[[MET-NEU], [CMP-NEU], [DIS], [MIN]] In this paper there is no generative model for the data and the\ndata obtained is not actually sampled from the model.[[DAT-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] The data are just targets\nobtained by the q-learning rule.[[DAT-NEU], [EMP-NEU], [DIS], [MIN]] This means that the authors are adapting\nQ-learning methods so that they look Bayesian, but in no way they are dealing\nwith a principled posterior distribution over Q-functions.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] At least this is my\nopinion, I would like to encourage the authors to be more precise and show in\nthe paper what is the exact posterior distribution over Q-functions and show\nhow they approximate that distribution, taking into account that a posterior\ndistribution is obtained as $p(theta|D) \\propto p(D|theta)p(\\theta)$.[[MET-NEU], [SUB-NEU], [SUG], [MIN]] In the\ncase addressed in the paper, what is the likelihood $p(D|\\theta)$ and what are\nthe modeling assumptions that explain how $D$ is generated by sampling from a\nmodel parameterized by \\theta?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nI am also concerned about the hyper-parameter tuning for the baselines.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MIN]] In\nsection 5 (choice of hyper-parameters) the authors describe a quite exhaustive\nhyper-parameter tuning procedure for BDQL.[[MET-NEU], [null], [DIS], [MIN]] However, they do not mention whether\nthey perform a similar hyper-parameter tuning for DDQN, in particular for the\nparameter epsilon which will determine the amount of exploration.[[MET-NEU], [null], [DIS], [MIN]] This makes me\nwonder if the comparison in table 2 is fair.[[TNF-NEU], [CMP-NEU], [DIS], [MIN]] Especially, because the authors\ntune the amount of data from the replay-buffer that is used to update their\nposterior distribution.[[DAT-NEU], [null], [DIS], [MIN]] This will have the effect of tuning the width of their\nposterior approximation which is directly related to the amount of exploration\nperformed by Thompson sampling.[[DAT-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]] You can, therefore, conclude that the authors are\ntuning the amount of exploration that they perform on each specific problem.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\nIs that also being done for the baseline DDQN, for example, by tuning epsilon in\neach problem?[[RWK-NEU,MET-NEU], [null], [SMY], [GEN]]\n\nThe authors also report in table 2 the scores obtained for DDQN by Osband et\nal. 2016.[[RWK-NEU,RES-NEU,TNF-NEU], [null], [DIS], [GEN]] What is the purpose of including two rows in table 2 with the same\nmethod?[[MET-NEU,TNF-NEU], [null], [QSN], [MIN]] It feels a bit that the authors want to hide the fact that they only\ncompare with a singe epsilon-greedy baseline (DDQN).[[RWK-NEG,MET-NEG], [CMP-NEG], [CRT], [MIN]] Epsilon-greedy methods\nhave already been shown to be less efficient than Bayesian methods with\nThompson sampling for exploration in q learning (Lipton et al. 2016).[[RWK-NEG,MET-NEG], [CMP-NEG], [CRT], [MAJ]]\n\nThe authors do not compare with variational approaches to Bayesian learning\n(Lipton et al. 2016).[[RWK-NEG,MET-NEG], [CMP-NEG], [CRT], [MIN]] They indicate that since Lipton et al. \"do not\ninvestigate the Atari games, we are not able to have their method as an\nadditional baseline\".[[RWK-NEU], [CMP-NEU], [DIS], [MIN]] This statement seems completely unjustified.[[RWK-NEG,MET-NEG], [CMP-NEG], [CRT], [MIN]] The authors\nshould clearly include a description of why Lipton's approach cannot be applied\nto the Atari games or include it as a baseline.[[RWK-NEU,ANA-NEU], [SUB-NEU,CMP-NEU], [DIS], [MIN]] \n\nThe method proposed by the authors is very similar to Lipton's approach.[[MET-NEU], [CMP-NEU], [DIS], [MIN]] The\nonly difference is that Lipton et al. use variational inference with a\nfactorized Gaussian distribution to approximate the posterior on all the\nnetwork weights.[[MET-NEU], [CMP-NEU], [DIS], [MIN]] The authors by contrast, perform exact Bayesian inference, but\nonly on the last layer of their neural network.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] It would be very useful to know\nwhether the exact linear Gaussian model in the last layer proposed by the\nauthors has advantages with respect to a variational approximation on all the\nnetwork weights.[[MET-NEU,ANA-NEU], [EMP-NEU], [DIS], [MIN]] If Lipton's method would be expensive to apply to large-scale\nsettings such as the Atari games, the authors could also compare with that\nmethod in smaller and simpler problems.[[MET-NEU], [CMP-NEU], [SUG], [MIN]]\n\nThe plots in Figure 2 include performance in terms of episodes.[[TNF-NEU], [null], [DIS], [MIN]] However, it\nwould also be useful to know how much is the extra computational costs of\nthe proposed method.[[MET-NEU,ANA-NEU], [SUB-NEU], [DIS], [MIN]] One could imagine that computing the posterior\napproximation in equation 6 has some additional cost.[[ANA-NEU], [null], [DIS], [MIN]] How do BDQN and DDQN\ncompare when one takes into account running time and not episode count into\naccount?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nClarity:\n\nThe paper is clearly written.[[OAL-POS], [CLA-POS], [APC], [MAJ]] However, I found a lack of motivation for the\nspecific design choices made to obtain equations 9 and 10.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] What is a_t in\nequation 9?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] The parameters \\theta are updated just after equation 10 by\nfollowing the gradient of the loss in which the weights of the last layer are\nfixed to a posterior sample, instead of the posterior mean.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Is this update rule\nguaranteed to produce convergence of \\theta?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] I could imagine that at different\ntimes, different posterior samples of the weights will be used to compute the\ngradients.[[MET-NEU], [null], [DIS], [MIN]] Does this create any instability in learning?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] \n\nI found the paragraph just above section 5 describing the maze-like\ndeterministic game confusing and not very useful. [[MET-NEG], [PNF-NEG], [CRT], [MIN]]The authors should improve\nthis paragraph.[[MET-NEG], [PNF-NEG], [SUG], [MIN]]\n\nOriginality:\n\nThe proposed approach in which the weights in the last layer of the neural\nnetwork are the only Bayesian ones is not new.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] The same method was proposed in\n\nSnoek, J., Rippel, O., Swersky, K., Kiros, R., Satish, N., Sundaram, N., ... &\nAdams, R. (2015, June).[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MIN]] Scalable Bayesian optimization using deep neural\nnetworks.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MIN]]  In International Conference on Machine Learning (pp. 2171-2180).\n\nwhich the authors fail to cite.[[RWK-NEG,MET-NEG], [SUB-NEG], [DFT], [MAJ]]  The use of Thompson sampling for efficient\nexploration in deep Q learning is also not new since it has been proposed by\nLipton et al. 2016.[[MET-NEG], [NOV-NEG], [CRT], [MIN]]  The main contribution of the paper is to combine these two\nmethods (equations 6-10) and evaluate the results in the large-scale setting of\nATARI games, showing that it works in practice.[[MET-NEU,RES-NEU], [null], [DIS], [MAJ]] \n\nSignificance:\n\nIt is hard to determine how significant the work is since the authors only\ncompare with a single baseline and leave aside previous work on efficient\nexploration with Thompson sampling based on variational approximations.[[RWK-NEG], [SUB-NEG,EMP-NEU], [DFT,CRT], [MAJ]] \n\nAs far as the method is described, I believe it would be impossible to\nreproduce their results because of the complexity of the hyper-parameter tuning\nperformed by the authors.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]]  I would encourage the authors to release code that can\ndirectly generate Figure 2 and table 2.\n"[[TNF-NEU], [null], [DIS], [MIN]] 