"In conventional boosting methods, one puts a weight on each sample.[[EXT-NEU], [null], [DIS], [GEN]] The wrongly classified samples get large weights such that in the next round those samples will be more likely to get right. [[EXT-NEU], [null], [DIS], [GEN]] Thus the learned weak learner at this round will make different mistakes.[[EXT-NEU], [null], [DIS], [GEN]]\nThis idea however is difficult to be applied to deep learning with a large amount of data.[[PDI-NEG], [EMP-NEG], [CRT], [MAJ]] This paper instead designed a new boosting method which puts large weights on the category with large error in this round.[[PDI-NEU], [null], [SMY], [GEN]]   In other words samples in the same category will have the same weight \n\nError bound is derived.[[PDI-NEU], [null], [SMY], [GEN]]  Experiments show its usefulness [[EXP-POS], [EMP-POS], [APC], [MAJ]]though experiments are limited\n"[[EXP-NEG], [SUB-NEG], [CRT], [MIN]]