"This paper leverages how deep Bayesian NNs, in the limit of infinite width, are Gaussian processes (GPs).[[INT-NEU], [null], [SMY], [GEN]] After characterizing the kernel function, this allows us to use the GP framework for prediction, model selection, uncertainty estimation, etc.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]]\n\n\n- Pros of this work\n\nThe paper provides a specific method to efficiently compute the covariance matrix of the equivalent GP and shows experimentally on CIFAR and MNIST the benefits of using the this GP as opposed to a finite-width non-Bayesian NN.[[DAT-POS,MET-POS], [EMP-POS], [APC], [MAJ]]\n\nThe provided phase analysis and its relation to the depth of the network is also very interesting.[[ANA-POS], [EMP-POS], [APC], [MAJ]]\n\nBoth are useful contributions as long as deep wide Bayesian NNs are concerned.[[ANA-POS], [EMP-POS], [APC], [MAJ]] A different question is whether that regime is actually useful.[[ANA-NEU], [EMP-NEU], [DIS], [MIN]]\n\n\n- Cons of this work\n\nAlthough this work introduces a new GP covariance function inspired by deep wide NNs, I am unconvinced of the usefulness of this regime for the cases in which deep learning is useful.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] \n\nFor instance, looking at the experiments, we can see that on MNIST-50k (the one with most data, and therefore, the one that best informs about the \"true\" underlying NN structure) the inferred depth is 1 for the GP and 2 for the NN, i.e., not deep.[[DAT-NEG,EXP-NEG], [EMP-NEG], [CRT], [MIN]] Similarly for CIFAR, where only up to depth 3 is used.[[DAT-NEU,EXP-NEU], [EMP-NEU], [DIS], [MIN]] None of these results beat state-of-the-art deep NNs.[[RWK-NEG,RES-NEG], [CMP-NEG], [CRT], [MIN]]\n\nAlso, the results about the phase structure show how increased depth makes the parameter regime in which these networks work more and more constrained. [[RES-NEG], [EMP-NEG], [CRT], [MIN]]\n\nIn [1], it is argued that kernel machines with fixed kernels do not learn a hierarchical representation.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] And such representation is generally regarded as essential for the success of deep learning.[[MET-NEU], [null], [DIS], [GEN]] \n\nMy impression is that the present line of work will not be relevant for deep learning and will not beat state-of-the-art results because of the lack of a structured prior.[[RWK-NEG,RES-NEG,FWK-NEG], [IMP-NEG,CMP-NEG], [CRT], [MAJ]] In that sense, to me this work is more of a negative result informing that to be successful, deep Bayesian NNs should not be wide and should have more structure to avoid reaching the GP regime.[[OAL-NEG], [EMP-NEG], [CRT], [MAJ]]\n\n\n- Other comments:\n\nIn Fig. 5, use a consistent naming for the axes (bias and variances).[[TNF-NEU], [null], [SUG], [MIN]]\n\nIn Fig. 1, I didn't find the meaning of the acronym NN with no specified width.[[TNF-NEU], [null], [SUG], [MIN]]\n\nDoes the unit norm normalization used to construct the covariance disallow ARD input selection?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\n\n[1] Yoshua Bengio, Olivier Delalleau, and Nicolas Le Roux.[[BIB-NEU], [null], [DIS], [GEN]] The Curse of Dimensionality for Local Kernel Machines. 2005."[[BIB-NEU], [null], [DIS], [GEN]]