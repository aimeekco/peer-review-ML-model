"The work was prompted by  an interesting observation: a phase transition can be observed in deep learning with stochastic gradient descent and Tikhonov regularization.[[INT-NEU], [null], [SMY], [GEN]] When the regularization parameter exceeds a (data-dependent) threshold, the parameters of the model are driven to zero, thereby preventing any learning.[[MET-NEU], [null], [SMY], [GEN]] The authors then propose to moderate this problem by letting the regularization parameter to be zero for 5 to 10 epochs, and then applying the \"strong\" penalty parameter.[[MET-NEU], [null], [SMY], [GEN]] In their experimental results, the phase transition is not observed anymore with their protocol.[[EXP-NEU,RES-NEG], [EMP-NEG], [CRT], [MAJ]] This leads to better performances, by using penalty parameters that would have prevent learning with the usual protocol.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]]\n\nThe problem targeted is important, in the sense that it reveals that some of the difficulties related to non-convexity and the use of SGD that are often overlooked.[[PDI-POS], [EMP-POS], [APC], [MAJ]] The proposed protocol is reported to work well, but since it is really ad hoc, it fails to convince the reader that it provides the right solution to the problem.[[MET-NEU], [EMP-NEU], [SMY], [MAJ]] I would have found much more satisfactory to either address the initialization issue by a proper warm-start strategy, or to explore standard optimization tools such as constrained optimization (i.e. Ivanov regularization) , that could be for example implemented by stochastic projected gradient or barrier functions.[[MET-NEU], [EMP-NEG], [CRT], [MAJ]] I think that the problem would be better handled that way than with the proposed strategy,;[[MET-NEU], [EMP-NEU], [SUG], [MAJ]] which seems to rely only on a rather limited amount of experiments, and which may prove to be inefficient when dealing with big databases.[[DAT-NEU,EXP-NEU], [SUB-NEG,EMP-NEG], [CRT], [MAJ]]\n\nTo summarize, I believe that the paper addresses an important point,;[[PDI-POS], [null], [APC], [MAJ]] but that the tools advocated are really rudimentary compared with what has been already proposed elsewhere.[[RWK-NEU,MET-NEG], [CMP-NEU,EMP-NEG], [CRT], [MAJ]]\n\nDetails :\n- there is a typo in the definition of the proximal operator in Eq. (9) [[MET-NEU], [CLA-NEG], [DFT], [MIN]]\n- there are many unsubstantiated speculations in the comments of the experimental section that do not add value to the paper [[EXP-NEG], [EMP-NEG], [CRT], [MAJ]]\n- the figure showing the evolution of the magnitude of parameters arrives too late and could be completed by the evolution of the data-fitting term of the training criterion"[[EXP-NEG,TNF-NEU], [EMP-NEG], [CRT], [MAJ]]