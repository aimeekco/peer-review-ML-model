"This paper describes DReLU, a shift version of ReLU.DReLU shifts ReLU from (0, 0) to (-\\sigma, -\\sigma). The author runs a few CIFAR-10/100 experiments with DReLU.[[INT-NEU], [null], [SMY], [GEN]]\n\nComments:\n\n1. Using expectation to explain why DReLU works well is not sufficient and convincing.[[MET-NEG], [EMP-NEG], [DFT], [MIN]] Although DReLU\u2019s expectation is smaller than expectation of ReLU, but it doesn\u2019t explain why DReLU is better than very leaky ReLU, ELU etc.[[MET-NEG], [EMP-NEG], [DFT], [MIN]]\n2. CIFAR-10/100 is a saturated dataset and it is not convincing DReLU will perform will on complex task, such as ImageNet, object detection, etc.[[DAT-NEG,MET-NEG], [EMP-NEG], [DFT], [MIN]]\n3. In all experiments, ELU/LReLU are worse than ReLU, which is suspicious.[[EXP-NEG], [EMP-NEG], [DFT,DIS], [GEN]] I personally have tried ELU/LReLU/RReLU on Inception V3 with Batch Norm, and all are better than ReLU.[[EXT-NEU], [null], [DIS], [GEN]] \n\nOverall, I don\u2019t think this paper meet ICLR\u2019s novelty standard, although the authors present some good numbers, but they are not convincing. [[OAL-NEG], [APR-NEG,NOV-NEG], [CRT], [MAJ]]\n\n\n"