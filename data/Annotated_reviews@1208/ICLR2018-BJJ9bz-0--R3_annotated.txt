"SUMMARY:\n\nThe motivation for this work is to have an RL algorithm that can use imperfect demonstrations to accelerate learning.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The paper proposes an actor-critic algorithm, called Normalized Actor-Critic (NAC), based on the entropy-regularized formulation of RL, which is defined by adding the entropy of the policy as an additional term in the reward function.[[MET-NEU], [null], [SMY], [GEN]]\nEntropy-regularized formulation leads to nice relationships between the value function and the policy, and has been explored recently by many, including [Ziebart, 2010], [Schulman, 2017], [Nachum, 2017], and [Haarnoja, 2017].[[RWK-POS,MET-POS], [EMP-POS], [APC], [MAJ]]\nThe paper benefits from such a relationship and derives an actor-critic algorithm.[[MET-POS,OAL-POS], [EMP-POS], [APC], [MAJ]] Specifically, the paper only parametrizes the Q function, and computes the policy gradient using the relation between the policy and Q function (Appendix A.1).[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\nThrough a set of experiments, the paper shows the effectiveness of the method.[[EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]]\n\n\nEVALUATION:\n\nI think exploring and understanding entropy-regularized RL algorithm is important.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] It is also important to be able to benefit from off-policy data.[[DAT-NEU,MET-NEU], [EMP-NEU], [DIS], [MAJ]] I also find the empirical results encouraging.[[RES-POS], [EMP-POS], [APC], [MAJ]] But I have some concerns about this paper:\n\n- The derivations of the paper are unclear.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n- The relation with other recent work in entropy-regularized RL should be expanded.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n- The work is less about benefiting from demonstration data and more about using off-policy data.[[OAL-NEU], [null], [DIS], [MIN]]\n- The algorithm that performs well is not the one that was actually derived.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n\n* Unclear derivations:\nThe derivations of Appendix A.1 is unclear.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] It makes it difficult to verify the derivations.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n\nTo begin with, what is the loss function of which (9) and (10) are its gradients?[[MET-NEG], [EMP-NEG], [QSN], [MIN]]\n\nTo be more specific, the choices of \\hat{Q} in (15) and \\hat{V} in (19) are not clear.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]  For example, just after (18) it is said that \u201c\\hat{Q} could be obtained through bootstrapping by R + gamma V_Q\u201d.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] But if it is the case, shouldn\u2019t we have a gradient of Q in (15) too?[[MET-NEG], [EMP-NEG], [QSN], [MIN]] (or show that it can be ignored?)\n\nIt appears that \\hat{Q} and \\hat{V} are parameterized independently from Q (which is a function of theta).[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Later in the paper they are estimated using a target network, but this is not specified in the derivations.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n\nThe main problem boils down to the fact that the paper does not start from a loss function and compute all the gradients in a systematic way.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Instead it starts from gradient terms, each of which seems to be from different papers, and then simplifies them.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] For example, the policy gradient in (8), which is further decomposed in Appendix A.1 as (15) and (16) and simplified, appears to be Eq. (50) of [Schulman et al., 2017] (https://arxiv.org/abs/1704.06440).[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MIN]] In that paper we have Q_pi instead of \\hat{Q} though.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n\nI suggest that the authors start from a loss function and clearly derive all necessary steps.[[MET-NEG], [EMP-NEG], [SUG], [MIN]]\n\n\n* Unclear relation with other papers:\nWhat part of the derivations of this work are novel?[[MET-NEU], [NOV-NEU], [QSN], [MAJ]] Currently the novelty is not obvious.[[OAL-NEG], [NOV-NEG], [CRT], [MAJ]]\nFor example, having the gradient of both Q and V, as in (9), has been stated by [Haarnoja et al., 2017] (very similar formulation is developed in Appendix B of https://arxiv.org/abs/1702.08165).[[RWK-NEG,MET-NEG], [NOV-NEG], [CRT], [MIN]]\nAn algorithm that can work with off-policy data has also been developed by [Nachum, 2017] (in the form of a Bellman residual minimization algorithm, as opposed to this work which essentially uses a Fitted Q-Iteration algorithm as the critic).[[RWK-NEU,MET-NEU], [EMP-NEG], [CRT], [MIN]]\n\nI think the paper could do a better job differentiating from those other papers.[[RWK-POS], [CMP-POS], [APC], [MAJ]]\n\n\n* The claim that this paper is about learning from demonstration is a bit questionable.[[OAL-NEG], [EMP-NEG], [CRT], [MIN]] The paper essentially introduces a method to use off-policy data, which is of course important,[[MET-POS], [EMP-POS], [APC], [MIN]] but does not cover the important scenario where we only have access to (state,action) pairs given by an expert.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Here it appears from the description of Algorithm 1 that the transitions in the demonstration data have the same semantic as the interaction data, i.e., (s,a,r,s\u2019).[[DAT-NEU,RES-NEU], [EMP-NEU], [DIS], [MIN]]\nThis makes it different from the work by [Kim et al., 2013], [Piot et al., 2014], and [Chemali et al., 2015], which do not require such a restriction on the demonstration data.[[RWK-NEG,DAT-NEG], [CMP-NEG], [CRT], [MIN]]\n\n\n* The paper mentions that to formalize the method as a policy gradient one, importance sampling should be used (the paragraph after (12)), but the performance of such a formulation is bad, as depicted in Figure 2.[[MET-NEG,TNF-NEG], [EMP-NEG,PNF-NEG], [CRT], [MAJ]] As a result, Algorithm 1 does not use importance sampling.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\nThis basically suggests that by ignoring the fact that the data is collected off-policy, and treating it as an on-policy data, the agent might perform better.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] This is an interesting phenomenon and deservers further study, as currently doing the \u201cwrong\u201d things is better than doing the \u201cright\u201d thing.[[MET-POS,FWK-POS], [IMP-POS], [APC], [MAJ]] I think a good paper should investigate this fact more."[[ANA-NEU,OAL-NEU], [SUB-NEU], [APC], [MAJ]]