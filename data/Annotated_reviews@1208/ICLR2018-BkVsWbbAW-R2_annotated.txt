"This paper reports on a system for sequential learning of several supervised classification tasks in a challenging online regime.[[INT-NEU], [null], [SMY], [GEN]] Known task segmentation is assumed and task specific input generators are learned in parallel with label prediction.[[MET-NEU], [null], [SMY], [GEN]] The method is tested on standard sequential MNIST variants as long as a class incremental variant.[[EXP-NEU,MET-NEU], [null], [SMY], [GEN]] Superior performance to recent baselines (e.g. EWC) is reported in several cases.[[RWK-NEU], [CMP-POS], [APC], [MAJ]] Interesting parallels with human cortical and hippocampal learning and memory are discussed.[[MET-NEU], [null], [SMY], [GEN]]\n\nUnfortunately, the paper does not go beyond the relatively simplistic setup of sequential MNIST, in contrast to some of the methods used as baselines.[[RWK-NEU,MET-NEG], [CMP-NEU,EMP-NEG], [CRT], [MAJ]] The proposed architecture implicitly reduces the continual learning problem to a classical multitask learning (MTL) setting for the LTM, where (in the best case scenario) i.i.d. data from all encountered tasks is available during training. This setting is not ideal, though.[[DAT-NEU,MET-NEU], [EMP-NEU], [SMY], [MAJ]] There are several example of successful multitask learning, but it does not follow that a random grouping of several tasks immediately leads to successful MTL.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Indeed, there is good reason to doubt this in both supervised and reinforcement learning domains.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] In the latter case it is well known that MTL with arbitrary sets of task does not guarantee superior, or even comparable performance to plain single-task learning, due to \u2018negative interference\u2019 between tasks [1, 2].[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] I agree that problems can be constructed where these assumptions hold, but this core assumption is limiting.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] The requirement of task labels also rules out important use cases such as following a non-stationary objective function, which is important in several realistic domains, including deep RL.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\n\n[1] Parisotto, Emilio; Lei Ba, Jimmy; Salakhutdinov, Ruslan: \t\nActor-Mimic: Deep Multitask and Transfer Reinforcement Learning. ICLR 2016.\n[2] Andrei A. Rusu, Sergio Gomez Colmenarejo, \u00c7aglar G\u00fcl\u00e7ehre, Guillaume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, Raia Hadsell: Policy Distillation. ICLR 2016."[[BIB-NEU], [null], [SUG], [GEN]]