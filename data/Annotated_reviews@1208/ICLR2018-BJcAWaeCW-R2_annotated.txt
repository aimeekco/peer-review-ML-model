"Quality: The work has too many gaps for the reader to fill in.[[OAL-NEG], [CNT], [CRT], [MAJ]] The generator (reconstructed matrix) is supposed to generate a 0-1 matrix (adjacency matrix) and allow backpropagation of the gradients to the generator.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] I am not sure how this is achieved in this work.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] The matrix is not isomorphic invariant and the different clusters don\u2019t share a common model.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Even implicit models should be trained with some way to leverage graph isomorphisms and pattern similarities between clusters.[[EXP-NEU], [EMP-NEU], [SUG], [MIN]] How can such a limited technique be generalizing?[[MET-NEU], [EMP-NEU], [QSN], [MAJ]] There is no metric in the results showing how the model generalizes, it may be just overfitting the data.\[[RES-NEG], [EMP-NEG], [CRT], [MAJ]]n\nClarity: The paper organization needs work; there are also some missing pieces to put the NN training together.[[MET-NEG], [CLA-NEG,SUB-NEG], [CRT], [MAJ]] It is only in Section 2.3 that the nature of G_i^\\prime becomes clear,[[EXP-POS], [EMP-POS], [APC], [MAJ]] although it is used in Section 2.2. Equation (3) is rather vague for a mathematical equation.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] From what I understood from the text, equation (3) creates a binary matrix from the softmax output using an indicator function.[[MET-NEU], [EMP-NEG], [CRT], [MAJ]] If the output is binary, how can the gradients backpropagate? Is it backpropagating with a trick like the Gumbel-Softmax trick of Jang, Gu, and Poole 2017 or Bengio\u2019s path derivative estimator?[[RWK-NEU], [EMP-NEU], [QSN], [MIN]] This is a key point not discussed in the manuscript.[[RWK-NEG], [CMP-NEG], [DFT], [MAJ]] \nAnd if I misunderstood the sentence \u201cturn re_G into a binary matrix\u201d and the values are continuous, wouldn\u2019t the discriminator have an easy time distinguishing the generated data from the real data.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] And wouldn\u2019t the generator start working towards vanishing gradients in its quest to saturate the re_G output?[[MET-NEG], [EMP-NEG], [QSN], [MIN]]\n\nOriginality: The work proposes an interesting approach: first cluster the network, then learning distinct GANs over each cluster.[[MET-POS], [EMP-POS], [APC], [MAJ]] There are many such ideas now on ArXiv but it would be unfair to contrast this approach with unpublished work. [[EXT-NEU], [null], [DIS], [GEN]]There is no contribution in the GAN / neural network aspect.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] It is also unclear whether the model generalizes.[[OAL-NEG], [EMP-NEG], [CRT], [MAJ]] I don\u2019t think this is a good fit for ICLR.[[OAL-NEG], [APR-NEG], [CRT], [MAJ]]\n\nSignificance: Generating graphs is an important task in in relational learning tasks, drug discovery, and in learning to generate new relationships from knowledge bases.[[MET-NEU], [IMP-NEU], [DIS], [MAJ]] The work itself, however, falls short of the goal.[[OAL-NEG], [IMP-NEG], [CRT], [MAJ]] At best the generator seems to be working but I fear it is overfitting.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] The contribution for ICLR is rather minimal, unfortunately.[[MET-NEG], [APR-NEG], [CRT], [MAJ]]\n\nMinor:\n\nGTI was not introduced before it is first mentioned in the into.[[INT-NEU], [NOV-NEU], [DIS], [MIN]]\n\nY. Bengio, N. Leonard, and A. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv:1308.3432, 2013.\n\n"[[RWK-NEU], [null], [DIS], [MIN]]