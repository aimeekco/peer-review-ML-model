"This paper aims to learn a single policy that can perform a variety of tasks that were experienced sequentially.[[PDI-NEU], [null], [SMY], [GEN]] The approach is to learn a policy for task 1, then for each task k+1: copy distilled policy that can perform tasks 1-k, finetune to task k+1, and distill again with the additional task.[[MET-NEU], [null], [SMY], [GEN]] The results show that this PLAID algorithm outperforms a network trained on all tasks simultaneously.[[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]] \n\nQuestions:\n- When distilling the policies, do you start from a randomly initialized policy, or do you start from the expert policy network?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n- What data do you use for the distillation?[[MET-NEU], [EMP-NEU], [APC], [MIN]] Section 4.1 states\"We use a method similar to the DAGGER algorithm\", but what is your method.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] If you generate trajectories form the student network, and label them with the expert actions, does that mean all previous expert policies need to be kept in memory?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] \n- I do not understand the purpose of \"input injection\" nor where it is used in the paper. [[MET-NEG], [EMP-NEG], [CRT], [MIN]] \n\nStrengths:\n- The method is simple but novel.[[MET-POS], [NOV-POS,EMP-POS], [APC], [MAJ]]  The results support the method's utility.[[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]] \n- The testbed is nice; the tasks seem significantly different from each other.[[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]] It seems that no reward shaping is used.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n- Figure 3 is helpful for understanding the advantage of PLAID vs MultiTasker.[[TNF-POS], [EMP-POS,PNF-POS], [APC], [MAJ]]\n\nWeaknesses:\n- Figure 2: the plots are too small.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]]\n- Distilling may hurt performance ( Figure 2.d)[[TNF-NEG], [PNF-NEG], [CRT], [MIN]]\n- The method lacks details (see Questions above)[[MET-NEG], [SUB-NEG], [DFT], [MIN]]\n- No comparisons with prior work are provided.[[RWK-NEG], [SUB-NEG], [CRT], [MAJ]] The paper cites many previous approaches to this but does not compare against any of them.[[RWK-NEG], [SUB-NEG], [CRT], [MAJ]] \n- A second testbed (such as navigation or manipulation) would bring the paper up a notch.[[MET-NEU], [EMP-NEU], [SUG,DIS], [MIN]] \n\nIn conclusion, the paper's approach to multitask learning is a clever combination of prior work.[[RWK-POS,MET-POS], [EMP-POS,CMP-POS], [APC], [MAJ]] The method is clear[[MET-POS], [EMP-POS,CMP-POS], [APC], [MAJ]] but not precisely described.[[MET-NEG], [CLA-NEG], [CRT], [MAJ]] The results are promising.[[RES-POS], [EMP-POS], [APC], [MAJ]] I think that this is a good approach to the problem that could be used in real-world scenarios.[[RES-POS], [EMP-POS], [APC], [MAJ]] With some filling out, this could be a great paper."[[OAL-POS], [EMP-POS], [APC], [MAJ]]