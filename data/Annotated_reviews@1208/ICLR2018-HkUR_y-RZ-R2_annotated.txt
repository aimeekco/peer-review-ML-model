"This paper proposes an adaptation of the SEARN algorithm to RNNs for generating text.[[INT-NEU,MET-NEU], [null], [SMY], [GEN]] In order to do so, they discuss various issues on how to scale the approach to large output vocabularies by sampling which actions the algorithm to explore.[[MET-NEU], [null], [SMY], [GEN]]\n\nPros:\n- Good literature review.[[RWK-POS], [CMP-POS], [APC], [MAJ]] But the future work on bandits is already happening:[[FWK-NEU], [IMP-NEU], [DIS], [MIN]]\nPaper accepted at ACL 2017: Bandit Structured Prediction for Neural Sequence-to-Sequence Learning. Julia Kreutzer, Artem Sokolov, Stefan Riezler.[[RWK-NEU,FWK-NEU], [IMP-NEU], [DIS], [MIN]]\n\n\nCons:\n- The key argument of the paper is that SEARNN is a better IL-inspired algorithm than the previously proposed ones.[[RWK-NEU,MET-NEU], [CMP-NEU], [SMY,DIS], [MIN]] However there is no direct comparison either theoretical or empirical against them.[[MET-NEG], [SUB-NEG,EMP-NEG], [DFT], [MIN]] In the examples on spelling using the dataset of Bahdanau et al. 2017, no comparison is made against their actor-critic method.[[RWK-NEU,DAT-NEU,MET-NEG], [CMP-NEG], [DFT], [MAJ]] Furthermore, given its simplicity, I would expect a comparison against scheduled sampling.[[MET-NEU], [SUB-NEU], [DIS], [MIN]]\n\n- A lot of important experimental details are in the appendices and they differ among experiments.[[EXP-NEU,RES-NEG], [EMP-NEG], [CRT], [MAJ]] For example, while mixed rollins are used in most experiments, reference rollins are used in MT, which is odd since it is a bad option theoretically.[[EXP-NEU], [EMP-NEG], [CRT], [MAJ]] Also,  no details are given on how the mixing in the rollouts was tuned.[[MET-NEG], [SUB-NEG], [DFT,CRT], [MAJ]] Finally, in the NMT comparison while it is stated that similar architecture is used in order to compare fairly against previous work, this is not the case eventually, as it is acknowledged at least in the case of MIXER.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] I would have expected the same encoder-decoder architecture to have been used for all the methods considered.[[EXP-NEG], [EMP-NEG], [DIS], [MIN]]\n \n- the two losses introduced are not really new.[[MET-NEG], [NOV-NEG], [CRT], [MIN]] The log-loss is just MLE, only assuming that instead of a fixed expert that always returns the same target, we have a dynamic one.[[RES-NEU], [EMP-NEU], [DIS], [MIN]] Note that the notion of dynamic expert is present in the SEARN paper too.[[RWK-NEU], [CMP-NEU], [DIS], [MIN]] Goldberg and Nivre just adapted it to transition-based dependency parsing.[[RWK-NEU], [CMP-NEU], [DIS], [MIN]] Similarly, since the KL loss is the same as XENT, why give it a new name?[[RES-NEU], [EMP-NEU], [QSN], [MIN]]\n\n- the top-k sampling method is essentially the same as the targeted exploration of Goodman et al. (2016) which the authors cite.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MIN]] Thus it is not a novel contribution.[[MET-NEG], [NOV-NEG], [CRT], [MAJ]]\n  \n- Not sure I see the difference between the stochastic nature of SEARNN and the online one of LOLS mentioned in section 7.[[MET-NEU], [CMP-NEU], [DIS], [MIN]] They both could be mini-batched similarly.[[MET-NEU], [CMP-NEU], [DIS], [MIN]] Also, not sure I see why SEARNN can be used on any task, in comparison to other methods.[[MET-NEU], [CMP-NEU], [DIS], [MIN]] They all seem to be equally capable.[[MET-NEU], [CMP-NEU], [DIS], [MIN]]\n\nMinor comments:\n- Figure 1: what is the difference between \"cost-sensitive loss\" and just \"loss\"?[[TNF-NEU], [PNF-NEU], [QSN], [MIN]]\n- local vs sequence-level losses: the point in Ranzato et al and Wiseman & Rush is that the loss they optimizise (BLEU/ROUGE) do not decompose over the the predictions of the RNNs.[[MET-NEU], [CMP-NEU], [DIS], [MIN]]\n- Can't see why SEARNN can help with the vanishing gradient problem.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Seem to be rather orthogonal.\n"[[MET-NEU], [EMP-NEU], [DIS], [MIN]]