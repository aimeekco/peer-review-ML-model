"This paper presents methods to reduce the variance of policy gradient using an action dependent baseline.[[INT-NEU], [null], [SMY], [GEN]] Such action dependent baseline can be used in settings where the action can be decomposed into factors that are conditionally dependent given the state.[[RWK-NEU,MET-NEU], [EMP-NEU], [DIS], [MAJ]] The paper:\n(1) shows that using separate baselines for actions, each of which can depend on the state and other actions is bias-free[[RWK-NEU,MET-NEU], [null], [SMY], [GEN]]\n(2) derive the optimal action-dependent baseline, showing that it does not degenerate into state-only dependent baseline, i.e. there is potentially room for improvement over state-only baselines.[[RWK-NEU,MET-NEU], [null], [SMY], [GEN]]\n(3) suggests using marginalized action-value (Q) function as a practical baseline, generalizing the use of value function in state-only baseline case.[[RWK-NEU,MET-NEU], [null], [SMY], [GEN]]\n(4) suggests using MC marginalization and also using the \"average\" action to improve computational feasibility[[MET-NEU], [null], [SMY], [GEN]]\n(5) combines the method with GAE techniques to further improve convergence by trading off bias and variance[[MET-POS], [null], [SMY], [GEN]]\n\nThe suggested methods are empirically evaluated on a number of settings.[[EXP-NEU,MET-NEU], [EMP-NEU], [SMY], [MAJ]] Overall action-dependent baseline outperform state-only versions.[[MET-POS], [EMP-POS], [SMY], [MAJ]] Using a single average action marginalization is on par with MC sampling, which the authors attribute to the low quality of the Q estimate. [[MET-NEG], [EMP-NEG], [CRT], [MAJ]]Combining GAE shows that a hint of bias can be traded off with further variance reduction to further improve the performance.[[MET-NEU], [EMP-NEU], [SUG], [MAJ]]\n\nI find the paper interesting and practical to the application of policy gradient in high dimensional action spaces with some level of conditional independence present in the action space.[[MET-POS], [EMP-POS], [APC], [MAJ]] In light of such results, one might change the policy space to enforce such structure.[[RES-POS], [IMP-POS], [APC], [MAJ]]\n\nNotes:\n- Elaborate further on the assumption made in Eqn 9.[[MET-NEU], [EMP-NEU], [SUG], [MIN]] Does it mean that the actions factors cannot share (too many) parameters in the policy construction, or that shared parameters can only be applied to the state?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n- Eqn 11 should use \\simeq.[[MET-NEU], [EMP-NEU], [SUG], [MIN]]\n- How can the notion of average be extended to handle multi-modal distributions, or categorical or structural actions?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Consider expanding on that in section 4.5.[[MET-NEU], [null], [SUG], [MIN]]\n- The discussion on the DAG graphical model is lacking experimental analysis (where separate baselines models are needed).[[EXP-NEU,ANA-NEG], [EMP-NEG], [CRT], [MAJ]] How would you train such baselines?[[EXP-NEU], [EMP-NEU], [QSN], [MAJ]]\n- Figure 4 is impossible to read in print.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]] The fonts are too small for the numbers and the legends.[[OAL-NEG], [PNF-NEG], [CRT], [MIN]]\n"