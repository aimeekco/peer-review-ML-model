"This paper shows a simple method for predicting the performance that neural networks will achieve with a given architecture, hyperparameters, and based on an initial part of the learning curve.[[INT-NEU,MET-NEU], [null], [SMY], [GEN]] \nThe method assumes that it is possible to first execute 100 evaluations up to the total number of epochs.[[MET-NEU], [null], [SMY], [GEN]] \nFrom these 100 evaluations (with different hyperparameters / architectures), the final performance y_T is collected.[[MET-NEU], [null], [SMY], [GEN]] Then, based on an arbitrary prefix of epochs y_{1:t}, a model can be learned to predict y_T.[[MET-NEU], [null], [SMY], [GEN]]\nThere are T different models, one for each prefix y_{1:t} of length t.[[MET-NEU], [null], [SMY], [GEN]] The type of model used is counterintuitive for me; why use a SVR model?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Especially since uncertainty estimates are required, a Gaussian process would be the obvious choice.[[MET-NEU], [null], [DIS], [MIN]] \n\nThe predictions in Section 3 appear to be very good, and it is nice to see the ablation study.[[RES-POS], [EMP-POS], [APC], [MAJ]]\n\nSection 4 fails to mention that its use of performance prediction for early stopping follows exactly that of Domhan et al (2015) and that this is not a contribution of this paper; this feels a bit disingenious and should be fixed.[[RWK-NEG,MET-NEG], [CMP-NEG], [CRT], [MAJ]]\nThe section should also emphasize that the models discussed in this paper are only applicable for early stopping in cases where the function evaluation budget N is much larger than 100.[[MET-NEU,ANA-NEU], [SUB-NEU,EMP-NEU], [DIS], [MIN]]\nThe emphasis on the computational demand of 1-3 minutes for LCE seems like a red herring: MetaQNN trained 2700 networks in 100 GPU days, i.e., about 1 network per GPU hour.[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]] It trained 20 epochs for the studied case of CIFAR, so 1-3 minutes per epoch on the CPU can be implemented with zero overhead while the network is training on the GPU.[[DAT-NEU,MET-NEU], [null], [DIS], [GEN]] Therefore, the following sentence seems sensational without substance: \"Therefore, on a full meta-modeling experiment involving thousands of neural network configurations, our method could be faster by several orders of magnitude as compared to LCE based on current implementations.[[EXP-NEU,MET-NEU], [null], [DIS], [MIN]]\"\n\nThe experiment on fast Hyperband is very nice at first glance, but the longer I think about it the more questions I have.[[EXP-POS], [EMP-POS], [APC], [MAJ]] During the rebuttal I would ask the authors to extend f-Hyperband all the way to the right in Figure 6 (left) and particularly in Figure 6 (right).[[EXP-NEU,TNF-NEU], [null], [DIS], [MIN]] Especially in Figure 6 (right), the original Hyperband algorithm ends up higher than f-Hyperband.[[MET-NEU,TNF-NEU], [null], [DIS], [MIN]] The question this leaves open is whether f-Hyperband would reach the same performance when continued or not. [[MET-NEU,RES-NEU], [EMP-NEU], [QSN], [MIN]]\nI would also request the paper not to casually mention the 7x speedup that can be found in the appendix, without quantifying this.[[CNT], [null], [SUG], [MIN]] This is only possible for a large number of 40 Hyperband iterations, and in the interesting cases of the first few iterations speedups are very small.[[EXP-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] Also, do the simulated speedup results in the appendix account for potentially stopping a new best configuration, or do they simply count how much computational time is saved, without looking at performance? The latter would of course be extremely misleading and should be fixed.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] I am looking forward to a clarification in the rebuttal period.[[RES-NEU], [EMP-NEU], [DIS], [MIN]] \nFor relating properly to the literatue, the experiment for speeding up Hyperband should also mention previous methods for speeding up Hyperband by a model (I only know one by the authors' reference Klein et al (2017)).[[RWK-NEU,EXP-NEU], [SUB-NEU,EMP-NEU], [SUG], [MAJ]]\n\nOverall, this paper appears very interesting.[[OAL-POS], [CNT], [APC], [MAJ]] The proposed technique has some limitations,[[MET-POS], [EMP-POS], [APC], [MAJ]] but in some settings it seems very useful.[[MET-POS], [EMP-POS], [APC], [MAJ]]  I am looking forward to the reply to my questions above; my final score will depend on these.[[OAL-NEU], [REC-NEU], [FBK], [MAJ]] \n\nTypos / Details: \n- The range of the coefficient of determination is from 0 to 1.[[OAL-NEG], [CLA-NEG], [CRT], [MIN]]  Table 1 probably reports 100 * R^2? Please fix the description.[[TNF-NEG], [CLA-NEG], [CRT], [MIN]] \n- I did not see Table 1 referenced in the text.[[TNF-NEG], [SUB-NEG], [DFT], [MIN]]\n- Page 3: \"more computationally and\" -> \"more computationally efficient and[[CNT], [CLA-NEG], [CRT], [MIN]]\"\n- Page 3: \"for performing final\" -> \"for predicting final[[CNT], [CLA-NEG], [CRT], [MIN]]\"\n\n\nPoints in favor of the paper:\n- Simple method[[MET-POS], [EMP-POS], [APC], [MAJ]]\n- Good prediction results[[RES-POS], [EMP-POS], [APC], [MAJ]]\n- Useful possible applications identified[[MET-POS], [EMP-POS], [APC], [MAJ]]\n\nPoints against the paper:\n- Methodological advances are limited / unmotivated choice of model[[MET-NEG], [SUB-NEG,EMP-NEG], [DFT], [MAJ]]\n- Limited applicability to settings where >> 100 configurations can be run fully[[MET-NEG], [SUB-NEG,EMP-NEG], [DFT], [MAJ]]\n- Possibly inflated results reported for Hyperband experiment"[[EXP-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]]