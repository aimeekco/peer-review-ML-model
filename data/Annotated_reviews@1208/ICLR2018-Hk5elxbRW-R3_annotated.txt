"This paper introduces a smooth surrogate loss function for the top-k SVM, for the purpose of plugging the SVM to the deep neural networks.[[RWK-NEU,EXP-NEU,MET-NEU], [null], [SMY], [GEN]] The idea is to replace the order statistics, which is not smooth and has a lot of zero partial derivatives, to the exponential of averages, which is smooth and is a good approximation of the order statistics by a good selection of the \"temperature parameter\". [[RWK-NEU,PDI-NEU,EXP-NEG], [EMP-NEG], [APC], [MAJ]]The paper is well organized and clearly written.[[OAL-POS], [CLA-POS,PNF-POS], [APC], [MAJ]] The idea deserves a publication.[[OAL-POS], [IMP-POS,REC-POS], [APC], [MAJ]]\n\nOn the other hand, there might be better and more direct solutions to reduce the combinatorial complexity.[[RWK-NEU,EXP-NEG], [EMP-NEG], [DFT], [MIN]] When the temperature parameter is small enough, both of the original top-k SVM surrogate loss (6) and the smooth loss (9) can be computed precisely by sorting the vector s first, and take a good care of the boundary around s_{[k]}.[[RWK-NEU,EXP-NEU], [EMP-NEU], [SMY], [GEN]]"
