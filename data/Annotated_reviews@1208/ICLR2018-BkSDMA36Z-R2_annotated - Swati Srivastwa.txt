"The authors present a model for text classification.[[INT-NEU], [null], [SMY], [GEN]] The parameters of the model are an embedding for each word and a local context unit. [[INT-NEU], [null], [SMY], [GEN]] The local context unit can be seen as a filter for a convolutional layer, but which filter is used at location i depends on the word at location i (i.e. there is one filter per vocabulary word).[[MET-NEU], [null], [SMY], [GEN]] After the filter is applied to the embeddings and after max pooling, the word-context region embeddings are summed and fed into a neural network for the classification task.[[MET-NEU], [null], [SMY], [GEN]] The embeddings, the context units and the neural net parameters are trained jointly on a supervised text classification task.[[MET-NEU], [null], [SMY], [GEN]] The authors also offer an alternative model, which changes the role of the embedding an the context unit, and results in context-word region embeddings.[[MET-NEU], [null], [SMY], [GEN]] Here the embedding of word i is combined with the elements of the context units of words in the context.[[MET-NEU], [null], [SMY], [GEN]] To get the region embeddings both model (word-context and context-word) combine attributes of the words (embeddings) with how their attributes should be emphasized or deemphasized based on nearby words (local context units and max pooling) while taking into account the relative position of the words in the context (columns of the context units).[[MET-NEU], [null], [SMY], [GEN]] \n\nThe method beats existing methods for text classification including d-LSTMs , BoWs, and ngram TFIDFs on held out classification accuracy.[[MET-POS], [CMP-POS], [APC], [MAJ]] the choice of baselines is convincing.[[RWK-POS], [CMP-POS], [APC], [MAJ]] What is the performance of the proposed method if the embeddings are initialized to pretrained word embeddings and a) trained for the classification task together with randomly initialized context units b) frozen to pretrained embeddings and only the context units are trained for the classification task?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nThe introduction was fine.[[INT-POS], [CNT], [APC], [MAJ]] Until page 3 the authors refer to the context units a couple of times without giving some simple explanation of what it could be.[[CNT], [CNT], [DFT], [MIN]] A simple explanation in the introduction would improve the writing.[[INT-NEU], [CLA-NEU], [SUG], [MIN]]\nThe related work section only makes sense *after* there is at least a minimal explanation of what the local context units do.[[RWK-NEU], [SUB-NEU], [DIS], [MIN]] A simple explanation of the method, for example in the introduction, would then make the connections to CNNs more clear.[[INT-NEU,MET-NEU], [CLA-NEU,SUB-NEU], [SUG], [MIN]] Also, in the related work, the authors could include more citations (e.g. the d-LSTM and the CNN based methods from Table 2) and explain the qualitative differences between their method and existing ones.[[RWK-NEU,MET-NEU], [SUB-NEU,CMP-NEU], [SUG], [MIN]]\n\nThe authors should consider adding equation numbers.[[MET-NEU], [PNF-NEU], [SUG], [MIN]] The equation on the bottom of page 3 is fine, but the expressions in 3.2 and 3.3 are weird.[[MET-NEG], [PNF-NEG], [CRT], [MIN]] A more concise explanation of the context-word region embeddings and the word-context region embeddings would be to instead give the equation for r_{i,c}.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]  \n\nThe included baselines are extensive and the proposed method outperforms existing methods on most datasets.[[RWK-POS,DAT-POS,MET-POS], [EMP-POS], [APC], [MAJ]] In section 4.5 the authors analyze region and embedding size, which are good analyses to include in the paper.[[ANA-POS], [EMP-POS], [APC], [MAJ]] Figure 2 and 3 could be next to each other to save space.[[TNF-NEU], [PNF-NEU], [SUG], [MIN]] \nI found the idea of multi region sizes interesting, but no description is given on how exactly they are combined.[[MET-POS], [EMP-POS], [APC], [MAJ]] Since it works so well, maybe it could be promoted into the method section?[[MET-NEU], [PNF-NEU], [QSN], [MIN]] Also, for each data set, which region size worked best?[[DAT-NEU], [EMP-NEU], [QSN], [MIN]]\n\nQualitative analysis: It would have been nice to see some analysis of whether the learned embeddings capture semantic similarities, both at the embedding level and at the region level.[[ANA-NEU], [SUB-NEU], [SUG], [MIN]] It would also be interesting to investigate the columns of the context units, with different columns somehow capturing the importance of relative position.[[ANA-NEU], [SUB-NEU], [SUG], [MIN]] Are there some words for which all columns are similar meaning that their position is less relevant in how they affect nearby words?[[CNT], [PNF-NEU], [SUG], [MIN]] And then for other words with variation along the columns of the context units, do their context units modulate the embedding more when they are closer or further away?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] \n\nPros:\n + simple model\n + strong quantitative results[[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]]\n\nCons:\n - notation (i.e. precise definition of r_{i,c})[[MET-NEU], [PNF-NEU], [DIS], [MIN]]\n - qualitative analysis could be extended\n - writing could be improved  "[[ANA-NEU,OAL-NEU], [EMP-NEU,CLA-NEU], [SUG], [MIN]]