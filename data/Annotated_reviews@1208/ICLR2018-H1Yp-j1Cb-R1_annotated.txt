"It is well known that the original GAN (Goodfellow et al.) suffers from instability and mode collapsing[[RWK-NEU,MET-NEG], [EMP-NEG], [SMY,DFT], [MIN]]. Indeed, existing work has pointed out that the standard GAN training process may not converge if we insist on obtaining pure strategies (for the minmax game). [[RWK-NEU,MET-NEG], [EMP-NEG], [SMY,DIS], [GEN]]The present paper proposes to obtain mixed strategy through an online learning approach.[[PDI-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] Online learning (no regret) algorithms have been used in finding an equilibrium for zero sum game. However, most theoretical convergence results are known for convex-concave loss.[[RWK-NEU,EXP-NEU,MET-NEU,RES-NEU], [IMP-NEU,EMP-NEU], [SMY], [GEN]] One interesting theoretical contribution of the paper is to show that convergence result can be proved if one player is a shallow network (and concave in M)[[RWK-POS,EXP-POS,RES-POS], [SUB-POS,EMP-POS], [APC], [MAJ]].In particular, the concave player plays the FTRL algorithm with standard L2 regularization term.[[RWK-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] The regret of concave player can be bounded using existing result for FTRL.[[RWK-NEU,EXP-NEU,RES-NEU], [EMP-NEU], [SMY], [GEN]]The regret for the other player is more interesting: it uses the fact the adversary's strategy doesn't change too drastically. [[RWK-NEU,EXP-NEU], [null], [SMY], [GEN]]Then a lemma by Kalai and Vempala can be used.[[RWK-NEU,EXP-NEU], [EMP-NEU], [DIS], [GEN]] The theory part of the paper is reasonable and quite well written.[[RWK-POS,EXP-POS], [CLA-POS,SUB-POS], [APC], [MAJ]] \n\nBased on the theory developed, the paper presents a practical algorithm.[[RWK-POS,EXP-POS], [SUB-POS,IMP-POS,EMP-POS], [APC], [MAJ]] Compared to the standard GAN training, the new algorithm returns mixed strategy and examine several previous models (instead of the latest) in each iteration.[[RWK-NEU,EXP-NEU,MET-NEU,ANA-NEU], [CMP-NEU,EMP-NEU], [SMY], [GEN]] The paper claims that this may help to prevent model collapsing.[[INT-NEU,PDI-NEU,MET-NEU], [IMP-NEU,EMP-NEU], [SMY], [GEN]]\n\nHowever, the experimental part is less satisfying.[[RWK-NEG,EXP-NEG], [EMP-NEG], [DFT], [MIN]] From figure 2, I don't see much advantage of Checkhov GAN.[[MET-NEG,TNF-NEG], [IMP-NEG], [DFT], [MIN]] In other experiments, I don't see much improvement neither (CIFAR10 and CELEBA).[[RWK-NEG,EXP-NEG], [EMP-NEG], [DFT], [MIN]]The paper didn't really compare other popular GAN models, especially WGAN and its improved version[[RWK-NEG,MET-NEG], [CMP-NEG,EMP-NEG], [DFT], [MIN]], which is already quite popular by now and should be compared with.[[RWK-POS,MET-POS], [CMP-POS], [APC], [MAJ]]\n\nOverall, I think it is a borderline paper.[[INT-NEU,RWK-NEU,OAL-NEU], [APR-NEU,REC-NEU], [SUG,FBK], [GEN]]\n\n-------------------------\nI read the response and the new experimental results regarding WGAN.[[RWK-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]]\nThe experimental results make more sense now.[[RWK-POS,EXP-POS], [IMP-POS,EMP-POS], [APC], [MAJ]]\nIt would be interesting to see whether the idea can be applied to more recent GAN models and still perform better[[RWK-POS,PDI-POS,MET-POS], [IMP-POS,EMP-POS], [APC], [MAJ]].\nI raised my score to 7.[[EXT-POS], [null], [APC], [MAJ]]\n\n"