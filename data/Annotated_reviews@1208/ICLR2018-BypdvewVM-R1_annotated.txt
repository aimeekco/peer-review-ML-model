"I liked this paper mostly because it surprised me and because it might spur the development of novel variants of Difference Target-Propagation (DTP).[[INT-POS], [null], [APC], [MAJ]] The paper does a good job of highlighting the relevant background and issues and introduces a slight variation to DTP which actually works as well while being more biologically plausible.[[INT-POS,RWK-POS], [null], [APC], [MAJ]] There was a concern or assumption in the original DTP paper about the target for the penultimate layer (before the output layer) which seems to have been excessive, i.e., the DTP propagation rule actually works on the last layer and there is no need to use the exact gradient propagation for it, at least according to these experiments.[[RWK-NEU,EXP-NEU], [EMP-NEU], [DIS], [GEN]] In call cases, the variant using the DTP target update everywhere works about as well as using the true gradient for the output layer.[[MET-POS], [EMP-POS], [APC], [MAJ]]  Another quirk that the proposed variant (SDTP) removes from the orignal DTP paper is the way noise is handled, and I agree that denoising makes a lot of sense (than noise preservation) while being more biologically plausible. [[MET-POS], [EMP-POS], [APC], [MAJ]] Finally, the authors did a good job of establishing a benchmark which could be used by others attempting to evaluate new biologically plausible alternatives to backprop.[[OAL-POS], [EMP-POS], [APC], [MAJ]] The paper is very clear and I have just outlined the original contributions and significance (DTP may have been a bit forgotten and is worth another look, apparently).[[OAL-POS], [CLA-POS], [APC], [GEN]]\n\nIn the negatives, the paper should mention in the discussion and intro that all the TP variants ignore the issue of dynamics.[[INT-NEG], [CMP-NEU], [SUG], [MIN]] We know that there are of course lateral connections and that feedback connections do not operate independently of the feedforward one (or there would be a need for a precise 'clockwork' mechanism to sweep layers forward and backward, which seems not very plausible).[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n\nIn the experimental results section, it would be good to report the CNN results as well (with shared weights, same architecture)[[EXP-NEU,RES-NEU], [NULL], [SUG], [MIN]]. Also, training errors should be shown, since I suspect that underfitting may be happening especially in the case of ImageNet.[[RES-NEG], [PNF-NEU], [SUG], [MIN]] If that was the case, future work should first explore higher capacity (which may require larger-memory GPUs...).[[FWK-NEG], [NULL], [SUG], [MAJ]] Finally, in the description of architectures, please define the structure notation, e.g. (3 x 3, 32, 2, SAME)."[[MET-NEU], [CLA-NEG], [SUG], [MIN]]