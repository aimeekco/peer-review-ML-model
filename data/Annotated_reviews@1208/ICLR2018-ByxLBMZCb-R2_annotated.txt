"Summary:\n\nThis paper studies the geometry of linear and neural networks and provides conditions under which the local minima of the loss are global minima for these non-convex problems.[[INT-NEU], [null], [SMY], [GEN]] The paper studies locally open maps, which preserve the local minima geometry.[[MET-NEU], [null], [SMY], [GEN]] Hence a local minima of l(F(W)) is a local minima of l(s) when s=F(W) is a locally open map.[[MET-NEU], [null], [SMY], [GEN]] Theorem 3 provides conditions under which the multiplication X*Y is a locally open map.[[MET-NEU], [null], [SMY], [GEN]] For a pyramidal feed forward net, if the weights in each layer have full rank,  input X is full rank, and the link function is invertible, then that local minima is a global minima.[[MET-NEU], [null], [SMY], [GEN]]  \n\nComments:\n\nThe locally open maps (Behrends 2017) is an interesting concept.[[RWK-NEU,PDI-POS], [null], [APC], [MAJ]] However I am not convinced that the paper is able to show stronger results about the geometry of linear/neural networks.[[MET-NEU,RES-NEU], [EMP-NEU], [CRT], [MAJ]] Further the claims all over the paper, comparing with the existing works. are over the top and not justified.[[RWK-NEU,OAL-NEG], [CMP-NEG], [CRT], [MAJ]] I believe the paper needs a significant rewriting[[OAL-NEG], [CLA-NEG], [CRT], [MAJ]].\n\nThe results are not a strict improvement over existing works.[[RES-NEG], [CMP-NEG], [CRT], [MAJ]] For neural networks, Nguyen and Hein (2017) assume the link function is differentiable.[[RWK-NEU,BIB-NEU], [CMP-NEU], [DIS], [MAJ]] This paper assumes the link function is invertible.[[MET-NEU], [EMP-NEU], [SMY], [MAJ]] Both papers can handle sigmoid/tanh, but cannot handle ReLU.[[MET-NEU], [EMP-NEU], [SMY], [MAJ]]\n\nResults for linear networks are not an improvement over existing works.[[RES-NEU], [CMP-NEG], [CRT], [MAJ]] Paper claims to remove assumption on Y, but they get much weaker results as they cannot differentiate between saddle points and global minima, for a critical point.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]  Results are also written in a confusing way as stating each critical point is a saddle or a global minima.[[RES-NEG], [CLA-NEG], [CRT], [MAJ]] Instead the presentation can be simplified by just discussing the equivalency between local minima and global minima, as the proposed framework cannot handle critical points directly.[[RES-NEU], [PNF-NEU], [SUG], [MAJ]]\n\nProof of Lemma 7 seems to have typos/mistakes.[[MET-NEG], [CLA-NEG], [CRT], [MIN]] What is \\bar{W_i}? Why are the first two equations just showing d_i \\leq d_i ? How do you use this to conclude locally openness of \\mathcal{M}?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nAuthors claim their result extends the results for matrix completion from Ge et al. (2016) . This is false claim as (10) is not the matrix completion problem with missing entries, and the results in Ge et al. (2016) do not assume any non-degeneracy conditions on W."[[RWK-NEU,RES-NEG,BIB-NEU], [CMP-NEG], [CRT], [MAJ]]