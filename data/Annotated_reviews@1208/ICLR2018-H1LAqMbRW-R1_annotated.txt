"The paper proposes to use a pretrained model-free RL agent to extract the developed state representation and further re-use it for learning forward model of the environment and planning.[[INT-NEU], [null], [SMY], [GEN]]\nThe idea of re-using a pretrained agent has both pros and cons.[[PDI-NEU], [null], [SMY], [GEN]] On one hand, it can be simpler than learning a model from scratch because that would also require a decent exploration policy to sample representative trajectories from the environment.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] On the other hand, the usefulness of the learned representation for planning is unclear.[[MET-NEU], [EMP-NEU], [CRT], [MAJ]] A model-free agent can (especially if trained with certain regularization) exclude a lot of information which is potentially useful for planning, but is it necessary for reactively taking actions.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]]\nA reasonable experiment/baseline thus would be to train a model-free agent with a small reconstruction loss on top of the learned representation.[[MET-NEU], [EMP-NEU], [SUG], [MAJ]]\u2028In addition to that, one could fine-tune the representation during forward model training.[[MET-NEU], [EMP-NEU], [SUG], [MAJ]] \nIt would be interesting to see if this can improve the results.[[RES-NEU], [EMP-NEU], [SUG], [MAJ]]\n\nI personally miss a more technical and detailed exposition of the ideas.[[MET-NEU], [EMP-NEU], [DFT], [MAJ]] For example, it is not described anywhere what loss is used for learning the model. [[MET-NEU], [EMP-NEG], [CRT], [MAJ]]MCTS is not described and a reader has to follow references and infer how exactly is it used in this particular application which makes the paper not self-contained.[[MET-NEU], [EMP-NEG], [SMY], [MAJ]] \nAgain, due to lack of equations, I don\u2019t completely understand the last paragraph of 3.2, I suggest re-writing it (as well as some other parts) in a more explicit way.[[MET-NEU], [CLA-NEU,EMP-NEG], [SUG,CRT], [MAJ]]\nI also could find the details on how figure 1 was produced[[TNF-NEG], [EMP-NEG], [QSN], [MAJ]]. As I understand, MCTS was not used in this experiment.[[EXP-NEG], [EMP-NEU], [DIS], [MAJ]] If so, how would one play with just a forward model?[[MET-NEU], [EMP-NEU], [QSN], [MAJ]]\n\nIt is a bit disappointing that authors seem to consider only deterministic models which clearly have very limited applicability.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Is mini-RTS a deterministic environment? [[MET-NEU], [null], [QSN], [MAJ]]\nWould it be possible to include a non-deterministic baseline in the experimental comparison?[[MET-NEU], [EMP-NEU], [QSN], [MAJ]]\n\nExperimentally, the results are rather weak compared to pure model-free agents.[[RWK-NEU,EXP-NEU,RES-NEG], [CMP-NEG], [CRT], [MAJ]] Somewhat unsatisfying, longer-term prediction results into weaker game play.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] Doesn\u2019t this support the argument about need in stochastic prediction?[[MET-NEU], [EMP-NEU], [QSN], [MAJ]] \n\nTo me, the paper in it\u2019s current form is not written well and does not contain strong enough empirical results, so that I can\u2019t recommend acceptance.[[RES-NEG], [CLA-NEG,REC-NEG], [FBK], [MAJ]] \n\nMinor comments:\n* MatchA and PredictPi models are not introduced under such names[[OAL-NEG], [CLA-NEG], [CRT], [MIN]]\n* Figure 1 that introduces them contains typos.[[TNF-NEG], [CLA-NEG], [CRT], [MIN]] \n* Formatting of figure 8 needs to be fixed.[[TNF-NEU], [PNF-NEU], [SUG], [MIN]] This figure does not seem to be referred to anywhere in the text and the broken caption makes it hard to understand what is happening there.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]]\n"