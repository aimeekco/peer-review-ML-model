"UPDATED COMMENT\nI've improved my score to 6 to reflect the authors' revisions to the paper and their response to my and R2's comments.[[OAL-NEU], [REC-NEU], [FBK], [MAJ]] I still think the work is somewhat incremental, but they have done a good job of exploring the idea (which is nice).[[PDI-POS,OAL-POS], [EMP-POS], [APC], [MAJ]]\n\nORIGINAL REVIEW BELOW\n\nThe paper introduces an architecture that linearly interpolates between ResNets and vanilla deep nets (without skip connections).[[MET-NEU], [null], [SMY], [GEN]] The skip connections are penalized by Lagrange multipliers that are gradually phased out during training.[[EXP-NEU,MET-NEU], [null], [SMY], [GEN]] The resulting architecture outperforms vanilla deep nets and sometimes approaches the performance of ResNets.[[MET-POS], [CMP-POS], [APC], [MAJ]]\n\nIt\u2019s a nice, simple idea.[[PDI-POS], [EMP-POS], [APC], [MAJ]]However, I don\u2019t think it\u2019s sufficient for acceptance.[[OAL-NEG], [REC-NEG], [CRT], [MAJ]] Unfortunately, this seems to be a simple idea that doesn't work as well as the simpler idea (ResNets) that inspired it.[[PDI-NEG], [EMP-NEG], [CRT], [MAJ]] Moreover, the experiments are weak in two senses: (i) there are lots of obvious open questions that should have been explored and closed, see below,[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]] and (ii) the results just aren\u2019t that good.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] \n\nComments:\n\n1. Why force the Lag. multipliers to 1 at the end of training? [[RES-NEU], [EMP-NEU], [QSN], [MIN]]It seems easy enough to treat the alphas as just more parameters to optimize with gradient descent.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]] I would expect the resulting architecture to perform at least as well as variable action nets.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] If not, I\u2019d be curious as to why.[[MET-NEU], [EMP-NEU], [DIS], [GEN]]\n\n2.Similarly, it\u2019s not obvious that initializing the multipliers at 0.5 is the best choice.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] The \u201clooks linear\u201d initialization proposed in \u201cThe shattered gradients problem\u201d (Balduzzi et al) implies that alpha=0 may work better.[[RWK-NEU], [CMP-NEU], [DIS], [MIN]] Did the authors try any values besides 0.5?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] \n\n3. The final paragraph of the paper discusses extending the approach to architectures with skip-connections.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Firstly, it\u2019s not clear to me what this would add, since the method is already interpolating in some sense between vanilla and resnets.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Secondly, why not just do it? \n\n"[[MET-NEU], [EMP-NEU], [QSN], [MIN]]