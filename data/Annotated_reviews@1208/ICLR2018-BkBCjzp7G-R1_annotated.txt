"The paper proposes a novel workflow for acceleration and compression of CNNs.[[PDI-POS], [EMP-POS], [APC], [MAJ]]  The proposed workflow consists of the novel two-pass decomposition of a group of layers, and the fine-tuning of the remaining network.[[MET-POS], [EMP-POS], [APC], [MAJ]] This process is applied iteratively to different groups of layers.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] The authors also propose a way to determine the target rank of each layer given the target overall acceleration.[[MET-NEU], [null], [SMY], [GEN]] The authors report the highest measured acceleration of VGG16 using low-rank approximation techniques (6.2x vs 5x previously) with a similar accuracy drop (1.2% vs 1.0% previously).[[RES-NEG], [EMP-NEG], [CRT], [MIN]]\nThe paper is well-structured, and the proposed method is clearly described.[[OAL-POS], [PNF-POS], [APC], [MAJ]] However it would be nice to see the difference to other related methods more clearly.[[RWK-NEU,MET-NEU], [EMP-NEU], [SUG], [MIN]] Some results are counterintuitive if the reader is not familiar with related works (e.g. the Zhang et al. 2016 achieves a lower acceleration with much lower ranks).[[RES-NEU], [EMP-NEU], [DIS], [MIN]]\nThe main concern is the motivation of the two-pass decomposition.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] It is not clear why the the optimized full-rank tensor is more easy to decompose if it was initialized with a low-rank tensor.[[MET-NEU,RES-NEU], [EMP-NEU], [CRT], [MIN]] There are no theoretical results regarding this question in the paper, and the empirical justification is also lacking.[[EXP-NEG,MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] It would be necessary to see the tensor reconstruction error during the following 2 scenarios:\nWe apply the CP decomposition to a pretrained network[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\nWe apply the CP decomposition to a pretrained network, then restore it back into the dense format, optimize it, and then apply the CP decomposition again[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\nWhat is the reconstruction error in case 1?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] What is the reconstruction error during the second CP decomposition in 2?[[RES-NEU], [EMP-NEU], [QSN], [MIN]] What is the accuracy drop after fine-tuning in both scenarios?[[RES-NEU], [EMP-NEU], [QSN], [MIN]] Figure 4 could have answered this question, however it is not clear from the paper whether the CP-ALS procedure was followed by fine-tuning or not.[[MET-NEG,MET-NEG], [EMP-NEG], [QSN], [MAJ]] If it wasn\u2019t, then the comparison is unfair, as the results for CP-ALS are drastically underestimated.[[RES-NEG], [EMP-NEG], [CRT], [MIN]]\nIt would also be nice to see the full learning curves for all experiments, where different stages (decompose->optimize->decompose->finetune->...) are explicitly marked.[[EXP-NEU], [EMP-NEU], [SUG], [MIN]] The reported tables seem to ignore a lot of the relevant information[[TNF-NEG], [EMP-NEG], [CRT], [MIN]].\nAlso Astrid and Lee 2017 do not seem to report the instabilities during fine-tuning of the decomposed layers, and argue that these layers should not be freezed.[[RWK-NEU], [CMP-NEU], [DIS], [MIN]] As they use a very similar iterative fine-tuning workflow, it is not clear why the two-pass decomposition + freezing should work better than one-pass decomposition + iterative fine-tuning with no freezing.[[MET-NEU], [CMP-NEU], [DIS], [MIN]] These two methods seem to be closely related and should be thoroughly compared.[[MET-NEU], [CMP-NEU], [SUG], [MIN]]\nThe improvement w.r.t. other methods seems marginal.[[RES-NEG], [CMP-NEG,EMP-NEG], [CRT], [MIN]] The previous SotA result on VGG16 was 5x acceleration with 1% accuracy drop, and here the reported result is 6.2x acceleration with 1.2% accuracy drop.[[RWK-NEU,RES-NEU], [EMP-NEU], [DIS], [MIN]] The authors claim that the previous SotA result was carefully fine-tuned with a low learning rate, and that in this paper they used only default fine-tuning with a high learning rate.[[RWK-NEU,RES-NEU], [CMP-NEU], [DIS], [MIN]] Is it possible to further improve the accuracy by a more careful fine-tuning?[[MET-NEU,RES-NEU], [EMP-NEU], [QSN], [MIN]] Right now the results are not very convincing.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]]\nI would be glad to reconsider my grade if the questions regarding the motivation of the two-pass decomposition and the comparison with Astrid and Lee 2017 are answered.[[RWK-NEU], [CMP-NEU], [DIS], [MIN]]\nOther comments and remarks:\nThe meaning of the following sentence is not clear, it probably should be rephrased: \u201cWe observed that if the network is trained in the restored dense form, the training result can be more stable because of its smoother convex.[[EXP-NEG,RES-NEG], [CLA-NEG], [CRT], [MAJ]]\u201d. What does \u201csmoother convex\u201d mean?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\nIt should be stated more clearly how the results from Figure 4 were obtained.[[RES-NEU,TNF-NEU], [SUB-NEU], [DIS], [MIN]]\nIt would be interesting to see the accuracy of the fitness approximation during the rank selection procedure.[[RES-NEU], [EMP-NEU], [SUG], [MAJ]]\nIs it possible to perform the CP decomposition by minimizing the activation reconstruction loss (like proposed by Zhang et al. 2016), and not the tensor reconstruction loss (as usual)?[[RES-NEU], [CMP-NEU,EMP-NEU], [QSN], [MIN]] It seems as a more natural way to do it.[[MET-NEU], [null], [DIS], [GEN]]\nThe convergence constraint procedure from Table 4 is not clear.[[TNF-NEG], [EMP-NEG], [CRT], [MAJ]] \u201cour experiment is extended with additional epochs to fine-tune until the accuracy improvement is smaller than 0.1%.[[EXP-NEU,RES-NEU], [null], [DIS], [MIN]]\u201d - what does \u201cthe accuracy improvement is smaller than 0.1%\u201d mean?"[[RES-NEU], [EMP-NEU], [QSN], [MIN]]