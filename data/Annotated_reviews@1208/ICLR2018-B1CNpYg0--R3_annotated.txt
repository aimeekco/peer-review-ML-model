"This paper examines ways of producing word embeddings for rare words on demand.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The key real-world use case is for domain specific terms, but here the techniques are demonstrated on rarer words in standard data sets.[[PDI-NEU], [null], [SMY], [GEN]] The strength of this paper is that it both gives a more systematic framework for and builds on existing ideas (character-based models, using dictionary definitions) to implement them as part of a model trained on the end task.[[PDI-POS], [EMP-POS], [APC], [MAJ]]\n\nThe contribution is clear[[OAL-POS], [CLA-POS], [APC], [MIN]] but not huge.[[OAL-NEG], [SUB-NEG], [CRT], [MIN]] In general, for the scope of the paper, it seems like what is here could fairly easily have been made into a short paper for other conferences that have that category.[[OAL-NEU], [EMP-NEU], [DIS], [MIN]] The basic method easily fits within 3 pages, and while the presentation of the experiments would need to be much briefer, this seems quite possible.[[MET-NEU,EXP-NEG], [EMP-NEG], [CRT], [MAJ]] More things could have been considered.[[EXP-NEG], [SUB-NEG], [DFT], [MIN]] Some appear in the paper, and there are some fairly natural other ones such as mining some use contexts of a word (such as just from Google snippets) rather than only using textual definitions from wordnet.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]] The contributions are showing that existing work using character-level models and definitions can be improved by optimizing representation learning in the context of the final task, and the idea of adding a learned linear transformation matrix inside the mean pooling model (p.3).[[RWK-NEU], [CMP-NEU], [SMY], [GEN]] However, it is not made very clear why this matrix is needed or what the qualitative effect of its addition is.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n\nThe paper is clearly written.[[OAL-POS], [CLA-POS], [APC], [MAJ]] \n\nA paper that should be referred to is the (short) paper of Dhingra et al. (2017): A Comparative Study of Word Embeddings\nfor Reading Comprehension https://arxiv.org/pdf/1703.00993.pdf .[[RWK-NEU,BIB-NEU], [null], [SUG], [MIN]] While it in no way covers the same ground as this paper it is relevant as follows: This paper assumes a baseline that is also described in that paper of using a fixed vocab and mapping other words to UNK.[[RWK-POS], [EMP-POS], [APC], [MIN]] However, they point out that at least for matching tasks like QA and NLI that one can do better by assigning random vectors on the fly to unknown words.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] That method could also be considered as a possible approach to compare against here.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\nOther comments:\n - The paper suggests a couple of times including at the end of the 2nd Intro paragraph that you can't really expect spelling models to perform well in representing the semantics of arbitrary words (which are not morphological derivations, etc.).[[RES-POS], [EMP-POS], [APC], [MAJ]] While this argument has intuitive appeal, it seems to fly in the face of the fact that actually spelling models, including in this paper, seem to do surprisingly well at learning such arbitrary semantics.[[RES-POS], [EMP-POS], [APC], [MAJ]]\n - p.2: You use pretrained GloVe vectors that you do not update.[[MET-NEG], [SUB-NEG], [DFT], [MIN]] My impression is that people have had mixed results, sometimes better, sometimes worse with updating pretrained vectors or not. Did you try it both ways?\n - fn. 1: Perhaps slightly exaggerates the point being made, since people usually also get good results with the GloVe or word2vec model trained on \"only\" 6 billion words \u2013 2 orders of magnitude less data.[[DAT-POS,MET-POS,RES-NEG], [EMP-POS], [APC], [MAJ]]\n - p.4. When no definition is available, is making e_d(w) a zero vector worse than or about the same as using a trained UNK vector?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n - Table 1: The baseline seems reasonable (near enough to the quality of the original Salesforce model from 2016 (66 F1)[[RWK-POS,EXP-POS], [CMP-POS], [APC], [MAJ]] but well below current best single models of around 76-78 F1.[[RWK-NEG], [CMP-NEG], [CRT], [MIN]] The difference between D1 and D3 does well illustrate that better definition learning is done with backprop from end objective.[[RWK-POS,EXP-POS], [CMP-POS], [APC], [MAJ]] This model shows the rather strong performance of spelling models \u2013 at least on this task \u2013 which he again benefit from training in the context of the end objective.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \n - Fig 2: It's weird that only the +dict (left) model learns to connect \"In\" and \"where\".[[MET-NEG], [EMP-NEG], [CRT], [MIN]] The point made in the text between \"Where\" and \"overseas\" is perfectly reasonable, but it is a mystery why the base model on the right doesn't learn to associate the common words \"where\" and \"in\" both commonly expressing a location.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n - Table 2: These results are interestingly different.[[TNF-POS,RES-POS], [EMP-POS], [APC], [MAJ]] Dict is much more useful than spelling here.[[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]] I guess that is because of the nature of NLI, but it isn't 100% clear why NLI benefits so much more than QA from definitional knowledge.[[MET-NEG], [EMP-NEG], [APC], [MAJ]]\n - p.7: I was slightly surprised by how small vocabs (3k and 5k words) are said to be optimal for NLI (and similar remarks hold for SQuAD).[[DAT-NEG], [SUB-NEG], [CRT], [MIN]] My impression is that most papers on NLI use much larger vocabs, no?[[DAT-NEG,RWK-NEG], [CMP-NEG], [QSN], [MIN]]\n - Fig 3: This could really be drawn considerably better: make the dots bigger and their colors more distinct.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]]\n - Table 3: The differences here are quite small and perhaps the least compelling, but the same trends hold.\n"[[TNF-NEG], [PNF-NEG], [CRT], [MIN]]