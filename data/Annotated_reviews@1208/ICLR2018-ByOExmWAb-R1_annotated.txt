"Generating high-quality sentences/paragraphs is an open research problem that is receiving a lot of attention.[[INT-NEU], [null], [SMY], [GEN]] This text generation task is traditionally done using recurrent neural networks.[[MET-NEU], [null], [SMY], [GEN]] This paper proposes to generate text using GANs.[[MET-NEU], [null], [SMY], [GEN]] GANs are notorious for drawing images of high quality but they have a hard time dealing with text due to its discrete nature.[[RWK-NEU], [CMP-NEU], [DIS], [MAJ]] This paper's approach is to use an actor-critic to train the generator of the GAN and use the usual maximum likelihood with SGD to train the discriminator.[[MET-NEU], [null], [SMY], [GEN]] The whole network is trained on the \"fill-in-the-blank\" task using the sequence-to-sequence architecture for both the generator and the discriminator.[[EXP-NEU,MET-NEU], [null], [SMY], [GEN]] At training time, the generator's encoder computes a context representation using the masked sequence.[[EXP-NEU], [null], [SMY], [GEN]] This context is conditioned upon to generate missing words.[[MET-NEU], [null], [SMY], [GEN]] The discriminator is similar and conditions on the generator's output and the masked sequence to output the probability of a word in the generator's output being fake or real.[[MET-NEU], [EMP-NEU], [SMY], [GEN]] With this approach, one can generate text at test time by setting all inputs to blanks.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] \n\nPros and positive remarks: \n--I liked the idea behind this paper.[[PDI-POS], [null], [APC], [MAJ]] I find it nice how they benefited from context (left context and right context) by solving a \"fill-in-the-blank\" task at training time and translating this into text generation at test time.[[EXP-POS], [EMP-POS], [APC], [MAJ]] \n--The experiments were well carried through and very thorough.[[EXP-POS], [EMP-POS], [APC], [MAJ]]\n--I second the decision of passing the masked sequence to the generator's encoder instead of the unmasked sequence.[[MET-POS], [EMP-POS], [APC], [MAJ]] I first thought that performance would be better when the generator's encoder uses the unmasked sequence.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] Passing the masked sequence is the right thing to do to avoid the mismatch between training time and test time.[[MET-POS], [EMP-POS], [APC], [MAJ]]\n\nCons and negative remarks:\n--There is a lot of pre-training required for the proposed architecture.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] There is too much pre-training. I find this less elegant. [[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n--There were some unanswered questions:\n            (1) was pre-training done for the baseline as well?[[RWK-NEU,MET-NEU], [null], [QSN], [MIN]]\n            (2) how was the masking done?[[MET-NEU], [null], [QSN], [MIN]] how did you decide on the words to mask? was this at random?[[MET-NEU], [null], [QSN], [MIN]]\n            (3) it was not made very clear whether the discriminator also conditions on the unmasked sequence. [[MET-NEG], [EMP-NEG], [CRT], [MAJ]]It needs to but \n                  that was not explicit in the paper.[[MET-NEU], [EMP-NEU], [DFT], [MIN]]\n--Very minor: although it is similar to the generator, it would have been nice to see the architecture of the discriminator with example input and output as well.[[EXP-NEU], [EMP-NEU], [SUG], [MIN]]\n\n\nSuggestion: for the IMDB dataset, it would be interesting to see if you generate better sentences by conditioning on the sentiment as well.[[DAT-NEU,EXP-NEU], [EMP-NEU], [SUG], [MAJ]]\n"