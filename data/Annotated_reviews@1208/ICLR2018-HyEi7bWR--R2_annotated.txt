"This paper suggests an RNN reparametrization of the recurrent weights with a skew-symmetric matrix using Cayley transform to keep the recurrent weight matrix orthogonal.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] They suggest that they reparametrization leads to superior performance compare to other forms of Unitary Recurrent Networks.[[MET-NEU,RES-NEU], [null], [SMY], [GEN]]\n\nI think the paper is well-written.[[OAL-POS], [CLA-POS], [APC], [MAJ]]  Authors have discussed previous works adequately and provided enough insight and motivation about the proposed method.[[RWK-POS,MET-POS], [SUB-POS,EMP-POS], [APC], [MAJ]]\n\nI have two questions from authors:\n\n1- What are the hyperparameters that you optimized in experiments?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\n2- How sensitive is the results to the number of -1 in the diagonal matrix?[[MET-NEU,RES-NEU], [EMP-NEU], [QSN], [GEN]]\n\n3- ince the paper is not about compression, it might be unfair to limit the number of hidden units in LSTMs just to match the number of parameters to RNNs.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] In MNIST experiment, for example, better numbers are reported for larger LSTMs.[[DAT-POS,MET-POS], [EMP-POS], [APC], [MAJ]] I think matching the number of hidden units could be helpful.[[DAT-NEU], [EMP-NEU], [SUG], [MIN]] Also, one might want to know if the scoRNN is still superior in the regime where the number of hidden units is about 1000.[[DAT-NEU], [SUB-NEU], [DIS], [MIN]] I appreciate if authors can provide more results in these settings.\n\n"[[RES-NEU], [SUB-NEU], [SUG], [MIN]]