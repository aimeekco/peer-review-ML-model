"The paper proposes to make the inner layers in a neural network be block diagonal, mainly as an alternative to pruning.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The implementation of this seems straightforward, and can be done either via initialization or via pruning on the off-diagonals.[[PDI-NEU], [null], [SMY], [GEN]] There are a few ideas the paper discusses:\n\n(1) compared to pruning weight matrices and making them sparse, block diagonal matrices are more efficient since they utilize level 3 BLAS rather than sparse operations which have significant overhead and are not \"worth it\" until the matrix is extremely sparse. I think this case is well supported via their experiments, and I largely agree.[[MET-POS], [CMP-POS,EMP-POS], [APC], [MAJ]]\n\n(2) that therefore, block diagonal layers lead to more efficient networks.[[RES-POS], [EMP-POS], [APC], [MAJ]] This point is murkier, because the paper doesn't discuss possible increases in *training time* (due to increased number of iterations) in much detail.[[EXP-NEG,ANA-NEG], [EMP-NEG], [DFT,CRT], [MAJ]] At if we only care about running the net, then reducing the time from 0.4s to 0.2s doesn't seem to be that useful (maybe it is for real-time predictions?[[EXP-NEU,ANA-NEU], [EMP-NEU], [QSN], [MIN]] Please cite some work in that case)[[BIB-NEG], [SUB-NEG], [DFT], [MIN]]\n\n(3) to summarize points (1) and (2), block diagonal architectures are a nice alternative to pruned architectures, with similar accuracy, and more benefit to speed (mainly speed at run-time, or speed of a single iteration, not necessarily speed to train)[[EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]]\n\n[as I am not primarly a neural net researcher, I had always thought pruning was done to decrease over-fitting, not to increase computation speed, so this was a surprise to me; also note that the sparse matrix format can increase runtime if implemented as a sparse object, as demonstrated in this paper, but one could always pretend it is sparse, so you never ought to be slower with a sparse matrix][[MET-NEU], [null], [DIS], [GEN]]\n\n(4) there is some vague connection to random matrices, with some limited experiments that are consistent with this observation but far from establish it, and without any theoretical analysis (Martingale or Markov chain theory)[[EXP-NEG,ANA-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nThis is an experimental/methods paper that proposes a new algorithm, explained only in general details, and backs up it up with two reasonable experiments (that do a good job of convincing me of point (1) above).[[EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]] The authors seem to restrict themselves to convolutional networks in the first paragraph (and experiments) but don't discuss the implications or reasons of this assumption.[[MET-NEU], [null], [DIS], [MIN]] The authors seem to understand the literature well, and not being an expert myself, I have the impression they are doing a fair job.[[OAL-POS], [CNT], [APC], [MAJ]]\n\n\nThe paper could have gone farther experimentally (or theoretically) in my opinion.[[EXP-POS,FWK-POS], [IMP-POS], [APC], [MAJ]] For example, with sparse and block diagonal matrices, reducing the size of the matrix to fit into the cache on the GPU must obviously make a difference, but this did not seem to be investigated.[[ANA-NEG], [SUB-NEG], [DFT], [MIN]] I was also wondering about when 2 or more layers are block sparse, do these blocks overlap?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] i.e., are they randomly permuted between layers so that the blocks mix?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] And even with a single block, does it matter what permutation you use?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] (or perhaps does it not matter due to the convolutional structure?)[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nThe section on the variance of the weights is rather unclear mathematically, starting with the abstract and even continuing into the paper.[[ABS-NEG,MET-NEG], [EMP-NEG], [CRT], [MIN]] We are talking about sample variance?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] What does DeltaVar mean in eq (2)?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] The Marchenko-Pastur theorem seemed to even be imprecise, since if y>1, then a < 0, implying that there is a nonzero chance that the positive semi-definite matrix XX' has a negative eigenvalue.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\nI agree this relationship with random matrices could be interesting, but it seems too vague right now.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Is there some central limit theorem explanation?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Are you sure that you've run enough iterations to fully converge?(Fig 4 was still trending up for b1=64).[[EXP-NEU], [EMP-NEU], [QSN], [MIN]]  Was it due to the convolutional net structure (you could test this)?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]]  Or, perhaps train a network on two datasets, one which is not learnable (iid random labels), and one which is very easily learnable (e.g., linearly separable).[[DAT-NEU,EXP-NEU], [EMP-NEU], [QSN], [MIN]]  Would this affect the distributions?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] \n\nFurthermore, I think I misunderstood parts, because the scaling in MNIST and CIFAR was different and I didn't see why (for MNIST, it was proportional to block size, and for CIFAR it was independent of block size almost).[[DAT-NEU], [null], [DIS], [MIN]] \n\nMinor comment: last paragraph of 4.1, comparing with Sindhwani et al., was confusing to me.[[CNT], [null], [DIS], [MIN]]  Why was this mentioned? And it doesn't seem to be comparable.[[CNT], [CNT], [CRT], [MIN]]  I have no idea what \"Toeplitz (3)\" is."[[CNT], [EMP-NEU], [DIS], [MIN]] 