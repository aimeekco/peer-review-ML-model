"Proposal is to restrict the feasible parameters to ones that have produce a function with small variance over pre-defined groups of images that should be classified the same.[[INT-NEU], [null], [SMY], [GEN]] As authors note, this constraint can be converted into a KKT style penalty with KKT multiplier lambda.[[PDI-NEU], [null], [SMY], [GEN]]   Thus this is very  similar to other regularizers that increase smoothness of the function, such as total variation or a graph Laplacian defined with graph edges connecting the examples in each group, as well as manifold regularization (see e.g. Belkin, Niyogi et al. JMLR).[[RWK-NEU,MET-NEU], [CMP-NEU], [SMY], [GEN]]   Heck, in practie ridge regularization will also do something similar for many function classes. [[RWK-NEU,MET-NEU], [CMP-NEU], [SMY], [GEN]] \n\nExperiments didn't compare to any similar smoothness regularization (and my preferred would have been a comparison to graph Laplacian or total variation on graphs formed by the same clustered examples).[[EXP-NEU], [EMP-NEU], [DIS], [MIN]]  It's also not clear either how important it is that they hand-define the groups over which to minimize variance or if just generally adding smoothness regularization would have achieved the same results.[[EXP-NEG,RES-NEG], [EMP-NEG], [DIS], [MAJ]]    That made it hard to get excited about the results in a vacuum.[[RES-NEG], [EMP-NEG], [DIS], [MAJ]]   \n\nWould this proposed strategy have thwarted the Russian tank legend problem?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]   Would it have fixed the Google gorilla problem?Why or why not?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] \n\nOverall, I found the writing a bit bombastic for a strategy that seems to require the user to hand-define groups/clusters of examples.[[OAL-POS], [CLA-POS], [APC], [MIN]]  \n\nPage 2: calling additional instances of the same person \u201ccounterfactual observations\u201d didn\u2019t seem consistent with the usual definition of that term\u2026 maybe I am just missing the semantic link here, but this isn't how we usually use the term counterfactual in my corner of the field.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] \n\nRe: \u201cone creates additional samples by modifying\u2026\u201d be nice to quote more of the early work doing this, I believe the first work of this sort was Scholkopf\u2019s, he called it \u201cvirtual examples\u201d and I\u2019m pretty sure he specifically did it for rotation MNIST images (and if not exactly that, it was implied).[[RWK-NEU,DAT-NEU], [CMP-NEU], [DIS], [MIN]]   I think the right citation is \u201cIncorporating invariances in support vector learning machines\n\u201c Scholkopf, Burges, Vapnik 1996, but also see Decoste * Scholkopf 2002 \u201cTraining invariant support vector machines.\u201d ".[[RWK-NEU,BIB-NEU], [SUB-NEU], [DIS], [MIN]] 