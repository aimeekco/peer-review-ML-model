"The paper discusses a phenomenon where neural network training in very specific settings can profit much from a schedule including large learning rates.[[INT-NEU], [null], [SMY], [GEN]] Unfortunately, this paper feels to be hastily written and can only be read when accompanied with several references as key parts (CLR) are not described and thus the work can not be reproduced from the paper.[[OAL-NEG], [CLA-NEG], [SMY,CRT], [MAJ]]\n\nThe main claim of the author hinges of the fact that in some learning problems the surface of the objective function can be very flat near the optimum.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] In this setting, a typical schedule with a decreasing learning rate would be a bad choice as the change of curvature must be corrected as well.[[MET-NEG], [EMP-NEG], [DFT], [MAJ]] However, this is not a general problem in neural network training and might not be generalizable to other datasets or architectures as the authors acknowledge.[[DAT-NEU,MET-NEG], [EMP-NEG], [DFT], [MIN]]\n\nIn the end, the actual gain of this paper is only in the form of a hypothesis but there is only very little enlightenment, especially as the only slightly theoretical contribution in section 5 does not predict the observed behavior.[[OAL-NEG], [IMP-NEU], [SMY], [MAJ]] \n\nPersonally i would not use the term \"convergence\" in this setting at all as the runs are very short and thus we might not be close to any region of convergence.[[MET-NEG], [SUB-NEG,EMP-NEG], [DFT], [MIN]] Most of the plots shown are actually not converged and convergence in test accuracy is not the same as convergence in training loss, which is not shown at all.[[RES-NEG], [EMP-NEG], [DFT], [MIN]] The results of smaller test error with larger learning rates on small training sets might therefore just be the inability of the optimizer to get closer to the optimum as steps are too long to decrease the expected loss, thus having a similar effect as early stopping.[[DAT-NEG,MET-NEG], [EMP-NEG], [DFT], [MAJ]]\n\nPros:\n- Many experiments which try to study the effect[[EXP-POS], [null], [SMY], [GEN]]\nCons:\n-The described phenomenon seems to depend strongly on the problem surface and might never \nbe encountered on any problem aside of Cifar-10[[MET-NEG], [EMP-NEG], [DFT], [MAJ]]\n- Only single runs are shown, considering the noise on those the results might not be reproducible.[[MET-NEG], [null], [DFT], [MIN]]\n-Experiments are not described in detail\n-Experiment design feels \"ad-hoc\" and unstructured\n-The role and value of the many LR-plots remains unclear to me.[[EXP-NEG], [CLA-NEG], [DFT], [MAJ]]\n\nForm:\n- The paper does not maker clear how the exact schedules work.[[MET-NEG], [EMP-NEG], [DFT], [MIN]] The terms are introduced but the paper misses the most basic formulas\n- Figures are not properly described, e.g. axes in Figures 3 a) and b)\n- Explicit references to code are made which require familiarity with the used framework(if at all published).[[TNF-NEG], [CLA-NEG], [DFT], [MIN]]  "