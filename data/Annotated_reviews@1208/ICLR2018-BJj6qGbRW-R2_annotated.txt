"This paper introduces a graph neural net approach to few-shot learning.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] Input examples form the nodes of the graph and edge weights are computed as a nonlinear function of the absolute difference between node features.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] In addition to standard supervised few-shot classification, both semi-supervised and active learning task variants are introduced.[[PDI-NEU], [null], [SMY], [GEN]] The proposed approach captures several popular few-shot learning approaches as special cases.[[MET-NEU], [null], [SMY], [GEN]] Experiments are conducted on both Omniglot and miniImagenet datasets.[[DAT-NEU,EXP-NEU], [null], [SMY], [GEN]]\n\nStrengths\n- Use of graph neural nets for few-shot learning is novel.[[MET-POS], [NOV-POS], [APC], [MAJ]]\n- Introduces novel semi-supervised and active learning variants of few-shot classification.[[MET-POS], [NOV-POS], [APC], [MAJ]]\n\nWeaknesses\n- Improvement in accuracy is small relative to previous work.[[RWK-NEG,RES-NEG], [CMP-NEG], [CRT], [MAJ]]\n- Writing seems to be rushed.[[OAL-NEG], [CLA-NEG], [CRT], [MIN]]\n\nThe originality of applying graph neural networks to the problem of few-shot learning and proposing semi-supervised and active learning variants of the task are the primary strengths of this paper.[[MET-POS], [NOV-POS], [APC], [MAJ]] Graph neural nets seem to be a more natural way of representing sets of items, as opposed to previous approaches that rely on a random ordering of the labeled set, such as the FCE variant of Matching Networks or TCML.[[RWK-POS,MET-POS], [NOV-POS], [APC], [MAJ]] Others will likely leverage graph neural net ideas to further tackle few-shot learning problems in the future, and this paper represents a first step in that direction.[[RWK-POS,MET-POS], [NOV-POS], [APC], [MAJ]]\n\nRegarding the graph, I am wondering if the authors can comment on what scenarios is the graph structure expected to help?[[TNF-NEU], [EMP-NEU], [QSN], [MIN]] In the case of 1-shot, the graph can only propagate information about other classes, which seems to not be very useful.[[TNF-POS], [EMP-POS], [APC], [MAJ]]\n\nThough novel,[[MET-POS], [NOV-POS], [APC], [MAJ]] the motivation behind the semi-supervised and active learning setup could use some elaboration.[[MET-NEU], [SUB-NEU], [SUG], [MIN]] By including unlabeled examples in an episode, it is already known that they belong to one of the K classes.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] How realistic is this set-up and in what application is it expected that this will show up?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nFor active learning, the proposed method seems to be specific to the case of obtaining a single label.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] How can the proposed method be scaled to handle multiple requested labels?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nOverall the paper is well-structured and related work covers the relevant papers,[[RWK-POS,OAL-POS], [PNF-NEU], [APC], [MAJ]] but the details of the paper seem hastily written.[[OAL-NEG], [CLA-NEG], [CRT], [MIN]]\n\nIn the problem set-up section, it is not immediately clear what the distinction between s, r, and t is.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Stating more explicitly that s is for the labeled data, etc. would make this section easier to follow.[[MET-NEU], [PNF-NEU], [SUG], [MIN]] In addition, I would suggest stating the reason why t=1 is a necessary assumption for the proposed model in the few-shot and semi-supervised cases.[[MET-NEU], [EMP-NEU], [SUG], [MIN]]\n\nRegarding the Omniglot dataset, Vinyals et al. (2016) augmented the classes so that 4,800 classes were used for training and 1,692 for test.[[RWK-NEU,DAT-NEU], [null], [DIS], [GEN]] Was the same procedure done for the experiments in the paper?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]] If yes, please update 6.1.1 to make this distinction more clear.[[EXP-NEU], [EMP-NEU], [SUG], [MIN]] If not, please update the experiments to be consistent with the baselines.[[RWK-NEU,EXP-NEU], [CMP-NEU,EMP-NEU], [SUG], [MIN]]\n\nIn the experiments, does the \\varphi MLP explicitly enforce symmetry and identity or is it learned?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nRegarding the Omniglot baselines, it appears that Koch et al. (2015), Edwards & Storkey (2016), and Finn et al. (2017) use non-standard class splits relative to the other methods.[[RWK-NEU,DAT-NEU], [null], [DIS], [GEN]] This should probably be noted.[[RWK-NEU], [CNT], [SUG], [MIN]]\n\nThe results for Prototypical Networks appear to be incorrect in the Omniglot and Mini-Imagenet tables.[[DAT-NEG,RES-NEG], [EMP-NEG], [CRT], [MIN]] According to Snell et al. (2017) they should be 49.4% and 68.2% for miniImagenet.[[DAT-NEU,RES-NEU], [EMP-NEU], [CRT], [MIN]] Moreover, Snell et al. (2017) only used 64 classes for training instead of 80 as utilized in the proposed approach.[[DAT-NEG,RWK-NEG], [CMP-NEG], [CRT], [MIN]] Given this, I am wondering if the authors can comment on the performance difference in the 5-shot case, even though Prototypical Networks is a special case of GNNs?[[MET-NEU,RES-NEU], [CMP-NEU], [QSN], [MIN]]\n\nFor semi-supervised and active-learning results, please include error bars for the miniImagenet results.[[DAT-NEU,RES-NEU], [SUB-NEU], [SUG], [MIN]] Also, it would be interesting to see 20-way results for Omniglot as the gap between the proposed method and the baseline would potentially be wider.[[RWK-NEU,DAT-NEU,RES-NEU], [EMP-NEU], [SUG], [MIN]]\n\nOther Comments:\n\n- In Section 4.2, Gc(.) is defined in Equation 2 but not mentioned in the text.[[MET-NEG], [SUB-NEG], [DFT], [MIN]]\n- In Section 4.3, adding an equation to clarify the relationship with Matching Networks would be helpful.[[MET-NEU], [EMP-NEU], [SUG], [MIN]]\n- I believe there is a typo in section 4.3 in that softmax(\\varphi) should be softmax(-\\varphi), so that more similar pairs will be more heavily weighted.[[MET-NEG], [CLA-NEG], [CRT], [MIN]\n- The equation in 5.1 appears to be missing a minus sign.[[MET-NEG], [CLA-NEG], [DFT], [MIN]\n\nOverall, the paper is novel and interesting,[[OAL-POS], [EMP-POS], [APC], [MAJ] though the clarity and experimental results could be better explained.[[EXP-NEG,RES-NEG], [SUB-NEG], [DFT], [MIN]\n\nEDIT: I have read the author's response.[[EXT-NEU], [null], [DIS], [GEN] The writing is improved and my concerns have largely been addressed.[[OAL-POS], [CLA-POS], [APC], [MAJ] I am therefore revising my rating of the paper to a 7."[[OAL-NEG], [REC-NEG], [FBK], [MAJ]