"--------------------\nReview updates:\nRating 6 -> 7\nConfidence 2 -> 4\n\nThe rebuttal and update addressed a number of my concerns, cleared up confusing sections, and moved the paper materially closer to being publication-worthy, thus I\u2019ve increased my score.\n-----[[EXT-POS], [REC-POS], [FBK], [MAJ]]---------------\n\nI want to love this paper.[[OAL-POS], [null], [APC], [GEN]] The results seem like they may be very important.[[RES-POS], [null], [SMY], [GEN]] However, a few parts were poorly explained, which led to this reviewer being unable to follow some of the jumps from experimental results to their conclusions.[[EXP-NEU,MET-NEU,RES-NEU,ANA-NEU], [CLA-NEG,EMP-NEG], [DFT], [MAJ]] I would like to be able to give this paper the higher score it may deserve, but some parts first need to be further explained.[[OAL-NEU], [EMP-NEU], [DFT], [MIN]]\n\nUnfortunately, the largest single confusion I had is on the first, most basic set of gradient results of section 4.1.[[DAT-NEU,RES-NEU], [EMP-NEU], [DFT], [MAJ]] Without understanding this first result, it\u2019s difficult to decide to what extent the rest of the paper\u2019s results are to be believed.[[RES-NEU], [IMP-NEU], [DIS], [MAJ]]\n\nFig 1 shows \u201cthe histograms of the average sign of partial derivatives of the loss with respect to activations, as collected over training for a random neuron in five different layers.[[DAT-NEU,MET-NEU,TNF-NEU], [EMP-POS], [SMY], [GEN]]\u201d Let\u2019s consider the top-left subplot of Fig 1, showing a heavily bimodal distribution (modes near -1 and +1.). Is this plot made using data from a single neuron or from  multiple neurons?[[DAT-NEU,TNF-NEU], [SUB-NEU], [QSN], [MIN]] For now let\u2019s assume it is for a single neuron, as the caption and text in 4.1 seem to suggest. If it is for a single neuron, then that neuron will have, for a single input example, a single scalar activation value and a single scalar gradient value.[[DAT-NEU,MET-NEU,TNF-NEU], [SUB-NEU], [DIS], [GEN]] The sign of the gradient will either be +1 or -1. If we compute the sign for each input example and then AGGREGATE over all training examples seen by this neuron over the course of training (or a subset for computational reasons), this will give us a list of signs.[[DAT-NEU,MET-NEU], [SUB-NEU], [DIS], [GEN]] Let\u2019s collect these signs into a long list: [+1, +1, +1, -1, +1, +1, \u2026].[[DAT-NEU], [SUB-NEU], [SUG], [GEN]] Now what do we do with this list?[[DAT-NEU], [SUB-NEU], [SUG], [GEN]] As far as I can tell, we can either average it (giving, say, .85 if the list has far more +1 values than -1 values) OR we can show a histogram of the list, which would just be two bars at -1 and +1.[[DAT-NEU], [SUB-NEU], [SUG], [GEN]] But we can\u2019t do both, indicating that some assumption above was incorrect. Which assumption in reading the text was incorrect?[[MET-NEG], [EMP-NEU], [SMY,QSN], [MIN]]\n\nFurther in this direction, Section 4.1 claims \u201cZero partial derivatives are ignored to make the signal more clear[[MET-NEU], [IMP-NEU], [DFT], [MIN]].\u201d Are these zero partial derivatives of the post-relu or pre-relu?[[MET-NEU], [null], [QSN], [MIN]] The text (Sec 3) points to activations as being post-relu, but in this case zero-gradients should be a very small set (only occuring if all neurons on the next layer had either zero pre-relu gradients, which is common for individual neurons but, I would think, not for all at once).[[DAT-NEU,MET-NEU], [SUB-NEU], [SMY], [GEN]] Or does this mean the pre-relu gradient is zero, e.g. the common case where the gradient is zeroed because the pre-activation was negative and the relu at that point has zero slope? [[DAT-NEU,MET-NEU,RES-NEU], [SUB-NEU,EMP-NEU], [QSN], [MIN]]In this case we would be excluding a large set (about half!) of the gradient values, and it didn\u2019t seem from the context in the paper that this would be desirable.[[DAT-NEG,MET-NEU], [SUB-NEG], [DFT], [MIN]]\n\nIt would be great if the above could be addressed.[[DAT-NEU,MET-NEU], [SUB-NEU,EMP-NEU], [DIS], [MIN]] Below are some less important comments.\n\nSec 5.1: great results![[RES-POS], [null], [APC], [MAJ]]\n\nFig 3: This figure studies \u201cthe first and last layers of each network\u201d. Is the last layer really the last linear layer, the one followed by a softmax?[[ANA-NEU,TNF-NEU], [null], [QSN], [MIN]] In this case there is no relu and the 0 pre-activation is not meaningful (softmax is shift invariant). [[MET-NEU], [null], [DIS], [MIN]]Or is the layer shown (e.g. \u201cstage3layer2\u201d) the penultimate layer?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Minor: in this figure, it would be great if the plots could be labeled with which networks/datasets they are from.[[TNF-NEU], [PNF-NEU], [SUG], [MIN]]\n\nSec 5.2 states \u201cneuron partitions the inputs in two distinct but overlapping categories of quasi equal size[[MET-NEU], [EMP-NEU], [DIS], [GEN]].\u201d This experiment only shows that this is true in aggregate, not for specific neurons?[[EXP-NEU], [null], [QSN], [MIN]] I.e. the partition percent for each neuron could be sampled from U(45, 55) or from U(10, 90) and this experiment would not tell us which, correct?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MAJ]] Perhaps this statement could be qualified.[[MET-NEU], [null], [DIS], [GEN]]\n\nTable 1: \u201c52th percentile vs actual 53 percentile shown\u201d. \n\n> Table 1: The more fuzzy, the higher the percentile rank of the threshold[[MET-NEU], [EMP-NEU], [SMY], [GEN]]\n\nThis is true for the CIFAR net but the opposite is true for ResNet, right?[[MET-NEU], [SUB-NEU], [QSN], [MIN]]\n"