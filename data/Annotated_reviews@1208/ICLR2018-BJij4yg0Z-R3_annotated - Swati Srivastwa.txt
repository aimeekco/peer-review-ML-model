"This paper builds on Zhang et al. (2016) (Understanding deep learning requires rethinking generalization).[[RWK-NEU,PDI-NEU], [null], [SMY], [GEN]] Firstly, it shows experimentally that the same effects appear even for simple models such as linear regression. [[RWK-NEU,PDI-NEU], [null], [SMY], [GEN]] It also shows that the phenomenon that sharp minima lead to worse result can be explained by Bayesian evidence.[[PDI-NEU], [null], [SMY], [GEN]]  Secondly, it views SGD with different settings as introducing different levels of noises that favors different minima.[[MET-NEU], [null], [SMY], [GEN]] With both theoretical and experimental analysis, it suggests the optimal batch-size given learning rate and training data size.[[DAT-NEU,EXP-NEU,ANA-NEU], [null], [DIS], [GEN]] The paper is well written and provides excellent insights. [[OAL-POS], [CLA-POS], [APC], [MAJ]]\n\nPros:\n1. Very well written paper with good theoretical and experimental analysis.[[EXP-POS,ANA-POS], [CLA-POS], [APC], [MAJ]]\n2. It provides useful insights of model behaviors which are attractive to a large group of people in the community.[[MET-POS], [EMP-POS], [APC], [MAJ]] \n3. The result of optimal batch size setting is useful to wide range of learning methods.[[MET-POS,ANA-POS], [EMP-POS], [APC], [MAJ]]\n\nCons and mainly questions:\n1. Missing related work.[[RWK-NEG], [SUB-NEG], [DFT], [MAJ]] \nOne important contribution of the paper is about optimal batch sizes, but related work in this direction is not discussed..[[RWK-NEG], [SUB-NEG], [DFT,CRT], [MAJ]] There are many related works concerning adaptive batch sizes, such as [1] (a summary in section 3.2 of [2]).[[RWK-NEU], [null], [DIS], [MIN]] \n\n2. It will be great if the author could provide some discussions with respect to the analysis of information bottleneck [3] which also discuss the generalization ability of the model.[[ANA-NEU], [SUB-NEU], [SUG], [MIN]] \n\n3. The result of the optimal mini-batch size depends on the training data size.[[DAT-NEU,RES-NEU], [null], [DIS], [GEN]] How about real online learning with streaming data where the total number of data points are unknown?.[[DAT-NEU,MET-NEU], [null], [QSN], [MIN]]\n\n4. The results are reported mostly concerning the training iterations, not the CPU time such as in figure 3.[[RES-NEG], [EMP-NEG], [CRT], [MIN]] It will be fair/interesting to see the result for CPU time where small batch maybe favored more.[[RES-NEU], [EMP-NEU], [DIS], [GEN]]   \n\n\n[1] Balles, Lukas, Javier Romero, and Philipp Hennig. \"Coupling Adaptive Batch Sizes with Learning Rates.\" arXiv preprint arXiv:1612.05086 (2016)..[[BIB-NEU], [null], [DIS], [GEN]]\n[2] Zhang, Cheng, Judith Butepage, Hedvig Kjellstrom, and Stephan Mandt. \"Advances in Variational Inference.\" arXiv preprint arXiv:1711.05597 (2017).[[BIB-NEU], [null], [DIS], [GEN]]\n[3] Tishby, Naftali, and Noga Zaslavsky. \"Deep learning and the information bottleneck principle.[[BIB-NEU], [null], [DIS], [GEN]]\" In Information Theory Workshop (ITW), 2015 IEEE, pp. 1-5. IEEE, 2015.[[BIB-NEU], [null], [DIS], [GEN]]\n\n\u2014\u2014\u2014\u2014\u2014-\nUpdate: I lowered my rating considering other ppl s review and comments. "[[OAL-NEG], [REC-NEG], [FBK], [MAJ]]