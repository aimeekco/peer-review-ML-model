"The paper proposes new RNN training method based on the SEARN learning to search (L2S) algorithm and named as SeaRnn.[[EXP-NEU,MET-NEU], [NOV-POS], [SMY], [MAJ]] It proposes a way of overcoming the limitation of local optimization trough the exploitation of the structured losses by L2S.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] It can consider different classifiers and loss functions, and a sampling strategy for making the optimization problem scalable is proposed.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] SeaRnn improves the results obtained by MLE training in three different problems, including a large-vocabulary machine translation.[[EXP-POS,RES-POS], [EMP-POS], [APC], [MAJ]] In summary, a very nice paper.[[OAL-POS], [CNT], [SMY,APC], [MAJ]]\n\nQuality: SeaRnn is a well rooted and successful application of the L2S strategy to the RNN training that combines at the same time global optimization and scalable complexity.[[EXP-POS,MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]] \n\nClarity: The paper is well structured and written, with a nice and well-founded literature review.[[RWK-POS,OAL-POS], [CLA-POS], [APC], [MAJ]]\n\nOriginality: the paper presents a new algorithm for training RNN based on the L2S methodology, and it has been proven to be competitive in both toy and real-world problems.[[MET-POS], [NOV-POS], [APC], [MAJ]]\n\nSignificance: although the application of L2S to RNN training is not new,[[MET-POS], [IMP-POS], [APC], [MAJ]] the contribution to the overcoming the limitations due to error propagation and MLE training of RNN is substantial.\n"