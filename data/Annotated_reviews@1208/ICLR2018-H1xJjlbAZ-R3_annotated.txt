"The paper shows that interpretations for DNN decisions, e.g. computed by methods such as sensitivity analysis or DeepLift, are fragile: Visually (to a human) inperceptibly different image cause greatly different explanations (and also to an extent different classifier outputs).[[PDI-NEU], [null], [SMY], [GEN]]  The authors perturb input images and create explanations using different methods.[[MET-NEU], [null], [SMY], [GEN]]  Even though the image is inperceptibly different to a human observer, the authors observe large changes in the heatmaps visualizing the explanation maps.[[MET-NEU], [null], [SMY], [GEN]]  This is true even for random perturbations.[[MET-NEU], [null], [DIS], [GEN]] \n\nThe images have been modified wrt. to some noise, such that they deviate from the natural statistics for images of that kind.[[MET-NEU], [null], [SMY], [GEN]]  Since the explanation algorithms investigated in this papers merely react to the interactions of the model to the input and thus are unsupervised processes in nature, the explanation methods merely show the model's reaction to the change.[[MET-NEU], [null], [DIS], [GEN]] \nFor one, the model itself reacts to the perturbation, which can be measured by the (considarbly) increased class probability.[[MET-NEU], [null], [DIS], [GEN]]  Since the prediction score is given in probabilty values, the reviewer assumes the final layer of the model is a SoftMax activation.[[MET-NEU], [null], [DIS], [GEN]]  In order to see change in the softmax output score, especially if the already dominant prediction score is further increased, a lot of change has to happen to the outputs of the layer serving as input to the SoftMax layer.[[MET-NEU], [null], [DIS], [GEN]] \n\nIt can thus be expected, that the input- and class specific explanations change as well, to an also not so small extent.[[MET-NEU], [null], [DIS], [GEN]]  The explanation maps mirror for the considered methods the model's reaction to the input.[[MET-NEU], [null], [DIS], [GEN]]  They are thus not meaningless, but are a measure to model reaction instead of an independent process.[[MET-NEU], [null], [DIS], [GEN]]  The excellent Figure 2 supports this point.[[TNF-POS], [CNT], [APC], [MAJ]]  Not the interpretation itself is fragile, but the model.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \nAdding a small delta to the sample x shifts its position in data space, completely altering the prediction rule applied by the model due to the change in proximity to another section of the decision hyperplane.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]  The fragility of DNN models to marginally perturbed inputs themselves is well known.[[EXT-NEU], [null], [DIS], [GEN]]  \nThis especially true for adversial perturbations, which have been used as test cases in this work.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]  The explanation methods are expected to highlight highly important areas in an image, which have been targetet by these perturbation approaches.[[MET-NEU], [SUB-NEU,CMP-NEU], [SUG], [MIN]] \n\nThe authors give an example of an adversary manipulating the input in order to draw the activation to specific features to draw confusing/malignant explanation maps.[[MET-NEU], [null], [DIS], [GEN]]  In a settig of model verification, the explanation via heatmaps is exactly what one wants to have: If tiny change to the image causes lots of change to the prediction (and explanation) we can visualize the instability of the model not the explanation method.[[MET-NEU], [null], [DIS], [GEN]] \nFurther do targeted perturbations not show the fragility of explanation methods, but rather that the models actually find what is important to the model.[[MET-NEU], [null], [DIS], [GEN]] It can be expected, that after a change to these parts of the input, the model will decide differently, albeit coming to the same conclusion (in terms of predicted class membership), which reflects in the explanation map computed for the perturbed input.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\nFurther remarks:\nIt would be interesting to see the size and position of the center of mass attacks in the appendix.[[CNT], [CNT], [DIS], [MIN]] The reviewer closely follows and is experienced with various explanation methods, their application and the quality of the expected explanations.[[EXT-NEU], [null], [DIS], [GEN]] The reviewer is therefore surprised by the poor quality and lack of structure in the maps obtained from the DeepLift method.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Can bugs and suboptimal configurations be ruled out during the experiments?[[EXP-NEU], [EMP-NEU], [QSN], [MAJ]] The DeepLift explanations are almost as noisy as the ones obtained for Sensitivity Analysis (i.e. the gradient at the input point).[[MET-NEU,ANA-NEU], [EMP-NEU], [DIS], [MIN]] However, recent work (e.g. Samek et al., IEEE TNNLS, 2017 or Montavon et al., Digital Signal Processing, 2017) showed that decomposition-based methods (such as DeepLift) provide less noisy explanations than Sensitivity Analysis.[[RWK-NEU], [CMP-NEU], [DIS], [MIN]]\n\nHave the authors considered training the net with small random perturbations added to the samples, to compare the \"vanilla\" model to the more robust one, which has seen noisy samples, and compared explanations?[[EXP-NEU,MET-NEU], [CMP-NEU], [QSN], [MAJ]] \nWhy not train (finetune) the considered models using softplus activations instead of exchanging activation nodes?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MAJ]]\nAppendix B: Heatmaps through the different stages of perturbation should be normalized using a common factor, not individually, in order to better reflect the change in the explanation[[CNT], [CNT], [DIS], [MIN]]\n\nConclusion:\nThe paper follows an interesting approach,[[MET-POS], [EMP-POS], [APC], [MAJ]] but ultimately takes the wrong view point:\nThe authors try to attribute fragility to explaining methods, which visualize/measure the reaction of the model to the perturbed inputs.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] A major rework should be considered."[[OAL-NEU], [null], [SUG], [MAJ]]