"\nSUMMARY\n\nThe paper proposes an RL algorithm that combines the DQN algorithm with a fear model.  The fear model is trained in parallel to predict catastrophic states.  Its output is used to penalize the Q learning target.\n\n\n\nCOMMENTS\n\nNot convinced about the fact that an agent forgets about catastrophic states. Because it does not experience it any more.  Shouldn\u2019t the agent stop learning at some point in time?  Why does it need to keep collecting good data?  How about giving more weight to catastrophic data (e.g., replicating it)\n\nIs the catastrophic scenario specific to DRL or RL in general with function approximation?\n\nWhy not specify catastrophic states with a large negative reward?\n\nIt seems that catastrophe states need to be experienced at least once.\nIs that acceptable for the autonomous car hitting a pedestrian?\n"