"This paper presents a new way of training stochastic neural network following an information relevance/compression framework similar to the Information Bottleneck.[[INT-NEU], [null], [SMY], [GEN]]  A new training objective is defined as a sum of mutual informations (MI) between the successive stochastic hidden layers plus a sum of mutual informations between each layer and the relevance variable.[[EXP-NEU], [null], [SMY], [GEN]]  \n\nThe idea is interesting and to my knowledge novel.[[PDI-POS], [EMP-POS], [APC], [MAJ]]  Experiments are carefully designed and presented in details, however assessing the impact of the proposed new objective is not straightforward.[[EXP-POS], [EMP-POS], [APC], [MAJ]]  It would have been interesting to compare not only with SFNN but also to a model with the same architecture and same gradient estimator (Raiko et al. 2014) using maximum likelihood.[[RWK-NEU,MET-NEU], [CMP-NEU], [SUG], [MIN]]  This would allow to disentangle the impact of the learning mechanism from the impact of the learning objective.[[RWK-NEU,MET-NEU], [CNT], [DIS], [MIN]]   \n\nWhy is it important to maximise I(X_l, Y) for every layer? [[MET-NEU], [EMP-NEU], [QSN], [MIN]] Does that impact the MI of the final layer and Y?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]   \n\nTo estimate the MI between a hidden layer and the relevance variable, a multilayer generalisation of the variational bound from Alemi et al. 2016.[[RWK-NEU,MET-NEU], [null], [DIS], [MIN]]  Computation of the bound requires integration over multiple layers (equation 15).[[MET-NEU], [null], [DIS], [MIN]]  How is this achieved in practice?[[RES-NEU], [EMP-NEU], [QSN], [MIN]]  With high-dimensional hidden layers a Monte-Carlo estimate on the minibatch can be very noisy and the resulting estimation of MI could be poor.[[RES-NEU], [null], [DIS], [MIN]] \n\nMutual information between the successive layers is decomposed as an entropy plus a conditional entropy term (eq 17).[[MET-NEU], [EMP-NEU], [DIS], [MIN]]  How is the conditional entropy term estimated?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] The entropy term is first bounded by conditioning on the previous layer and then estimated using Monte Carlo sampling with a plug-in estimator. Plug-in estimators are known to be inefficient in high dimensions even using a full dataset unless the number of samples is very large.[[DAT-NEU], [EMP-NEU], [DIS], [MIN]] It thus seems challenging to use mini batch MC, how does the mini batch estimation compare to an estimation using the full dataset? [[DAT-NEU], [EMP-NEU], [QSN], [MIN]]What is the variance of the mini batch estimate?[[DAT-NEU], [EMP-NEU], [QSN], [MIN]]\n\nIn the related work section, the IB problem can also be solved efficiently for meta-Gaussian distribution as explained in Rey et al. 2012 (Meta-gaussian information bottleneck).[[RWK-NEU], [EMP-NEU], [DIS], [MIN]] \n\nThere is a small typo in (eq 5).\n"[[OAL-NEG], [CLA-NEG], [CRT], [MIN]]