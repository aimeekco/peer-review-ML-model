"The paper describes a deep Q-learning approach to the problem of lane changing, whereby the action space is abstracted to high-level maneuvers that are then associated with low-level controllers.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The paper proposes a \"Q-masking\" strategy that reduces the action space according to constraints or prior knowledge.[[PDI-NEU], [null], [SMY], [GEN]] The method is trained and evaluated in a multi-lane simulator and compared against a baseline approach and human drivers.[[MET-NEU], [null], [SMY], [GEN]]\n\nPlanning lane-change maneuvers is an interesting, important problem for self-driving vehicles.[[PDI-POS,MET-POS], [EMP-POS], [APC], [MAJ]] What makes this problem particularly challenging is the need to predict/respond to the actions of other drivers.[[PDI-NEG], [EMP-NEG], [CRT], [MIN]] However, these issues are ignored here,  and it is is unclear why existing optimization/planning approaches are poorly suited to this problem, which is a fundamental assumption being made here. [[PDI-NEG], [SUB-NEG], [DFT], [MAJ]]Indeed, there is a long history of motion planning research that specifically addresses the problem of planning in the face of dynamic obstacles, as well as work that plans using predictive models of vehicle behavior (e.g., see the work by Jon How's group).[[EXT-NEU], [null], [DIS], [GEN]] However, the related work discussion is significantly lacking.[[RWK-NEG], [SUB-NEG], [DFT], [MAJ]] The paper does an insufficient job describing why deep RL is the right way to formulate this problem.[[PDI-NEU], [null], [DIS], [MIN]] There are vague references to the policy being difficult to define, but that motivates the importance of learning in general, not deep RL.[[RWK-NEG], [CMP-NEG], [CRT], [MIN]] Why is it reasonable given: (i) the challenge in defining appropriate rewards (i.e., it's not clear to me what would constitute the right reward for this problem);[[PDI-NEU], [null], [DIS], [MIN]] (ii) the large amount of data required to learn the policy; [[DAT-NEG], [SUB-NEG], [CRT], [MIN]]and (iii) the significant risks associated with training with a physical vehicle;[[EXP-NEU], [null], [DIS], [MIN]] One can see the merits in employing a hierarchical action space, whereby decision making operates over high-level actions, each associated with low-level controllers, but that the adopted formulation is not fundamental to this abstraction.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Indeed, this largely regulates the hard problems (i.e., controlling the low-level actions of the vehicle while avoiding collisions) to a separate controller.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Further, Q-masking largely amounts to simply removing actions that are infeasible (e.g., changing lanes to the left when in the left-most lane), but is seems to be no more than a heuristic, the advantages of which are not evaluated.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \n\nThe method is evaluated in simulation with comparisons to a simple baseline that tries to get over to the right lane as well as human performance.[[RWK-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]]  In the runs that reach the goal, the proposed method is about 20% faster than the simple baseline, though it does not reach the goal every time.[[RWK-POS,MET-POS], [EMP-POS], [APC], [MAJ]]  Given the claim that not reaching the goal is considered a failure, it isn't clear which performance is preferred.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]]  Meanwhile, the evaluation could be improved with the use of a better baseline (e.g., using an existing planning framework such as a predictive RRT that plans to the goal).[[RWK-NEG,MET-NEG], [CMP-NEG], [CRT], [MIN]] \n\nAdditional comments/questions:\n\n* The description of the Q-learning implementation is unclear.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]  How is the terminal time known a priori?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]  Why are two buffers necessary?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] \n\n* The paper claims that the method permits training without any collisions, even for real training runs (strong claim), however it isn't clear how this is guaranteed beyond the assumption that you have a low-level controller that can ensure collisions are avoided.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]  This is secondary to the proposed framework.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \n\n* The paper overstates the contributions of Q-masking, emphasizing improvements to data efficiency among others.[[MET-NEU], [null], [DIS], [MIN]]  The authors should validate these claims with an ablation study that compares performance with and without masking.[[PDI-NEU], [null], [DIS], [MIN]]  This would help to address the contribution of Q-masking vs. simply abstracting the action space.[[MET-NEU], [null], [DIS], [MIN]] \n\n* The network takes as input a 2.5m (this is large) occupancy grid representation of the local environment.[[MET-NEU], [null], [QSN], [MIN]]  How sensitive is the network to errors in this model?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]  Does the occupancy grid account for sensing limitations (e.g., occlusions)?\n\n"[[MET-NEU], [EMP-NEU], [QSN], [MIN]] 