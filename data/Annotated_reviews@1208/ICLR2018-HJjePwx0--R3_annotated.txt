"The paper develops an efficient algorithm to solve the subproblem of the trust region method with an asymptotic linear convergence guarantee, and they demonstrate the performances of the trust region method incorporating their efficient solver in deep learning problems.[[PDI-POS,MET-POS], [EMP-POS], [APC], [MAJ]]  It shows better generation errors by trust region methods than SGD in different tasks, despite slower running time, and the authors speculate that trust-region method can escape sharp minima and converge to wide minima and they illustrated that through some hybrid experiment.[[EXP-POS,MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]]\nThe paper is organized well.[[OAL-POS], [PNF-POS], [APC], [MAJ]]\n\n1.  The result in Section 4.3 empirically showed that Trust Region Method could escape from sharp local minimum.[[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]]  The results are interesting[[RES-POS], [EMP-POS], [APC], [MAJ]] but not quite convincing.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]]  The terms about sharp and wide minima are ambiguous.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]   At best, this provides a data point in an area that has received attention, but the lack of precision about sharp and wide makes it difficult to know what the more general conclusions are.[[RES-NEU], [EMP-NEU], [DIS], [MIN]]   It might help to show the distance between the actual model parameters that those algorithms converge to.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] \n\n2. As well know, VGG16 with well training strategy (learning rate decay) could achieve at least 92 percent accuracy.[[EXT-NEU], [null], [DIS], [GEN]]  In the paper, the author only got around 83 percent accuracy with SGD and 85 percent accuracy with TR.[[RES-NEU], [EMP-NEU], [DIS], [MAJ]]   Why is this.[[RES-NEU], [EMP-NEU], [QSN], [MIN]] \n\n3. In section 4.2, it said \"Although we can also define Hessian on ReLU function, it is not well supported on major platforms (Theano/PyTorch).[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]   Likewise, we find max-pooling is also not supported by platforms to calculate higher order derivative, one way to walk around is to change all the max-pooling layers to avg- pooling, it hurts accuracy a little bit, albeit this is not our primary concern.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] \" It is my understanding that Pytorch support higher order derivative both for ReLu and Max-pooling.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]]   Hence, it is not an explanation for not using ReLu and Max-pooling. Please clarify[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]  \n\n4. In section 4.3, the authors claimed that numerical diffentiation only hurts 1 percent error for second derivative.[[MET-NEU,RES-NEU], [null], [DIS], [GEN]]   Please provide numerical support.[[ANA-NEU], [SUB-NEU], [DIS], [GEN]]  \n\n5. The setting of numerical experiments is not clear, e.g. value of N1 and N2.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]]    This makes it hard to reproduce results.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]]\n\n5. It's not clear whether this is a theoretical paper or an empirical paper.[[OAL-NEG], [CNT], [CRT], [MAJ]]  For example, there is a lot of math, but in Section 4.5 the authors seem to hedge and say \"We give an intuitive explanation ... and leave the rigorous analysis to future works.\"  Please clarify.\n\n"[[ANA-NEG,FWK-NEU], [SUB-NEG], [DFT,CRT], [MAJ]]