"This paper proposes a small modification to the monotonic attention in [1] by adding a soft attention to the segment predicted by the monotonic attention.[[INT-NEU,PDI-NEU,MET-NEU], [EMP-NEU], [SMY,DIS], [GEN]] The paper is very well written and easy to follow[[INT-POS], [CLA-POS,IMP-POS,REC-POS], [APC,FBK], [MAJ]]. The experiments are also convincing. Here are a few suggestions and questions to make the paper stronger.[[RWK-NEU,EXP-POS], [SUB-NEG,IMP-POS,EMP-POS], [SUG,QSN], [GEN]]\n\nThe first set of questions is about the monotonic attention.[[RWK-NEG,MET-NEU], [null], [DIS], [GEN]] Training the monotonic attention with expected context vectors is intuitive, but can this be justified further?[[RWK-NEU,MET-NEG], [EMP-NEG], [DFT,QSN], [MIN]] For example, how far does using the expected context vector deviate from marginalizing the monotonic attention? [[EXP-NEU,MET-NEU], [EMP-NEU], [SMY,QSN], [GEN]]The greedy step, described in the first paragraph of page 4, also has an effect on the produced attention[[RWK-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]]. How does the greedy step affect training and decoding?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [GEN]] It is also unclear how tricks in the paragraph above section 2.4 affect training and decoding[[RWK-NEG,EXP-NEU], [CLA-NEG,IMP-NEU,EMP-NEU], [DFT,DIS,FBK], [GEN]]. These questions should really be answered in [1][[RWK-NEU,RES-NEU], [null], [QSN], [GEN]]. Since the authors are extending their work and since these issues might cause training difficulties,[[RWK-NEU,PDI-NEU,EXP-NEG], [EMP-NEG], [DFT,FBK], [MIN]] it might be useful to look into these design choices[[RWK-NEG], [PNF-NEG], [SUG,DFT], [MIN]].\n\nThe second question is about the window size $w$[[RWK-NEU], [null], [QSN], [GEN]]. Instead of imposing a fixed window size, which might not make sense for tasks with varying length segments such as the two in the paper,[[RWK-NEG,ANA-NEG], [IMP-NEG], [DFT,DIS], [MIN]] why not attend to the entire segment, i.e., from the current boundary to the previous boundary[[RWK-NEG], [SUB-NEG], [DFT], [MIN]]?\n\nIt is pretty clear that the model is discovering the boundaries in the utterance shown in Figure 2.[[MET-POS,TNF-NEU], [CLA-POS,SUB-POS,EMP-POS], [APC], [MAJ]] (The spectrogram can be made more visible by removing the delta and delta-delta in the last subplot.)[[EXP-NEG,MET-NEG], [EMP-NEG], [DFT], [MIN]] How does the MoCha attention look like for words whose orthography is very nonphonemic, for example, AAA and WWW[[RWK-NEU,MET-NEU], [EMP-NEU], [DIS,QSN], [GEN]]?\n\nFor the experiments, it is intriguing to see that $w=2$ works best for speech recognition[[RWK-POS,EXP-POS], [EMP-POS], [APC], [MAJ]]. If that's the case, would it be easier to double the hidden layer size and use the vanilla monotonic attention? [[RWK-NEU,DAT-NEU,MET-NEU], [EMP-NEU], [DIS,QSN], [GEN]]The latter should be a special case of the former, and in general you can always increase the size of the hidden layer to incorporate the windowed information.[[RWK-NEU,PDI-NEU,MET-NEU], [null], [SMY,DIS], [GEN]] Would the special cases lead to worse performance and if so why is there a difference?\[[EXT-NEU], [null], [QSN], [GEN]]n\n[1] C Raffel, M Luong, P Liu, R Weiss, D Eck, Online and linear-time attention by enforcing monotonic alignments, 2017"[[RWK-NEU,BIB-NEU], [null], [FBK], [GEN]]