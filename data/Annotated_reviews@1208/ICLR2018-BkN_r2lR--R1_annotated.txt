"This paper presents an image-to-image cross domain translation framework based on generative adversarial networks.[[INT-NEU], [null], [SMY], [GEN]] The contribution is the addition of an explicit exemplar constraint into the formulation which allows best matches from the other domain to be retrieved.[[MET-POS], [EMP-POS], [SMY], [MAJ]] The results show that the proposed method is superior for the task of exact correspondence identification and that AN-GAN rivals the performance of pix2pix with strong supervision.[[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]]\n\n\nNegatives:\n1.) The task of exact correspondence identification seems contrived.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] It is not clear which real-world problems have this property of having both all inputs and all outputs in the dataset, with just the correspondence information between inputs and outputs missing.[[DAT-NEG], [EMP-NEG], [CRT], [MAJ]]\n2.) The supervised vs unsupervised experiment on Facades->Labels (Table 3) is only one scenario where applying a supervised method on top of AN-GAN\u2019s matches is better than an unsupervised method. [[EXP-NEG,TNF-NEU], [EMP-NEG], [CRT], [MAJ]] More transfer experiments of this kind would greatly benefit the paper and support the conclusion that \u201cour self-supervised method performs similarly to the fully supervised method.[[EXP-NEU,MET-NEU], [SUB-NEU,EMP-NEU], [SUG], [MAJ]]\u201d \n\nPositives:\n1.) The paper does a good job motivating the need for an explicit image matching term inside a GAN framework.[[MET-POS], [EMP-POS], [APC], [MAJ]]\n2.) The paper shows promising results on applying a supervised method on top of AN-GAN\u2019s matches.[[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]]\n\nMinor comments:\n1. The paper sometimes uses L1 and sometimes L_1, it should be L_1 in all cases.[[OAL-NEU], [CLA-NEG], [DFT], [MIN]]\n2. DiscoGAN should have the Kim et al citation, right after the first time it is used. I had to look up DiscoGAN to realize it is just Kim et al.[[BIB-NEG], [PNF-NEG], [DFT], [MIN]]"