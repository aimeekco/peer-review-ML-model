"This paper proposes using a feedforward neural network (FFNN) to extract intermediate features which are input to a 1NN classifier.[[PDI-NEU], [null], [SMY], [GEN]]  The parameters of the FFNN are updated via a genetic algorithm with a fitness function defined as the error on the downstream classification, on a held-out set.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]]  The performance of this approach is measured on several UCI datasets and compared with baselines.[[RWK-NEU,DAT-NEU], [null], [SMY], [GEN]]\n\u2013 The paper\u2019s main contribution seems to be a neural network with a GA optimization for classification that can learn \u201cintelligent combinations of features\u201d, which can be easily classified by a simple 1NN classifier.[[PDI-NEU,MET-NEU], [null], [DIS], [MAJ]] But isn't this exactly what neural networks do \u2013 learn intelligent combinations of features optimized (in this case, via GA) for a downstream task?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]] This has already been successfully applied in multiple domains eg. in computer vision (Krizhevsky et al, NIPS 2011), NLP (Bahdanau et al 2014), image retrieval (Krizhevsky et al. ESANN 2011) etc, and also studied comprehensively in autoencoding literature.[[RWK-NEU], [CMP-NEU], [DIS], [MAJ]] There also exists prior work on optimizing neural nets via GA (Leung, Frank Hung-Fat et al., IEEE Transactions on Neural networks 2003).[[RWK-NEU], [null], [DIS], [MAJ]] However, this paper claims both as novelties while not offering any improvement / comparison. [[OAL-NEG], [NOV-NEG], [CRT], [MAJ]]\n\u2013 The claim \u201cthere is no need to use more powerful and complex classifier anymore\u201d is unsubstantiated, as the paper\u2019s approach still entails using a complex classifier (a FFNN) to learn an optimal intermediate representation.[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MAJ]]\n\u2013 The choice of activations is not motivated, and performance on variants is not reported.[[MET-NEG,MET-NEG], [SUB-NEG,EMP-NEG], [DFT,CRT], [MAJ]] For instance, why is that particular sigmoid formulation used?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MAJ]] \n\u2013 The use for a genetic algorithm for optimization is not motivated, and no comparison is made to the performance and efficiency of other approaches (like standard backpropagation).[[EXP-NEG,MET-NEG], [CMP-NEG,EMP-NEG], [DFT,CRT], [MAJ]] So it is unclear why GA makes for a better choice of optimization, if at all.[[MET-NEG], [CMP-NEG], [CRT], [MAJ]]\n\u2013 The primary baselines compared to are unsupervised methods (PCA and LDA), and so demonstrating improvements over those with a supervised representation does not seem significant or surprising.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] It would be useful to compare with a simple neural network baseline trained for K-way classification with standard backpropagation (though the UCI datasets may potentially be too small to achieve good performance).[[RWK-NEU,DAT-NEU,MET-NEU], [null], [SUG], [MAJ]]\n\u2013 The paper is poorly written, containing several typos and incomplete, unintelligible sentences, incorrect captions (eg. Table 4) etc.\n"[[OAL-NEG], [CLA-NEG], [CRT], [MIN]]