"The paper suggests taking GloVe word vectors, adjust them, and then use a non-Euclidean similarity function between them.[[PDI-NEU], [null], [SMY], [GEN]] The idea is tested on very small data sets (80 and 50 examples, respectively).[[DAT-NEG], [SUB-NEG], [DFT], [MIN]] The proposed techniques are a combination of previously published steps, and the new algorithm fails to reach state-of-the-art on the tiny data sets.[[RWK-NEG,MET-NEG], [CMP-NEG], [CRT], [MAJ]]\n\nIt isn't clear what the authors are trying to prove, nor whether they have successfully proven what they are trying to prove[[RES-NEG], [EMP-NEG], [CRT], [MAJ]]. Is the point that GloVe is a bad algorithm?[[MET-NEU], [EMP-NEU], [QSN,CRT], [MIN]] That these steps are general?[[MET-NEU], [EMP-NEU], [QSN,CRT], [MIN]] If the latter, then the experimental results are far weaker than what I would find convincing.[[EXP-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] Why not try on multiple different word embeddings?[[EXP-NEU], [EMP-NEU], [QSN], [MAJ]] What happens if you start with random vectors?[[EXP-NEU], [EMP-NEU], [QSN], [MAJ]] What happens when you try a bigger data set or a more complex problem?"[[DAT-NEU], [EMP-NEU], [QSN], [MAJ]]