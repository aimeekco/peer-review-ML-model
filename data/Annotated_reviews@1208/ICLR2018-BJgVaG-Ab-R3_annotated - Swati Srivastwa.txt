"This paper proposes to join temporal logic with hierarchical reinforcement learning to simplify skill composition.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]]  The combination of temporal logic formulas with reinforcement learning was developed previously in the literature, and the main contribution of this paper is for fast skill composition.[[RWK-NEU], [CMP-NEU], [DIS], [GEN]]  The system uses logic formulas in truncated linear temporal logic (TLTL), which lacks an Always operator and where the LTL formula (A until B) also means that B must eventually hold true. [[EXP-NEG,MET-NEG], [SUB-NEG,EMP-NEG], [DFT], [MAJ]]The temporal truncation also requires the use of a specialized MDP formulation with an explicit and fixed time horizon T.[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MAJ]]  The exact relationship between the logical formulas and the stochastic trajectories of the MDP is not described in detail here, but relies on a robustness metric, rho.[[MET-NEG,ANA-NEG], [SUB-NEG], [DFT,CRT], [MAJ]]  The main contributions of the paper are to provide a method that converts a TLTL formula that specifies a task into a reward function for a new augmented MDP (that can be used by a conventional RL algorithm to yield a policy), and a method for quickly combining two such formulas (and their policies) into a new policy.[[MET-NEU], [EMP-NEU], [DIS], [GEN]]  The proposed method is evaluated on a small Markov chain and a simulated Baxter robot.[[MET-NEU], [null], [DIS], [GEN]]\n\nThe main problem with this paper is that the connections between the TLTL formulas and the conventional RL objectives are not made sufficiently clear.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]  The robustness term rho is essential, but it is not defined.[[MET-NEG], [EMP-NEG], [CRT], [GEN]]  I was also confused by the notation $D_\\phi^q$, which was described but not defined.[[MET-NEG,OAL-NEG], [PNF-NEG], [CRT], [MIN]]  The method for quickly combining known skills (the zero-shot skill composition in the title) is switching between the two policies based on rho.[[MET-NEU], [EMP-NEU], [DIS], [GEN]]  The fact that there may be many policies which satisfy a particular reward function (or TLTL formula) is ignored.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]  This means that skill composition that is proposed in this paper might be quite far from the best policy that could be learned directly from a single conjunctive TLTL formula.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] It is unclear how this approach manages tradeoffs between objectives that are specified as a conjunction of TLTL goals.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]  is it better to have a small probability of fulfilling all goals, or to prefer a high probability of fulfilling half the goals?[[MET-NEU], [EMP-NEU], [QSN], [MAJ]]   In short the learning objectives of the proposed composition algorithm are unclear after translation from TLTL formulas to rewards.\n"[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] 