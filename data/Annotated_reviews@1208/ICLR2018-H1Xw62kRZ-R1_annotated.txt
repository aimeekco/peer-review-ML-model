"The authors consider the task of program synthesis in the Karel DSL.[[PDI-NEU], [null], [SMY], [GEN]] Their innovations are to use reinforcement learning to guide sequential generation of tokes towards a high reward output, incorporate syntax checking into the synthesis procedure to prune syntactically invalid programs.[[PDI-NEU], [null], [SMY], [GEN]] Finally they learn a model that predicts correctness of syntax in absence of a syntax checker.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] \n\nWhile the results in this paper look good,[[RES-POS], [EMP-POS], [APC], [MAJ]] I found many aspects of the exposition difficult to follow.[[OAL-NEG], [CNT], [CRT], [MIN]] In section 4, the authors define objectives, but do not clearly describe how these objectives are optimized, instead relying on the read to infer from context how REINFORCE and beam search are applied.[[MET-NEG,ANA-NEG], [CLA-NEU], [CRT], [MIN]] I was not able to understand whether syntactic corrected is enforce by way of the reward introduced in section 4, or by way of the conditioning introduced in section 5.1. Discussion of the experimental results coould similarly be clearer.[[RES-NEU], [CLA-NEU], [SUG], [MIN]] The best method very clearly depends on the taks and the amount of available data, but I found it difficult to extract an intuition for which method works best in which setting and why.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] \n\nOn the whole this seems like a promising paper.[[OAL-POS], [CNT], [APC], [MAJ]] That said, I think the authors would need to convincingly address issues of clarity in order for this to appear.[[OAL-NEU], [CLA-NEU], [SUG], [MIN]] \n\nSpecific comments \n\n- Figure 2 is too small[[TNF-NEG], [PNF-NEG], [CRT], [MIN]] \n\n- Equation 8 is confusing in that it defines a Monte Carlo estimate of the expected reward, rather than an estimator of the gradient of the expected reward (which is what REINFORCE is).[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] \n\n- It is not clear the how beam search is carried out.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]] In equation (10) there appear to be two problems.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]  The first is that the index i appears twice (once in i=1..N and once in i \\in 1..C), the second is that \u03bb_r refers to an index that does not appear.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]  More generally, beam search is normally an algorithm where at each search depth, the set of candidate paths is pruned according to some heuristic.[[MET-NEU], [EMP-NEU], [DIS], [GEN]]  What is the heuristic here?[[MET-NEU], [EMP-NEU], [QSN], [GEN]]  Is syntax checking used at each step of token generation, or something along these lines?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]] \n \n- What is the value of the learned syntax in section 5.2?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]]  Presumaly we need a large corpus of syntax-checked training examples to learn this model, which means that, in practice, we still need to have a syntax-checker available, do we not?"[[DAT-NEU,EXP-NEU], [SUB-NEU,EMP-NEU], [QSN], [MIN]] 