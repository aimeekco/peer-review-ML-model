"Summary: \nBased on ideas within the context of kernel theory, the authors consider post-training of NNs as an extra training step, which only optimizes the last layer of the network.[[INT-NEU], [null], [SMY], [GEN]]\nThis additional step makes sure that the embedding, or representation, of the data is used in the best possible way for the considered task (which is also reflected in the experiments).[[MET-NEU], [EMP-NEU], [SMY], [GEN]]\n\nAccording to the authors, the contributions are the following:\n1. Post-training step: keeping the rest of the NN frozen (after training), the method trains the last layer in order to \"make sure\" that the representation learned is used in the most efficient way.[[MET-POS], [null], [SMY], [GEN]]\n2. Highlighting connections with kernel techniques and RKHS optimization (like kernel ridge regression).[[MET-NEU], [null], [SMY], [GEN]]\n3. Experimental results.[[EXP-POS,RES-POS], [null], [SMY], [GEN]]\n\nClarity:\nThe paper is well-written, the main ideas well-clarified.[[PDI-POS], [CLA-POS], [APC], [MAJ]] \n\nImportance:\nWhile the majority of papers nowadays focuses on the representation part (i.e., how we get to \\Phi_{L-1}(x)), this paper assumes this is given and proposes how to optimize the weights in the final step of the algorithm.[[MET-NEG], [PNF-NEG], [DFT], [MAJ]] This by itself is not enough to boost the performance universally (e.g., if \\Phi_{L-1} is not well-trained, the problem is deeper than training the last layer); however, it proposes an additional step that can be used in most NN architectures.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] From that front (i.e., proposing to do something different than simply training a NN), I find the paper interesting, that might attract some attention at the conference.[[MET-POS], [EMP-POS], [DIS], [MAJ]]\n\nOn the other hand, to my humble opinion, the experimental results do not show a significant gain in the performances of all networks (esp. Figure 3 and Table 1 are within the range of statistical error).[[EXP-NEG,TNF-NEU], [EMP-NEG], [CRT], [MAJ]] In order to state something like this universally, either one needs to perform experiments with more than just MNIST/CIFAR datasets, or even more preferably, prove that the algorithm performs better.[[DAT-NEU,EXP-NEG], [SUB-NEG], [DFT], [MAJ]]\n\nOriginality:\nIt would be great to have some more theory (if any) for the post-training step, or investigate more cases, rather than optimizing only the last layer.[[DAT-NEU,EXP-NEG], [SUB-NEU], [SUG], [MAJ]]\n\nComments:\n1. I assume the authors focused in the last layer of the NN for simplicity, but is there a reason why one might want to focus only on the last layer?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] One reason is convexity in W of the problem (2). Any other?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] \n\n2. Have the authors considered (even in practice only) to include training of the last 2 layers of the NN?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]] The authors state this question in the future direction, but it would make the paper more complete to consider it here.\n"[[EXP-NEU,FWK-NEU], [SUB-NEU], [SUG], [MIN]]