"This paper explores the idea of using policy gradients to learn a stochastic policy on complex control problems.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]]  The central idea is to frame learning in terms of a new kind of Q-value that attempts to smooth out Q-values by framing them in terms of expectations over Gaussian policies.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]]\n\nTo be honest, I didn't really \"get\" this paper.\n*[[OAL-NEG], [CNT], [CRT], [MAJ]] As far I understand, all of the original work policy gradients involved stochastic policies.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]  Many are/were Gaussian.\n*[[MET-NEU], [null], [DIS], [GEN]] All Q-value estimators are designed to marginalize out the randomness in these stochastic policies.\n*[[MET-NEU], [null], [DIS], [GEN]] As far as I can tell, this is equivalent to a slightly different formulation, where the agent emits a deterministic action (\\mu,\\Sigma) and the environment samples an action from that distribution.[[MET-NEU], [EMP-NEU], [DIS], [GEN]]  In other words, it seems that if we just draw the box a bit differently, the environment soaks up the nondeterminism, instead of needing to define a new type of Q-value.[[MET-NEU], [EMP-NEU], [DIS], [GEN]]\n\nUltimately, I couldn't discern /why/ this was a significant advance for RL, or even a meaningful new perspective on classic ideas.[[MET-NEU], [EMP-NEU], [DIS], [GEN]]\n\nI thought the little 2-mode MOG was a nice example of the premise of the model.[[MET-POS], [EMP-POS], [APC], [MAJ]]\n\nWhile I may or may not have understood the core technical contribution, I think the experiments can be critiqued: they didn't really seem to work out.[[EXP-NEG], [null], [DIS], [MAJ]]  Figures 2&3 are unconvincing - the differences do not appear to be statistically significant.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]]  Also, I was disappointed to see that the authors only compared to DDPG; they could have at least compared to TRPO, which they mention.[[MET-NEU], [SUB-NEG], [DFT], [MIN]]  They dismiss it by saying that it takes 10 times as long, but gets a better answer - to which I respond, \"Very well, run your algorithm 10x longer and see where you end up!\" [[MET-NEG], [SUB-NEG,EMP-NEG], [CRT], [MIN]] I think we need to see a more compelling demonstration of why this is a useful idea before it's ready to be published.[[PDI-NEG,OAL-NEG], [APR-NEG,REC-NEG], [FBK], [MAJ]]\n\nThe idea of penalizing a policy based on KL-divergence from a reference policy was explored at length by Bert Kappen's work on KL-MDPs.  Perhaps you should cite that?\n"[[RWK-NEU,BIB-NEG], [PNF-NEG], [QSN], [MIN]]