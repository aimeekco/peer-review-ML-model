"The paper presents a model titled the \"unsupervised tree-LSTM,\" in which the authors mash up a dynamic-programming chart and a recurrent neural network.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] As far as I can glean, the topology of the neural network is constructed using the chart of a CKY parser.[[MET-NEU], [null], [SMY], [GEN]] When combining different constituents, an energy function is computed (equation 6) and the resulting energies are passed through a softmax.[[EXP-NEU,MET-NEU,RES-NEU], [null], [SMY], [GEN]] The architecture achieves impressive results on two tasks: SNLI and the reverse dictionary of Hill et al. (2016).[[RWK-POS,RES-POS], [CMP-POS,EMP-POS], [APC], [MAJ]]\n\nOverall, I found the paper deeply uninspired.[[RWK-NEG,OAL-NEG], [CMP-NEG], [CRT], [MAJ]] The authors downplay the similarity of their paper to that of Le and Zuidema (2015), which  I did not appreciate.[[RWK-NEG], [CMP-NEG], [CRT], [MAJ]] It's true that Le and Zuidema take a parse forest from an existing parser, but it still contains an exponential number of trees, as does the work in here.[[MET-NEU], [CMP-NEU], [DIS], [MIN]] Note that exposition in Le and Zuidema (2015) discusses the pruned case as well, i.e., a compete parse forest.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MIN]] The authors of this paper simply write \"Le and Zuidema (2015) propose a model that takes as input a parse forest from an external parser, in order to deal with uncertainty.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [GEN]]\" I would encourage the authors to revisit Le and Zuidema (2015), especially section 3.2, and consider the technical innovations over the existing work.[[RWK-NEU,MET-NEU], [CMP-NEU], [SUG], [MIN]] I believe the primary difference (other using an LSTM instead of a convnet) is to replace max-pooling with softmax-pooling. Do these two architectural changes matter?[[MET-NEU], [EMP-NEU], [DIS,QSN], [GEN]] The experiments offer no empirical comparison.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]] In short, the insight of having an end-to-end differentiable function based on a dynamic-programming chart is pretty common -- the idea is in the air.[[EXP-NEG], [NOV-NEG], [CRT], [MIN]] The authors provide yet another instantiation of such an approach, but this time with an LSTM.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \n\nThe technical exposition is also relatively poor.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] The authors could have expressed their network using a clean recursion, following the parse chart, but opted not to, and, instead,  provided a round-about explanation in English.[[MET-NEU,OAL-NEG], [PNF-NEG], [CRT], [MAJ]] Thus, despite the strong results,[[RES-POS], [EMP-POS], [APC], [MAJ]] I would not like to see this work in the proceedings, due to the lack of originality and poor technical discussion.[[EXP-NEG,MET-NEG], [APR-NEG,NOV-NEG,EMP-NEG], [CRT], [MAJ]] If the paper were substantially cleaned-up, I would be willing to increase my rating. "[[OAL-NEU], [PNF-NEU], [SUG], [MAJ]]