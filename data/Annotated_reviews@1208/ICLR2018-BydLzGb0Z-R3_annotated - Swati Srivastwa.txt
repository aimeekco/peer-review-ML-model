"Twin Networks: Using the Future as a Regularizer[[EXT-NEU], [null], [DIS], [GEN]]\n\n** PAPER SUMMARY **\n\nThe authors propose to regularize RNN for sequence prediction by forcing states of the main forward RNN to match the state of a secondary backward RNN.[[INT-NEU], [null], [SMY], [GEN]] Both RNNs are trained jointly and only the forward model is used at test time.[[PDI-NEU], [null], [SMY], [GEN]]  Experiments on conditional generation (speech recognition, image captioning), and unconditional generation (MNIST pixel RNN, language models) show the effectiveness of the regularizer.[[EXP-NEU,MET-NEU], [null], [SMY], [GEN]] \n\n** REVIEW SUMMARY **\n\nThe paper reads well, has sufficient reference.[[OAL-POS,BIB-POS], [CLA-POS], [APC], [MAJ]]  The idea is simple and well explained.[[PDI-POS], [EMP-POS], [APC], [MAJ]]  Positive empirial results support the proposed regularizer.[[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]] \n\n** DETAILED REVIEW **\n\nOverall, this is a good paper.[[OAL-POS], [CNT], [APC], [MAJ]]  I have a few suggestions along the text but nothing major.[[OAL-NEU], [CLA-NEU], [DIS], [GEN]] \n\nIn related work, I would cite co-training approaches.[[RWK-NEU,EXP-NEU], [CNT], [DIS], [GEN]]  In effect, you have two view of a point in time, its past and its future and you force these two views to agree, see  (Blum and Mitchell, 1998) or Xu, Chang, Dacheng Tao, and Chao Xu.[[RWK-NEU], [CMP-NEU], [DIS], [GEN]]  \"A survey on multi-view learning.\" arXiv preprint arXiv:1304.5634 (2013).[[RWK-NEU], [CMP-NEU], [DIS], [GEN]] I would also relate your work to distillation/model compression which tries to get one network to behave like another. On that point, is it important to train the forward and backward network jointly or could the backward network be pre-trained?[[RWK-NEU,EXP-NEU,MET-NEU], [CMP-NEU,EMP-NEU], [QSN], [MIN]] \n\nIn section 2, it is not obvious to me that the regularizer (4) would not be ignored in absence of regularization on the output matrix.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] I mean, the regularizer could push h^b to small norm, compensating with higher norm for the output word embeddings.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Could you comment why this would not happen?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] \n\nIn Section 4.2, you need to refer to Table 2 in the text.[[TNF-NEU], [PNF-NEU], [DIS], [MIN]]  You also need to define the evaluation metrics used.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]  In this section, why are you not reporting the results from the original Show&Tell paper?[[RES-NEU], [EMP-NEU], [QSN], [MIN]]  How does your implementation compare to the original work?[[RWK-NEU,MET-NEU], [CMP-NEU], [QSN], [MIN]] \n\nOn unconditional generation, your hypothesis on uncertainty is interesting and could be tested.[[ANA-POS], [EMP-POS], [APC], [MAJ]] You could inject uncertainty in the captioning task for instance, e.g. consider that multiple version of each word e.g. dogA, dogB, docC which are alternatively used instead of dog with predefined substitution rates.[[MET-NEU], [EMP-NEU], [SUG,DIS], [MIN]] Would your regularizer still be helpful there?[[MET-NEU], [EMP-NEU], [DIS], [MIN]] At which point would it break?"[[MET-NEU], [EMP-NEU], [QSN], [MIN]]