"This paper proposes a method for learning from noisy labels, particularly focusing on the case when data isn't redundantly labeled (i.e. the same sample isn't labeled by multiple non-expert annotators).[[INT-NEU], [null], [SMY], [GEN]] The authors provide both theoretical and experimental validation of their idea.[[EXP-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] \n\nPros:\n+ The paper is generally very clearly written.[[OAL-POS], [CLA-POS], [APC], [MAJ]] The motivation, notation, and method are clear.[[PDI-POS], [IMP-POS], [APC], [MAJ]]\n+ Plentiful experiments against relevant baselines are included, validating both the no-redundancy and plentiful redundancy cases. [[RWK-NEU,EXP-POS], [EMP-POS], [APC], [MAJ]]\n+ The approach is a novel twist on an existing method for learning from noisy data.[[DAT-NEG,MET-NEU], [NOV-POS], [SMY], [GEN]] \n\nCons: \n- All experiments use simulated workers; this is probably common but still not very convincing.[[EXP-NEG,MET-NEU], [EMP-NEU], [DFT], [MIN]]\n- The authors missed an important related work which studies the same problem and comes up with a similar conclusion: Lin, Mausam, and Weld. \"To re (label), or not to re (label).\" HCOMP 2014.[[RWK-NEG], [CMP-NEG], [DFT], [MIN]]\n- The authors should have compared their approach to the \"base\" approach of Natarajan et al.[[RWK-NEU], [CMP-NEU], [SUG], [MIN]] \n- It seems too simplistic too assume all workers are either hammers or spammers; the interesting cases are when annotators are neither of these.[[DAT-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]]\n- The ResNet used for each experiment is different, and there is no explanation of the choice of architecture.[[EXP-NEU,MET-NEU], [null], [SMY], [GEN]]\n\nQuestions: \n- How would the model need to change to account for example difficulty?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] \n- Why are Joulin 2016, Krause 2016 not relevant?[[RWK-NEU], [null], [QSN], [MIN]]\n- Best to clarify what the weights in the weighted sum of Natarajan are. [[MET-NEU], [null], [QSN], [MIN]]\n- \"large training error on wrongly labeled examples\" -- how do we know they are wrongly labeled, i.e. do we have a ground truth available apart from the crowdsourced labels?[[OAL-NEU], [CLA-NEU], [QSN], [MIN]] Where does this ground truth come from?[[MET-NEU], [null], [QSN], [MIN]]\n- Not clear what \"Ensure\" means in the algorithm description.[[MET-NEU], [null], [QSN], [MIN]]\n- In Sec. 4.4, why is it important that the samples are fresh?[[DAT-NEU], [SUB-NEU], [QSN], [MIN]]\n"