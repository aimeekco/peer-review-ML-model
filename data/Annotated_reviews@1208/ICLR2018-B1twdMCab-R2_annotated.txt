"The main emphasis of this paper is how to add background knowledge so as to improve the performance of NLU (specifically QA and NLI) systems[[PDI-NEU], [null], [SMY], [GEN]]. They adopt the sensible perspective that background knowledge might most easily be added by providing it in text format.[[PDI-NEU], [null], [SMY], [GEN]] However, in this paper, the way it is added is simply by updating word representations based on this extra text.[[MET-NEU], [null], [SMY], [GEN]] This seems too simple to really be the right way to add background knowledge.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] \n\nIn practice, the biggest win of this paper turns out to be that you can get quite a lot of value by sharing contextualized word representations between all words with the same lemma (done by linguistic preprocessing;[[RES-POS], [EMP-POS], [APC], [MAJ]] the paper never says exactly how, not even if you read the supplementary material).[[RES-NEG], [SUB-NEG], [DFT], [MIN]] This seems a useful observation which it would be easy to apply everywhere and which shows fairly large utility from a bit of linguistically sensitive matching![[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]]  As the paper notes, this type of sharing is the main delta in this paper from simply using a standard deep LSTM (which the paper claims to not work on these data sets, though I'm not quite sure couldn't be made to work with more tuning).[[DAT-NEG,MET-NEG], [SUB-NEG,EMP-NEG], [CRT], [MIN]]\n\npp. 6-7: The main thing of note seems to be that sharing of representations between words with the same lemma (which the tables refer to as \"reading\" is worth a lot (3.5-6.0%), in every case rather more than use of background knowledge (typically 0.3-1.5%).[[TNF-NEU,MET-NEU], [null], [DIS], [GEN]] A note on the QA results: The QA results are certainly good enough to be in the range of \"good systems\",[[RES-POS], [EMP-POS], [APC], [MAJ]] but none of the results really push the SOTA.[[RWK-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] The best SQuAD (devset) results are shown as several percent below the SOTA.[[RWK-NEG,RES-NEG], [CMP-NEG], [CRT], [MAJ]] In the table the TriviaQA results are shown as beating the SOTA, and that's fair wrt published work at the time of submission,[[RWK-POS,RES-POS], [EMP-POS], [APC], [MAJ]] but other submissions show that all of these results are below what you get by running the DrQA (Chen et al. 2017) system off-the-shelf on TriviaQA, so the real picture is perhaps similar to SQuAD, especially since DrQA is itself now considerably below the SOTA on SQUAD.[[RWK-NEG,DAT-NEG,RES-NEG], [CMP-NEG], [CRT], [MAJ]] Similar remarks perhaps apply to the NLI results.[[RES-NEU], [EMP-NEU], [DIS], [MIN]]\n\np.7 In the additional NLI results, it is interesting and valuable to note that the lemmatization and knowledge help much more when amounts of data (and the covarying dimensionality of the word vectors) is much smaller,[[DAT-POS,MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]] but the fact that the ideas of this paper have quite little (or even negative) effects when run on the full data with full word vectors on top of the ESIM model again draws into question whether enough value is being achieved from the world knowledge.[[DAT-NEG,MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nBiggest question:\n - Are word embeddings powerful enough as a form of memory to store the kind of relational facts that you are accessing as background knowledge?[[MET-NEG], [EMP-NEG], [QSN], [MAJ]]\n\nMinor notes:\n - The paper was very well written/edited.[[OAL-POS], [CLA-POS,PNF-POS], [APC], [MAJ]] The only real copyediting I noticed was in the conclusion: and be used \u2794 and can be used; that rely on \u2794 that relies on.[[OAL-POS], [PNF-POS], [APC], [MAJ]]\n - Should reference to (Manning et al. 1999) better be to (Manning et al. 2008) since the context here appears to be IR systems?[[RWK-NEU], [CMP-NEU], [QSN], [MIN]]\n - On p.3 above sec 3.1: What is u?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Was that meant to be z?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n - On p.8, I'm a bit suspicious of the \"Is additional knowledge used?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\" experiment which trains with knowledge and then tests without knowledge.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]] It's not surprising that this mismatch might hurt performance, even if the knowledge provided no incremental value over what could be gained from standard word vectors alone.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MIN]]\n - In the supplementary material the paper notes that the numbers are from the best result from 3 runs.[[RES-POS], [PNF-POS], [APC], [MAJ]] This seems to me a little less good experimental practice than reporting an average of k runs, preferably for k a bit bigger than 3.\n\n\n"[[EXP-POS], [PNF-POS], [APC], [MAJ]]