"=======\nUpdate:\n\nThe new version addresses some of my concerns.[[EXT-NEU], [null], [DIS], [GEN]]  I think this paper is still pretty borderline, but I increased my rating to a 6.[[OAL-NEU], [REC-NEU], [FBK], [MAJ]] \n=======\n\nThis article examines the two sources of loose bounds in variational autoencoders, which the authors term \u201capproximation error\u201d (slack due to using a limited variational family) and \u201camortization error\u201d (slack due to the inference network not finding the optimal member of that family).[[PDI-NEU], [null], [SMY], [GEN]] \n\nThe existence of amortization error is often ignored in the literature, but (as the authors point out) it is not negligible.[[PDI-NEU], [null], [SMY], [GEN]]  It has been pointed out before in various ways, however:\n* Hjelm et al. (2015; https://arxiv.org/pdf/1511.06382.pdf) observe it for directed belief networks (admittedly a different model class).[[RWK-NEU], [null], [DIS], [MIN]] \n* The ladder VAE paper by Sonderby et al. (2016, https://arxiv.org/pdf/1602.02282.pdf) uses an architecture that reduces the work that the encoder network needs to do, without increasing the expressiveness of the variational approximation.[[RWK-NEU], [null], [DIS], [MIN]] That this approach works well implies that amortization error cannot be ignored.[[RWK-NEU], [null], [DIS], [MIN]]\n* The structured VAE paper by Johnson et al. (2016, https://arxiv.org/abs/1603.06277) also proposes an architecture that reduces the load on the inference network.[[RWK-NEU], [null], [DIS], [MIN]]\n* The very recent paper by Krishnan et al. (posted to arXiv days before the ICLR deadline, although a workshop version was presented at the NIPS AABI workshop last year; http://approximateinference.org/2016/accepted/KrishnanHoffman2016.pdf) examines amortization error as a core cause of training failures in VAEs.[[RWK-NEU], [null], [DIS], [MIN]] They also observe that the gap persists at test time, although it does not examine how it relates to approximation error.[[RWK-NEU], [null], [DIS], [MIN]]\n\nSince these earlier results existed, and approximation-amortization decomposition is fairly simple (although important!), the main contributions of this paper are the empirical studies.[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MAJ]] I will try to summarize the main novel (i.e., not present elsewhere in the literature) results of these:[[RES-NEU], [NOV-NEU], [DIS], [GEN]]\n\nSection 5.1:\nInference networks with FFG approximations can produce qualitatively embarrassing approximations.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nSection 5.2:\nWhen trained on a small dataset, training amortization error becomes negligible.[[DAT-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] I found this surprising, since it\u2019s not at all clear why dataset size should lead to \u201cstrong inference\u201d.[[DAT-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] It seems like a more likely explanation is that the decoder doesn\u2019t have to work as hard to memorize the training set, so it has some extra freedom to make the true posterior look more like a FFG.[[DAT-NEG,EXP-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nAlso, I think it\u2019s a bit of an exaggeration to call a gap of 2.71 nats \u201cmuch tighter\u201d than a gap of 3.01 nats.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nSection 5.3:\nAmortization error is an important contributor to the slack in the ELBO on MNIST, and the dominant contributor on the more complicated Fashion MNIST dataset.[[DAT-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] (This is totally consistent with Krishnan et al.\u2019s finding that eliminating amortization error gave a bigger improvement for more complex datasets than for MNIST.)[[RWK-NEU,DAT-NEU], [CMP-NEU], [DIS], [MIN]]\n\nSection 5.4:\nUsing a restricted variational family causes the decoder to learn to induce posteriors that are easier to approximate with that variational family.[[EXP-NEU], [EMP-NEU], [DIS], [MAJ]] This idea has been around for a long time (although I\u2019m having a hard time coming up with a reference).[[RWK-NEU], [null], [DIS], [GEN]]\n\nThese results are interesting,[[RES-POS], [EMP-POS], [APC], [MAJ]] but given the empirical nature of this paper I would have liked to see results on more interesting datasets (Celeb-A, CIFAR-10, really anything but MNIST).[[DAT-NEU,RES-NEU], [SUB-NEU], [DIS], [MIN]] Also, it seems as though none of the full-dataset MNIST models have been trained to convergence, which makes it a bit difficult to interpret some results.[[EXP-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]]\n\n\nA few more specific comments:\n\n2.2.1: The \\cdot seems extraneous to me.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\n5.1: What dataset/model was this experiment done on?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]]\n\nFigure 3: This can be inferred from the text (I think), but I had to remind myself that \u201cIW train\u201d and \u201cIW test\u201d refer only to the evaluation procedure, not the training procedure.[[EXP-NEU,TNF-NEU], [null], [DIS], [MIN]] It might be good to emphasize that you don\u2019t train on the IWAE bound in any experiments.[[EXP-NEG], [EMP-NEG], [CRT], [MIN]]\n\nTable 2: It would be good to see standard errors on these numbers; they may be quite high given that they\u2019re only evaluated on 100 examples.[[DAT-NEU,RES-NEU], [EMP-NEU], [DIS], [MIN]]\n\n\u201cWe can quantitatively determine how close the posterior is to a FFG distribution by comparing the Optimal FFG bound and the Optimal Flow bound.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\u201d: Why not just compare the optimal with the AIS evaluation?[[MET-NEU], [CMP-NEU], [QSN], [MIN]] If you trust the AIS estimate, then the result will be the actual KL divergence between the FFG and the true posterior."[[MET-NEU], [null], [DIS], [MIN]]