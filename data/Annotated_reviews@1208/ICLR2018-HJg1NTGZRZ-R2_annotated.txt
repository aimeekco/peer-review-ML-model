"This paper proposes to optimize neural networks considering the three different terms: original loss function, quantization error and the sum of bits.[[PDI-NEU], [null], [SMY], [GEN]] While the idea makes sense,[[PDI-POS], [EMP-POS], [APC], [MAJ]] the paper is not well executed, and I cannot understanding how gradient descend is performed based on the description of Section 4.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\n1. After equation (5), I don't understand how the gradient of L(tilde_W) w.r.t. B(i) is computed. B(i) is discrete.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] The update rule seems to be clearly wrong.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n2. The experimental section of this paper needs improvement.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n   a. End-to-end trained quantized networks have been studied in various previous works including stochastic neuron (Bengio et al 2013), quantization + fine tuning (Wu et al 2016 Quantized Convolutional Neural Networks for Mobile Devices), Binary connect (Courbariaux et al 2016) etc.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [GEN]] None of these works have been compared with.\n   b. All the baseline methods use 8 bits per value.[[RWK-NEG,MET-NEG], [CMP-NEG,SUB-NEG], [DFT,CRT], [MAJ]] This choice is quite ad-hoc.[[RWK-NEG], [CMP-NEG], [CRT], [MAJ]]\n   c. Only MNIST and CIFAR10 dataset with Lenet32 are used in the experiment.[[DAT-NEG,EXP-NEG], [SUB-NEG], [DFT], [MAJ]] I find the findings not conclusive based on these.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]]\n   d. No wall-time and real memory numbers are reported."[[EXP-NEG,MET-NEG], [SUB-NEG], [DFT], [MAJ]]