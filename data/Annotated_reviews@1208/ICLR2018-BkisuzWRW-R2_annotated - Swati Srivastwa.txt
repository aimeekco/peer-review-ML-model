"Summary:\nThe authors present a paper about imitation of a task presented just during inference, where the learning is performed in a completely self-supervised manner.[[INT-NEU,PDI-NEU], [null], [DIS], [GEN]]\nDuring training, the agent explores by itself related (but different) tasks, learning a) how actions affect the world state,[[EXP-NEU], [null], [DIS], [GEN]] b) which action to perform given the previous action and the world state,[[EXP-NEU], [null], [DIS], [GEN]] and c) when to stop performing actions.[[EXP-NEU], [null], [DIS], [GEN]] This learning is done without any supervision, with a loss that tries to predict actions which result in the state achieved through self exploration (forward consistency loss).[[EXP-NEG,MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]]\nDuring testing, the robot is presented with a sequence of goals in a related but different task.[[EXP-NEU], [null], [DIS], [GEN]] Experiments show that the system achieves a better performance than different subparts of the system (through an ablation study), state of the art and common open source systems.[[RWK-POS,EXP-POS], [CMP-POS,EMP-POS], [APC], [MAJ]]\n\nPositive aspects:\nThe paper is well written and clear to understand.[[OAL-POS], [CLA-POS], [APC], [MAJ]] Since this is not my main area of research I cannot judge its originality in a completely fair way, but it is original AFAIK.[[OAL-NEU], [NOV-NEU], [DIS], [GEN]] The idea of learning the basic relations between actions and state through self exploration is definitely interesting.[[PDI-POS], [EMP-POS], [APC], [MAJ]]\nThis line of work is specially relevant since it attacks one of the main bottlenecks in learning complex tasks, which is the amount of supervised examples.[[PDI-POS], [EMP-POS], [APC], [MAJ]]\nThe experiments show clearly that a) the components of the proposed pipeline are important since they outperform ablated versions of it and b) the system is better than previous work in those tasks[[EXP-POS], [CMP-POS,EMP-POS], [APC], [MAJ]]\n\nNegative aspects:\nMy main criticism to the paper is that the task learning achieved through self exploration seems relatively shallow.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] From the navigation task, it seems like the system mainly learns a discover behavior that is better than random motion.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]]It definitely does not seem able to learn higher level concepts like certain scenes being more likely to be close to each other than others (e.g. it is likely to find an oven in the same room as a kitchen sink but not in a toilet).[[CNT], [CNT], [DIS], [GEN]] It is not clear whether this is achievable by the current system even with more training data.[[DAt-NEG,RES-NEG], [EMP-NEG], [CRT], [MIN]]\nAnother aspect that worries me about the system is how it can be extended to higher dimensional action spaces.[[MET-NEG], [EMP-NEG], [DIS], [MIN]] Extending control laws through self-exploration under random disturbances has been studied in character control (e.g. \"Domain of Attraction Expansion for Physics-based Character Control\" by Borno et al.), but the dimensionality of the problem makes this exploration very expensive (even for short time frames, and even in simulation).[[RWK-NEU], [CMP-NEU], [DIS], [MIN]] I wonder if the presented ideas won't suffer from the same curse of dimensionality.[[PDI-NEG], [null], [DIS], [GEN]]\nIn terms of experiments, it is shown that the system is more effective than others but not so much *how* it achieves this efficiency.[[EXP-NEU,RES-NEG], [EMP-NEG], [CRT], [MAJ]] It would be good to show whether part of its efficiency comes from effective image-guided navigation: does a partial image match entail with targetted navigation (e.g. matches in the right side of the image make the robot turn right)?[[RES-NEU], [EMP-NEU], [DIS], [MIN]]\nA couple more specific comments:\n- I think that dealing with multimodal distributions of actions with the forward consistency loss is effective for achieving the goal, but not necessarily good for modeling multimodality.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Isn't it possible that the agent learns only one way of achieving such goal?[[MET-NEG], [SUB-NEG], [QSN], [MIN]]\n - It is not clear how the authors achieve to avoid the problem of starting from scratch by \"pre-train the forward model and PSF separately by blocking gradient flow\".[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]] Isn't it still challenging to update them independently, given that at the beginning both components are probably not very accurate?[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]]\n\n\nConclusion:\nI think the paper presents an interesting idea which should be exposed to the community.[[PDI-POS], [EMP-POS], [APC], [MAJ]] The paper is easy to read and its experiments show the effectiveness of the method.[[MET-POS,OAL-POS], [CLA-POS,EMP-POS], [APC], [MAJ]] The relevance of the method to achieve a deeper sense of learning and performing more complex tasks is however unclear to me."[[MET-NEG], [EMP-NEG], [CRT], [MIN]]