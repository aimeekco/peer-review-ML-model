"The paper proposes a novel way of causal inference in situations where in causal SEM notation the outcome Y = f(T,X) is a function of a treatment T and covariates X.[[INT-NEU], [null], [SMY], [GEN]] The goal is to infer the treatment effect E(Y|T=1,X=x) - E(Y|T=0,X=x) for binary treatments at every location x.[[PDI-NEU], [null], [SMY], [GEN]] If the treatment effect can be learned, then forecasts of Y under new policies that assign treatment conditional on X will still \"work\" and the distribution of X can also change without affecting the accuracy of the predictions.[[MET-NEU], [null], [SMY], [GEN]] \n\nWhat is proposed seems to be twofold:\n- instead of using a standard inverse probability weighting, the authors construct a bound for the prediction performance under new distributions of X and new policies and learn the weights by optimizing this bound.[[INT-NEU], [null], [SMY], [GEN]] The goal is to avoid issues that arise if the ratio between source and target densities become very large or small and the weights in a standard approach would become very sparse, thus leading to a small effective sample size.[[INT-NEU], [null], [SMY], [GEN]]\n- as an additional ingredient the authors also propose \"representation learning\" by mapping x to some representation Phi(x).[[INT-NEU], [null], [SMY], [GEN]] \nThe goal is to learn the mapping Phi (and its inverse) and the weighting function simultaneously by optimizing the derived bound on the prediction performance.[[INT-NEU], [null], [SMY], [GEN]] \n\nPros: \n- The problem is relevant and also appears in similar form in domain adaptation and transfer learning.[[PDI-POS], [EMP-POS], [APC], [MAJ]] \n- The derived bounds and procedures are interesting and nontrivial, even if there is some overlap with earlier work of Shalit et al.[[RWk-POS,MET-POS], [EMP-POS], [APC], [MAJ]] \n\nCons:\n- I am not sure if ICLR is the optimal venue for this manuscript but will leave this decision to others.[[OAl-NEU], [APR-NEU], [DIS], [GEN]] \n- The manuscript is written in a very compact style and I wish some passages would have been explained in more depth and detail.[[OAL-NEG], [SUB-NEG], [SUG,DFT], [MAJ]] Especially the second half of page 5 is at times very hard to understand as it is so dense.[[CNT], [PNF-NEG], [CRT], [MAJ]] \n- The implications of the assumptions in Theorem 1 are not easy to understand, especially relating to the quantities B_\\Phi, C^\\mathcal{F}_{n,\\delta} and D^{\\Phi,\\mathcal{H}}_\\delta.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Why would we expect these quantities to be small or bounded?[[MET-NEG], [EMP-NEG], [QSN], [MIN]] How does that compare to the assumptions needed for standard inverse probability weighting?[[MET-NEG], [EMP-NEG], [QSN], [MIN]]  \n- I appreciate that it is difficult to find good test datasets for evaluating causal estimator.[[DAT-POS], [EMP-POS], [APC], [MAJ]]   The experiment on the semi-synthetic IHDP dataset is ok, even though there is very little information about its structure in the manuscript (even basic information like number of instances or dimensions seems missing?).[[DAT-POS,EXP-POS], [EMP-POS], [APC], [MAJ]] The example does not provide much insight into the main ideas and when we would expect the procedure to work more generally.\n\n\n\n\n\n\n\n\n\n"[[EXP-NEG,MET-NEG], [SUB-NEG], [CRT], [MIN]]