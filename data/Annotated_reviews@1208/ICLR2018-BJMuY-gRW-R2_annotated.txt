"Summary: The paper proposes to use the CYK chart-based mechanism to compute vector representations for sentences in a bottom-up manner as in recursive NNs[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]]. The key idea is to maintain a chart to take into account all possible spans.[[PDI-NEU], [null], [SMY], [GEN]] The paper also introduces an attention method over chart cells.[[MET-NEU], [null], [SMY], [GEN]] The experimental results show that the propped model outperforms tree-lstm using external parsers.[[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]]\n\nComment: I kinda like the idea of using chart, and the attention over chart cells.[[MET-POS], [EMP-POS], [APC], [MAJ]] The paper is very well written.[[OAL-POS], [CLA-POS], [APC], [MAJ]]\n- My only concern about the novelty of the paper is that the idea of using CYK chart-based mechanism is already explored in Le and Zuidema (2015).[[RWK-NEG], [NOV-NEG], [CRT], [MAJ]]\n- Le and Zudema use pooling and this paper uses weighted sum.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MIN]] Any differences in terms of theory and experiment?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]]\n- I like the new attention over chart cells.[[EXP-POS], [EMP-POS], [APC], [MAJ]] But I was surprised that the authors didn\u2019t use it in the second experiment (reverse dictionary).[[EXP-NEG], [EMP-NEG], [CRT], [MIN]]\n- In table 2, it is difficult for me to see if the difference between unsupervised tree-lstm and right-branching tree-lstm (0.3%) is \u201cgood enough\u201d.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]] In which cases the former did correctly but the latter didn\u2019t?[[TNF-NEG], [PNF-NEG], [QSN], [MIN]]\n- In table 3, what if we use the right-branching tree-lstm with attention?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n- In table 4, why do Hill et al lstm and bow perform much better than the others?\n"[[MET-NEU,TNF-NEU], [EMP-NEU], [QSN], [MIN]]