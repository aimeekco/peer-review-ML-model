"This paper proposes a new machine comprehension model, which integrates several contributions like different embeddings for gate function and passage representation function, self-attention layers and highway network based fusion layers.[[INT-NEU], [null], [SMY], [GEN]] The proposed method was evaluated on the SQuAD dataset only, and marginal improvement was observed compared to the baselines.[[RWK-NEU,DAT-NEU,MET-NEU], [CMP-POS], [SMY], [MAJ]]\n\n(1) One concern I have for this paper is about the evaluation. The paper only evaluates the proposed method on the SQuAD data with systems submitted in July 2017, and the improvement is not very large.[[RWK-NEU,DAT-NEU], [CMP-NEU], [CRT], [MAJ]] As a result, the results are not suggesting significance or generalizability of the proposed method.[[RES-NEU,ANA-NEU], [EMP-NEG], [CRT], [MAJ]]\n\n(2) The paper gives some ablation tests like reducing the number of layers and removing the gate-specific question embedding, which help a lot for understanding how the proposed methods contribute to the improvement.[[EXP-POS,MET-NEU], [EMP-POS], [APC], [MAJ]] However, the results show that the deeper self-attention layers are indeed useful (but still not improving a lot, about 0.7-0.8%).[[RES-POS], [EMP-POS], [APC], [MAJ]] The other proposed components contribute less significant.[[RES-NEU,ANA-NEU], [IMP-NEU], [CRT], [MAJ]] As a result, I suggest the authors add more ablation tests regarding (1) replacing the outer-fusion with simple concatenation (it should work for two attention layers); (2) removing the inner-fusion layer and only use the final layer's output, and using residual connections (like many NLP papers did) instead of the more complicated GRU stuff.[[EXP-NEU,MET-NEU], [EMP-NEU], [SUG], [MAJ]]\n\n(3) Regarding the ablation in Table 2, my first concern is that the improvement seems small (~0.5%). [[RES-NEU], [IMP-NEU], [DIS], [MAJ]]As a result, I am wondering whether this separated question embedding really brings new information, or the similar improvement can be achieved by increasing the size of LSTM layers.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] For example, if we use the single shared question embeddings, but increase the size from 128 to some larger number like 192, can we observe similar improvement.[[EXP-NEU], [EMP-NEU], [DIS], [MAJ]] I suggest the authors try this experiment as well and I hope the answer is no, as separated input embeddings for gate functions was verified to be useful in some \"old\" works with syntactic features as gate values, like \"Semantic frame identification with distributed word representations\" and \"Learning composition models for phrase embeddings\" etc.[[EXP-NEU,MET-NEU], [EMP-NEU], [CRT], [MAJ]]\n\n(4) Please specify which version of the SQuAD leaderboard is used in Table 3.[[TNF-NEU,DAT-NEU], [SUB-NEU], [SUG], [MIN]] Is it a snapshot of the Jul 14 one?[[DAT-NEU], [null], [QSN], [MIN]] Because this paper is not comparing to the state-of-the-art, no specification of the leaderboard version may confuse the other reviewers and readers. [[RWK-NEU], [CMP-NEG], [CRT], [MAJ]]By the way, it will be better to compare to the snapshot of Oct 2017 as well, indicating the position of this work during the submission deadline.[[RWK-NEU], [CMP-NEU], [SUG], [MAJ]]\n\nMinor issues:\n\n(1) There are typos in Figure 1 regarding the notations of Question Features and Passage Features.[[TNF-NEG], [PNF-NEG], [DFT], [MIN]]\n\n(2) In Figure 1, I suggest adding an \"N \\times\" symbol to the left of the Q-P Attention Layer and remove the current list of such layers, in order to be consistent to the other parts of the figure.[[TNF-NEU], [PNF-NEU], [SUG], [MIN]]\n\n(3) What is the relation between the \"PhaseCond, QPAtt+\"\b in Table 2 and the \"PhaseCond\" in Table 3?[[TNF-NEU], [EMP-NEU], [QSN], [MIN]] I was assuming that those are the same system but did not see the numbers match each other.[[TNF-NEU], [null], [SMY], [GEN]]"