"* Paper Summary\nThis paper addresses the problem of learning a low rank tensor filter operation for filtering layers in deep neural networks (DNNs).[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] It makes use of the CP or rank-1 tensor decomposition to define the meaning of rank for a tensor.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] When a tensor is decomposed as a sum of rank-1 tensors (outer products), the number of operations in a DNN forward pass decreases leading to a faster testing runtime.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]]  This form of network compression has been worked on before.[[MET-NEU], [CMP-NEU], [DIS], [MIN]]  The contribution of this paper seems to be the specific way the decomposition is used in training the DNN.[[EXP-NEU], [null], [DIS], [GEN]]  It seems that this training process follows a projected gradient descent procedure, where the filter weights of the network are iteratively updated using regular (stochastic) gradient descent and then they are projected onto the set of rank-R tensors.[[EXP-NEU,MET-NEU], [null], [SMY], [GEN]]  The authors devise a heuristic way (based on an innovated measure that combines computational complexity with performance) to select the tensor rank to be used.[[EXP-NEU,MET-NEU], [null], [SMY], [GEN]]  Experiments are conducted on the task of image classification for a couple well-known DNN architectures (VGG and Resnet) to show a speedup of runtime in testing, significant compression of the network, and minimal degradation in performance.[[EXP-POS,MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]] \n\n* Related Work\nThe authors do a good job describing and listing the papers most related to the current submission.[[RWK-POS], [CMP-POS], [APC], [MAJ]]\n\n* Technical Novelty\nOne main limitation of the paper is the lack of technical contribution.[[EXP-NEG,MET-NEG], [SUB-NEG], [CRT], [MAJ]] The idea of using rank-1 tensor decomposition for training low-rank filtering operations in DNNs has already been proposed and used in several other work.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [GEN]] From what I understood from the paper, the only technical contribution is the use of a so called 2-pass decomposition, which is simply an implementation of projected gradient descent on the set of rank-R tensors.[[EXP-NEU,MET-NEU], [null], [DIS], [GEN]] In particular, if we seek to minimize f(W) such that W belongs to asset that can be easily projected on, then projected gradient descent would apply traditional gradient descent on the current iterate, followed by a projection step onto this set.[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [GEN]] This paper seems to be applying this exact same strategy in training for a cross-entropy classification loss f(.).[[MET-NEU], [EMP-NEU], [DIS], [MIN]] This iterative projection tends to perform better than iteratively optimizing f(W) and then applying the projection step only once at the very end of the optimization (assumedly the CP-ALS method that is used for comparison).[[MET-POS], [CMP-POS], [APC], [MAJ]] Put in this light, the proposed paper does not contribute much.[[OAL-NEG], [SUB-NEG], [CRT], [MAJ]]\n\n* Paper Presentation\nIn the reviewer\u2019s opinion, the primary limitation of the paper is how it is written and organized.[[OAL-NEG], [CLA-NEG,PNF-NEG], [CRT], [MIN]] The paper is badly written.[[OAL-NEG], [CLA-NEG], [CRT], [MIN]] It is riddled with grammar, choice of word, and spelling mistakes.[[OAL-NEG], [CLA-NEG], [CRT], [MIN]] The paper organization needs to be revamped with emphasis on the proposed ideas of the paper and how it differs from the rich related work.[[RWK-NEU], [CLA-NEU,PNF-NEU], [SUG], [MIN]] These issues make the paper hard to read.[[OAL-NEG], [CLA-NEG,PNF-NEG], [CRT], [MIN]] For example, the authors spend quite a bit of space focusing on the rank-1 (CP) decomposition, which is well known, as opposed to focusing on the merits of their technical contributions.[[OAL-NEG], [PNF-NEG], [CRT], [MIN]] Also, the experiments are not clearly explained.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]] It is hard to understand the experimental setup of each experiment and what the conclusions are.[[EXP-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] For example, it is unclear whether the baseline in Table 3 also uses the two-pass decomposition or not.[[RWK-NEG,TNF-NEG], [EMP-NEG], [CRT], [MAJ]] Also, the authors should provide a clear and standard description of the experimental setup for each experiment (e.g. which network, which dataset, which task/loss, which measure, etc.).[[OAL-NEG], [CLA-NEG,PNF-NEG], [CRT], [MAJ]]\n\n* Experimental Results\nFrom what I understood from the experiments, it seems that using the \u201ctwo-pass decomposition\u201d (i.e. projected gradient descent) is better than CP-ALS (gradient descent ended with a single projection step).[[RES-POS], [CMP-POS], [APC], [MAJ]] This conclusion seems to be intuitive and expected.[[RES-POS], [CLA-NEG,EMP-POS], [APC], [MAJ]] However, as mentioned earlier, the paper writing and organization makes it hard to understand what exactly is being shown.[[OAL-NEG], [CLA-NEG,PNF-NEG], [CRT], [MAJ]] For example, Table 1 shows that the baseline method uses less filters than the proposed method that selects the number of filters through an innovated heuristic measure.[[MET-NEG,TNF-NEG], [CLA-NEG,PNF-NEG], [CRT], [MAJ]] Then in Table 3, we see that the baseline is less stable (i.e. its performance decreases across the different iterations of projected gradient descent).[[TNF-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] Isn\u2019t this expected since the baseline uses less filters?[[RWK-NEG], [SUB-NEG], [CRT], [MIN]] It is unclear from the text if this is the case.[[CNT], [CLA-NEG], [CRT], [GEN]] \n                   The authors should do a better job explaining and comparing the overall experimental results.[[EXP-NEU,RES-NEU], [EMP-NEU], [SUG], [MIN]]  For example, it seems that the proposed projected gradient descent method leads to better speedup results in VGG as opposed to Resnet, with very similar reduction in accuracy.[[MET-POS,RES-POS], [CMP-POS], [APC], [MAJ]] The authors do not comment on this.[[CNT], [CNT], [DFT], [MIN]] It would be nice for them to explain the circumstances under which the proposed method is best suited and any potential failure cases (e.g. cases when the low-rank decomposition leads to a significant decrease in performance).[[MET-NEU], [EMP-NEU], [SUG], [MIN]] All of this analysis provided more insight into the method and helps the reader understand its extents. \n"[[ANA-NEU], [CNT], [SUG], [MIN]]