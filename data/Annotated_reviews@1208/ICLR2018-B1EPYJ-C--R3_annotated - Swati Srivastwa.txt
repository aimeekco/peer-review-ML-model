"This paper proposes a new learning method, called federated learning, to train a centralized model while training data remains distributed over a large number of clients each with unreliable and relatively slow network connections.[[EXP-NEG,MET-NEG], [NOV-NEG,EMP-NEG], [CRT], [MIN]] Experiments on both convolutional and recurrent networks are used for evaluation.[[EXP-NEU,MET-NEU], [null], [SMY], [GEN]] \n\nThe studied problem in this paper seems to be interesting, and with potential application in real settings like mobile phone-based learning.[[PDI-POS], [EMP-POS], [APC], [MAJ]] Furthermore, the paper is easy to read with good organization.[[OAL-NEU], [PNF-NEU], [DIS], [MIN]] \n\nHowever, there exist several major issues which are listed as follows:[[OAL-NEU], [null], [SMY], [MIN]]\n\nFirstly, in federated learning, each client independently computes an update to the current model based on its local data, and then communicates this update to a central server where the client-side updates are aggregated to compute a new global model.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] This learning procedure is heuristic, and there is no theoretical guarantee about the correctness (convergence) of this learning procedure.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] The authors do not provide any analysis about what can be learned from this learning procedure.[[ANA-NEU], [EMP-NEU], [DFT], [MAJ]] \n\nSecondly, both structured update and sketched update methods adopted by this paper are some standard techniques which have been widely used in existing works.[[MET-NEU], [EMP-NEU], [DFT], [MAJ]] Hence, the novelty of this paper is limited.[[OAL-NEG], [NOV-NEG], [CRT], [MAJ]] \n\nThirdly, experiments on larger datasets, such as ImageNet, will improve the convincingness. \n"[[DAT-NEG,EXP-NEG], [SUB-NEU], [SUG], [MAJ]]