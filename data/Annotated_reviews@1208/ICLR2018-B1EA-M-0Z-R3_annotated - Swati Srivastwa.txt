"This paper presents a new covariance function for Gaussian processes (GPs) that is equivalent to a Bayesian deep neural network with a Gaussian prior on the weights and an infinite width.[[INT-NEU,MET-NEU], [NOV-NEU], [SMY], [GEN]] As a result, exact Bayesian inference with a deep neural network can be solved with the standard GP machinery.[[MET-NEU], [null], [DIS], [GEN]]\n\n\nPros:\n\nThe result highlights an interesting relationship between deep nets and Gaussian processes.[[RES-POS], [EMP-POS], [APC], [MAJ]] (Although I am unsure about how much of the kernel design had already appeared outside of the GP literature.)[[RES-NEU], [null], [DIS], [GEN]]\n\nThe paper is clear and very well written.[[OAL-POS], [CLA-POS], [APC], [MAJ]]\n\nThe analysis of the phases in the hyperparameter space is interesting and insightful.[[ANA-POS], [EMP-POS], [APC], [MAJ]] On the other hand, one of the great assets of GPs is the powerful way to tune their hyperparameters via maximisation of the marginal likelihood but the authors have left this for future work![[FWK-NEU], [IMP-NEU], [DIS], [MIN]]\n\n\nCons:\n\nAlthough the computational complexity of computing the covariance matrix is given, no actual computational times are reported in the article.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nI suggest using the same axis limits for all subplots in Figure 3."[[TNF-NEU], [null], [SUG], [MIN]]