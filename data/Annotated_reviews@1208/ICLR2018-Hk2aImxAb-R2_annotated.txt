"This paper presents a method for image classification given test-time computational budgeting constraints.[[INT-NEU], [null], [SMY], [GEN]]  Two problems are considered:  \"any-time\" classification, in which there is a time constraint to evaluate a single example, and batched budgets, in which there is a fixed budget available to classify a large batch of images.[[RWK-NEU,PDI-NEU], [EMP-NEU], [SMY], [GEN]]  A convolutional neural network structure with a diagonal propagation layout over depth and scale is used, so that each activation map is constructed using dense connections from both same and finer scale features.[[RWK-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]]  In this way, coarse-scale maps are constructed quickly, then continuously updated with feed-forward propagation from lower layers and finer scales, so they can be used for image classification at any intermediate stage.[[RWK-NEU,PDI-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SUG], [GEN]]  Evaluations are performed on ImageNet and CIFAR-100.[[ANA-NEU], [null], [SMY], [GEN]]\n\nI would have liked to see the MC baselines also evaluated on ImageNet --- I'm not sure why they aren't there as well?[[RWK-NEU,PDI-NEG], [EMP-NEG], [CRT], [MIN]] Also on p.6 I'm not entirely clear on how the \"network reduction\" is performed ---[[RWK-NEG], [CLA-NEG], [CRT], [MIN]] it looks like finer scales are progressively dropped in successive blocks,[[EXT-NEU], [null], [CNT], [CNT]] but I don't think they exactly correspond to those that would be needed to evaluate the full model (this is \"lazy evaluation\").[[PDI-NEG,EXP-NEG,MET-NEU,ANA-NEG], [EMP-NEG], [CRT], [MIN]]  A picture would help here, showing where the depth-layers are divided between blocks.\[[RWK-NEG], [EMP-NEG], [DFT], [MIN]]n\nI was also initially a bit unclear on how the procedure described for batched budgeted evaluation achieves the desired result: [[PDI-NEG,RES-NEU,ANA-NEU], [CLA-NEG,EMP-NEG], [CRT], [MIN]] It seems this relies on having a batch that is both large and varied, so that its evaluation time will converge towards the expectation.[[RWK-NEU,MET-NEU,RES-NEU,ANA-NEU], [EMP-NEU], [SMY], [GEN]]  So this isn't really a hard constraint (just an expected result for batches that are large and varied enough).[[RWK-NEG], [null], [DFT], [MIN]]  This is fine, but could perhaps be pointed out if that is indeed the case.[[RWK-NEU], [null], [SMY], [GEN]]\n\nOverall, this seems like a natural and effective approach, and achieves good results.[[RES-POS,OAL-NEU], [IMP-POS], [APC], [MAJ]]