"In this paper, the authors propose  a new network architecture, CrescendoNet, which is a simple stack of building blocks without residual connections.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] To reduce the memory required for training, the authors also propose a path-wise training procedure based on the independent convolution paths of CrescendoNet.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] The experimental results on CIFAR-10, CIFAR-100 and SVHN show that CrescendoNet outperforms most of the networks without residual connections.[[DAT-POS,EXP-POS,RES-POS], [CMP-POS,EMP-POS], [APC], [MAJ]]\n \nContributions:\n\n1 The authors proposed Crescendo block that consists of convolution paths with increasing depth.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] \n\n3 The authors conducted experiments on three benchmark datasets and show promising performance of CrescendoNet .[[DAT-POS,EXP-POS,RES-POS], [EMP-POS], [APC], [MAJ]]\n\n3 The authors proposed a path-wise training procedure to reduce memory requirement in training.[[PDI-NEU,EXP-NEU], [null], [SMY], [GEN]]\n\nNegative points:\n\n1 The motivation of the paper is not clear.[[PDI-NEG], [CNT], [SMY], [MAJ]] It is well known that the residual connections are important in training deep CNNs and have shown remarkable performance on many tasks.[[EXT-NEU], [null], [DIS], [GEN]] The authors propose the CrescendoNet which is without residual connections.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] However, the experiments 2show that CrescendoNet is worse than ResNet.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] \n\n2  The contribution of this paper is not clear.[[OAL-NEG], [CNT], [CRT], [MAJ]] In fact, the performance of CrescendoNet is worse than most of the variants of residual networks, e.g., Wide ResNet, DenseNet, and ResNet with pre-activation.[[MET-NEG], [CMP-NEG], [CRT], [MAJ]] Besides, it seems that the proposed path-wise training procedure also leads to significant performance degradation.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]]\n\n3 The novelty of this paper is insufficient.[[OAL-NEG], [NOV-NEG], [DFT], [MAJ]] The CrescendoNet is like a variant of the FractalNet, and the only difference is that the number of convolutional layers in Crescendo blocks grows linearly.[[MET-NEU], [null], [DIS], [MIN]]\n\n4 The experimental settings are unfair.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]] The authors run 700 epochs and even 1400 epochs with path-wise training on CIFAR, while the baselines only have 160~400 epochs for training.[[DAT-NEU,EXP-NEU], [CMP-NEU], [DIS], [MIN]]\n\n5 The authors should provide the experimental results on large-scale data sets (e.g. ImageNet) to prove the effectiveness of the proposed method, as they only conduct experiments on small data sets, including CIFAR-10, CIFAR-100, and SVHN.[[DAT-NEG,EXP-NEG,MET-NEU,RES-NEU], [SUB-NEG], [DFT,CRT], [MAJ]]\n\n\n6 The model size of CrescendoNet is larger than residual networks with similar performance.[[MET-NEU], [CMP-NEU], [DIS], [MIN]]\n\n\nMinor issues:\n\n1In line 2, section 3.4, the period after \u201c(128, 256, 512)\u201d should be removed.\n"[[CNT], [CLA-NEU], [SUG], [MIN]]