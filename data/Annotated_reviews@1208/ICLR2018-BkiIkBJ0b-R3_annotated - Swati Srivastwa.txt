"This paper proposes to re-evaluate some of the methods presented in a previous paper with a somewhat more general evaluation method.[[RWK-NEU,PDI-NEG], [null], [SMY], [GEN]] \n\nThe previous paper (Mirowski et al. 2016) introduced a deep RL agent with auxiliary losses that facilitates learning in navigation environments, where the tasks were to go from a location to another in a first person viewed fixed 3d maze, with the starting and goal locations being either fixed or random.[[RWK-NEU], [null], [SMY], [GEN]] This proposed paper rejects some of the claims that were made in Mirowski et al. 2016, mainly the capacity of the deep RL agent to learn to navigate in such environments.[[RWK-NEU], [CMP-NEU], [DIS], [MIN]] \n\nThe proposed refutation is based on the following experiments:[[EXP-NEU], [null], [DIS], [GEN]]\n- an agent trained on random maps does much worse on fixed random maps that an agent trained on the same maps its being evaluated on (figure 4)[[EXP-NEU,TNF-NEU], [null], [DIS], [GEN]]\n- when an agent is trained on fixed number of random map, its performance on random unseen maps doesn't increase with the number of training maps beyond ~100 maps. (figure 5).[[EXP-NEU,TNF-NEU], [null], [DIS], [GEN]] The authors argue that the reason for those diminishing returns is that the agent is actually learning a trivial wall following strategy that doesn't benefit from more maps.[[EXP-NEU], [null], [DIS], [GEN]]\n- when evaluated on hand designed small maps, the agent doesn't perform very well (figure 6).[[EXP-NEU], [null], [DIS], [GEN]]\n\nThere is addition experimental data reported which I didn't find very conclusive nor relevant to the analysis, particularly the attention heat map and the effect of apples and texture.[[DAT-NEG,EXP-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nI don't think any of the experiments reported actually refute any of the original paper's claim.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]] All of the reported results are what you would expect.[[RES-NEU], [EMP-NEU], [DIS], [MIN]] It boils down to these simple commonly known facts about deep RL agents:\n- When evaluated outside of its training distribution, it might not generalized very well (figure 4/6)[[MET-NEG,TNF-NEG], [EMP-NEG], [CRT], [MAJ]]\n- It has a limited capacity so if the distribution of environments is too large, its performance will plateau (figure 5).[[TNF-NEU], [EMP-NEU], [DIS], [MIN]] By the way to me results presented in figure 5 are not enough to claim that the agent trained on random map is implementing a purely reactive wall-following strategy.[[RES-NEG,TNF-NEG], [EMP-NEG], [CRT], [MAJ]] In fact, an interesting experiment here would have been to do ablation studies e.g. by replacing the LSTM with a feed forward fully connected network.[[EXP-NEU], [EMP-NEU], [SUG], [MIN]] To me the reported performance plateau with number of map size is normal expected behavior, only symptomatic that this deep RL agent has finite capacity.[[RES-NEU], [null], [DIS], [MIN]]\n\nI think this paper does not provide compelling pieces of evidence of unexpected pathological behavior in the previous paper, and also does not provide any insight of how to improve upon and address the obvious limitations of previous work.[[RWK-NEG,OAL-NEG], [SUB-NEG,EMP-NEG], [CRT], [MAJ]] I therefore recommend not to accept this paper in its current form."[[OAL-NEG], [REC-NEG], [CRT], [MAJ]]