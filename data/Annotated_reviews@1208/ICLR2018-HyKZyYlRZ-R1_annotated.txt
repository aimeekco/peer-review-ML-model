"The paper presents a multi-task, multi-domain model based on deep neural networks.[[INT-NEU], [null], [SMY], [GEN]] The proposed model is able to take inputs from various domains (image, text, speech) and solves multiple tasks, such as image captioning, machine translation or speech recognition.[[PDI-NEU], [null], [SMY], [GEN]] The proposed model is composed of several features learning blocks (one for each input type) and of an encoder and an auto-regressive decoder, which are domain-agnostic.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The model is evaluated on 8 different tasks and is compared with a model trained separately on each task, showing improvements on each task.[[MET-NEU,RES-POS], [EMP-POS], [APC], [MAJ]]\n\nThe paper is well written and easy to follow.[[OAL-POS], [CLA-POS,PNF-POS], [APC], [MAJ]]\n\nThe contributions of the paper are novel and significant.[[OAL-POS], [NOV-POS,IMP-POS], [APC], [MAJ]] The approach of having one model able to perform well on completely different tasks and type of input is very interesting and inspiring.[[MET-POS], [EMP-POS], [APC], [MAJ]] The experiments clearly show the viability of the approach and give interesting insights.[[EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]] This is surely an important step towards more general deep learning models.[[RES-POS], [EMP-POS], [APC], [MAJ]] \n\nComments:\n\n* In the introduction where the 8 databases are presented, the tasks should also be explained clearly, as several domains are involved and the reader might not be familiar with the task linked to each database.[[DAT-NEU,MET-NEU], [EMP-NEU], [SUG], [MIN]] Moreover, some databases could be used for different tasks, such as WSJ or ImageNet.[[DAT-NEU], [CNT], [SUG,DIS], [MIN]]\n\n* The training procedure of the model is not explained in the paper.[[MET-NEU], [SUB-NEU], [DFT], [MIN]] What is the cost function and what is the strategy to train on multiple tasks ?[[EXP-NEU,ANA-NEU], [EMP-NEU], [QSN], [MIN]] The paper should at least outline the strategy.[[MET-NEU], [SUB-NEU], [SUG,DFT], [MIN]]\n\n* The experiments are sufficient to demonstrate the viability of the approach,[[EXP-POS], [SUB-POS], [APC], [MAJ]] but the experimental setup is not clear.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]] Specifically, there is an issue about the speech recognition part of the experiment.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]] It is not clear what the task exactly is: continuous speech recognition, isolated word recognition ?[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT,QSN], [MAJ]] The metrics used in Table 1 are also not clear, they should be explained in the text.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]] Also, if the task is continuous speech recognition, the WER (word error rate) metric should be used.[[MET-NEU], [EMP-NEU], [SUG], [MAJ]] Information about the detailed setup is also lacking, specifically which test and development sets are used (the WSJ corpus has several sets).[[DAT-NEG,EXP-NEG,MET-NEG], [SUB-NEG], [DFT,CRT], [MAJ]]\n\n* Using raw waveforms as audio modality is very interesting,[[EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]] but this approach is not standard for speech recognition,[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] some references should be provided, such as:\nP. Golik, Z. Tuske, R. Schluter, H. Ney, Convolutional Neural Networks for Acoustic Modeling of Raw Time Signal in LVCSR, in: Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015, pp. 26\u201330.[[BIB-NEU], [SUB-NEU], [SUG], [MIN]]\nD. Palaz, M. Magimai Doss and R. Collobert, (2015, April). Convolutional neural networks-based continuous speech recognition using raw speech signal.[[BIB-NEU], [SUB-NEU], [SUG], [MIN]] In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on (pp. 4295-4299). IEEE.[[BIB-NEU], [SUB-NEU], [SUG], [MIN]]\nT. N. Sainath, R. J. Weiss, A. Senior, K. W. Wilson, and O. Vinyals.[[BIB-NEU], [SUB-NEU], [SUG], [MIN]] Learning the Speech Front-end With Raw Waveform CLDNNs.[[BIB-NEU], [SUB-NEU], [SUG], [MIN]] Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015.[[BIB-NEU], [SUB-NEU], [SUG], [MIN]]\n\nRevised Review:\nThe main idea of the paper is very interesting and the work presented is impressive.[[PDI-POS], [EMP-POS], [APC], [MAJ]] However, I tend to agree with Reviewer2, as a more comprehensive analysis should be presented to show that the network is not simply multiplexing tasks.[[ANA-NEU], [SUB-NEU], [SUG], [MIN]] The experiments are interesting,[[EXP-POS], [EMP-POS], [APC], [MAJ]] except for the WSJ speech task, which is almost meaningless.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]] Indeed, it is not clear what the network has learned given the metrics presented, as the WER on WSJ should be around 5% for speech recognition.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]]\nI thus suggest to either drop the speech experiment, or the modify the network to do continuous speech recognition.[[EXP-NEU], [EMP-NEU], [SUG], [MIN]] A simpler speech task such as Keyword Spotting could also be investigated.\n"[[MET-NEU], [EMP-NEU], [SUG], [MIN]]