"\n1) Summary\nThis paper proposes a flow-based neural network architecture and adversarial training for multi-step video prediction.[[PDI-NEU], [null], [SMY], [GEN]] The neural network in charge of predicting the next frame in a video implicitly generates flow that is used to transform the previously observed frame into the next.[[PDI-NEU], [null], [SMY], [GEN]] Additionally, this paper proposes a new quantitative evaluation criteria based on the observed flow in the prediction in comparison to the groundtruth.[[MET-NEU], [null], [SMY], [GEN]] Experiments are performed on a new robot arm dataset proposed in the paper where they outperform the used baselines.[[RWK-POS,EXP-POS], [EMP-POS], [APC], [MAJ]]\n\n\n2) Pros:\n+ New quantitative evaluation criteria based on motion accuracy.[[EXP-POS,MET-POS], [NOV-POS], [APC], [MAJ]]\n+ New dataset for robot arm pushing objects.[[DAT-POS], [NOV-POS], [APC], [MAJ]]\n\n3) Cons:\nOverall architectural prediction network differences with baseline are unclear:\nThe differences between the proposed prediction network and [1] seem very minimal.[[RWK-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] In Figure 3, it is mentioned that the network uses a U-Net with recurrent connections.[[TNF-NEU], [null], [SMY], [GEN]] This seems like a very minimal change in the overall architecture proposed.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Additionally, there is a paragraph of \u201carchitecture improvements\u201d which also are minimal changes.[[CNT], [EMP-NEG], [CRT], [MAJ]] Based on the title of section 3, it seems that there is a novelty on the \u201cprediction with flow\u201d part of this method.[[MET-POS], [NOV-POS], [APC], [MAJ]] If this is a fact, there is no equation describing how this flow is computed.[[EXP-NEG], [SUB-NEG], [DFT], [MAJ]] However, if this \u201cflow\u201d is computed the same way [1]  does it, then the title is misleading.[[CNT], [null], [CRT], [MIN]]\n\n\nAdversarial training objective alone is not new as claimed by the authors:\nThe adversarial objective used in this paper is not new.[[MET-NEG], [NOV-NEG], [CRT], [MAJ]] Works such as [2,3] have used this objective function for single step and multi-step frame prediction training, respectively.[[EXP-NEU], [null], [DIS], [GEN]] If the authors refer to the objective being new in the sense of using it with an action conditioned video prediction network, then this is again an extremely minimal contribution.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] Essentially, the authors just took the previously used objective function and used it with a different network.[[MET-NEG], [NOV-NEG], [CRT], [MAJ]] If the authors feel otherwise, please comment on why this is the case.[[EXT-NEU], [null], [DIS], [GEN]]\n\n\nIncomplete experiments:\nThe authors only show experiments on videos containing objects that have already been seen, but no experiments with objects never seen before.[[EXP-NEG], [SUB-NEG,EMP-NEG], [DFT,CRT], [MAJ]] The missing experiment concerns me in the sense that the network could just be memorizing previously seen objects.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]] Additionally, the authors present evaluation based on PSNR and SSIM on the overall predicted video, but not in a per-step paradigm.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] However, the authors show this per-step evaluation in the Amazon Mechanical Turk, and predicted object position evaluations.[[MET-NEU], [null], [DIS], [GEN]]\n\n\nUnclear evaluation:\nThe way the Amazon Mechanical Turk experiments are performed are unclear and/or not suited for the task at hand.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]]\nBased on the explanation of how these experiments are performed, the authors show individual images to mechanical turkers.[[EXP-NEU], [EMP-NEU], [DIS], [GEN]] If we are evaluating the video prediction task for having real or fake looking videos, the turkers need to observe the full video and judge based on that.[[EXP-NEU,MET-NEU], [null], [DIS], [GEN]] If we are just showing images, then they are evaluating image synthesis, which do not necessarily contain the desired properties in videos such as temporal coherence.[[EXP-NEU,MET-NEU], [null], [DIS], [GEN]]\n\n\nAdditional comments:\nThe paper needs a considerable amount of polishing.[[OAL-NEU], [PNF-NEU], [SUG], [MIN]]\n\n\n4) Conclusion:\nThis paper seems to contain very minimal changes in comparison to the baseline by [1].[[RWK-NEG], [CMP-NEG], [CRT], [MIN]] The adversarial objective is not novel as mentioned by the authors and has been used in [2,3].[[MET-NEG], [NOV-NEG], [CRT], [MAJ]] Evaluation is unclear and incomplete.[[EXP-NEG,MET-NEG], [SUB-NEG], [DIS], [GEN]]\n\n\nReferences:\n[1] Chelsea Finn, Ian Goodfellow, and Sergey Levine.[[BIB-NEU], [null], [DIS], [GEN]] Unsupervised learning for physical interaction through video prediction. In NIPS, 2016.[[BIB-NEU], [null], [DIS], [GEN]]\n[2] M. Mathieu, C. Couprie, and Y. LeCun. Deep multi-scale video prediction beyond mean square error. In ICLR, 2016.[[BIB-NEU], [null], [DIS], [GEN]]\n[3] Ruben Villegas, Jimei Yang, Seunghoon Hong, Xunyu Lin, Honglak Lee. Decomposing Motion and Content for Natural Video Sequence Prediction. In ICLR, 2017\n"[[BIB-NEU], [null], [DIS], [GEN]]