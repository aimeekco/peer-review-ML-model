"The paper presents a new CNN architecture: CrescendoNet.[[PDI-NEU,MET-POS], [NOV-POS], [APC], [MAJ]] It does not have skip connections yet performs quite well.[[MET-NEU], [null], [SMY], [GEN]]\n\nOverall, I think the contributions of this paper are too marginal for acceptance in a top tier conference.[[OAL-NEU], [REC-NEU], [FBK], [MAJ]]\n\nThe architecture is competitive on SVHN and CIFAR 10 but not on CIFAR 100.[[DAT-NEU,MET-NEG], [EMP-NEG], [CRT], [MAJ]] The performance is not strong enough to warrant acceptance by itself.[[RES-NEG,OAL-NEG], [EMP-NEG,REC-NEG], [CRT,FBK], [MAJ]]\n\nFractalNets amd DiracNets (https://arxiv.org/pdf/1706.00388.pdf) have demonstrated that it is possible to train deep networks without skip connections and achieve high performance.[[EXT-NEU], [null], [DIS], [GEN]] While CrescendoNet seems to slightly outperform FractalNet in the experiments conducted, it is itself outperformed by DiracNet.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MAJ]] Hence, CrescendoNet does not have the best performance among skip connection free networks.[[MET-NEG], [CMP-NEG], [CRT], [MAJ]]\n\nYou claim that FractalNet shows no ensemble behavior.[[RWK-NEU,MET-NEU], [null], [DIS], [GEN]] This is clearly not true because FractalNet has ensembling directly built in, i.e. different paths in the network are explicitly averaged.[[RWK-NEG,MET-NEG], [CNT], [CRT], [MAJ]] If averaging paths leads to ensembling in CrescendoNet, it leads to ensembling in FractalNet.[[RWK-NEU], [CMP-NEU], [DIS], [MIN]]  While the longest path in FractalNet is stronger than the other members of the ensemble, it is nevertheless an ensemble.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MIN]]  Besides, as Veit showed, ResNet also shows ensemble behavior.[[DAT-NEU], [CMP-NEU], [DIS], [MIN]]  Hence, using ensembling in deep networks is not a significant contribution.[[MET-NEG], [EMP-NEG,IMP-NEG], [CRT], [MAJ]] \n\nThe authors claim that \"Through our analysis and experiments, we note that the implicit ensemble behavior of CrescendoNet leads to high performance\".[[EXP-NEU,MET-NEU,ANA-NEU], [null], [DIS], [GEN]]  I don't think the experiments show that ensemble behavior leads to high performance. [[EXP-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] Just because a network performs averaging of different paths and individual paths perform worse than sets of paths doesn't imply that ensembling as a mechanism is in fact the cause of the performance of the entire architecture.[[MET-NEU,RES-NEG], [EMP-NEG], [CRT], [MAJ]]  Similary, you say \"On the other hand, the ensemble model can explain the performance improvement easily.[[MET-NEU], [null], [DIS], [GEN]] \" Veit et al only claimed that ensembling is a feature of ResNet, but they did not claim that this was the cause of the performance of ResNet.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] \n\nPath-wise training is not original enough or indeed different enough from drop-path to count as a major contribution.\" Veit et al only claimed that ensembling is a feature of ResNet, but they did not claim that this was the cause of the performance of ResNet.[[RWK-NEU], [null], [DIS], [GEN]]\n\nYou claim that the number of layers \"increase exponentially\" in FractalNet. This is misleading.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] The number of layers increases exponentially in the number of paths, but not in the depth of the network.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] In fact, the number of layers is linear in the depth of the network.[[MET-NEU], [null], [DIS], [MIN]] Since depth is the meaningful quantity here, CrescendoNet does not have an advantage over FractalNet in terms of layer number.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Also, it is always possible to simply add more paths to FractalNet if desired without increasing depth.[[MET-NEU], [null], [DIS], [GEN]] Instead of using 1 long paths, one can simply use 2, 3, 4 etc.[[MET-NEU], [null], [DIS], [GEN]] While this is not explicitly mentioned in the FractalNet paper, it clearly would not break the design principle of FractalNet which is to train a path of multiple layers by ensembling it with a path of fewer layers. [[EXT-NEU], [null], [DIS], [GEN]]CrescendoNets do not extend beyond this design principle.[[RWK-NEG,MET-NEG], [SUB-NEG], [DFT], [MIN]]\n\nYou say that \"First, path-wise training procedure significantly reduces the memory requirements for convolutional layers, which constitutes the major memory cost for training CNNs.[[EXP-NEU], [null], [DIS], [GEN]] For example, the higher bound of the memory required can be reduced to about 40% for a Crescendo block with 4 paths where interval = 1.\"[[EXP-NEU], [EMP-NEU], [DIS], [MIN]] This is misleading, as you need to store the weights of all convolutional layers to compute the forward pass and the majority of the weights of all convolutional layers to compute the backward pass, no matter how many weights you intend to update.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]] In a response to a question I posed, you mentioned that we you meant was \"we use about 40% memory for the gradient computation and storage\".[[EXP-NEU], [null], [DIS], [MIN]] Fair enough, but \"gradient computation and storage\" is not mentioned in the paper.[[EXP-NEG], [SUB-NEG], [DFT], [MAJ]] Also, the reduction to 40% does not apply e.g. to vanilla SGD because the computed gradient can be immediately added to the weights and does not need to be stored or combined with e.g. a stored momentum term.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nFinally, nowhere in the paper do you mention which nonlinearities you used or if you used any at all.[[MET-NEU,ANA-NEG], [SUB-NEG], [DFT], [MIN]] In future revisions, this should be rectified.[[MET-NEU], [CNT], [SUG], [MIN]]\n\nWhile I can definitely imagine that your network architecture is well-designed and a good choice for image classification tasks,[[MET-NEU], [null], [DIS], [MAJ]] there is a very saturated market of papers proposing various architectures for CIFAR-10 and related datasets.[[DAT-NEU,MET-NEU], [null], [DIS], [MIN]] To be accepted to ICLR, either outstanding performance or truly novel design principles are required."[[OAL-NEU], [REC-NEU], [SUG], [MAJ]]