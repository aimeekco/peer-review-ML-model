"This paper describes an implementation of reduced precision deep learning using a 16 bit integer representation.[[INT-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] This field has recently seen a lot of publications proposing various methods to reduce the precision of weights and activations.[[RWK-NEU,PDI-NEU,EXP-NEU,ANA-NEU], [null], [SMY], [GEN]] These schemes have generally achieved close-to-SOTA accuracy for small networks on datasets such as MNIST and CIFAR-10.[[PDI-NEU,DAT-NEU,RES-NEU], [EMP-NEU], [SMY], [GEN]] However, for larger networks (ResNET, Vgg, etc) on large dataset such as ImageNET, a significant accuracy drop are reported.[[PDI-NEU,DAT-NEU,RES-NEG], [IMP-NEG], [DFT], [MAJ]] In this work, the authors show that a careful implementation of mixed-precision dynamic fixed point computation can achieve SOTA on 4 large networks on the ImageNET-1K datasets.[[RWK-NEU,DAT-NEU,EXP-NEU,MET-NEU], [null], [SMY], [GEN]] Using a INT16 (as opposed to FP16) has the advantage of enabling the use of new SIMD mul-acc instructions such as QVNNI16.[[RWK-NEU], [EMP-NEU], [SMY], [GEN]] \n\nThe reported accuracy numbers show convincingly that INT16 weights and activations can be used without loss of accuracy in large CNNs.[[PDI-NEU,EXP-NEU,MET-NEU,RES-NEU,ANA-NEU], [IMP-NEU,EMP-NEU], [SMY], [GEN]] However, I was hoping to see a direct comparison between FP16 and INT16. [[RWK-NEG,EXP-NEG], [CMP-NEG], [DFT], [MIN]] \n\nThe paper is written clearly and the English is fine."[[OAL-POS], [CLA-POS], [APC], [MAJ]]