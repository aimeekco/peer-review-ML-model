"The authors discuss the regularized objective function minimized by standard SGD in the context of neural nets, and provide a variational inference perspective using the Fokker-Planck equation.[[INT-NEU,RWK-NEU,PDI-NEU], [null], [SMY], [GEN]] They note that the objective can be very different from the desired loss function if the SGD noise matrix is low rank, as evidenced in their experiments.[[EXP-NEU,MET-NEU], [null], [SMY], [GEN]]\n\nOverall the paper is written quite well, and the authors do a good job of explaining their thesis.[[OAL-POS], [CLA-POS,PNF-POS], [APC], [MAJ]] However I was unable to identify any real novelty in the theory: the Fokker-Planck equation has been widely used in analysis of stochastic noise in MCMC samplers in recent years, and this paper mostly rephrases those results.[[RWK-NEG,MET-NEG,OAL-NEG], [NOV-NEG,CMP-NEG], [CRT], [MAJ]] Also the fact that SGD theory only works for isotropic noise is well known, and that there is divergence from the true loss function in case of low rank noise is obvious.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Thus I found most of section 3 to be a reformulation of known results, including Theorem 5 and its proof.\n\nSame goes for section 5; the symmetric- anti symmetric split is a common technique used in the stochastic MCMC literature over the last few years, and I did not find any new insight into those manipulations of the Fokker-Planck equation from this paper.[[RWK-NEG,MET-NEG,OAL-NEG], [NOV-NEG,CMP-NEG], [CRT], [MAJ]]\n\nThus I think that although this paper is written well,[[OAL-POS], [CLA-POS], [APC], [MAJ]] the theory is mostly recycled and the empirical results in Section 4 are known; thus it is below acceptance threshold due to lack of novelty."[[MET-NEG,RES-NEG], [NOV-NEG,REC-NEG], [CRT,FBK], [MAJ]]