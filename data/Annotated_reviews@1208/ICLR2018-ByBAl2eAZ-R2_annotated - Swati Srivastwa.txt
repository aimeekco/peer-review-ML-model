"In recent years there have been many notable successes in deep reinforcement learning.[[EXT-NEU], [null], [DIS], [GEN]] However, in many tasks, particularly sparse reward tasks, exploration remains a difficult problem.[[EXT-NEU], [null], [DIS], [GEN]] For off-policy algorithms it is common to explore by adding noise to the policy action in action space, while on-policy algorithms are often regularized in the action space to encourage exploration.[[EXT-NEU], [null], [DIS], [GEN]] This work introduces a simple, computationally straightforward approach to exploring by perturbing the parameters (similar to exploration in some evolutionary algorithms) of policies parametrized with deep neural nets.[[INT-POS,MET-POS], [EMP-POS], [APC], [MAJ]] This work argues this results in more consistent exploration and compares this approach empirically on a range of continuous and discrete tasks.[[MET-NEU,RES-NEU], [null], [SMY], [GEN]] By using layer norm and adaptive noise, they are able to generate robust parameter noise (it is often difficult to estimate the appropriate variance of parameter noise, as its less clear how this relates to the magnitude of variance in the action space).[[MET-NEU,RES-NEU], [EMP-NEU], [DIS], [MAJ]]\n\nThis work is well-written and cites previous work appropriately.[[RWK-POS,OAL-POS], [CLA-POS], [APC], [MAJ]] Exploration is an important topic, as it often appears to be the limiting factor of Deep RL algorithms.[[MET-POS], [CNT], [APC], [MAJ]] The authors provide a significant set of experiments using their method on several different RL algorithms in both continuous and discrete cases, and find it generally improves performance, particularly for sparse rewards.[[EXP-POS,MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]]\n\nOne empirical baseline that would helpful to have would be a stochastic off-policy algorithm (both off-policy algorithms compared are deterministic), as this may better capture uncertainty about the value of actions (e.g. SVG(0) [3]).[[RWK-NEU,MET-NEU], [null], [SUG], [MAJ]]\n\nAs with any empirical results with RL, it is a challenging problem to construct comparable benchmarks due to minor variations in implementation, environment or hyper-parameters all acting as confounding variables [1].[[MET-NEU,RES-NEU], [EMP-NEU], [DIS], [MIN]] It would be helpful if the authors are able to make their paper reproducible by releasing the code on publication.[[EXP-NEU], [EMP-NEU], [DIS], [GEN]] As one example, figure 4 of [1] seems to show DDPG performing much better than the DDPG baseline in this work on half-cheetah.[[MET-POS,TNF-POS], [CMP-POS], [APC], [MAJ]]\n\nMinor points:\n- The definition of a stochastic policy (section 2) is unusual (it is defined as an unnormalized distribution).[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Usually it would be defined as $\\mathcal{S} \\rightarrow \\mathcal{P}(\\mathcal{A})$[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\n- This work extends DQN to learn an explicitly parametrized policy (instead of the greedy policy) in order to useful perturb the parameters of this policy.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Instead of using a single greedy target, you could consider use the relationship between the advantage function and an entropy-regularized policy [2] to construct a target.[[MET-NEU], [EMP-NEU], [SUG], [MIN]]\n\n[1] Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., & Meger, D. (2017).[[BIB-NEU], [null], [DIS], [GEN]] Deep reinforcement learning that matters.[[BIB-NEU], [null], [DIS], [GEN]]  arXiv preprint arXiv:1709.06560.\n\n[2] O'Donoghue, B., Munos, R., Kavukcuoglu, K., & Mnih, V. (2016).[[BIB-NEU], [null], [DIS], [GEN]]  Combining policy gradient and Q-learning.[[BIB-NEU], [null], [DIS], [GEN]] \n\n[3] Heess, N., Wayne, G., Silver, D., Lillicrap, T., Erez, T., & Tassa, Y. (2015).[[BIB-NEU], [null], [DIS], [GEN]]  Learning continuous control policies by stochastic value gradients. In Advances in Neural Information Processing Systems (pp. 2944-2952)."[[BIB-NEU], [null], [DIS], [GEN]] 