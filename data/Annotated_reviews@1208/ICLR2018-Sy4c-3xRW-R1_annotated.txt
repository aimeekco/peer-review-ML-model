"This paper propose an adaptive dropout strategy for class logits.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] They learn a distribution q(z | x, y) that randomly throw class logits.[[MET-NEU], [null], [SMY], [GEN]][[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] By doing so they ensemble predictions of the models between different set of classes, and focuses on more difficult discrimination tasks.[[MET-NEU], [null], [SMY], [GEN]] They learn the dropout distribution by variational inference with concrete relaxation.[[MET-NEU], [null], [SMY], [GEN]] \n\nOverall I think this is a good paper.[[OAL-POS], [CNT], [APC], [MAJ]] The technique sounds, the presentation is clear and I have not seen similar paper elsewhere[[MET-POS,OAL-POS], [PNF-POS,EMP-POS], [APC], [MAJ]] (not 100% sure about the originality of the work though).[[OAL-NEG], [NOV-NEG], [DIS], [MIN]] \n\nPro:\n* General algorithm\n\nCon:\n* The experiment is a little weak.[[EXP-NEG], [EMP-NEG], [CRT], [MIN]] Only on CIFAR100 the proposed approach is much better than other approaches.[[RWK-POS,DAT-POS,MET-POS], [CMP-POS,EMP-POS], [APC], [MAJ]] I would like to see the results on more datasets.[[DAT-NEU,RES-NEU], [SUB-NEU], [SUG,DIS], [MIN]] Maybe should also compare with more dropout algorithms, such as DropConnect and MaxOut."[[RWK-NEU,MET-NEU], [CMP-NEU], [SUG,DIS], [MIN]]