"This paper introduces a method for learning representations for prepositions.[[INT-NEU], [null], [SMY], [GEN]] They first take co-occurrence counts counts of pairs of words in a local window of each preposition, and then factorize the matrix to find low dimensional word representations.[[PDI-NEU], [null], [SMY], [GEN]] The main difference from previous work is restricting the context to be close to a preposition.[[RWK-POS], [CMP-POS], [APC], [MAJ]] The authors report improved paraphrasing of phrasal verbs, and state-of-the-art accuracy in correcting grammatical errors involving prepositions, and good results on prepositional phrase attachment.[[RWK-POS,MET-POS,RES-POS], [CMP-POS], [APC], [MAJ]] \n\n- The paper frequently overclaims.[[OAL-NEG], [SUB-NEG], [CRT], [MAJ]] For one example, we\u2019re told that \u201cPreposition selection [is] a major area of study in both syntactic and semantic computational linguistics\u201d, but at best it\u2019s quite a specialized niche.[[PDI-NEG], [SUB-NEG], [APC], [MAJ]] The paper would be much improved if it was generally toned down.[[OAL-NEG], [SUB-NEG], [CRT], [MAJ]]\n- The authors claim their method is \u201cvastly\u201d better at paraphrasing phrasal verbs than baselines, based on qualitative comparison.[[RWK-NEU,MET-NEU], [CMP-NEU], [APC], [MIN]] However, I couldn\u2019t find any details on how the phrasal verbs were chosen, or what (if any) held out data was used for tuning the models.[[DAT-NEU,MET-NEU], [SUB-NEU], [DIS], [MIN]] Even assuming this is a meaningful task, surely the natural baseline would be to treat these phrasal verbs as non-compositional (e.g. extend the vocab with words like \u201csparked_off\u201d)  and train Word2Vec.[[RWK-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]]\n- The other experiments are lacking important details.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]] For example, we\u2019re just told some values that hyperparameters were fixed at for both tasks - how were these chosen (including for the baselines)?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Was the model architecture tuned based on the proposed representations? [[MET-NEU], [EMP-NEU], [QSN], [MIN]]Were the word representations fixed, or fine tuned during training?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]] \n- Despite the author\u2019s expectations that their representations will be \u2018widely used\u2019, I am struggling to think of cases where they would be useful, outside of the very specific tasks involving prepositions that they use.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] That is because almost all tasks require good representations for all words, not just prepositions.[[MET-NEU], [EMP-NEU], [CRT], [MIN]] The authors should add more justification for the where/how these representations will be useful.[[RES-NEG], [EMP-NEG], [CRT], [MIN]]\n\nOverall, I think the technical contributions of the paper are quite limited, and the experiments are not well enough described for publication.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n"