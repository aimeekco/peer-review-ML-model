"The paper proposes a regularizer that encourages a GAN discriminator to focus its capacity in the region around the manifolds of real and generated data points, even when it would be easy to discriminate between these manifolds using only a fraction of its capacity, so that the discriminator provides a more informative signal to the generator.[[INT-NEU], [null], [SMY], [GEN]] The regularizer rewards high entropy in the signs of discriminator activations.[[MET-NEU], [null], [SMY], [GEN]] Experiments show that this helps to prevent mode collapse on synthetic Gaussian mixture data and improves Inception scores on CIFAR10.[[EXP-POS], [null], [SMY], [GEN]] \n\nThe high-level idea of guiding model capacity by rewarding high-entropy activations  is interesting and novel to my knowledge (though I am not an expert in this space).[[MET-POS], [NOV-POS], [APC], [MAJ]] Figure `1 is a fantastic illustration that presents the core idea very clearly.[[TNF-POS], [PNF-POS], [APC], [MAJ]] That said I found the intuitive story a little bit difficult to follow;[[MET-NEU], [EMP-NEG], [CRT], [MAJ]] -- it's true that in Figure 1b the discriminator won't communicate the detailed structure of the data manifold to the generator, but it's not clear why this would be a problem;[[TNF-NEG], [EMP-NEG], [CRT], [MAJ]] -- the gradients should still pull the generator *towards* the manifold of real data, and as this happens and the manifolds begin to overlap, the discriminator will naturally be forced to allocate its capacity towards finer-grained details.[[DAT-NEU,EXP-NEU], [EMP-NEU], [DIS], [MAJ]] Is the implicit assumption that for real, high-dimensional data the generator and data manifolds will *never* overlap?[[DAT-NEU,MET-NEU], [EMP-NEU], [QSN], [MAJ]] But in that case much of the theoretical story goes out the window.[[MET-NEU], [EMP-NEU], [CRT], [MAJ]] I'd also appreciate further discussion of the relationship of this approach to Wasserstein GANs, which also attempt to provide a clearer training gradient when the data and generator manifolds do not overlap.[[EXP-NEU,MET-NEU], [EMP-POS], [APC,DIS], [MAJ]]\n\nMore generally I'd like to better understand what effect we'd expect this regularizer to have.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] It appears to be motivated by improving training dynamics, which is understandably a significant concern.[[EXP-NEU], [EMP-NEU], [DIS], [MAJ]] Does it also change the location of the Nash equilibria?[[MET-NEU], [EMP-NEU], [QSN], [MAJ]] (or equivalently, the optimal generator under the density-ratio-estimator interpretation of discriminators proposed by https://arxiv.org/abs/1610.03483).[[RWK-NEU,BIB-NEU], [CMP-NEU], [SUG], [MAJ]] I'd expect that it would but the effects of this changed objective are not discussed in the paper.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] \n\n The experimental results seem promising, although not earthshattering.[[EXP-NEU], [null], [SMY], [MAJ]] I would have appreciated a comparison to other methods for guiding discriminator representation capacity, e.g. autoencoding (I'd also imagine that learning an inference network (e.g. BiGAN) might serve as a useful auxiliary task?).[[RWK-NEU,MET-NEU], [CMP-NEU], [SUG], [MAJ]] \n\nOverall this feels like an cute hack, supported by plausible intuition but without deep theory or compelling results on real tasks (yet).[[MET-NEU], [EMP-NEU], [SMY], [GEN]] As such I'd rate it as borderline; though perhaps interesting enough to be worth presenting and discussing.[[OAL-POS], [REC-NEU], [FBK], [MAJ]]\n\nA final note: this paper was difficult to read due to many grammatical errors and unclear or misleading constructions, as well as missing citations (e.g. sec 2.1).[[OAL-NEG], [CLA-NEG], [CRT], [MAJ]] From the second paragraph alone:\n\"impede their wider applications in new data domain\" -> domains\n\"extreme collapse and heavily oscillation\" -> heavy oscillation\n\"modes of real data distribution\" -> modes of the real data distribution\n\"while D fails to exploit the failure to provide better training signal to G\" -> should be \"this failure\" to refer to the previously-described generator mode collapse, or rewrite entirely\n\"even when they are their Jensen-Shannon divergence\" -> even when their Jensen-Shannon divergence\n I'm sympathetic to the authors who are presumably non-native English speakers;[[OAL-NEG], [CLA-NEG], [CRT], [MIN]] many good papers contain mistakes, but in my opinion the level in this paper goes beyond what is appropriate for published work.[[OAL-NEG], [CLA-NEG], [CRT], [MAJ]] I encourage the authors to have the work proofread by a native speaker; clearer writing will ultimately increase the reach and impact of the paper.[[OAL-NEU], [CLA-NEU,IMP-NEU], [SUG], [MAJ]]  \n"