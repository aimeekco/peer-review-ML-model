"\n* In the \"flat vs sharp\" dilemma, the experiments display that the dilemma, if any, is subtle.[[RWK-NEU], [null], [SMY], [GEN]] Table 1 does not necessarily contradict this view. [[RWK-NEG,TNF-NEG], [IMP-NEG], [DFT], [MIN]]It would be a good idea to put the test results directly on Fig. 4 as it does not ease reading currently (and postpone ResNet-56 in the appendix).[[RWK-NEU,RES-NEG,TNF-NEG,BIB-NEU], [CLA-NEG,IMP-NEG], [DFT], [MIN]]\n\nHow was Figure 5 computed ?[[TNF-NEU], [null], [QSN], [GEN]] It is said that *a* random direction was used from each minimiser to plot the loss, so how the 2D directions obtained ?[[RWK-NEU,EXP-NEU,MET-NEU], [null], [QSN], [GEN]]\n\n* On the convexity vs non-convexity (Sec. 6), it is interesting to see how pushing the Id through the net changes the look of the loss for deep nets.[[RWK-NEU,EXP-NEU,MET-NEU], [IMP-NEU,EMP-NEU], [SMY], [GEN]] The difference VGG - ResNets is also interesting, but it would have been interesting to see how this affects the current state of the art in understanding deep learning, something that was done for the \"flat vs sharp\" dilemma, but is lacking here.[[RWK-NEU,EXP-NEU,MET-NEU], [NOV-NEU,EMP-NEU], [SMY], [GEN]] For example, does this observation that the local curvature of the loss around minima is different for ResNets and VGG allows to interpret the difference in their performances ?[[RWK-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SMY,QSN], [GEN]]\n\n* On optimisation paths, the choice of PCA directions is wise compared to random projections, and results are nice as plotted.[[RWK-POS,EXP-POS,MET-POS], [SUB-POS,EMP-POS], [APC], [MAJ]] There is however a phenomenon I would have liked to be discussed, the fact that the leading eigenvector captures so much variability, which perhaps signals that optimisation happens in a very low dimensional subspace for the experiments carried, and could be useful for optimisation algorithms (you trade dimension d for a much smaller \"effective\" d', you only have to figure out a generating system for this subspace and carry out optimisation inside).[[RWK-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] Can this be related to the \"flat vs sharp\" dilemma ?[[RWK-NEU], [null], [QSN], [GEN]] I would suppose that flatness tends to increase the variability captured by leading eigenvectors ?[[RWK-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SMY,QSN], [GEN]]\n\n\nTypoes:\n\nLegend of Figure 2: red lines are error -> red lines are accuracy\nTable 1: test accuracy -> test error\nBefore 6.2: architecture effects -> architecture affects[[RWK-NEU,TNF-NEG], [PNF-NEG], [DFT,CRT], [MIN]]"