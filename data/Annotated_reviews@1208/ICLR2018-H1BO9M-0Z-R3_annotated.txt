"In this paper, the authors proposed to learn word embedding for the target domain in the lifelong learning manner.[[INT-NEU], [null], [SMY], [GEN]] The basic idea is to learn a so-call meter learner to measure similarities of the same words between the target domain and the source domains for help learning word embedding for the target domain with a small corpus.[[MET-NEU], [null], [SMY], [GEN]] \n\nOverall, the descriptions of the proposed model (Section 3 - Section 5) are hard to follow.[[OAL-NEG], [CLA-NEG], [CRT], [MAJ]] This is not because the proposed model is technically difficult to understand.[[MET-NEU], [EMP-NEU], [SMY], [MAJ]] On the contrary, the model is heuristic, and simple, but the descriptions are unclear.[[MET-POS], [CLA-NEG], [CRT], [MAJ]] Section 3 is supposed to give an overview and high-level introduction of the whole model using the Figure 1, and Figure 2 (not Figure 3 mentioned in text).[[MET-NEU,TNF-NEG], [PNF-NEG,EMP-NEU], [SMY], [GEN]] However, after reading Section 3, I do not catch any useful information about the proposed model expect for knowing that a so-called meta learner is used.[[MET-NEU], [EMP-NEG], [CRT], [MAJ]] Section 4 and  Section 5 are supposed to give details of different components of the proposed model and explain the motivations.[[MET-NEU], [null], [SMY], [GEN]] However, descriptions in these two sections are very confusing, e.g, many symbols in Algorithm 1 are presented with any descriptions.[[MET-NEU], [CLA-NEG,EMP-NEG], [CRT], [MAJ]] Moreover, the motivations behind the proposed methods for different components are missing.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]  Also, a lot of types make the descriptions more difficult to follow, e.g., \"may not helpful or even harmful\", '\"Figure 3\", \"we show this Section 6\", \"large size a vocabulary\", etc.[[OAL-NEG], [CLA-NEG], [CRT], [MAJ]]\n\nAnother major concern is that the technical contributions of the proposed model is quite limited.[[MET-NEU], [EMP-NEU], [DFT], [MAJ]] The only technical contributions are (4) and the way to construct the co-occurrence information A.[[MET-NEU], [EMP-NEU], [SMY], [MAJ]] However, such contributions are quite minor, and technically heuristic.[[MET-NEU], [EMP-NEU], [SMY], [MAJ]] Moreover, regarding the aggregation layer in the pairwise network, it is similar to feature engineering.[[RWK-NEU,MET-NEU], [CMP-NEU], [SMY], [GEN]] In this case, why not just train a flat classifier, like logistic regression, with rich feature engineering, in stead of using a neural network.[[EXP-NEU,MET-NEU], [EMP-NEU], [SUG], [MAJ]]\n\nRegarding experiments, one straight-forward baseline is missing.[[RWK-NEG,EXP-NEU], [EMP-NEU], [CRT], [MAJ]] As n domains are supposed to be given in advance before the n+1 domain (target domain) comes, one can use multi-domain learning approaches with ensemble learning techniques to learn word embedding for the target domain.[[DAT-NEU,EXP-NEU], [EMP-NEU], [SMY], [GEN]] For instance, one can learn n pairwise (1 out of n sources + the target) cross-domain word embedding, and combine them using the similarity between each source and the target as the weight.[[DAT-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SUG], [MAJ]]"