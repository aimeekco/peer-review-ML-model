"This paper presents a neural architecture for converting natural language queries to SQL statements.[[INT-NEU], [null], [SMY], [GEN]] The model utilizes a simple typed decoder that chooses to copy either from the question / table or generate a word from a predefined SQL vocabulary. [[MET-NEU], [null], [SMY], [GEN]]The authors try different methods of aggregating attention for the decoder copy mechanism and find that summing token probabilities works significantly better than alternatives; this result could be useful beyond just Seq2SQL models (e.g., for summarization).[[RWK-POS,MET-POS], [CMP-POS,EMP-POS], [APC], [MAJ]] Experiments on the WikiSQL dataset demonstrate state-of-the-art results, and detailed ablations measure the impact of each component of the model. [[DAT-POS,EXP-POS,MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]]Overall, even though the architecture is not very novel,;[[OAL-NEU], [NOV-NEG], [CRT], [MAJ]] the paper is well-written and the results are strong;[[RES-POS], [CLA-POS], [APC], [MAJ]] as such, I'd recommend the paper for acceptance.[[OAL-POS], [REC-POS], [FBK], [MAJ]]\n\nSome questions:\n- How can the proposed approach scale to more complex queries (i.e., those not found in WikiSQL)?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Could the output grammar be extended to support joins, for instance? [[MET-NEU], [EMP-NEU], [QSN], [MIN]]As the grammar grows more complex, the typed decoder may start to lose its effectiveness.Some discussion of these issues would be helpful.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \n- How does the additional preprocessing done by the authors affect the performance of the original baseline system of Zhong et al.?[[RWK-NEU,EXP-NEU], [EMP-NEU], [QSN], [MAJ]] In general, some discussion of the differences in preprocessing between this work and Zhong et al. would be good (do they also use column annotation)?"[[RWK-NEU,MET-NEU], [CMP-NEU], [SUG], [MAJ]]