"This paper presents a variational inference algorithm for models that contain\ndeep neural network components and probabilistic graphical model (PGM)\ncomponents.[[MET-NEU], [null], [SMY], [GEN]]\nThe algorithm implements natural-gradient message-passing where the messages\nautomatically reduce to stochastic gradients for the non-conjugate neural\nnetwork components.[[MET-NEU], [null], [SMY], [GEN]] The authors demonstrate the algorithm on a Gaussian mixture\nmodel and linear dynamical system where they show that the proposed algorithm\noutperforms previous algorithms.[[RWK-NEU,MET-NEU], [null], [SMY], [GEN]] Overall, I think that the paper proposes some\ninteresting ideas,[[PDI-POS], [EMP-POS], [APC], [MAJ]] however, in its current form I do not think that the novelty\nof the contributions are clearly presented and that they are not thoroughly\nevaluated in the experiments.[[MET-NEG,EXP-NEG], [NOV-NEG,EMP-NEG], [CRT], [MAJ]]\n\nThe authors propose a new variational inference algorithm that handles models\nwith deep neural networks and PGM components.[[MET-NEU], [NOV-NEU], [SMY], [GEN]] However, it appears that the\nauthors rely heavily on the work of (Khan & Lin, 2017) that actually provides\nthe algorithm.[[RWK-NEU,MET-NEU], [NOV-NEU], [DIS], [GEN]] As far as I can tell this paper fits inference networks into\nthe algorithm proposed in (Khan & Lin, 2017) which boils down to i) using an\ninference network to generate potentials for a conditionally-conjugate\ndistribution[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [GEN]] and ii) introducing new PGM parameters to decouple the inference\nnetwork from the model parameters.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [GEN]] These ideas are a clever solution to work\ninference networks into the message-passing algorithm of (Khan & Lin, 2017),[[RWK-POS,PDI-POS], [CMP-NEU,EMP-POS], [APC], [MAJ]]\nbut I think the authors may be overselling these ideas as a brand new algorithm.[[RWK-NEG,MET-NEG], [NOV-NEG], [CRT], [MAJ]]\nI think if the authors sold the paper as an alternative to (Johnson, et al., 2016)\nthat doesn't suffer from the implicit gradient problem the paper would fit into\nthe existing literature better.[[RWK-NEU], [CMP-NEU], [DIS], [GEN]]\n\nAnother concern that I have is that there are a lot of conditiona-conjugacy\nassumptions baked into the algorithm that the authors only mention at the end\nof the presentation of their algorithm.[[MET-NEG], [SUB-NEG], [CRT], [MAJ]] Additionally, the authors briefly state\nthat they can handle non-conjugate distributions in the model by just using\nconjugate distributions in the variational approximation.[[MET-NEU], [null], [DIS], [GEN]] Though one could do\nthis, the authors do not adequately show that one should, or that one can do this\nwithout suffering a lot of error in the posterior approximation.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] I think that\nwithout an experiment the small section on non-conjugacy should be removed.[[EXP-NEU], [EMP-NEU], [SUG], [MIN]]\n\nFinally, I found the experimental evaluation to not thoroughly demonstrate the\nadvantages and disadvantages of the proposed algorithm.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] The algorithm was applied\nto the two models originally considered in (Johnson, et al., 2016) and the\nproposed algorithm was shown to attain lower mean-square errors for the two\nmodels.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [GEN]] The experiments do not however demonstrate why the algorithm is\nperforming better.[[EXP-NEG,MET-NEG], [SUB-NEG,EMP-NEG], [DFT,CRT], [MIN]] For instance, is the (Johnson, et al., 2016) algorithm\nsuffering from the implicit gradient? [[RWK-NEU,MET-NEU], [null], [QSN], [GEN]]It also would have been great to have\nconsidered a model that the (Johnson, et. al., 2016) algorithm would not work\nwell on or could not be applied to show the added applicability of the proposed\nalgorithm.[[RWK-NEU,MET-NEU], [CMP-NEU], [SUG], [GEN]]\n\nI also have some minor comments on the paper:\n- There are a lot of typos.[[OAL-NEG], [CLA-NEG], [CRT], [MIN]]\n- The first two sentences of the abstract do not really contribute anything to the paper.[[ABS-NEG], [CNT], [CRT], [CNT]] What is a powerful model?[[MET-NEU], [CNT], [QSN], [CNT]] What is a powerful algorithm?[[MET-NEU], [CNT], [QSN], [CNT]]\n- DNN was used in Section 2 without being defined.[[MET-NEU], [EMP-NEG], [CRT], [MIN]]\n- Using p() as an approximate distribution in Section 3 is confusing notation[[MET-NEG], [PNF-NEG], [CRT], [MIN]]\n  because p() was used for the distributions in the model.[[MET-NEG], [PNF-NEG], [CRT], [MIN]]\n- How is the covariance matrix parameterized that the inference network produces?[[MET-NEU,RES-NEU], [EMP-NEU], [QSN], [MIN]\n- The phrases \"first term of the inference network\" are not clear.[[CNT], [PNF-NEU], [CRT], [MIN] Just use The\n  DNN term and the PGM term of the inference networks, and better still throw in a reference to Eq. (4).[[MET-NEU], [CNT], [SUG,DIS], [MIN]\n- The term \"deterministic parameters\" was used and never introduced.[[CNT], [null], [CRT], [MIN]\n- At the bottom of page 5 the extension to the non-conjugate case should be\ presented somewhere (probably the appendix) since the fact that you can do this is a part of your algorithm that's important.\n"[[MET-NEU], [PNF-NEU], [SUG], [MIN]