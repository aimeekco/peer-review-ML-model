"The majority of the paper is focused on the observation that (1) making policies that condition on the time step is important in finite horizon problems, and a much smaller component on that (2) if episodes are terminated early during learning (say to restart and promote exploration) that the values should be bootstrapped to reflect that there will be additional rewards received in the true infinite-horizon setting.\n\n1 is true and is well known.[[PDI-NEU,OAL-NEU], [EMP-NEU], [DIS], [GEN]] This is typically described as finite horizon MDP planning and learning and the optimal policy is well known to be nonstationary and depend on the number of remaining time steps.[[CNT], [null], [DIS], [GEN]] There are a number of papers focusing on this for both planning and learning though these are not cited in the current draft.[[RWK-NEG], [CMP-NEG], [DFT], [MAJ]] \n\nI don\u2019t immediately know of work that suggests bootstrapping if an episode is terminated early artificially during training but it seems a very reasonable and straightforward thing to do. \n\n"[[EXP-NEU], [EMP-NEU], [DIS], [MIN]]