"The authors propose a method for reducing the computational burden when performing inference in deep neural networks.[[INT-NEU], [null], [SMY], [GEN]] The method is based a previously-developed approach called incomplete dot products, which works by pruning some of the inputs in the dot products via the introduction of pre-specified coefficients.[[RWK-NEU,MET-NEU], [null], [SMY], [GEN]] The authors of this paper extend the method by introducing a task-wise learning procedure that sequentially optimizes a loss function for decreasing percentage of included features in the dot product.[[MET-POS], [null], [SMY], [GEN]] \n\nUnfortunately, this paper was hard to follow for someone who does not actively work in this field, making it hard to judge if the contribution is significant or not.[[EXT-NEG], [null], [CRT], [MAJ]] While the description of the problem itself is adequate, when it comes to describing the TESLA procedure and the alternative training procedure, the relevant passages are, in my opinion, too vague to allow other researchers to implement this procedure.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nPositive points:\n- The application seems relevant, and the task-wise procedure seems like an improvement over the original IDP proposal.\n- Application to two well-known benchmarking datasets.[[MET-POS], [EMP-POS], [APC], [MAJ]]\n\nNegative points:\n- The method is not described in sufficient detail to allow reproducibility, the algorithms are no more than sketches.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n- It is not clear to me what the advantage of this approach is, as opposed to alternative ways of compressing the network (e.g. via group lasso regularization), or training an emulator on the full model for each task.[[MET-NEG], [EMP-NEG], [DFT], [MAJ]]\n\nMinor point:\n- Figure 1 is unclear and requires a better caption. "[[TNF-NEG], [PNF-NEG], [DFT], [MIN]]