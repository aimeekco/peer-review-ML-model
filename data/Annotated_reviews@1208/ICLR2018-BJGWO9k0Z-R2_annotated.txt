"This paper thoroughly analyzes an algorithmic task (determining if two points in a maze are connected, which requires BFS to solve) by constructing an explicit ConvNet solution and analytically deriving properties of the loss surface around this analytical solution.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] They show that their analytical solution implements a form of BFS algorithm, characterize the probability of introducing \"bugs\" in the algorithm as the weights move away from the optimal solution, and how this influences the error surface for different depths.[[MET-NEU,ANA-NEU], [null], [SMY], [GEN]] This analysis is conducted by drawing on results from the field of critical percolation in physics.[[ANA-NEU], [null], [SMY], [GEN]]\n\nOverall, I think this is a good paper and its core contribution is definitely valuable: it provides a novel analysis of an algorithmic task which sheds light on how and when the network fails to learn the algorithm, and in particular the role which initialization plays.[[MET-POS,ANA-POS,OAL-POS], [NOV-POS,EMP-POS], [APC], [MAJ]] The analysis is very thorough and the methods described may find use in analyzing other tasks.[[MET-POS,ANA-POS], [EMP-POS], [APC], [GEN]] In particular, this could be a first step towards better understanding the optimization landscape of memory-augmented neural networks (Memory Networks, Neural Turing Machines, etc) which try to learn reasoning tasks or algorithms.[[MET-POS,FWK-POS], [IMP-POS], [APC], [MAJ]] It is well-known that these are sensitive to initialization and often require running the optimizer with multiple random seeds and picking the best one.[[MET-POS], [EMP-POS], [APC], [MAJ]] This work actually explains the role of initialization for learning BFS and how certain types of initialization lead to poor solutions.[[MET-NEU], [null], [DIS], [MIN]] I am curious if a similar analysis could be applied to methods evaluated on the bAbI question-answering tasks (which can be represented as graphs, like the maze task) and possibly yield better initialization or optimization schemes that would remove the need for multiple random seeds.[[MET-NEU], [null], [DIS], [MIN]]   \n\nWith that being said, there is some work that needs to be done to make the paper clearer.[[OAL-NEU], [SUB-NEU], [DIS], [MIN]]  In particular, many parts are quite technical and may not be accessible to a broader machine learning audience.[[OAL-NEG], [APR-NEG], [DIS], [MIN]]  It would be good if the authors spent more time developing intuition (through visualization for example) and move some of the more technical proofs to the appendix.[[MET-NEU], [PNF-NEU], [SUG], [MIN]]  Specifically:\n- I think Figure 3 in the appendix should be moved to the main text, to help understand the behavior of the analytical solution.[[TNF-NEG], [PNF-NEG], [SUG], [MAJ]]  \n- Top of page 5, when you describe the checkerboard BFS: please include a visualization somewhere, it could be in the Appendix.[[CNT], [PNF-NEU], [SUG], [MIN]]\n- Section 6: there is lots of math here, but the main results don't obviously stand out.[[MET-NEG,RES-NEG], [PNF-NEG], [CRT], [MIN]] I would suggest highlighting equations 2 and 4 in some way (for example, proposition/lemma + proof), so that the casual reader can quickly see what the main results are.[[MET-NEU,RES-NEU], [EMP-NEU], [SUG], [MIN]] Interested readers can then work through the math if they want to.[[EXT-NEU], [null], [DIS], [GEN]] Also, some plots/visualizations of the loss surface given in Equations 4 and 5 would be very helpful.[[MET-NEU], [EMP-NEU], [SUG], [MIN]] \n\nAlso, although I found their work to be interesting after finishing the paper, I was initially confused by how the authors frame their work and where the paper was heading.[[OAL-NEG], [null], [CRT], [MAJ]] They claim their contribution is in the analysis of loss surfaces (true) and neural nets applied to graph-structured inputs.[[MET-NEU,ANA-NEU], [null], [SUG], [MIN]] This second part was confusing - although the maze can be viewed as a graph, many other works apply ConvNets to maze environments [1, 2, 3], and their work has little relation to other work on graph CNNs.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Here the assumptions of locality and stationarity underlying CNNs are sensible and I don't think the first paragraph in Section 3 justifying the use of the CNN on the maze environment is necessary.[[MET-NEG], [EMP-NEG], [DIS], [MIN]]  However, I think it would make much more sense to mention how their work relates to other neural network architectures which learn algorithms (such as the Neural Turing Machine and variants) or reasoning tasks more generally (for example, memory-augmented networks applied to the bAbI tasks).[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MIN]]  \n\nThere are lots of small typos, please fix them.[[OAL-NEG], [CLA-NEG], [CRT], [MIN]]  Here are a few:\n- \"For L=16, batch size of 20, ...\": not a complete sentence. [[OAL-NEG], [CLA-NEG], [CRT], [MIN]] \n- Right before 6.1.1: \"when the these such\" -> \"when such\"[[OAL-NEG], [CLA-NEG], [CRT], [MIN]] \n- Top of page 8: \"it also have a\" -> \"it also has a\", \"when encountering larger dataset[[DAT-NEG], [CLA-NEG], [CRT], [MIN]] \" -> \"...datasets\"\n-  First sentence of 6.2: \"we turn to the discuss a second\" -> [[OAL-NEG], [CLA-NEG], [CRT], [MIN]] \"we turn to the discussion of a second\"\n- etc.[[CNT], [CLA-NEG], [CRT], [MIN]]  \n\nQuality: High\nClarity: medium-low\nOriginality: high[[OAL-NEU], [CLA-NEU], [CRT], [MIN]] \nSignificance: medium-high\n\nReferences:\n[1] https://arxiv.org/pdf/1602.02867.pdf\n[2] https://arxiv.org/pdf/1612.08810.pdf\n[3] https://arxiv.org/pdf/1707.03497.pdf"[[BIB-NEU], [null], [DIS], [GEN]] 