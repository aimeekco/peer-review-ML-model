"I was asked to contribute this review rather late in the process, and in order\nto remain unbiased I avoided reading other reviews.[[EXT-NEU], [null], [SMY], [GEN]]  I apologize if some of\nthese comments have already been addressed in replies to other reviewers.[[EXT-NEU], [null], [SMY], [GEN]]\n\nThis paper proposes a regularization strategy for autoencoders that is very\nsimilar to the adversarial autoencoder of Makhzani et al.[[INT-NEU,RWK-NEU], [null], [SMY], [GEN]] The main difference\nappears to be that rather than using the classic GAN loss to shape the\naggregate posterior of an autoencoder to match a chosen, fixed distribution,\nthey instead employ a Wasserstein GAN loss (and associated weight magnitude\nconstraint, presumably enforced with projected gradient descent) on a system\nwhere the matched distribution is instead learned via a parameterized sampler\n(\"generator\" in the GAN lingo).[[MET-NEU], [null], [SMY], [GEN]] Gradient steps that optimize the encoder,\ndecoder and generator are interleaved.[[MET-NEU], [null], [SMY], [GEN]] The authors apply an extension of this\nmethod to topic and sentiment transfer and show moderately good latent space\ninterpolations between generated sentences.[[MET-POS], [EMP-POS], [APC], [MAJ]]\n\nThe difference from the original AAE is rather small and straightforward, making the\nnovelty mainly in the choice of task, focusing on discrete vectors and sequences.[[RWK-NEG,MET-NEG], [CMP-NEG], [CRT], [MAJ]]\n\nThe exposition leaves ample room for improvement.[[MET-NEU], [null], [DIS], [GEN]] For one thing, there is the\nirksome and repeated use of \"discrete structure\" when discrete *sequences* are\nconsidered almost exclusively (with the exception of discretized MNIST digits).[[DAT-NEU], [CNT], [DIS], [MIN]]\nThe paper is also light on discussion of related work other than Makhzani et al[[RWK-NEU], [CMP-NEU], [DIS], [MIN]]\n-- the wealth of literature on combining autoencoders (or autoencoder-like\nstructures such as ALI/BiGAN) and GANs merits at least passing mention.[[RWK-NEU], [CMP-NEU], [DIS], [MIN]]\n\nThe empirical work is somewhat compelling,[[CNT], [EMP-POS], [APC], [MAJ]] though I am not an expert in this\ntask domain.[[EXT-NEU], [null], [DIS], [GEN]] The annealed importance sampling technique of Wu et al (2017) for\nestimating bounds on a generator's log likelihood could be easily applied in\nthis setting and would give (for example, on binarized MNIST) a quantitative\nmeasurement of the degree of overfitting, and this would have been preferable\nthan inventing new heuristic measures.[[RWK-NEU,MET-NEU], [CMP-NEU,EMP-NEU], [DIS], [MIN]] The \"Reverse PPL\" metric requires more\njustification, and it looks an awful lot like the long-since-discredited Parzen\nwindow density estimation technique used in the original GAN paper.[[MET-NEG,ANA-NEU], [SUB-NEU,EMP-NEG], [CRT], [MAJ]]\n\nHigh-level comments:\n\n- It's not clear why the optimization is done in 3 separate steps.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Aside\nfrom the WGAN critic needing to be optimized for more steps, couldn't the\nremaining components be trained jointly, with a weighted sum of terms for the\nencoder?[[EXP-NEU], [EMP-NEU], [QSN], [MAJ]]\n- In section 2, \"This [pre-training or co-training with maximum likelihood]\n  precludes there being a latent encoding of the sentence.[[EXP-NEU], [EMP-NEU], [DIS], [MAJ]]\" It is not at all\n  clear to me why this would be the case.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]]\n- \"One benefit of the ARAE framework is that it compresses the input to a\n  single code vector.[[MET-NEU], [null], [DIS], [GEN]]\" This is true of any autoencoder.[[MET-NEU], [null], [DIS], [GEN]]\n- It would be worth explaining, in a sentence, the approach in Shen et al for\n  those who are not familiar with it, seeing as it is used as a baseline.[[RWK-NEU], [CMP-NEU], [SUG], [MIN]]\n- We are told that the encoder's output is l2-normalized but the generator's\n  is not, instead output units of the generator are squashed with the tanh\n  activation.[[EXP-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] The motivation for this choice would be helpful.[[ANA-NEU], [SUB-NEU], [SUG], [MIN]] Shortly\n  thereafter we are told that the generator quickly learns to produce norm 1\n  outputs as evidence that it is matching the encoder's distribution, but this\n  is something that could have just as easily have been built-in, and is a\n  trivial sort of \"distribution matching[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]]\"\n- In general, tables that report averages would do well to report error bars as\n  well.[[TNF-NEU], [EMP-NEU], [DIS], [MIN]] In general some more nuanced statistical analysis of these results\n  would be worthwhile, especially where they concern human ratings.\n- The dataaset fractions chosen for the semi-supervised experience seem\n  completely arbitrary.[[DAT-NEU], [CNT], [DIS], [MIN]] Is this protocol derived from some other source?[[DAT-NEU], [CNT], [QSN], [MIN]]\n  Putting these in a table along with the results would improve readability.[[DAT-NEU,TNF-NEU,RES-NEU], [CLA-NEU], [SUG], [MIN]] \n- Linear interpolation in latent space may not be the best choice here\n  seeing as e.g. for a Gaussian code the region near the origin has rather low\n  probability.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Spherical interpolation as recommended by White (2016) may\n  improve qualitative results.[[MET-NEU,RES-NEU], [EMP-NEU], [SUG], [MIN]]\n- For the interpolation results you say \"we output the argmax\", what is meant?[[RES-NEU], [EMP-NEU], [QSN], [MIN]]\n  Is beam search performed in the case of sequences?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n- Finally, a minor point: I will challenge the authors to justify their claim\n  that the learned generative model is \"useful\" (their word).[[MET-NEU], [null], [DIS], [MIN]] Interpolating\n  between two sentences sampled from the prior is a neat parlour trick, but the\n  model as-is has little utility.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Even some speculation on how this aspect\n  could be applied would be appreciated (admittedly, many GAN papers could use\n  some reflection of this sort)."[[ANA-NEU], [SUB-NEU], [DIS], [MIN]]