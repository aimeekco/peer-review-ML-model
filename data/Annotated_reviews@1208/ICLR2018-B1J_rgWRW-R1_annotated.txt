"This paper presents several theoretical results regarding the expressiveness and learnability of ReLU-activated deep neural networks.[[MET-NEU,RES-NEU], [null], [SMY], [GEN]] I summarize the main results as below:\n\n(1) Any piece-wise linear function can be represented by a ReLU-acteivated DNN.[[MET-NEU,RES-NEU], [null], [SMY], [GEN]] Any smooth function can be approximated by such networks.[[MET-NEU,RES-NEU], [null], [SMY], [GEN]]\n\n(2) The expressiveness of 3-layer DNN is stronger than any 2-layer DNN.[[MET-NEU,RES-NEU], [null], [SMY], [GEN]]\n\n(3) Using a polynomial number of neurons, the ReLU-acteivated DNN can represent a piece-wise linear function with exponentially many pieces[[MET-NEU], [null], [SMY], [GEN]]\n\n(4) The ReLU-activated DNN can be learnt to global optimum with an exponential-time algorithm.[[MET-NEU,RES-NEU], [null], [SMY], [GEN]]\n\nAmong these results (1), (2), (4) are sort of known in the literature.[[RES-NEU], [null], [SMY], [GEN]] This paper extends the existing results in some subtle ways.[[RES-NEU], [EMP-NEU], [DIS], [GEN]] For (1), the authors show that the DNN has a tighter bound on the depth.[[RES-NEU], [null], [SMY], [GEN]] For (2), the \"hard\" functions has a better parameterization, and the gap between 3-layer and 2-layer is proved bigger. [[RES-NEU], [EMP-NEU], [DIS], [GEN]]For (4), although the algorithm is exponential-time, it guarantees to compute the global optimum.[[MET-NEU,RES-NEU], [null], [SMY], [GEN]]\n\nThe stronger results of (1), (2), (4) all rely on the specific piece-wise linear nature of ReLU.[[RES-POS], [EMP-POS], [APC], [MAJ]] Other than that, I don't get much more insight from the theoretical result.[[RES-NEG], [EMP-NEG], [DFT], [MIN]] When the input dimension is n, the representability result of (1) fails to show that a polynomial number of neurons is sufficient.[[MET-NEU,RES-NEG], [EMP-NEG], [CRT], [MIN]] Perhaps an exponential number of neurons is necessary in the worst case, but it will be more interesting if the authors show that under certain conditions a polynomial-size network is good enough.[[MET-NEG], [SUB-NEG,EMP-NEG], [SUG], [MIN]]\n\nResult (3) is more interesting as it is a new result.[[RES-POS], [NOV-POS], [APC], [MAJ]] The authors present a constructive proof to show that ReLU-activated DNN can represent many linear pieces.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]  However, the construction seems artificial and these functions don't seem to be visually very complex.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MIN]]\n\nOverall, this is an incremental work in the direction of studying the representation power of neural networks.[[FWK-POS,OAL-POS], [NOV-POS], [APC], [MAJ]] The results might be of theoretical interest,[[RES-POS], [EMP-POS], [APC], [MAJ]] but I doubt if a pragmatic ReLU network user will learn anything by reading this paper."[[RES-NEG], [EMP-NEG], [CRT], [MIN]]