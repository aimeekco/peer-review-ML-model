"This paper describes PLAID, a method for sequential learning and consolidation of behaviours via policy distillation; the proposed method is evaluated in the context of bipedal motor control across several terrain types, which follow a natural curriculum.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]]\n\nPros:\n- PLAID masters several distinct tasks in sequence, building up \u201cskills\u201d by learning \u201crelated\u201d tasks of increasing difficulty.[[MET-POS], [EMP-POS], [APC], [MAJ]]\n- Although the main focus of this paper is on continual learning of \u201crelated\u201d tasks, the authors acknowledge this limitation and convincingly argue for the chosen task domain.[[MET-POS], [EMP-POS], [APC], [MAJ]]\n\nCons:\n- PLAID seems designed to work with task curricula, or sequences of deeply related tasks; for this regime, classical transfer learning approaches are known to work well (e.g finetunning), and it is not clear whether the method is applicable beyond this well understood case.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n- Are the experiments single runs?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]] Due to the high amount of variance in single RL experiments it is recommended to perform several re-runs and argue about mean behaviour.[[EXP-NEU], [SUB-NEU,EMP-NEU], [DFT], [MIN]]\n\nClarifications:\n- What is the zero-shot performance of policies learned on the first few tasks, when tested directly on subsequent tasks?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n- How were the network architecture and network size chosen, especially for the multitasker?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Would policies generalize to later tasks better with larger, or smaller networks?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n- Was any kind of regularization used, how does it influence task performance vs. transfer?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n- I find figure 1 (c) somewhat confusing.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]] Is performance maintained only on the last 2 tasks, or all previously seen tasks?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] That\u2019s what the figure suggests at first glance, but that\u2019s a different goal compared to the learning strategies described in figures 1 (a) and (b).\n"[[TNF-NEU], [EMP-NEU], [DIS], [MIN]]