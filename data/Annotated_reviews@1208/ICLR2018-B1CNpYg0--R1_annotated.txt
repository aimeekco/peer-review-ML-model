"This paper describes a method for computing representations for out-of-vocabulary words, e.g. based on their spelling or dictionary definitions.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The main difference from previous approaches is that the model is that the embeddings are trained end-to-end for a specific task, rather than trying to produce generically useful embeddings.[[RWK-NEU,MET-NEU], [CMP-NEU], [SMY], [GEN]] The method leads to better performance than using no external resources,[[MET-POS], [EMP-POS], [APC], [MAJ]] but not as high performance as using Glove embeddings.[[RWK-NEG], [CMP-NEG], [CRT], [MIN]]  The paper is clearly written, and has useful ablation experiments.[[EXP-POS,OAL-POS], [CLA-POS,EMP-POS], [APC], [MAJ]]  However, I have a couple of questions/concerns:\n- Most of the gains seem to come from using the spelling of the word.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]]  As the authors note, this kind of character level modelling has been used in many previous works.[[RWK-NEG], [CMP-NEG], [CRT], [MIN]]  \n- I would be slightly surprised if no previous work has used external resources for training word representations using an end-task loss,[[RWK-NEU,EXP-NEU], [CMP-NEU], [DIS], [MIN]]  but I don\u2019t know the area well enough to make specific suggestions [[EXT-NEU], [null], [DIS], [MIN]] \n- I\u2019m a little skeptical about how often this method would really be useful in practice.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]  It seems to assume that you don\u2019t have much unlabelled text (or you\u2019d use Glove), but you probably need a large labelled dataset to learn how to read dictionary definitions well.[[DAT-NEG], [SUB-NEG], [DFT], [MIN]]  All the experiments use large tasks - it would be helpful to have an experiment showing an improvement over character-level modelling on a smaller task.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] \n- The results on SQUAD seem pretty weak - 52-64%, compared to the SOTA of 81.[[RWK-NEG,DAT-NEG], [CMP-NEG,EMP-NEG], [CRT], [MAJ]]  It seems like the proposed method is quite generic, so why not apply it to a stronger baseline?\n"[[RWK-NEG,RES-NEG], [EMP-NEG], [QSN], [MAJ]]