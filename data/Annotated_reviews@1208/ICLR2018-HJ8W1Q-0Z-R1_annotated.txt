"The authors present an evolution of the idea of fast weights: training a double recurrent neural network, one \"slow\" trained as usual and one \"fast\" that gets updated in every time-step based on the slow network.[[RWK-NEU,PDI-NEU,EXP-NEU,MET-NEU], [null], [SMY], [GEN]] The authors generalize this idea in a nice  way and present results on 1 experiment.[[PDI-POS,EXP-POS,RES-POS], [IMP-POS,EMP-POS], [APC], [MAJ]] On the positive side, the paper is clearly written and while the fast-weights are not new, the details of the presented method are original.[[RWK-POS,OAL-POS], [CLA-POS,NOV-POS,EMP-POS], [APC], [MAJ]] On the negative side, the experimental results are presented on only 1 experiment with a data-set and task made up by the authors.[[EXP-NEG,RES-NEG,OAL-NEG], [IMP-NEG], [CRT], [MIN]] The results are good but the improvements are not too large, and they are measured over weak baselines implemented by the authors.[[EXP-NEG,RES-POS,OAL-NEU], [IMP-NEU,EMP-NEG], [SMY], [MIN]] For a convincing result, one would require an evaluation on a number of tasks, including long-studied ones like language modeling, and comparison to stronger related models, such as the Neural Turing Machine or the Transformer (from \"Attention is All You Need\").[[EXT-NEU], [null], [SUG], [GEN]] Without comparison to stronger baselines and with results only on 1 task constructed by the authors, we have to recommend rejection."[[RWK-NEG,EXP-NEG,RES-NEG], [SUB-NEG,CMP-NEG,REC-NEG], [SUG,DFT,CRT,FBK], [GEN]]