"The paper investigates the representation of polynomials by neural networks up to a certain degree and implied uniform approximations.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] It shows exponential gaps between the width of shallow and deep networks required for approximating a given sparse polynomial.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] \n\nBy focusing on polynomials, the paper is able to use of a variety of tools (e.g. linear algebra) to investigate the representation question.[[MET-POS], [null], [DIS], [GEN]] Results such as Proposition 3.3 relate the representation of a polynomial up to a certain degree, to the approximation question.[[RES-NEU], [null], [SMY], [MIN]] Here it would be good to be more specific about the domain, however, as approximating the low order terms certainly does not guarantee a global uniform approximation.[[MET-NEU], [SUB-NEU,EMP-NEG], [SUG], [MIN]] \n\nTheorem 3.4 makes an interesting claim, that a finite network size is sufficient to achieve the best possible approximation of a polynomial (the proof building on previous results, e.g. by Lin et al that I did not verify).[[RWK-POS,MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]] The idea being to construct a superposition of Taylor approximations of the individual monomials.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Here it would be good to be more specific about the domain.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Also, in the discussion of Taylor series, it would be good to mention the point around which the series is developed, e.g. the origin.[[RWK-NEU], [CMP-NEU], [SUG], [MIN]] \n\nThe paper mentions that ``the theorem is false for rectified linear units (ReLUs), which are piecewise linear and do not admit a Taylor series''.[[MET-NEU], [null], [SMY,DIS], [GEN]] However, a ReLU can also be approximated by a smooth function and a Taylor series.[[MET-NEU], [null], [DIS], [GEN]] \n\nTheorem 4.1 seems to be implied by Theorem 4.2.[[MET-NEU], [null], [DIS], [MIN]] Similarly, parts of Section 4.2 seem to follow directly from the previous discussion.[[MET-NEU], [null], [DIS], [MIN]] \n\nIn page 1 ```existence proofs' without explicit constructions'' This is not true, with numerous papers providing explicit constructions of functions that are representable by neural networks with specific types of activation functions. \n\n"[[RWK-NEG, MET-NEG], [CMP-NEG], [CRT], [MAJ]]