"The main strengths of the paper are the supporting experimental results in comparison to plain feed-forward networks (FNNs).[[EXP-POS,RES-POS], [EMP-POS], [APC], [MAJ]]  The proposed method is focused on discovering sparse neural networks.[[MET-NEU], [null], [SMY], [GEN]]  The experiments show that sparsity is achieved and still the discovered sparse networks have comparable or better performance compared to dense networks.[[EXP-NEU], [null], [SMY], [GEN]]\n\nThe main weakness of the paper is lack of cohesion in contributions and difficulty in delineating the scope of their proposed approach.[[MET-NEG,ANA-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nBelow are some suggestions for improving the paper:\n\nCan you enumerate the paper\u2019s contributions and specify the scope of this work?[[PDI-NEU], [EMP-NEU], [QSN], [MIN]]  Where is this method most applicable and where is it not applicable?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nWhy is the paper focused on these specific contributions? [[PDI-NEU], [EMP-NEU], [QSN], [MIN]] What problem does this particular set of contributions solve that is not solvable by the baselines? [[PDI-NEU,RWK-NEU], [EMP-NEU], [QSN], [MIN]] There needs to be a cohesive story that puts the elements together.[[PDI-NEU], [null], [DIS], [GEN]]  For example, you explain how the algorithm for creating the backbone can use unsupervised data.[[MET-NEU], [null], [SUG], [MIN]]    On the other hand, to distinguish this work from the baselines you mention that this work is the first to apply the method to supervised learning problems.[[RWK-NEU,MET-NEU], [null], [DIS], [GEN]]  \n\nThe motivation section in the beginning of the paper motivates using the backbone structure to get a sparse network.[[PDI-NEU], [null], [DIS], [GEN]]  However, it does not adequately motivate the skip-path connections or applications of the method to supervised tasks.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nIs this work extending the applicability of baselines to new types of problems?[[RWK-NEU,OAL-NEU], [NOV-NEU], [QSN], [MIN]]  Or is this work focused on improving the performance of existing methods?[[RWK-NEU,OAL-NEU], [CNT], [QSN], [MIN]]  Answers to these questions can automatically determine suitable experiments to run as well. [[EXP-NEU], [CNT], [DIS], [MIN]] It's not clear if Pruned FNNs are the most suitable baseline for evaluating the results.[[RWK-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]]  Can your work be compared experimentally with any of the constructive methods from the related work section?[[RWK-NEG,EXP-NEG], [EMP-NEG], [QSN], [MAJ]]  If not, why?\n\nWhen contrasting this work with existing approaches, can you explain how existing work builds toward the same solution that you are focusing on?[[RWK-NEU,MET-NEU], [EMP-NEU], [QSN], [MAJ]]  It would be more informative to explain how the baselines contribute to the solution instead of just citing them and highlighting their differences.[[RWK-NEG], [CMP-NEG], [DIS], [MAJ]]\n\nRegarding the experimental results, is there any insight on why the dense networks are falling short?[[EXP-NEU,RES-NEU], [EMP-NEU], [QSN], [MIN]]  For example, if it is due to overfitting, is there a correlation between performance and size of FNNs? [[EXP-NEU,RES-NEU], [EMP-NEU], [QSN], [MIN]] Do you observe a similar performance vs FNNs in existing methods?[[MET-NEU,RES-NEU], [EMP-NEU], [QSN], [MIN]]  Whether this good performance is due to your contributions or due to effectiveness of the baseline algorithm, proper analysis and discussion is required and counts as useful research contribution.\n"[[ANA-NEU], [SUB-NEU], [DIS], [MAJ]]