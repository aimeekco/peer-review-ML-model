"Summary: This paper explores how to handle two practical issues in reinforcement learning.[[INT-NEU], [null], [SMY], [GEN]] The first is including time remaining in the state, for domains where episodes are cut-off before a terminal state is reached in the usual way.[[PDI-NEU], [null], [SMY], [GEN]] The second idea is to allow bootstrapping at episode boundaries, but cutting off episodes to facilitate exploration.[[PDI-NEU], [null], [SMY], [GEN]] The ideas are illustrated through several well-worked micro-world experiments.[[PDI-POS,EXP-POS], [EMP-POS], [APC], [MAJ]]\n\nOverall the paper is well written and polished.[[OAL-POS], [CLA-POS], [APC], [MAJ]] They slowly worked through a simple set of ideas trying to convey a better understanding to the reader, with a focus on performance of RL in practice.[[PDI-POS,RES-POS], [EMP-POS], [APC], [MAJ]]\n\nMy main issue with the paper is that these two topics are actually not new and are well covered by the existing RL formalisms.[[PDI-NEG], [NOV-NEG], [CRT], [MAJ]] That is not to say that an empirical exploration of the practical implications is not of value, but that the paper would be much stronger if it was better positioned in the literature that exists.[[OAL-NEG], [NOV-NEG], [SUG,CRT], [MIN]]\n\nThe first idea of the paper is to include time-remaining in the state.[[PDI-NEU], [null], [DIS], [GEN]] This is of course always possible in the MDP formalism.[[MET-NEU], [null], [DIS], [GEN]] If it was not done, as in your examples, the state would not be Markov and thus it would not be an MDP at all.[[RWK-NEG], [CMP-NEG], [CRT], [MIN]] In addition, the technical term for this is finite horizon MDPs (in many cases the horizon is taken to be a constant, H).[[MET-NEU], [null], [DIS], [GEN]] It is not surprising that algorithms that take this into account do better, as your examples and experiments illustrate.[[EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]] The paper should make this connection to the literature more clear and discuss what is missing in our existing understanding of this case, to motivate your work.[[RWK-NEG], [SUB-NEG,CMP-NEG], [DFT], [MIN]] See Dynamic Programming and Optimal Control and references too it.[[RWK-NEU], [CMP-NEU], [DIS], [GEN]]\n\nThe second idea is that episodes may terminate due to time out, but we should include the discounted value of the time-out termination state in the return.[[PDI-NEU], [null], [DIS], [MIN]] I could not tell from the text but I assume, the next transition to the start state is fully discounted to zero, otherwise the value function would link the values of S_T and the next state, which I assume you do not want.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] The impact of this choice is S_T is no longer a termination state, and there is a direct fully discounted transition to the start states.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] This is in my view is how implementations of episodic tasks with a timeout should be done and is implemented this way is classic RL frameworks (e.g., RL glue).[[MET-NEU], [EMP-NEU], [DIS], [MIN]] If we treat the value of S_T as zero or consider gamma on the transition into the time-out state as zero, then in cost to goal problems the agent will learn that these states are good and will seek them out leading to suboptimal behavior.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] The literature might not be totally clear about this, but it is very well discussed in a recent ICML paper: White 2017 [1]\n\nAnother way to pose and think about this problem is using the off-policy learning setting---perhaps best described in the Horde paper [2].[[RWK-NEU], [CMP-NEU], [DIS], [MIN]] In this setting the behavior policy can have terminations and episodes in the classic sense (perhaps due to time outs).[[MET-NEU], [null], [DIS], [MIN]] However, the agent's continuation function (gamma : S -> [0,1]) can specify weightings on states representing complex terminations (or not), completely independent of the behavior policy or actual state transition dynamics of the underlying MDP.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] To clearly establish your contributions, the authors must do a better job of relating their work to [1] and [2].[[RWK-NEG], [CMP-NEG], [CRT], [MIN]]\n\n[1] White. Unifying task specification in reinforcement learning. Martha White. International Conference on Machine Learning (ICML), 2017.[[BIB-NEU], [null], [DIS], [GEN]]\n\n[2] Sutton, R. S., Modayil, J., Delp, M., Degris, T., Pilarski, P. M., White, A., & Precup, D. (2011).[[BIB-NEU], [null], [DIS], [GEN]] Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction.[[BIB-NEU], [null], [DIS], [GEN]] In The 10th International Conference on Autonomous Agents and Multiagent Systems: 2, 761--768.[[BIB-NEU], [null], [DIS], [GEN]] \n\nSmall comments that did not impact paper scoring:\n1) eq 1 we usually don't use the superscript \\gamma[[MET-NEU], [EMP-NEG], [CRT], [MIN]]\n2) eq2, usually we talk about truncated n-step returns include the value of the last state to correct the return.[[MET-NEU], [null], [DIS], [GEN]] You should mention this\n3) Last paragraph of page 2 should not be in the intro[[INT-NEG], [PNF-NEG], [CRT], [MIN]]\n4) in section 2.2 why is the behavior policy random instead of epsilon greedy?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n5) It would be useful to discuss the average reward setting and how it relates to your work.[[ANA-NEU], [SUB-NEU], [SUG], [MIN]]\n6) Fig 5. What does good performance look like in this domain.[[TNF-NEU], [PNF-NEU], [DIS], [MIN]] I have no reference point to understand these graphs[[TNF-NEU], [null], [DIS], [GEN]]\n7) page 9, second par outlines alternative approaches but they are not presented as such. Confusing "[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]