"This paper presents a new multi-task network architecture within which low-rank parameter spaces were found using matrix factorization.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] As carefully proved and tested, only one pass of the training data would help recover the parametric subspace, thus network could be easily trained layer-wise and expanded.[[DAT-NEU,EXP-NEU], [EMP-NEU], [DIS], [GEN]]\n\nSome novel contributions:\n1. Layer by layer feedforward training process, no back-prop.[[EXP-POS], [NOV-POS,EMP-POS], [APC], [MAJ]]\n2. On-line settings to train parameters ( guaranteed convergence in a single pass of the data)[[DAT-POS,EXP-POS], [NOV-POS,EMP-POS], [APC], [MAJ]]\n\nWeakness :\n1. The assumption that a low-rank parameter space exists among tasks rather than original feature spaces is not new and widely used in literature.[[RWK-NEG,MET-NEG], [NOV-NEG], [CRT], [MIN]]\n2. The proof part(Section 2.2) can be extended with more details in Appendix.[[MET-NEG], [SUB-NEG,PNF-NEG], [SUG], [MIN]]\n3. In synthetic data experiments (Table1), only small margins could be observed between SN, f-MLP and rf-MLP, and only Layer 1 of SN performs better above all others.[[EXP-NEU], [EMP-NEU], [CRT], [MIN]] \n4. Typo: In Table2,3,5, Multi-l_{2,1} (denotes the L2,1 norm) were written wrong.[[TNF-NEG], [CLA-NEG], [CRT], [MIN]]\n5. In the synthetic data experiments on comparison with single-task and multi-task models, counter-intuitive results (with larger training data split, ANMSE raises instead of decreases) of multi-task models may need further explanation.[[EXP-NEG,RES-NEG], [SUB-NEG], [SUG,DFT], [MIN]] \n6. Extra models like Deep Networks with/without matrix factorization could be added.[[MET-NEU], [SUB-NEU], [SUG], [MIN]] ( As proposed model is a deep model, the lack of comparison with deep methods is dubious)[[MET-NEU], [SUB-NEU], [SUG], [MIN]] \n7. In Section 4.2, the real dataset is rather small thus the results on this small dataset were not convincing enough. SN model outperforms the state-of-the-art with only small margin.[[RWK-POS,MET-POS], [EMP-POS], [APC], [MAJ]]  Extensive experiments could be added.[[EXP-NEU], [SUB-NEU], [SUG], [MIN]] \n8. The performance on One-Layer Subspace Network (with only the input features) could be added.[[EXP-NEU,MET-NEU], [SUB-NEU], [SUG], [MIN]] \n\nConclusion:\nThough with a quite novel idea on solving multi-task censored regression problem,[[PDI-POS], [EMP-POS], [APC], [MAJ]] the experiments conducted on synthetic data and real data are not convincing enough to ensure the contribution of the Subspace Network. \n"[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]]