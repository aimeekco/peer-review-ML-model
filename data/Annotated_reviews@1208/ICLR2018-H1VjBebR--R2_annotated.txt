"Quality:\nThe paper appears to be correct\n\nClarity:\nthe paper is clear, although more formalization would help sometimes\n\nOriginality\nThe paper presents an analysis for unsupervised learning of mapping between 2 domains that is totally new as far as I know.[[INT-NEU,RWK-POS,PDI-POS,ANA-POS], [EMP-POS], [APC], [MAJ]]\n\nSignificance\nThe points of view defended in this paper can be a basis for founding a general theory for unsupervised learning of mappings between domains.[[RWK-NEU,ANA-NEU], [CMP-NEU], [DIS], [GEN]]\n\nPros/cons\nPros\n-Adresses an important problem in representation learning[[PDI-NEU,ANA-NEU], [CMP-NEU], [DIS], [GEN]]\]n-The paper proposes interesting assumptions and results for measuring the complexity of semantic mappings[[PDI-NEU,ANA-NEU], [CMP-NEU], [DIS], [GEN]]\n-A new cross domain mapping is proposed\n-Large set of experiments\nCons\n-Some parts deserve more formalization/justification\n-Too many materials for a conference paper\n-The cost of the algorithm seems [[RWK-NEU,ANA-NEU], [CMP-NEU], [DIS], [GEN]]high \n\nSummary:\nThis paper studies the problem of unsupervised learning of semantic mappings.[[RWK-NEU,ANA-NEU], [CMP-NEU], [DIS], [GEN]] It proposes a notion of low complexity networks in this context used for identifying  minimal complexity mappings which is assumed to be central for recovering the best cross domain mapping.[[RWK-NEU,ANA-NEU], [CMP-NEU], [DIS], [GEN]] A theoretical result shows that the number of low-discrepancy (between cross-domains) mappings of low complexity is rather small.[[RWK-NEU,RES-NEG], [EMP-NEG], [DFT], [MIN]]\nA large set of experiments are provided to support the claims of the paper.[[RWK-NEU,EXP-NEU], [CMP-NEU], [DIS], [GEN]]\n\n\nComments:\n\n-The work is interesting, for an important problemin representation learning, while in machine learning in general with the unsupervised aspect[[RWK-POS,MET-POS], [CMP-POS], [APC], [MAJ]].\n\n-In a sense, I find that the approach suggested by algorithm 1 has some connections with structural risk minimization: by increasing k1 and k2 - when looking for the mapping - you increase the complexity of the model searched while trying to optimize the risk which is measured by the discrepancies and loss[[RWK-NEU,MET-NEU,ANA-NEU], [EMP-NEU], [DIS], [GEN]].\nThe approach seems costly anyway and I wonder if the authors could think of a smoother version of the algorithm to make it more efficient.\[[RWK-NEU,MET-NEU,ANA-NEU], [EMP-NEU], [DIS], [GEN]]n\n-For counting the minimal complexity mappings, I wonder if one can make a connection with Algorithm robustness of Xu&Mannor(COLT,2012) where instead of comparing losses, you work with discrepancies.\n\nTypo:\nSection 5.1 is build of -> is built of\n"[[RWK-NEU,MET-NEU,ANA-NEU], [EMP-NEU], [DIS], [GEN]]