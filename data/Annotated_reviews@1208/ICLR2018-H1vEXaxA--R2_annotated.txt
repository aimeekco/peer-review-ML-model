"--------------\nSummary and Evaluation:\n--------------\nThis work present a novel multi-agent reference game designed to train monolingual agents to perform translation between their respective languages -- all without parallel corpora.[[OAL-POS], [NOV-POS], [APC], [MAJ]] The proposed approach closely mirrors that of Nakayama and Nishida, 2017 in that image-aligned text is encouraged to map to similarly to the grounded image.[[MET-NEU], [null], [SMY], [GEN]] Unlike in this previous work, the approach proposed here induces this behavior though a multi-agent reference game. [[RWK-NEU], [null], [DIS], [GEN]]The key distinction being that in this gamified setting, the agents sample many more descriptions from their stochastic policies than would otherwise be covered by the human ground truth.[[PDI-NEU,MET-NEU], [null], [DIS], [GEN]] The authors demonstrate that this change results in significantly improved BLEU scores across a number of translation tasks. Furthermore, increasing the number of agents/languages in this setting seems to \n\nOverall I think this is an interesting paper[[RWK-POS,PDI-POS,RES-POS], [null], [APC], [MAJ]]. The technical novelty is somewhat limited to a minor (but powerful) change in approach from Nakayama and Nishida, 2017; however, the resulting translators outperform this previous method.[[EXP-NEG,MET-NEG], [NOV-NEG,SUB-NEG], [DFT], [MAJ]] I have a few things listed in the weaknesses section that I found unclear or think would make for a stronger submission.[[OAL-NEG], [null], [CRT], [MAJ]] \n\n\n--------------\nStrengths:\n--------------\n\n- The paper is fairly clearly written and the figures appropriately support the text.[[TNF-POS,OAL-POS], [CLA-POS,PNF-POS], [APC], [MAJ]]\n\n- Learning translation without parallel corpora is a useful task and leveraging a pragmatic reference game to induce additional semantically valid samples of a source language is an interesting approach to do so.[[RWK-NEU,MET-NEU], [null], [APC], [MAJ]]\n\n- I'm also excited by the result that multi-agent populations tend to improve the rate of convergence and final translation abilities of these models; though I'm slightly confused about some of the results here (see weaknesses).[[MET-NEG,RES-NEG], [EMP-NEG],  [CRT], [MAJ]]\n\n--------------\nWeaknesses:\n--------------\n\n- Perhaps I'm missing something, but shouldn't the Single EN-DE/DE-EN results in Table 2 match the not pretrained EN-DE/DE-EN Multi30k Task 1 results? I understand that this is perhaps on a different data split into M1/2 but why is there such a drastic difference?[[DAT-NEU,TNF-NEU,RES-NEU], [EMP-NEU],  [DIS], [MIN]]\n\n- I would have liked to see some context as how these results compare to an approach trained with aligned corpora. [[RWK-NEU,RES-NEU], [CMP-NEU], [DIS], [GEN]]Perhaps a model trained on the human-translated pairs from Task 1 of Multi30k? Obviously, outperforming such a model is not necessary for this approach to be interesting, but it would provide useful context on how well this is doing.\[[MET-POS], [EMP-POS], [APC], [MAJ]]n\n- A great deal of the analysis and qualitative examples are pushed to the supplement which is a bit of a shame[[MET-NEG], [PNF-NEG], [CRT], [MAJ]] given they are quite interesting.\n\n\n"[[MET-POS], [EMP-POS], [APC], [MAJ]]