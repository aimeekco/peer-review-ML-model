"This paper presents an iterative approach to sparsify a network already during training.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] During the training process, the amount of connections in the network is guaranteed to stay under a specific threshold.[[EXP-NEU,MET-NEU], [null], [SMY], [GEN]] This is a big advantage when training is performed on hardware with computational limitations, in comparison to \"post-hoc\" sparsification methods, that compress the network after training.[[RWK-POS,EXP-POS,MET-POS], [CMP-POS,EMP-POS], [APC], [MAJ]]\nThe method is derived by considering the \"rewiring\" of an (artificial) neural network as a stochastic process.[[EXP-NEU,MET-NEU], [null], [SMY], [GEN]] This perspective is based on a recent model in computational biology but also can be interpreted as a (sequential) monte carlo sampling based stochastic gradient descent approach. [[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MIN]]References to previous work in this area are missing, e.g.\n\n[1] de Freitas et al., Sequential Monte Carlo Methods to Train Neural Network\nModels, Neural Computation 2000\n[2] Welling et al., Bayesian Learning via Stochastic Gradient Langevin Dynamics, ICML 2011\n\n[[RWK-NEG,BIB-NEG], [CMP-NEG], [DFT], [MIN]]Especially the stochastic gradient method in [2] is strongly related to the existing approach.[[MET-NEG], [NOV-NEG], [CRT], [MIN]]\n\nPositive aspects\n\n- The presented approach is well grounded in the theory of stochastic processes.[[MET-POS], [EMP-POS], [APC], [MAJ]] The authors provide proofs of convergence by showing that the iterative updates converge to a fixpoint of the stochastic process\n\n-[[EXP-NEU,MET-NEU], [null], [DIS], [MIN]] By keeping the temperature parameter of the stochastic process high, it can be directly applied to online transfer learning.\n\n[[MET-NEU], [null], [DIS], [MIN]]- The method is specifically designed for online learning with limited hardware ressources.\n\n[[MET-POS], [EMP-POS], [APC], [MAJ]]Negative aspects\n\n- The presented approach is outperformed for moderate compression levels (by Han's pruning method for >5% connectivity on MNIST, Fig. 3 A, and by l1-shrinkage for >40% connectivity on CIFAR-10 and TIMIT, Fig. 3 B&C).[[DAT-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] Especially the results on MNIST suggest that this method is most advantageous for very high compression levels.[[DAT-NEG,MET-NEU,RES-NEG], [SUB-NEG], [CRT], [MAJ]] However in these cases the overall classification accuracy has already dropped significantly which could limit the practical applicability.\n\n[[RES-NEG], [NOV-NEG,IMP-NEG], [CRT], [MAJ]]- A detailled discussion of the relation to previously existing very similar work is missing (see above)[[RWK-NEG,ANA-NEG], [CMP-NEG], [DFT], [MAJ]]\n\n\nTechnical Remarks\n\nFig. 1, 2 and 3 are referenced on the pages following the page containing the figure.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]] Readibility could be slightly increased by putting the figures on the respective pages.\n"[[TNF-NEU], [CLA-NEU,PNF-NEU], [SUG], [MIN]]