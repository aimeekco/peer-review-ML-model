"This paper studies the problem of learning one-hidden layer neural networks and is a theory paper.[[INT-NEU], [null], [SMY], [GEN]] A well-known problem is that without good initialization, it is not easy to learn the hidden parameters via gradient descent.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] This paper establishes an interesting connection between least squares population loss and Hermite polynomials.[[MET-POS], [EMP-POS], [APC], [MAJ]] Following from this connection authors propose a new loss function.[[MET-POS], [NOV-POS], [APC], [MAJ]] Interestingly, they are able to show that the loss function globally converges to the hidden weight matrix, Simulations confirm the findings.[[MET-POS], [EMP-POS], [APC], [MAJ]]\n\nOverall, pretty interesting result and solid contribution.[[RES-POS], [NOV-POS], [APC], [MAJ]] The paper also raises good questions for future works.[[FWK-POS], [null], [APC], [MAJ]] For instance, is designing alternative loss function useful in practice? [[MET-NEU], [EMP-NEU], [QSN], [MIN]]In summary, I recommend acceptance.[[OAL-POS], [REC-POS], [FBK], [MAJ]] The paper seems rushed to me so authors should polish up the paper and fix typos.[[OAL-NEU], [CLA-NEU], [SUG], [MIN]]\n\nTwo questions:\n1) Authors do not require a^* to recover B^*. Is that because B^* is assumed to have unit length rows?[[MET-NEU], [EMP-NEU], [QSN], [MAJ]] If so they should clarify this otherwise it confuses the reader a bit.[[MET-NEU], [EMP-NEU], [SUG], [MAJ]]\n2) What can be said about rate of convergence in terms of network parameters?[[MET-NEU], [EMP-NEU], [QSN], [MAJ]] Currently a generic bound is employed which is not very insightful in my opinion.[[MET-NEU], [EMP-NEG], [CRT], [MIN]]\n\n"