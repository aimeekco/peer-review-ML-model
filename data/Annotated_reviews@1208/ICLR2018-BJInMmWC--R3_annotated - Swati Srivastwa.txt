"Summary: The authors observe that the current image generation models generate realistic images however as the dimensions of the latent vector is fully entangled, small changes to a single neuron can effect every output pixel in arbitrary ways.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] In this work, they explore the effect of using partial natural language scene descriptions for the task of disentangling the latent entities visible in the image.[[PDI-NEU], [null], [SMY], [GEN]]  The proposed Generative Entity Networks jointly generates the natural language descriptions and images from scratch.[[PDI-NEU], [null], [SMY], [GEN]] The core model is Variational Autoencoders (VAE) with an integrated visual attention mechanism that also generates the associated text.[[MET-NEU], [null], [SMY], [GEN]] The experiments are conducted on the Shapeworld dataset.[[EXP-NEU], [null], [SMY], [GEN]]\n\nStrengths:\nSimultaneous text and image generation is an interesting research topic that is relevant for the community.[[PDI-POS], [EMP-POS], [APC], [MAJ]]\nThe paper is well written, the model is formulated with no errors (although it could use some more detail) and supported by illustrations (although there are some issues with the illustrations detailed below).[[MET-POS,OAL-POS], [CLA-POS,EMP-POS], [APC], [MAJ]] \nThe model is evaluated on tasks that it was not trained on which indicate that this model learns generalizable latent representations.[[MET-POS], [EMP-POS], [APC], [MAJ]] \n\nWeaknesses:\nThe paper gives the impression to be rushed, i.e. there are citations missing (page 3 and 6), the encoder model illustration is not as clear as it could be.[[PDI-NEU], [null], [SMY], [GEN]] Especially the white boxes have no labels, the experiments are conducted only on one small-scale proof of concept dataset, several relevant references are missing, e.g. GAN, DCGAN, GAWWN, StackGAN.[[RWK-NEG,EXP-NEG], [SUB-NEG,EMP-NEG], [DFT], [MAJ]] Visual Question answering is mentioned several times in the paper, however no evaluations are done in this task.[[MET-NEG], [SUB-NEG], [DFT], [MAJ]]\n\nFigure 2 is complex and confusing due to the lack of proper explanation in the text.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]] The reader has to find out the connections between the textual description of the model and the figure themselves due to no reference to particular aspects of the figure at all.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]] In addition the notation of the modules in the figure is almost completely disjoint so that it is initially unclear which terms are used interchangeably.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]]\n\nDetails of the \u201cwhite components\u201d in Figure 2 are not mentioned at all.[[TNF-NEG], [SUB-NEG], [DFT], [MIN]] E.g., what is the purpose of the fully connected layers, why do the CNNs split and what is the difference in the two blocks (i.e. what is the reason for the addition small CNN block in one of the two)[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n\nThe optimization procedure is unclear.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] What is the exact loss for each step in the recurrence of the outputs (according to Figure 5)?[[TNF-NEU], [EMP-NEU], [QSN], [MIN]] Or is only the final image and description optimized.[[TNF-NEU], [EMP-NEU], [DIS], [MIN]] If so, how is the partial language description as a target handled since the description for a different entity in an image might be valid, but not the current target.[[MET-NEG,TNF-NEG], [EMP-NEG], [CRT], [MIN]] (This is based on my understanding that each data point consists of one image with multiple entities and one description that only refers to one of the entities).[[TNF-NEU,DAT-NEU], [EMP-NEU], [DIS], [MIN]]\n\nAn analysis or explanation of the following would be desirable: How is the network trained on single descriptions able to generate multiple descriptions during evaluation.[[EXP-NEU,MET-NEU,ANA-NEU], [EMP-NEU], [DIS], [GEN]] How does thresholding mentioned in Figure 5 work?[[TNF-NEU], [EMP-NEU], [QSN], [MIN]]\n\nIn the text, k suggests to be identical to the number of entities in the image.[[MET-NEU,TNF-NEU], [null], [DIS], [GEN]] In Figure 5, k seems to be larger than the number of entities.[[TNF-NEU], [PNF-NEU], [DIS], [GEN]] How is k chosen? Is it fixed or dynamic?[[MET-NEU], [EMP-NEU], [QSN], [GEN]]\n\nEven though the title claims that the model disentangles the latent space on an entity-level, it is not mentioned in the paper.[[MET-NEG], [EMP-NEG], [DFT], [MAJ]] Intuitively from Figure 5, the network generates black images (i.e. all values close to zero) whenever the attention is on no entity and, hence, when attention is on an entity the latent space represents only this entity and the image is generated only showing that particular entity.[[MET-NEU,TNF-NEU], [EMP-NEU], [DIS], [MIN]] However, confirmation of this intuition is needed since this is a central claim of the paper.[[MET-NEU,ANA-NEU], [SUB-NEU], [DIS], [MAJ]]\n\nAs the main idea and the proposed model is simple and intuitive, the evaluation is quite important for this paper to be convincing.[[PDI-POS,MET-POS], [EMP-POS], [APC], [MAJ]] Shapeworlds dataset seems to be an interesting proof-of-concept dataset[[DAT-POS], [EMP-POS], [APC], [MAJ]] however it suffers from the following weaknesses that prevent the experiments from being convincing especially as they are not supported with more realistic setups.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] First, the visual data is composed of primitive shapes and colors in a black background.[[DAT-NEG], [PNF-NEG], [CRT], [MIN]] Second, the sentences are simple and non-realistic.[[CNT], [CNT], [CRT], [MIN]] Third, it is not used widely in the literature, therefore no benchmarks exist on this data.[[RWK-NEG,DAT-NEG], [EMP-NEG], [CRT], [MAJ]] \n\nIt is not easy to read the figures in the experimental section, no walkthrough of the results are provided.[[EXP-NEG,RES-NEG,TNF-NEG], [PNF-NEG], [CRT], [MIN]] For instance in Figure 4a, the task is described as \u201cshowing the changes in the attribute latent variables\u201d which gives the impression that, e.g. for the first row the interpolation would be between a purple triangle to a purple rectangle however in the middle the intermediate shapes also are painted with a different color. It is not clear why the color in the middle changes.[[EXP-NEG,RES-NEG,TNF-NEG], [PNF-NEG], [CRT], [MIN]]\n\nThe evaluation criteria reported on Table 1 is not clear.[[MET-NEG,TNF-NEG], [EMP-NEG], [CRT], [MIN]] How is the accuracy measured, e.g. with respect to the number of objects mentioned in the sentence, the accuracy of the attribute values, the deviation from the ground truth sentence (if so, what is the evaluation metric)? [[RES-NEU], [EMP-NEU], [QSN], [MIN]]No example sentences are provided for a qualitative comparisons.[[RES-NEG], [SUB-NEG], [CRT], [MIN]] In fact, it is not clear if the model generates full sentences or attribute phrases.[[EXP-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nAs a summary, this paper would benefit significantly with a more extensive overview of the existing relevant models, clarification on the model details mentioned above and a more through experimental evaluation with more datasets and clear explanation of the findings."[[DAT-NEU,RWK-NEU,EXP-NEU,ANA-NEU], [IMP-NEU,SUB-NEU], [SUG], [MAJ]]