"The paper introduces two alternatives to value iteration network (VIN) proposed by Tamar et al. VIN was proposed to tackle the task of learning to plan using as inputs a position and an image of the map of the environment.[[RWK-NEU], [NULL], [SMY], [GEN]] The authors propose two new updates value propagation (VProp) and max propagation (MVProp), which are roughly speaking additive and multiplicative versions of the update used in the Bellman-Ford algorithm for shortest path.[[PDI-NEU], [CMP-POS], [SMY], [GEN]] The approaches are evaluated in grid worlds with and without other agents.[[MET-NEU], [NULL], [SMY], [GEN]]\n\nI had some difficulty to understand the paper because of its presentation and writing (see below).[[OAL-NEG], [CLA-NEG,PNF-NEG], [CRT], [MIN]]\n\nIn Tamar's work, a mapping from observation to reward is learned.[[RWK-NEU], [null], [DIS], [GEN]] It seems this is not the case for VProp and MVProp, given the gradient updates provided in p.5.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] As a consequence, those two methods need to take as input a new reward function for every new map.[[MET-NEU], [EMP-NEG], [DIS], [GEN]] Is that correct?[[MET-NEU], [EMP-NEG], [QSN], [MIN]]\nI think this could explain the better experimental results\n\nIn the experimental part, the results for VIN are worse than those reported in Tamar et al.'s paper.[[RWK-NEU,EXP-NEG], [CMP-POS], [CRT], [MAJ]] Why did you use your own implementation of VIN and not Tamar et al.'s, which is publicly shared as far as I know?[[MET-NEG], [EMP-NEG], [QSN], [MAJ]]\n\nI think the writing needs to be improved on the following points:\n- The abstract doesn't fit well the content of the paper.[[ABS-NEG], [CLA-NEG], [SUG], [MIN]] For instance, \"its variants\" is confusing because there is only other variant to VProp.[[MET-NEU], [CLA-NEG], [DFT], [MIN]] \"Adversarial agents\" is also misleading because those agents act like automata.[[MET-NEU], [CLA-NEG], [DFT], [MIN]]\n\n- The authors should recall more thoroughly and precisely the work of Tamar et al., on which their work is based to make the paper more self-contained, e.g., (1) is hardly understandable.[[RWK-NEU,MET-NEG], [CMP-NEU], [SUG], [MIN]]\n\n- The writing should be careful, e.g., \nvalue iteration is presented as a learning algorithm (which in my opinion is not)[[OAL-NEU], [CLA-NEG], [SUG], [MIN]]\n\\pi^* is defined as a distribution over state-action space and then \\pi is defined as a function; [[EXP-NEU,MET-NEU], [CLA-NEG], [DIS], [MIN]]...\n\n- The mathematical writing should be more rigorous;[[MET-NEU], [PNF-NEU], [SUG], [MIN]] e.g., \np.2:\nT: s \\to a \\to s', \\pi : s \\to a\nA denotes a set and its cardinal\nIn (1), shouldn't it be \\Phi(o)?[[EXP-NEU,MET-NEU], [CLA-NEG], [SUG,QSN], [MIN]]all the new terms should be explained\np. 3:\ndefinition of T and R \nshouldn't V_{ij}^k depend on Q_{aij}^k?\nT_{::aij} should be defined\nIn the definition of h_{aij}, should \\Phi and b be indexed by a?[[OAL-NEG], [CLA-NEG], [SUG,QSN], [MIN]]\n\n- The typos and other issues should be fixed:\np. 3:\nK iteration\nwith capable\np.4:\nclose 0\np.5:\nour our\ns^{t+1} should be defined like the other terms\n\"The state is represented by the coordinates of the agent and 2D environment observation\" should appear much earlier in the paper.[[OAL-NEG], [CLA-NEG], [SUG], [MIN]]\n\"\\pi_\\theta described in the previous sections\", notation \\pi_\\theta appears the first time here...\n3x3 -> 3 \\times 3\nofB\nV_{\\theta^t w^t}[[CNT], [CLA-NEG], [DIS], [MIN]]\np.6:\nthe the\nFig.2's caption:\nWhat does \"both cases\" refer to? They are three models.[[TNF-NEU], [CLA-NEG], [QSN], [MIN]]\nReferences:\net al.\nYI WU\n"[[BIB-NEU], [null], [DIS], [MIN]]