"This paper introduces a new nonlinear activation function for  neural networks, i.e., Inverse Square Root Linear Units (ISRLU).[[INT-NEU,PDI-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] Experiments show that ISRLU is promising compared to competitors like ReLU and ELU.[[PDI-POS,EXP-POS,MET-POS,ANA-POS], [CMP-POS], [APC], [MAJ]]\n\nPros:\n(1) The paper is clearly written.[[INT-POS], [CLA-POS], [APC], [MAJ]]\n\n(2) The proposed ISRLU function has similar curves with ELU and has a learnable parameter \\alpha (although only fixed value is used in the experiments) to control the negative saturation zone.[[PDI-NEU,EXP-NEU,ANA-NEU], [EMP-NEU], [SMY,DIS], [GEN]] \n\nCons:\n(1) Authors claim that ISRLU is faster than ELU, while still achieves ELU\u2019s performance.[[RWK-NEG], [EMP-NEG], [CRT], [MIN]]However, they only show the reduction of computation complexity for convolution, and speed comparison between ReLU, ISRLU and ELU on high-end CPU[[RWK-NEG,MET-NEG], [SUB-NEG,EMP-NEG], [DFT], [MIN]]. As far as I know, even though modern CNNs have reduced convolution\u2019s computation complexity,[[RWK-NEU,MET-NEG,EXT-NEG], [SUB-NEG], [DFT], [GEN]] the computation cost of activation function is still only a very small part (less than 1%) in the overall running time of training/inference[[ANA-NEG,OAL-NEG], [SUB-NEG,IMP-NEG], [DFT], [MIN]]. \n\n(2) Authors only experimented with two very simple CNN architectures and with three different nonlinear activation functions, i.e., ISRLU/ELU/ReLU and showed their accuracies on MNIST[[RWK-NEG,EXP-NEU,MET-NEU], [SUB-NEG,EMP-NEU], [SMY,DFT], [MIN]]. They did not provide the comparison of running time which I believe is important here as the efficiency is emphasized a lot throughout the paper[[INT-NEG,RWK-NEG,ANA-NEG], [SUB-NEG,CMP-NEG], [DFT], [MIN]].\n\n(3) For ISRLU of CNN, experiments on larger scale dataset such as CIFAR or ImageNet would be more convincing[[RWK-POS,DAT-POS,EXP-POS,MET-POS], [IMP-POS,EMP-POS], [APC], [MAJ]]. Moreover, authors also propose ISRU which is similar to tanh for RNN, but do not provide any experimental results.[[RWK-NEU,EXP-NEG,MET-NEU,RES-NEG], [IMP-NEG,CMP-NEG,EMP-NEU], [SMY,DFT,DIS], [GEN]]\n\nOverall, I think the current version of the paper is not ready for ICLR conference.[[OAL-NEG], [APR-NEG,REC-NEG], [FBK], [MIN]] As I suggested above, authors need more experiments to show the effectiveness of their approach.\n"[[PDI-NEG,EXP-NEG], [SUB-NEG], [SUG], [GEN]]