"In this paper, the authors explore how using random projections can be used to make OCSVM robust to adversarially perturbed training data.[[INT-NEU,PDI-NEU,DAT-NEU], [null], [SMY], [GEN]]  While the intuition is nice and interesting,[[PDI-POS], [EMP-POS], [APC], [MAJ]] the paper is not very clear in describing the attack and the experiments do not appropriately test whether this method actually provides robustness.[[EXP-NEG,MET-NEG], [CLA-NEG,EMP-NEG], [CRT], [MAJ]]\n\nDetails:\nhave been successfully in anomaly detection --> have been successfully used in anomaly detectionP[[MET-POS], [EMP-POS], [APC], [MAJ]]\n\n\"The adversary would select a random subset of anomalies, push them towards the normal data cloud and inject these perturbed points into the training set\" -- This seems backwards.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]]  As in the example that follows, if the adversary wants to make anomalies seem normal at test time, it should move normal points outward from the normal point cloud (eg making a 9 look like a weird 7).[[EXP-NEG], [EMP-NEG], [SUG], [MIN]]\n\nAs s_attack increases, the anomaly data points are moved farther away from the normal data cloud, altering the position of the separating hyperplane. -- This seems backwards from Fig 2.[[MET-NEU,TNF-NEG], [EMP-NEU,PNF-NEG], [CRT], [MIN]]   From (a) to (b) the red points move closer to the center while in (c) they move further away (why?).[[MET-NEU], [EMP-NEU], [QSN], [MIN]]   The blue points seem to consistently become more dense from (a) to (c).[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \n\nThe attack model is too rough.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]   It seems that without bounding D, we can make the model arbitrarily bad, no? [[MET-NEG], [EMP-NEG], [QSN], [MAJ]]  Assumption 1 alludes to this but doesn't specify what is \"small\"?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]   Also the attack model is described without considering if the adversary knows the learner's algorithm.[[MET-NEU], [EMP-NEU], [CRT], [MIN]]   Even if there is randomness, can the adversary take actions that account for that randomness?[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \n\nDoes selecting a projection based on compactness remove the randomness?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] \n\nExperiments -- why/how would you have distorted test data?[[DAT-NEU,EXP-NEG], [EMP-NEG], [CRT], [MIN]]   Making an anomaly seem normal by distorting it is easy.[[EXP-NEG], [EMP-NEG], [CRT], [MIN]] \n\nI don't see experiments comparing having random projections and not.[[EXP-NEG], [CMP-NEG], [CRT], [MIN]]   This seems to be the fundamental question -- do random projects help in the train_D | test_C case?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] \n\nExperiments don't vary the attack much to understand how robust the method is."[[MET-NEU], [EMP-NEU], [DIS], [MIN]] 