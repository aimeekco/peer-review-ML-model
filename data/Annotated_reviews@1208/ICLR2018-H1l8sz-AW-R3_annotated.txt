"The paper presents an additive regularization scheme to encourage parameter updates that lead to small changes in prediction (i.e. adjusting updates based on their size in the output space instead of the input space).[[INT-NEU], [null], [SMY], [GEN]] This goal is to achieve a similar effect to that of natural gradient, but with lighter computation.[[RWK-NEU,MET-NEU], [CMP-NEU], [SMY], [MAJ]] The authors claim that their regularization is related to Wasserstein metric (but the connection is not clear to me, read below).[[RWK-NEU,MET-NEU], [null], [SMY], [GEN]] Experiments on MNIST with show improved generalization (but the baseline is chosen poorly, read below).[[RWK-NEG,EXP-POS], [null], [APC], [MAJ]]\n\nThe paper is easy to read and organized very well, and has adequate literature review.[[RWK-POS,OAL-POS], [CLA-POS,PNF-POS], [APC], [MAJ]] However, the contribution of the paper itself needs to be strengthened in both the theory and empirical sides.[[MET-NEU], [EMP-NEU], [SUG], [MAJ]]\n\nOn the theory side, the authors claim that their regularization is based on Wasserstein metric (in the title of the paper as well as section 2.[[MET-NEU], [EMP-NEU], [SMY], [MAJ]]2). However, this connection is not very clear to me [if there is a rigorous connection, please elaborate].[[RWK-NEU,MET-NEU], [CMP-NEG], [CRT], [MAJ]] From what I understand, the authors argue that their proposed loss+regularization is equivalent to the Kantorovich-Rubinstein form.[[RWK-NEU,MET-NEU], [EMP-NEU], [DIS], [MAJ]] However, in the latter, the optimization objective is the f itself (sup E[f_1]-E[f_2]) but in your scheme you propose adding the regularization term (which can be added to any objective function, and then the whole form loses its connection to Wasserstrin metric).[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nOn the practical side, the chosen baseline is very poor.[[RWK-NEU], [null], [CRT], [MAJ]] The authors only experiment with MNIST dataset.[[DAT-NEU,EXP-NEU], [SUB-NEG], [SMY], [MAJ]] The baseline model lacks both \"batch normalization\" and \"dropout\", which I guess is because otherwise the proposed method would under-perform against the baseline.[[RWK-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] It is hard to tell if the proposed regularization scheme is something significant under such poorly chosen baseline.[[RWK-NEG,MET-NEU], [CMP-NEG], [CRT], [MAJ]]\n"