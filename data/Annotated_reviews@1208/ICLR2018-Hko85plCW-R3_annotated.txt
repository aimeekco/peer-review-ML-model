"This paper extends a previously proposed monotonic alignment based attention mechanism by considering local soft alignment across features in a chunk (certain window).[[INT-NEU,PDI-NEU,MET-NEU], [EMP-NEU], [SMY,DIS], [GEN]]  \n\nPros.\n- the paper is clearly written.\[[INT-POS], [CLA-POS], [APC], [MAJ]]n- the proposed method is applied to several sequence-to-sequence benchmarks, and the paper show the effectiveness of the proposed method (comparable to full attention and better than previous hard monotonic assignments)[[PDI-NEU,MET-NEU,ANA-NEU], [CMP-NEU,EMP-NEU], [SMY,DIS], [GEN]].\nCons.\n- in terms of the originality, the methodology of this method is rather incremental from the prior study (Raffel et al),[[RWK-NEU,MET-NEU,BIB-NEU], [NOV-NEU], [SMY], [GEN]] but it shows significant gains from it.[[RWK-NEU], [IMP-POS,EMP-POS], [APC], [MAJ]]\n- in terms of considering a monotonic alignment, Hori et al, \"Advances in Joint CTC-Attention based End-to-End Speech Recognition with a Deep CNN Encoder and RNN-LM,[[RWK-NEU,MET-NEU], [EMP-NEU], [SMY,DIS], [GEN]]\" in Interspeech'17, also tries to solve this issue by combining CTC and attention-based methods.[[RWK-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SMY,DIS], [GEN]] The paper should also discuss this method in Section 4.\[[RWK-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]]n\nComments:\n- Eq. (16): $j$ in the denominator should be $t_j$.\n"[[RWK-NEU], [EMP-NEU], [SMY], [GEN]]