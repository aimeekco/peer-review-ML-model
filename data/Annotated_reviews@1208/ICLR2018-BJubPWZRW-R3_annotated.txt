"This paper proposes a multi-view semi-supervised method.[[INT-NEU,MET-NEU], [null], [SMY], [GEN]] For the unlabelled data, a single input (e.g., a picture) is partitioned into k new inputs permitting overlap.[[MET-NEU], [null], [SMY], [GEN]] Then a new objective is to obtain k predictions as close as possible to the prediction from the model learned from mere labeled data.[[DAT-NEU,MET-NEU], [null], [SMY], [GEN]]\n\nTo be more precise, as seen from the last formula in section 3.1, the most important factor is the D function (or KL distance used here).[[MET-NEU], [null], [SMY], [GEN]] As the author said, we could set the noisy parameter in the first part to zero, but have to leave this parameter non-zero in the second term.[[MET-NEU], [null], [SMY], [GEN]] Otherwise, the model can't learn anything.[[MET-NEU], [null], [DIS], [MIN]]\n\nMy understanding is that the key factor is not the so called k views (as in the first sight, this method resembles conventional ensemble learning very much), but the smoothing distribution around some input x (consistency related loss).[[MET-NEU], [EMP-NEU], [SMY], [GEN]] In another word, we set the k for unlabeled data as 1, but use unlabeled data k times in the scale (assuming no duplicate unlabeled data), keeping the same training (consistency objective) method, would this new method obtain a similar performance?[[DAT-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]] If my understanding is correct, the authors should further discuss the key novelty compared to the previous work stated in the second paragraph of section 1.[[RWK-NEU], [NOV-NEU], [DIS], [GEN]] One obvious merit is that the unlabeled data is utilized more efficiently, k times better.\n\n\n"[[MET-POS], [EMP-POS], [APC], [MAJ]]