"This paper includes several controlled empirical studies comparing MC and TD methods in predicting of value function with complex DNN function approximators.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] Such comparison has been carried out both in theory and practice for simple low dimensional environments with linear (and RKHS) value function approximation showing how TD methods can have much better sample complexity and overall performance compared to pure MC methods.[[PDI-NEU,MET-NEU], [CMP-NEU], [SMY], [GEN]]  This paper shows some results to the contrary when applying RL to complex perceptual observation space.[[RES-NEU], [null], [SMY], [GEN]] \n\nThe main results include:\n(1) In a rollout update a mix of MC and TD update (i.e. a rollout of > 1 and < horizon) outperforms either extreme. [[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]] This is inline with TD-lambda analysis in previous work.[[RWK-NEU,ANA-NEU], [CMP-NEU], [DIS], [GEN]] \n(2) Pure MC methods can outperform TD methods when the rewards becomes noisy.[[MET-POS], [CMP-POS], [APC], [MAJ]] \n(3) TD methods can outperform pure MC methods when the return is mostly dominated by the reward in the terminal state.[[MET-POS], [CMP-POS], [APC], [MAJ]]\n(4) MC methods tend to degrade less when the reward signal is delayed.[[MET-POS], [EMP-POS], [APC], [MAJ]]\n(5) Somewhat surprising: MC methods seems to be on-par with TD methods when the reward is sparse and even longer than the rollout horizon.[[MET-POS], [CMP-POS], [APC], [MAJ]]\n(6) MC methods can outperform TD methods with more complex and high dimensional perceptual inputs.[[MET-POS], [CMP-POS], [APC], [MAJ]]\n\nThe authors conjecture that several of the above observations can be explained by the fact that the training target in MC methods is \"ground truth\" and do not rely on bootstrapping from the current estimates as is done in a TD rollout.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] They suggest that training on such signal can be beneficial when training deep models on complex perceptual input spaces.[[EXP-NEU], [EMP-NEU], [DIS], [GEN]]\n\nThe contributions of the paper are in parts surprising and overall interesting.[[OAL-POS], [EMP-POS], [APC], [MAJ]] I believe there are far more caveats in this analysis than what is suggested in the paper and the authors should avoid over-generalizing the results based on a few domains and the analysis of a small set of algorithms.[[MET-NEG,RES-NEG,ANA-NEG], [SUB-NEG], [CRT], [MIN]] Nonetheless I find the results interesting to the RL community and a starting point to further analysis of the MC methods (or adaptations of TD methods) that work better with image observation spaces.[[RES-POS], [EMP-POS], [APC], [MAJ]] Publishing the code, as the authors mentioned, would certainly help with that.[[MET-NEU], [SUB-NEU], [SUG], [MIN]] \n\nNotes:\n- I find the description of the Q_MC method presented in the paper very confusing and had to consult the reference to understand the details.[[RWK-NEG,MET-NEG], [EMP-NEG], [CRT], [MIN]] Adding a couple of equations on this would improve the readability of the paper.[[MET-NEG], [SUB-NEG,CLA-NEG], [CRT], [MIN]]\n\n- The first mention of partial observability can be moved to the introduction.[[INT-NEU], [PNF-NEU], [SUG], [MIN]]\n\n- Adding results for m=3 to table 2 would bring further insight to the comparison.[[MET-NEU,RES-NEU], [CMP-NEU], [SUG], [MIN]]\n\n- The results for the perceptual complexity experiment seem contradictory and inconclusive.[[INT-NEU], [PNF-NEU], [SUG], [MIN]] One would expect Q_MC to work well in Grid Map domain if the conjecture put forth by the authors was to hold universally.[[MET-NEU], [EMP-NEU], [DIS], [GEN]]\n\n- In the study on reward sparsity, although a prediction horizon of 32 is less than the average steps needed to get to a rewarding state, a blind random walk might be enough to take the RL agent to a close-enough neighbourhood from which a greedy MC-based policy has a direct path to the goal.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] What is missing from this picture is when a blind walk cannot reach such a state, e.g. when a narrow corridor is present in the environment.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Such a case cannot be resolved by a short horizon MC method.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] In other words, a sparse reward setting is only \"difficult\" if getting into a good neighbourhood requires long term planning and cannot be resolved by a (pseudo) blind random walk.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n\n- The extrapolation of the value function approximator can also contribute to why the limited horizon MC method can see beyond its horizon in a sparse reward setting.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] That is, even if there is no way to reach a reward state in 32 steps, an MC value function approximation with horizon 32 can extrapolate from similar looking observed states that have a short path to a rewarding state, enough to be better than a blind random walk.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] It would have been nice to experiment with increasing model complexity to study such effect. "[[EXP-NEU], [EMP-NEU], [DIS], [MIN]]