"In this very good paper, the objective is to perform robust learning: to minimize not only the risk under some distribution P_0, but also against the worst case distribution in a ball around P_0.\[[EXP-POS,OAL-POS], [IMP-POS], [APC], [MAJ]]n\nSince the min-max problem is intractable in general,[[PDI-NEU], [null], [SMY], [GEN]] what is actually studied here is a relaxation of the problem: it is possible to give a non-convex dual formulation of the problem.[[PDI-NEU], [null], [SMY], [GEN]] If the duality parameter is large enough, the functions become convex given that the initial losses are smooth.[[PDI-NEU,EXP-NEU,ANA-NEU], [EMP-NEU], [SMY], [GEN]] \n\nWhat follows are certifiable bounds for the risk for robust  learning and stochastic optimization over a ball of distributions.[[PDI-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] Experiments show that this performs as expected, and gives a good intuition for the reasons why this occurs: separation lines are 'pushed away' from samples, and a margin seems to be increased with this procedure."[[RWK-NEU,PDI-NEU,EXP-NEU], [EMP-NEU], [SMY], [GEN]]