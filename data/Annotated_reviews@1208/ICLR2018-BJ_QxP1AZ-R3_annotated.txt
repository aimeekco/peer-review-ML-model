"The paper proposes a method for few-shot learning using a new image representation called visual concept embedding.[[INT-NEU,MET-NEU], [null], [SMY], [GEN]] Visual concepts were introduced in Wang et al. 2015, which are clustering centers of feature vectors in a lattice of a CNN.[[INT-NEU,RWK-NEU], [null], [SMY], [GEN]] For a given image, its visual concept embedding is computed by thresholding the distances between feature vectors in the lattice of the image to the visual concepts.[[MET-NEU], [null], [SMY], [GEN]] Using the visual concept embedding, two simple methods are used for few-shot learning: a nearest neighbor method and a probabilistic model with Bernoulli distributions.[[MET-NEU], [null], [SMY], [GEN]] Experiments are conducted on the Mini-ImageNet dataset and the PASCAL3D+ dataset for few-shot learning.[[DAT-NEU,EXP-NEU], [null], [SMY], [GEN]]\n\nPositives:\n- The three properties of visual concepts described in the paper are interesting.[[MET-POS], [EMP-POS], [APC], [MAJ]]\n\nNegatives:\n- The novelty of the paper is limited.[[OAL-NEG], [NOV-NEG], [CRT], [MIN]] The idea of visual concept has been proposed in Wang et al. 2015.[[PDI-NEG], [NOV-NEG], [CRT], [MAJ]] Using a embedding representation based on visual concepts is straightforward.[[PDI-NEG], [EMP-NEG], [CRT], [MAJ]] The two baseline methods for few-shot learning provide limited insights in solving the few-shot learning problem.[[RWK-NEG,MET-NEG], [SUB-NEG], [CRT], [MIN]]\n\n- The paper uses a hard thresholding  in the visual concept embedding.[[MET-NEG], [EMp-NEG], [CRT], [MAJ]] It would be interesting to see the performance of other strategies in computing the embedding, such as directly using the distances without thresholding."[[ANA-NEU], [SUB-NEU], [SUG], [MIN]]