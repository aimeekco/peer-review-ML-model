"In this paper an alternating optimization approach is explored for training Auto Encoders (AEs).[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]]\nThe authors treat each layer as a generalized linear model, and suggest to use the stochastic normalized GD of [Hazan et al., 2015] as the minimization algorithm in each (alternating) phase.[[RWK-NEU,MET-NEU], [null], [SMY], [GEN]]\nThen they apply the suggested method to several single layer and multi layer AEs, comparing its performance to standard SGD.[[MET-NEU], [null], [SMY], [GEN]] The paper suggests an interesting approach and provides experimental evidence for its usefulness, especially for multi-layer AEs.[[EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]]\n\n\nSome comments on the theoretical part:\n-The theoretical part is partly misleading.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] While it is true that every layer can be treated a generalized linear model, the SLQC property only applies for the last layer.[[MET-NEU], [EMp-NEU], [DIS], [MIN]]\nRegarding the intermediate layers, we may indeed treat them as generalized linear models, but with non-monotone activations, and therefore the SLQC property does not apply.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]]\nThe authors should mention this point.[[RES-NEG], [SUB-NEG], [DFT], [MAJ]]\n\n-Showing that generalized ReLU is SLQC with a polynomial dependence on the domain is interesting.[[MET-POS], [EMP-POS], [APC], [MAJ]] \n\n-It will be interesting if the authors can provide an analysis/relate to some theory related to alternating minimization of bi-quasi-convex objectives.[[ANA-NEU,MET-NEU], [SUB-NEU], [SUG], [MIN]] Concretely: Is there any known theory for such objectives?[[RWK-NEU,ANA-NEU], [CMP-NEU], [QSN], [MIN]] What guarantees can we hope to achieve?[[RES-NEU], [EMP-NEU], [QSN], [MIN]]\n\n\nThe extension to muti-layer AEs makes sense and seems to works quite well in practice[[MET-POS], [EMP-POS], [APC], [MAJ]].\n\nThe experimental part is satisfactory, and seems to be done in a decent manner.[[EXP-POS], [EMP-POS], [APC], [MAJ]] \nIt will be useful if the authors could relate to the issue of parameter tuning for their algorithm.[[MET-NEU], [EMP-NEU], [SUG], [MIN]]\nConcretely: How sensitive/robust is their approach compared to SGD with respect to hyperparameter misspecification.\n"[[MET-NEU], [CMP-NEU], [QSN], [MIN]]