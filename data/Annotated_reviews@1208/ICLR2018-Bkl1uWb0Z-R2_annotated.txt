"This paper describes a method to induce source-side dependency structures in service to neural machine translation.[[INT-NEU], [null], [SMY], [GEN]] The idea of learning soft dependency arcs in tandem with an NMT objective is very similar to recent notions of self-attention (Vaswani et al., 2017, cited) or previous work on latent graph parsing for NMT (Hashimoto and Tsuruoka, 2017, cited).[[RWK-NEU,PDI-NEU], [CMP-NEU], [SMY], [MAJ]] This paper introduces three innovations: (1) they pass the self-attention scores through a matrix-tree theorem transformation to produce marginals over tree-constrained head probabilities; (2) they explicitly specify how the dependencies are to be used, meaning that rather than simply attending over dependency representations with a separate attention, they select a soft word to attend to through the traditional method, and then attend to that word\u2019s soft head (called Shared Attention in the paper); and (3) they gate when attention is used.[[MET-NEU], [null], [SMY], [GEN]] I feel that the first two ideas are particularly interesting. [[PDI-POS], [null], [APC], [MAJ]]Unfortunately, the results of the NMT experiments are not particularly compelling, with overall gains over baseline NMT being between 0.6 and 0.8 BLEU.[[EXP-NEU,RES-NEG], [EMP-NEG], [CRT], [MAJ]] However, they include a useful ablation study that shows fairly clearly that both ideas (1) and (2) contribute equally to their modest gains, and that without them (FA-NMT Shared=No in Table 2), there would be almost no gains at all.[[PDI-POS,MET-POS], [EMP-POS], [SMY], [MAJ]] Interesting side-experiments investigate their accuracy as a dependency parser, with and without a hard constraint on the system\u2019s latent dependency decisions.[[EXP-POS], [EMP-POS], [APC], [MAJ]]\n\nThis paper has some very good ideas, and asks questions that are very much worth asking.[[PDI-POS], [IMP-POS], [APC], [MAJ]] In particular, the question of whether a tree constraint is useful in self-attention is very worthwhile.[[MET-POS], [EMP-POS], [SMY], [MAJ]] Unfortunately, this is mostly a negative result, with gains over \u201cflat attention\u201d being relatively small.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] I also like the \u201cShared Attention\u201d - it makes a lot of sense to say that if the \u201csemantic\u201d attention mechanism has picked a particular word, one should also attend to that word\u2019s head; it is not something I would have thought of on my own.[[MET-POS], [EMP-POS], [APC], [MAJ]] The paper is also marred by somewhat weak writing, with a number of disfluencies and awkward phrasings making it somewhat difficult to follow.[[OAL-NEG], [CLA-NEG], [CRT], [MAJ]]\n\nIn terms of specific criticisms:\n\nI found the motivation section to be somewhat weak.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] We need a better reason than morphology to want to do source-side dependency parsing.[[MET-NEU], [EMP-NEU], [SUG], [MAJ]] All published error analyses of strong NMT systems (Bentivogli et al, EMNLP 2016; Toral and Sanchez-Cartagena, EACL 2017; Isabelle et al, EMNLP 2017) have shown that morphology is a strength, not a weakness of these systems, and the sorts of head selection problems shown in Figure 1 are, in my experience, handled capably by existing LSTM-based systems.[[RWK-NEU,EXP-POS,TNF-NEU,BIB-NEU], [EMP-POS], [CRT], [MAJ]]\n\nThe paper mentions \u201csignificant improvements\u201d in only two places: the introduction and the conclusion.[[INT-POS,ANA-POS], [IMP-NEU], [SMY], [MAJ]] With BLEU score differences being so low, the authors should specify how statistical significance is measured;[[EXP-NEU,RES-NEG], [IMP-NEU], [SUG], [MAJ]] ideally using a technique that accounts for the variance of random restarts (i.e.: Clark et al, ACL 2011).[[MET-NEU,BIB-NEU], [EMP-NEU], [SMY], [MAJ]]\nEquation (3): I couldn\u2019t find the definition for H anywhere.[[MET-NEG], [null], [CRT], [MIN]]\n\nSentence before Equation (5): I believe there is a typo here, \u201cf takes z_i\u201d should be \u201cf takes u_t\u201d.[[MET-NEU], [CLA-NEG], [CRT], [MIN]]\n\nFirst section of Section 3: please cite the previous work you are talking about in this sentence.[[RWK-NEU,BIB-NEU], [null], [SUG], [GEN]]\n\nMy understanding was that the dependency marginals in p(z_{i,j}=1|x,\\phi) in Equation (11) are directly used as \\beta_{i,j}.[[MET-NEU], [null], [DIS], [MAJ]] If I\u2019m correct, that\u2019s probably worth spelling out explicitly in Equation (11): \\beta_{i,j} = p(z_{i,j}=1|x,\\phi) = \u2026.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]]\n\nI don\u2019t don\u2019t feel like the clause between equations (17) and (18), \u201cwhen sharing attention weights from the decoder with the encoder\u201d is a good description of your clever \u201cshared attention\u201d idea.[[PDI-POS,MET-NEU], [EMP-NEU], [DIS], [MAJ]] In general, I found this region of the paper, including these two equations and the text between them, very difficult to follow.[[MET-NEU], [CLA-NEG], [CRT], [MAJ]]\n\nSection 4.4: It\u2019s very very good that you compared to \u201cflat attention\u201d,[[RWK-NEU], [CMP-POS], [APC], [MAJ]] but it\u2019s too bad for everyone cheering for linguistically-informed syntax that the results weren\u2019t better.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nTable 5: I had a hard time understanding Table 5 and the corresponding discussion.[[TNF-NEG], [CLA-NEG], [CRT], [MIN]] What are \u201cproduction percentages\u201d?[[EXP-NEU], [null], [QSN], [MIN]]\n\nFinally, it would have been interesting to include the FA system in the dependency accuracy experiment (Table 4), to see if it made a big difference there.[[MET-NEU,TNF-NEU], [EMP-NEU], [SUG], [MAJ]]"