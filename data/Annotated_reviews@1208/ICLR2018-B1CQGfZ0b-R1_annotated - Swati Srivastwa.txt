"The paper proposes a method for identifying representative examples for program\nsynthesis to increase the scalability of existing constraint programming\nsolutions.[[INT-NEU,PDI-NEU,MET-NEU], [null], [SMY], [MIN]]  The authors present their approach and evaluate it empirically.[[EXP-NEU,MET-NEU], [null], [SMY], [MIN]] \n\nThe proposed approach is interesting,[[MET-POS], [EMP-POS], [APC], [MAJ]]  but I feel that the experimental section\ndoes not serve to show its merits for several reasons.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]]  First, it does not\ndemonstrate increased scalability. [[EXP-NEG], [EMP-NEG], [CRT], [MAJ]] Only 1024 examples are considered, which is\nby no means large.[[DAT-NEG], [SUB-NEG], [DFT,CRT], [MAJ]]  Even then, the authors approach selects the highest number of\nexamples (figure 4). [[DAT-NEU,MET-NEU], [SUB-NEU], [DIS], [MIN]]CEGIS both selects fewer examples and has a shorter median\ntime for complete synthesis.[[DAT-NEU,MET-NEU], [SUB-NEG], [DFT], [MIN]] Intuitively, the authors' method should scale\nbetter, but they fail to show this -- a missed opportunity to make the paper\nmuch more compelling.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] This is especially true as a more challenging benchmark\ncould be created very easily by simply scaling up the image.[[MET-NEU,FWK-NEU], [IMP-NEU], [SUG], [MIN]]\n\nSecond, there is no analysis of the representativeness of the found sets of\nconstraints.[[ANA-NEG], [SUB-NEG], [DFT], [MAJ]] Given that the results are very close to other approaches, it\nremains unclear whether they are simply due to random variations, or whether the\nproposed approach actually achieves a non-random improvement.[[EXP-NEG,MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nIn addition to my concerns about the experimental evaluation, I have concerns\nabout the general approach.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MIN]] It is unclear to me that machine learning is the\nbest approach for modeling and solving this problem.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] In particular, the\nselection probability of any particular example could be estimated through a\nheuristic, for example by simply counting the number of neighbouring examples\nthat have a different color, weighted by whether they are in the set of examples\nalready, to assess its \"borderness\", with high values being more important to\nachieve a good program.[[MET-NEU,RES-NEU], [EMP-NEG], [DIS], [MIN]] The border pixels are probably sufficient to learn the\nprogram perfectly, and in fact this may be exactly what the neural net is\nlearning.[[MET-NEU], [CNT], [DIS], [MIN]] The above heuristic is obviously specific to the domain, but similar\nheuristics could be easily constructed for other domains.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] I feel that this is\nsomething the authors should at least compare to in the empirical evaluation.[[EXP-NEG,MET-NEG], [CMP-NEG], [DFT], [MAJ]]\n\nAnother concern is that the authors' approach assumes that all parameters have\nthe same effect.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Even for the example the authors give in section 2, it is\nunclear that this would be true.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nThe text says that rand+cegis selects 70% of examples of the proposed approach,\nbut figure 4 seems to suggest that the numbers are very close -- is this initial\nexamples only?[[MET-NEU], [EMP-NEU], [QSN], [MAJ]]\n\nOverall the paper appears rushed -- the acknowledgements section is left over\nfrom the template and there is a reference to figure \"blah\".[[TNF-NEG,OAL-NEG], [PNF-NEG], [CRT], [MIN]] There are typos and\ngrammatical mistakes throughout the paper.[[OAL-NEG], [CLA-NEG], [CRT], [MAJ]] The reference to \"Model counting\" is\nincomplete.[[BIB-NEG], [SUB-NEG], [DFT], [MIN]]\n\nIn summary, I feel that the paper cannot be accepted in its current form."[[OAL-NEG], [REC-NEG], [FBK], [MAJ]]