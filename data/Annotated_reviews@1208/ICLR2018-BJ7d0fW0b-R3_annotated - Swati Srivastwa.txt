"The authors propose to speed up RL techniques, such as DQN, by utilizing expert demonstrations.[[INT-NEU], [null], [SMY], [GEN]]  The  expert demonstrations are sequences of consecutive states that do not include actions, which is closer to a real setting of imitation learning.[[PDI-NEU], [null], [SMY], [GEN]] The goal of this process is to extract a function that maps any given state to a subgoal.[[MET-NEU], [null], [SMY], [GEN]] Subgoals are then used to learn different Q-value functions, one per subgoal.[[MET-NEU], [null], [SMY], [GEN]] \nTo learn the function that maps states into subgoals, the authors propose a surrogate reward model that corresponds to the angle between: the difference between two consecutive states (which captures velocity or direction) and a given subgoal.[[MET-NEU], [null], [SMY], [GEN]] A von Mises- Fisher distribution policy is then assumed to be used by the expert to generate actions that guide the agent toward the subgoal.[[MET-NEU], [null], [SMY], [MIN]] Finally, the mapping function state->subgoal is learned by performing a gradient descent on the expected total cost (based on the surrogate reward function, which also has free parameters that need to be learned).[[MET-NEU], [null], [SMY], [MIN]]\nFinally, the authors use the DQN platform to learn a Q-value function using the learned  surrogate reward function that guides the agent to specific subgoals, depending on the situation.[[MET-NEU], [null], [SMY], [MIN]]\nThe paper is overall well-written, and the proposed idea seems interesting.[[PDI-POS,OAL-POS], [CLA-POS,EMP-POS], [APC], [MAJ]] However, there are rather little explanations provided to argue for the different modeling choices made, and the intuition behind them.[[EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]] From my understanding, the idea of subgoal learning boils down to a non-parametric (or kernel) regression where each state is mapped to a subgoal based on its closeness to different states in the expert's demonstration.[[PDI-NEU,MET-NEU], [null], [DIS], [GEN]] It is not clear how this method would generalize to new situations.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] There is also the issue of keeping tracking of a large number of demonstration states in memory. This technique reminds me of some common methods in learning from demonstrations, such as those using GPs or GMMs, but the novelty of this technique is the fact that the subgoal mapping function is learned in an IRL fashion, by tacking into account the sum of surrogate rewards in the expert's demonstration.[[MET-POS], [NOV-POS], [APC], [MAJ]] \nThe architecture of the action value estimator does not seem novel, it's basically just an extension of DQN with an extra parameter (subgoal g).[[PDI-NEG], [NOV-NEG], [CRT], [MIN]]\nThe empirical evaluation seems rather mixed.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MIN]] Figure 3 shows that the proposed method learns faster than DQN,[[MET-POS], [CMP-POS], [APC], [MAJ]] but Table I shows that the improvement is not statistically significant, except in two games, DefendCenter and PredictPosition.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] Are these the results after all agents had converged?[[RES-NEU], [EMP-NEU], [QSN], [MIN]] \nOverall, this is a good paper,[[OAL-POS], [CNT], [APC], [MAJ]] but focusing on only a single game (Doom) is a weakness that needs to be addressed because one cannot tell if the choices were tailored to make the method work well for this game.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Since the paper does not provide significant theoretical or algorithmic contribution, at least more realistic and diverse experiments should be performed. "[[EXP-NEG,MET-NEG], [EMP-NEG], [SUG,CRT], [MIN]]