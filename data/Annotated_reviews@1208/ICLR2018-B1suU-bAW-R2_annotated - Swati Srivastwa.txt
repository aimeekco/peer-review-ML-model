"The authors present a method for learning word embeddings from related groups of data.[[INT-NEU], [null], [DIS], [MIN]] The model is based on tensor factorization which extends GloVe to higher order co-ocurrence tensors, where the co-ocurrence is of words within subgroups of the text data.[[MET-NEU,DAT-NEU], [null], [SMY], [GEN]] These two papers need to be cited:\n\nRudolph et al., NIPS 2017, \"Sturctured Embedding Models for Grouped Data\":[[BIB-NEU], [null], [DIS], [MIN]] This paper also presents a method for learning embeddings specific for subgroups of the data, but based on hierarchical modeling.[[MET-NEU], [null], [SMY], [GEN]] An experimental comparison is needed.[[EXP-NEU], [CMP-NEU], [SUG], [GEN]]\n\nCotterell et al., EACL 2017 \"Explaining and Generalizing Skip-Gram through Exponential Family Principal Component Analysis\": This paper also derives a tensor factorization based approach for learning word embeddings for different covariates.[[RWK-NEU], [CMP-NEU], [DIS], [MIN]] Here the covariates are morphological tags such as part-of-speech tags of the words.[[MET-NEU], [null], [DIS], [GEN]]\n\nDue to these two citations, the novelty of both the problem set-up of learning different embeddings for each covariate and the novelty of the tensor factorization based model are limited.[[RWK-NEG,pDI-NEG], [NOV-NEG], [CRT], [MIN]]\n\nThe writing is ok.[[OAL-NEU], [CLA-NEU], [DIS], [MIN]] I appreciated the set-up of the introduction with the two questions.[[INT-POS], [null], [APC], [MAJ]] However, the questions themselves could have been formulated differently: \nQ1: the way Q1 is formulated makes it sound like the covariates could be both discrete and continuous while the method presented later in the paper is only for discrete covariates (i.e. group structure of the data).[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\nQ2: The authors mention topic alignment without specifying what the topics are aligned to.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] It would be clearer if they stated explicitly that the alignment is between covariate-specific embeddings.[[MET-NEU], [EMP-NEU], [SUG], [MIN]] It is also distracting that they call the embedding dimensions topics.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\nAlso, why highlight the problem of authorship attribution of Shakespear's work in the introduction, if that problem is not addressed later on?[[INT-NEU], [null], [QSN], [MIN]]\n\nIn the model section, the paragraphs \"notation\" and \"objective function and discussion\" are clear.[[MET-POS], [CLA-POS], [APC], [MAJ]]  I also liked the idea of having the section \"A geometric view of embeddings and tensor decomposition\", but that section needs to be improved.[[MET-POS], [EMP-POS], [DIS], [MIN]] For example, the authors describe RandWalk (Arora et al. 2016) but how their work falls into that framework is unclear.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\nIn the third paragraph, starting with \"Therefore we consider a natural extension of this model, ...\" it is unclear which model the authors are referring to. (RandWalk or their tensor factorization?).[[RWK-NEG,MET-NEG], [CMP-NEG], [CRT], [MIN]]\nWhat are the context vectors in Figure 1? [[TNF-NEU], [EMP-NEU], [DIS], [MIN]] I am guessing the random walk transitions are the ellipsoids?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] How are they to be interpreted?[[TNF-NEG], [EMP-NEU], [QSN], [MIN]] \n\nIn the last paragraph, beginning with \"Note that this is essentially saying...\", I don't agree with the argument that the \"base embeddings\" decompose into independent topics.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] The dimensions of the base embeddings are some kind of latent attributes and each individual dimension could be used by the model to capture a variety of attributes.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] There is nothing that prevents the model from using multiple dimensions to capture related structure of the data.[[MET-NEU], [null], [DIS], [MIN]] Also, the qualitative results in Table 3 do not convince me that the embedding dimensions represent topics.[[RES-NEG,TNF-NEG], [EMP-NEG], [CRT], [MIN]] For example \"horses\" has highest value in embedding dimension 99.[[RES-NEG], [EMP-NEG], [CRT], [MIN]] It's nearest neighbours in the embedding space (i.e. semantically similar words) will also have high values in coordinate 99.[[RES-NEU], [EMP-NEU], [DIS], [MIN]] Hence, the apparent semantic coherence in what the authors call \"topics\".[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\nThe authors present multiple qualitative and quantitative evaluations.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] The clustering by weight (4.1.) is nice and convincing that the model learns something useful.[[MET-POS], [EMP-POS], [APC], [MAJ]] 4.2, the only quantitative analysis was missing some details.[[ANA-NEG], [SUB-NEG], [DFT], [MAJ]] Please give references for the evaluation metrics used, for proper credit and so people can look up these tasks.[[BIB-NEU], [null], [DIS], [MIN]] Also, comparison needed to fitting GloVe on the entire corpus (without covariates) and existing methods Rudolph et al. 2017 and Cotterell et al. 2017.[[RWK-NEU], [CMP-NEU], [SUG], [MIN]]   \nSection 5.2 was nice and so was 5.3.[[CNT], [null], [APC], [MAJ]] However, for the covariate specific analogies (5.3.) the authors could also analyze word similarities without the analogy component and probably see similar qualitative results.[[RES-NEU,ANA-NEU], [EMP-NEU], [SUG], [MIN]] Specifically, they could analyze for a set of query words, what the most similar words are in the embeddings obtained from different subsections of the data.[[DAT-NEU,ANA-NEU], [EMP-NEU], [SUG], [MIN]]\n\nPROS:\n+ nice tensor factorization model for learning word embeddings specific to discrete covariates.[[MET-POS], [EMP-POS], [APC], [MAJ]]\n+ the tensor factorization set-up ensures that the embedding dimensions are aligned \n+ clustering by weights (4.1) is useful and seems coherent\n+ covariate-specific analogies are a creative analysis[[ANA-POS], [EMP-POS], [APC], [MAJ]]\n\nCONS:\n- problem set-up not novel and existing approach not cited (experimental comparison needed)[[PDI-NEG], [NOV-NEG], [CRT], [MAJ]]\n- interpretation of embedding dimensions as topics not convincing[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n- connection to Rand-Walk (Aurora 2016) not stated precisely enough[[MET-NEG], [CLA-NEG], [CRT], [MIN]]\n- quantitative results (Table 1) too little detail:\n        * why is this metric appropriate[[RES-NEU], [EMP-NEU], [QSN], [MIN]]?\n        * comparison to GloVe on the entire corpus (not covariate specific)\n        * no reference for the metrics used (AP, BLESS, etc.?)[[RWK-NEU], [CMP-NEU], [QSN], [MIN]]\n- covariate specific analogies presented confusingly and similar but simpler analysis might be possible by looking at variance in neighbours v_b and v_d without involving v_a and v_c (i.e. don't talk about analogies but about similarities)"[[MET-NEU,ANA-NEU], [null], [SUG], [MIN]]