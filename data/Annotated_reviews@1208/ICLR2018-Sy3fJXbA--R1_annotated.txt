"The authors extend the ResNeXt architecture.[[INT-NEU], [null], [SMY], [GEN]] They substitute the simple add operation with a selection operation for each input in the residual module.[[INT-NEU, PDI-NEU], [null], [SMY], [GEN]] The selection of the inputs happens through gate weights, which are sampled at train time.[[EXP-NEU,MET-NEU], [null], [SMY], [GEN]] At test time, the gates with the highest values are kept on, while the other ones are shut.[[EXP-NEU], [null], [SMY], [GEN]] The authors fix the number of the allowed gates to K out of C possible inputs (C is the multi-branch factor in the ResNeXt modules).[[EXP-NEU,MET-NEU], [null], [SMY], [GEN]] They show results on CIFAR-100 and ImageNet (as well as mini ImageNet).[[DAT-NEU,RES-NEU], [null], [SMY], [GEN]] They ablate the choice of K, the binary nature of the gate weights.[[MET-NEU], [null], [SMY], [GEN]]\n\nPros:\n(+) The paper is well written and the method is well explained[[MET-POS,OAL-POS], [CLA-POS,EMP-POS], [APC], [MAJ]]\n(+) The authors ablate and experiment on large scale datasets[[EXP-POS, DAT-POS], [SUB-POS], [APC], [MAJ]]\n\nCons:\n(-) The proposed method is a simple extension of ResNeXt[[MET-NEG], [CMP-NEG], [CRT], [MIN]] \n(-) The gains are reasonable,[[RES-POS], [EMP-POS], [APC], [MIN]] yet not SOTA, and come at a price of more complex training protocols (see below)[[RWK-NEG, EXP-NEG], [CMP-NEG], [CRT], [MIN]]\n(-) Generalization to other tasks not shown[[ANA-NEG], [SUB-NEG], [DFT], [MIN]]\n\nThe authors do a great job walking us through the formulation and intutition of their proposed approach.[[MET-POS], [EMP-POS], [CRT], [MIN]] They describe their training procedure and their sampling approach for the gate weights.[[EXP-NEU, MET-NEU], [null], [SMY], [MIN]] However, the training protocol gets complicated with the introduction of gate weights.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]] In order to train the gate weights along with the network parameters, the authors need to train the parameters jointly followed by the training of only the network parameters while keeping the gates frozen.[[EXP-NEU], [EMP-NEU], [SUG,DIS], [MAJ]] This makes training of such networks cumbersome.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]]\n\nIn addition, the authors report a loss in performance when the gates are not discretized to {0,1}.[[EXP-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] This means that a liner combination with the real-valued learned gate parameters is suboptimal.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]] Could this be a result of suboptimal, possibly compromised training?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]] \n\nWhile the CIFAR-100 results look promising, the ImageNet-1k results are less impressive.[[DAT-POS, RES-POS], [EMP-POS], [APC], [MAJ]] The gains from introducing gate weights in the input of the residual modules vanish when increasing the network size.[[RES-NEG], [EMP-NEG], [CRT], [MIN]] \n\nLast, the impact of ResNeXt/ResNet lies in their ability to generalize to other tasks.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Have the authors experimented with other tasks, e.g. object detection, to verify that their approach leads to better performance in a more diverse set of problems?\n"[[EXP-NEU,MET-NEU], [SUB-NEU], [QSN], [MIN]]