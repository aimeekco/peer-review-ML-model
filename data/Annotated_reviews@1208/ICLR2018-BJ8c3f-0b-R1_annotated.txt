"Update:\nOn further consideration (and reading the other reviews), I'm bumping my rating up to a 7.[[OAL-POS], [REC-POS], [FBK], [MAJ]] I think there are still some issues, but this work is both valuable and interesting, and it deserves to be published (alongside the Naesseth et al. and Maddison et al. work).[[OAL-POS], [REC-POS], [FBK], [MAJ]]\n\n-----------\n\nThis paper proposes a version of IWAE-style training that uses SMC instead of classical importance sampling.[[INT-NEU], [null], [SMY], [GEN]] Going beyond the several papers that proposed this simultaneously, the authors observe a key issue: the variance of the gradient of these IWAE-style bounds (w.r.t. the inference parameters) grows with their accuracy.[[RWK-NEU,PDI-NEU], [null], [SMY], [GEN]] They therefore propose using a more-biased but lower-variance bound to train the inference parameters, and the more-accurate bound to train the generative model.[[EXP-POS], [EMP-POS], [APC], [MAJ]]\n\nOverall, I found this paper quite interesting.[[OAL-POS], [EMP-POS], [APC], [MAJ]] There are a few things I think could be cleared up, but this seems like good work (although I'm not totally up to date on the very recent literature in this area).[[OAL-POS], [EMP-POS], [APC], [MAJ]]\n\nSome comments:\n\n* Section 4: I found this argument extremely interesting.[[CNT], [null], [APC], [MAJ]] However, it\u2019s worth noting that your argument implies that you could get an O(1) SNR by averaging K noisy estimates of I_K.[[MET-POS], [EMP-POS], [APC], [MAJ]] Rainforth et al. suggest this approach, as well as the approach of averaging K^2 noisy estimates, which the theory suggests may be more appropriate if the functions involved are sufficiently smooth, which even for ReLU networks that are non-differentiable at a finite number of points I think they should be.[[RWK-NEU,MET-NEU], [CMP-NEU,EMP-NEU], [SUG,DIS], [MIN]]\n\nThis paper would be stronger if it compared with Rainforth et al.\u2019s proposed approaches.[[RWK-NEU,MET-NEU], [CMP-NEU,EMP-NEU], [SUG,DIS], [MIN]] This would demonstrate the real tradeoffs between bias, variance, and computation.[[MET-NEU], [CMP-NEU], [DIS], [MIN]] Of course, that involves O(K^2) or O(K^3) computation, which is a weakness.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]  But one could use a small value of K (say, K=5).[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\nThat said, I could also imagine a scenario where there is no benefit to generating multiple noisy samples for a single example versus a single noisy sample for multiple examples.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Basically, these all seem like interesting and important empirical questions that would be nice to explore in a bit more detail.[[MET-NEU,ANA-NEU], [SUB-NEU], [SUG], [MIN]]\\n\n* Section 3.3: Claim 1 is an interesting observation.[[MET-POS], [EMP-POS], [APC], [MAJ]] But Propositions 1 and 2 seem to just say that the only way to get a perfectly tight SMC ELBO is to perfectly sample from the joint posterior.[[MET-NEU,RES-NEU], [null], [DIS], [MIN]] I think there\u2019s an easier way to make this argument:\n\nGiven an unbiased estimator \\hat{Z} of Z, by Jensen\u2019s inequality E[log \\hat{Z}] \u2264 log Z, with equality iff the variance of \\hat{Z} = 0.[[MET-NEU], [null], [DIS], [MIN]] The only way to get an SMC estimator\u2019s variance to 0 is to drive the variance of the weights to 0.[[MET-NEU,RES-NEU], [EMP-NEU], [DIS], [MIN]] That only happens if you perfectly sample each particle from the true posterior, conditioned on all future information.[[EXP-NEU], [null], [DIS], [MIN]]\n\nAll of which is true as far as it goes, but I think it\u2019s a bit of a distraction.[[EXP-NEG], [EMP-NEG], [CRT], [MIN]] The question is not \u201cwhat\u2019s it take to get to 0 variance\u201d but \u201chow quickly can we approach 0 variance\u201d.[[MET-NEU], [null], [DIS], [MIN]] In principle IS and SMC can achieve arbitrarily high accuracy by making K astronomically large.[[MET-NEU,RES-NEU], [EMP-NEU], [DIS], [MIN]] (Although [particle] MCMC is probably a better choice if one wants extremely low bias.)[[MET-POS], [EMP-POS], [APC], [MAJ]]\n\n* Section 3.2: The choice of how to get low-variance gradients through the ancestor-sampling choice seems seems like an important technical challenge in getting this approach to work, but there\u2019s only a very cursory discussion in the main text.[[MET-POS], [EMP-POS], [APC], [MAJ]] I would recommend at least summarizing the main findings of Appendix A in the main text.[[CNT], [null], [SUG], [MIN]]\n\n* A relevant missing citation: Turner and Sahani\u2019s \u201cTwo problems with variational expectation maximisation for time-series models\u201d (http://www.gatsby.ucl.ac.uk/~maneesh/papers/turner-sahani-2010-ildn.pdf).[[BIB-NEG], [null], [DFT], [MIN]] They discuss in detail some examples where tighter variational bounds in state-space models lead to worse parameter estimates (though in a quite different context and with a quite different analysis).[[DAT-NEU,ANA-NEU], [null], [DIS], [MIN]]\n\n* Figure 1: What is the x-axis here?[[MET-NEU,TNF-NEU], [null], [QSN], [MIN]] Presumably phi is not actually 1-dimensional?[[MET-NEU,TNF-NEU], [EMP-NEU], [QSN], [MIN]]\n\nTypos etc.:\n\n* \u201clearn a particular series intermediate\u201d missing \u201cof\u201d.[[CNT], [CLA-NEU], [CRT], [MIN]]\n\n* \u201cTo do so, we generate on sequence y1:T\u201d s/on/a/, I think?[[MET-NEU], [CLA-NEU], [QSN], [MIN]]\n\n* Equation 3: Should there be a (1/K) in Z?"[[MET-NEU], [CLA-NEU], [QSN], [MIN]]