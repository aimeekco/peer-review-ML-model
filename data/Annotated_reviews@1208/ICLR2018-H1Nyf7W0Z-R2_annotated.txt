"This paper considers a dichitomy between ML and RL based methods for sequence generation.[[INT-NEU], [null], [SMY], [GEN]] It is argued that the ML approach has some \"discrepancy\" between the optimization objective and the learning objective, and the RL approach suffers from bad sample complexity.[[RWK-NEU], [CMP-POS], [SMY], [GEN]] An alpha-divergence formulation is considered to combine both methods.[[MET-NEU], [null], [SMY], [GEN]]\n\nUnfortunately, I do not understand main points made in this paper and am thus not able to give an accurate evaluation of the technical content of this paper. [[OAL-NEG], [EMP-NEG], [DFT], [MAJ]]I therefore have no option but to vote for reject of this paper, based on my educated guess.[[OAL-NEG], [REC-NEG], [FBK], [MAJ]] \n\nBelow are the points that I'm particularly confused about:\n\n1. For the ML formulation, the paper made several particularly confusing remarks. Some of them are blatantly wrong to me[[MET-NEG], [EMP-NEG], [DFT], [MAJ]]. For example, \n\n1.1 The q(.|.) distribution in Eq. (1) *cannot* really be the true distribution, because the true distribution is unknown and therefore cannot be used to construct estimators.[[MET-NEG], [EMP-NEG], [DFT], [MAJ]] From the context, I guess the authors mean \"empirical training distribution\"?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\n1.2 I understand that the ML objective is different from what the users really care about (e.g., blue score), but this does not seem a \"discrepancy\" to me.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] The ML estimator simply finds a parameter that is the most consistent to the observed sequences; and if it fails to perform well in some other evaluation criterion such as blue score, it simply means the model is inadequate to describe the data given, or the model class is so large that the give number of samples is insufficient, and as a result one should change his/her modeling to make it more apt to describe the data at hand. [[MET-NEU], [EMP-NEU], [DIS], [GEN]]In summary, I'm not convinced that the fact that ML optimizes a different objective than the blue score is a problem with the ML estimator.[[MET-NEG], [EMP-NEG], [DIS], [MIN]]\n\nIn addition, I don't see at all why this discrepancy is a discrepancy between training and testing data.[[DAT-NEG], [SUB-NEU], [DFT], [MAJ]] As long as both of them are identically distributed, then no discrepancy exists.[[DAT-NEU], [SUB-NEU], [DIS], [GEN]]\n\n1.3 In point (ii) under the maximum likelihood section, I don't understand it at all and I think both sentences are wrong.[[MET-NEG], [EMP-NEG], [DFT], [MAJ]] First, the model is *not* trained on the true distribution which is unknown.[[DAT-NEU,MET-NEG], [EMP-NEU], [DFT], [MAJ]] The model is trained on an empirical distribution whose points are sampled from the true distribution.[[MET-NEU], [null], [SMY], [GEN]] I also don't understand why it is evaluated using p_theta; if I understand correctly, the model is evaluated on a held-out test data, which is also generated from the underlying true distribution.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\n2. For the RL approach, I think it is very unclear as a formulation of an estimator.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] For example, in Eq. (2), what is r and what is y*? It is mentioned that r is a \"reward\" function, but I don't know what it means and the authors should perhaps explain further.[[MET-NEU], [EMP-NEG], [QSN], [MIN]] I just don't see how one obtains an estimated parameter theta from the formulation in Eq. (2), using training examples."[[MET-NEG], [EMP-NEG], [CRT], [MIN]]