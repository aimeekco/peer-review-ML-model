"  The main idea in the paper is fairly simple:\n\n The paper considers SGD over an objective of the form of a sum over examples of a quadratic loss.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]]\nThe basic form of SGD selects an example uniformly.[[MET-NEU], [null], [SMY], [GEN]]   Instead,  one can use any probability distribution over examples and apply inverse probability weighting to retain unbiasedness of the gradient[[MET-NEU], [null], [SMY], [GEN]].\n\n  A good method (that builds on classic pps sampling) is to select examples with higher normed gradients with higher probability [Alain et al 2015].[[RWK-NEU,MET-NEU], [null], [SMY,DIS], [GEN]]\n\n  With quadratic loss,  the gradient increases with the inner product of the parameter vector (concatenated with -1) and the example vector x_i (concatenated with the label y_i).\n\n  For the current parameter vector \\theta,  we would like to sample examples so that the probability of sampling larger inner products is larger.[[RES-NEU], [null], [DIS], [MIN]]\n\n  The paper uses LSH structures, computed over the set of examples,[[MET-NEU], [null], [SMY], [GEN]]\n to quickly sample examples with large inner products with the current parameter vector \\theta.[[MET-NEU], [null], [SMY], [GEN]]   Essentially, two vectors are hashed to the same bucket with probability that increases with their cosine similarity.[[MET-NEU], [null], [SMY], [GEN]]\n So we select examples in the same LSH bucket as \\theta (for rubstness, we use multiple LSH mappings).[[MET-NEU], [null], [SMY], [GEN]]\n\n\nstrengths:  simple idea that can work well in the context of sampling examples for SGD\n\nweaknesses: \n\n  The novelty in the paper is limited.[[OAL-NEG], [NOV-NEG], [CRT], [MAJ]] The use of LSH for sampling is a common technique to sample more similar vectors with higher probability.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]  There are theorems,  but they are trivial, straightforward applications of importance sampling.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \n\n The paper is not well written.[[OAL-NEG], [CLA-NEG], [CRT], [MAJ]] The presentation is much more complex that need be.[[OAL-NEG], [PNF-NEG], [CRT], [MAJ]] References to classic weighted sampling are[[CNT], [CNT], [CNT], [CNT]] \n\n  The application is limited to certain loss functions for which we can compute LSH structures.[[MET-NEU], [null], [SUG], [MIN]]  This excludes NN models and even the addition of regularization to the quadratic loss can affect the effectiveness.\n"[[RES-NEU], [EMP-NEU], [DIS], [MIN]]