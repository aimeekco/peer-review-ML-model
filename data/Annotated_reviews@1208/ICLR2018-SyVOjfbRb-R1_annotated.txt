"Authors propose sampling stochastic gradients from a monotonic function proportional to gradient magnitudes by using LSH.[[INT-NEU,PDI-NEU,MET-NEU], [null], [SMY], [GEN]] I found the paper relatively creative and generally well-founded and well-argued.[[OAL-POS], [EMP-POS], [APC], [MAJ]]\n\nNice clear example with least squares linear regression, though a little hard to tell how generalizable the given ideas are to other loss functions/function classes, given the authors seem to be taking heavy advantage of the inner product. [[OAL-POS], [EMP-POS], [APC], [MAJ]]\n\nExperiments: appreciated the wall clock timings.[[EXP-POS], [EMP-POS], [APC], [MAJ]]\n\nSGD comparison: \u201cfixed learning rate.[[MET-POS], [EMP-POS], [APC], [MAJ]]\u201d Didn't see how the initial (well constant here) step size was tuned?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]] Why not use the more standard 1/t decay?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nFig 1: Suspicious CIFAR100 that test objective is so much better than train objective?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Legend backwards?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nWhy were so many of the chosen datasets have so few training examples?[[DAT-NEU], [SUB-NEU], [QSN], [MIN]]\n\nPaper is mostly very clearly written,[[OAL-POS], [CLA-POS], [APC], [MAJ]] though a bit too redundant and some sentences are oddly ungrammatical as if a word is missing - just needs a careful read-through. \n"[[OAL-NEG], [CLA-NEG,PNF-NEG], [CRT], [MIN]]