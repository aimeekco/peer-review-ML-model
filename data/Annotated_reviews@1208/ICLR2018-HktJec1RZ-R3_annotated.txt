"This paper introduces a new architecture for end to end neural machine translation.[[INT-NEU], [null], [SMY], [GEN]] Inspired by the phrase based approach, the translation process is decomposed as follows : source words are embedded and then reordered; a bilstm then encodes the reordered source; a sleep wake network finally generates the target sequence as a phrase sequence built from left to right.[[EXP-NEU,MET-NEU], [null], [SMY], [GEN]] \n\nThis kind of approach is more related to ngram based machine translation than conventional phrase based one.[[MET-NEU], [null], [SMY], [GEN]]  \n\nThe idea is nice.[[PDI-POS], [EMP-POS], [APC], [MAJ]] The proposed approach does not rely on attention based model.[[MET-NEU], [null], [DIS], [GEN]] This opens nice perpectives for better and faster inference.[[MET-POS], [EMP-POS], [APC], [MAJ]] \n\nMy first concern is about the architecture description.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] For instance, the swan part is not really stand alone.[[MET-NEG], [EMP-NEG], [DIS], [MIN]] For reader who does not already know this net, I'm not sure this is really clear.[[MET-NEG], [PNF-NEG], [CRT], [MIN]] Moreover, there is no link between notations used for the swan part and the ones used in the reordering part.[[MET-NEG], [PNF-NEG], [CRT], [MIN]] \n\nThen, one question arises. Why don't you consider the reordering of the whole source sentence.[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Maybe you could motivate your choice at this point.[[MET-NEU], [null], [DIS], [MIN]] This is the main contribution of the paper, since swan already exists.[[MET-NEU], [NOV-NEU], [CRT], [MIN]]\n\nFinally, the experimental part shows nice improvements [[EXP-POS], [EMP-POS], [APC], [MAJ]]but: 1/ you must provide baseline results with a well tuned phrase based mt system;[[RWK-NEU,EXP-NEU], [SUB-NEU,CMP-NEU], [SUG], [MIN]] 2/ the datasets are small ones, as well as the vocabularies, you should try with larger datasets and bpe for sake of comparison. "[[DAT-NEG], [SUB-NEG], [SUG,DFT,CRT], [MAJ]]