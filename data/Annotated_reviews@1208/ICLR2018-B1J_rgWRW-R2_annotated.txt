"The paper presents a series of definitions and results elucidating details about the functions representable by ReLU networks, their parametrisation, and gaps between deep and shallower nets.[[INT-NEU], [null], [SMY], [GEN]] \n\nThe paper is easy to read,[[OAL-POS], [CLA-POS], [APC], [MAJ]] although it does not seem to have a main focus (exponential gaps vs. optimisation vs. universal approximation).[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MIN]] The paper makes a nice contribution to the details of deep neural networks with ReLUs,[[RES-POS], [EMP-POS], [APC], [MAJ]] although I find the contributed results slightly overstated.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] The 1d results are not difficult to derive from previous results.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] The advertised new results on the asymptotic behaviour assume a first layer that dominates the size of the network.[[RES-NEU], [null], [DIS], [GEN]] The optimisation method appears close to brute force and is limited to 2 layers.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] \n\nTheorem 3.1 appears to be easily deduced from the results from Montufar, Pascanu, Cho, Bengio, 2014.[[RWK-NEU,MET-NEU,RES-NEU], [CMP-NEU], [DIS], [MIN]] For 1d inputs, each layer will multiply the number of regions at most by the number of units in the layer, leading to the condition w\u2019 \\geq w^{k/k\u2019}.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Theorem 3.2 is simply giving a parametrization of the functions, removing symmetries of the units in the layers.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \n\nIn the list at the top of page 5. Note that, the function classes might be characterized in terms of countable properties, such as the number of linear regions as discussed in MPCB, but still they build a continuum of functions.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Similarly, in page 5 ``Moreover, for fixed n,k,s, our functions are smoothly parameterized''.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] This should not be a surprise.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \n\nIn the last paragraph of Section 3 ``m = w^k-1'' This is a very big first layer.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] This also seems to subsume the first condition, s\\geq  w^k-1 +w(k-1) for the network discussed in Theorem 3.9.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] In the last paragraph of Section 3 ``To the best of our knowledge''.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]. In the construction presented here, the network\u2019s size is essentially in the layer of size m..[[MET-NEU], [null], [DIS], [MIN]] Under such conditions, Corollary 6 of MPCB also reads as s^n..[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Here it is irrelevant whether one artificially increases the depth of the network by additional, very narrow, layers, which do not contribute to the asymptotic number of units..[[MET-NEU], [null], [DIS], [MIN]] \n\nThe function class Zonotope is a composition of two parts.[[MET-NEU], [null], [DIS], [MIN]] It would be interesting to consider also a single construction, instead of the composition of two constructions..[[MET-NEU], [EMP-NEU], [SUG], [MIN]] \n\nTheorem 3.9 (ii) it would be nice to have a construction where the size becomes 2m + wk when k\u2019=k..[[MET-NEU], [EMP-NEU], [SUG], [MIN]] \n\nSection 4, while interesting, appears to be somewhat disconnected from the rest of the paper..[[CNT], [EMP-NEG], [CRT], [MIN]] \n\nIn Theorem 2.3. explain why the two layer case is limited to n=1..[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \n\nAt some point in the first 4 pages it would be good to explain what is meant by ``hard\u2019\u2019 functions (e.g. functions that are hard to represent, as opposed to step functions, etc.) \n".[[MET-NEU], [EMP-NEU], [SUG], [MIN]]