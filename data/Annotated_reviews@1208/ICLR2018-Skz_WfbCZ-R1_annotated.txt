"The authors prove a generalization guarantee for deep\nneural networks with ReLU activations, in terms of margins of the\nclassifications and norms of the weight matrices.[[INT-NEU,PDI-NEU,MET-NEU], [null], [SMY], [GEN]]  They compare this\nbound with a similar recent bound proved by Bartlett, et al.[[RWK-NEU], [CMP-NEU], [SMY], [GEN]]  While,\nstrictly speaking, the bounds are incomparable in strength, the\nauthors of the submission make a convincing case that their new bound\nmakes stronger guarantees under some interesting conditions.[[MET-POS], [CMP-POS,EMP-POS], [APC], [MAJ]]\n\nThe analysis is elegant.[[ANA-POS], [EMP-POS], [APC], [MAJ]]  It uses some existing tools, but brings them\nto bear in an important new context, with substantive new ideas needed.\nThe mathematical writing is excellent.[[MET-POS], [CLA-POS], [APC], [MAJ]]\n\nVery nice paper.[[OAL-POS], [CNT], [APC], [MAJ]]\n\nI guess that networks including convolutional layers are covered by\ntheir analysis.[[ANA-NEU], [EMP-NEU], [DIS], [MIN]]  It feels to me that these tend to be sparse,[[ANA-NEU], [EMP-POS], [APC], [MIN]] but that\ntheir analysis still my provides some additional leverage for such\nlayers.[[ANA-NEU], [SUB-NEU], [SUG], [MIN]]  Some explicit discussion of convolutional layers may be\nhelpful.  "[[MET-NEU,ANA-NEU], [SUB-NEU], [SUG], [MIN]]