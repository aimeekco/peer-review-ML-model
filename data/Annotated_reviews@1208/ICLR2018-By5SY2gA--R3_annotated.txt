"This paper introduces modifications the word2vec and GloVe loss functions to incorporate affect lexica to facilitate the learning of affect-sensitive word embeddings.[[INT-NEU], [null], [SMY], [GEN]] The resulting word embeddings are evaluated on a number of standard tasks including word similarity, outlier prediction, sentiment detection, and also on a new task for formality, frustration, and politeness detection.[[MET-NEU], [null], [SMY], [GEN]]\n\nA considerable amount of prior work has investigated reformulating unsupervised word embedding objectives to incorporate external resources for improving representation learning.[[RWK-NEU], [SUB-POS], [SMY], [GEN]] The methodologies of Kiela et al (2015) and Bollegala et al (2016) are very similar to those proposed in this work.[[RWK-NEU], [CMP-NEU], [DIS], [MAJ]] The main originality seems to be captured in Algorithm 1, which computes the strength between two words.[[MET-NEU], [NOV-NEU], [DIS], [MAJ]] Unlike prior work, this is a real-valued instead of a binary quantity.[[RWK-NEU,MET-NEU], [CMP-POS], [SMY], [MAJ]] Because this modification is not particularly novel, I believe this paper should primarily be judged based upon the effectiveness of the method rather than the specifics of the underlying techniques.[[MET-NEU], [NOV-NEU,EMP-NEU], [DFT], [MAJ]] In this light, the performance relative to the baselines is particularly important.[[RWK-NEU], [CMP-NEU], [SUG], [MAJ]] From the results reported in Tables 1, 2, and 3, I do not see compelling evidence that +V, +A, +D, or +VAD consistently lead to significant performance increases relative to the baseline methods. [[RWK-NEU,TNF-NEG], [IMP-NEG,CMP-NEG], [CRT], [MAJ]]I therefore cannot recommend this paper for publication."[[OAL-NEG], [REC-NEG], [FBK], [MAJ]]