"This paper considers the task of aspect sentiment classification, which entails categorizing texts with respect to the sentiment expressed concerning particular aspects (e.g., television resolution or price).[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]]  Toward this end, the authors adopt a memory-network based approach.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]]  The idea is to explicitly model interactions between aspects and words expressing sentiment about them.[[PDI-NEU], [null], [SMY], [GEN]]  This is achieved via an attention mechanism.[[MET-NEU], [null], [SMY], [GEN]]  \n\nOverall, this paper does seem to identify a concrete problem, and I liked the use of explicit aspect embeddings for sentiment analysis.[[PDI-POS,MET-POS,ANA-POS], [EMP-POS], [APC], [MAJ]]  However, the contribution is relatively minor here.[[OAL-NEG], [SUB-NEG], [CRT], [MIN]]  Furthermore, the approaches did not seem particularly well motivated in my view; inconsistent and seemingly erroneous notation in places complicate the picture.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]   Moreover, the experimental evaluation is small, considering only two datasets.[[DAT-NEG,EXP-NEG], [SUB-NEG], [DFT], [MIN]]   My feeling is that this work is a bit preliminary at present,[[OAL-NEU], [EMP-NEU], [DIS], [MIN]]   but could be expanded into a nice contribution eventually. [[FWK-NEU], [IMP-NEU], [SUG], [MIN]]  \n\nSpecific comments\n---\n- I had some trouble following the notation in places, and I think this is due to a bit of sloppiness.[[OAL-NEG], [PNF-NEG], [CRT], [MIN]]   Specifically:\n\n    1. Target aspect vectors live in R^V, but it's not clear to me what V represents; this is the number of distinct aspects?[[MET-NEG], [EMP-NEG], [CRT], [MIN]]   This should be clarified, ideally with concrete examples.[[ANA-NEU], [SUB-NEG], [SUG], [MIN]]   \n\n    2. Are the x_1 ... x_n here word embeddings or one-hot encodings?[[MET-NEG], [EMP-NEG], [QSN], [MIN]]   I believe the latter, but this implies that the vocabulary dimension (V) is the same as the number of aspects, since A is apparently shared.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]   This seems a bit surprising, or at least would seem to warrant further explanation.[[OAL-NEU], [SUB-NEU], [DIS], [MIN]]   In Eq. 2, below, it actually seems words are embedded separately via C, although again the dimensions are not provided.[[MET-NEG], [PNF-NEG], [CRT], [MIN]]   Regardless, more discussion regarding what the Ax_i embeddings are meant to capture (in contrast to the Cx_i vectors) would be appreciated.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]   \n\n    3. What is u is equation 3?[[MET-NEU], [SUB-NEG], [QSN], [MIN]]   As far as I can tell, this term is undefined.[[MET-NEG], [SUB-NEG], [CRT], [MIN]]   In Equation 5 this is mysteriously replaced with v_t, which is the target aspect embedding and so may have been the intention for u all along?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]   \n\n    4. In Equation 6, on the RHS in the expansion, the \\alpha_1 W c_i should be \\alpha_1 W c_1.[[MET-NEG], [EMP-NEG], [SUG], [MIN]]   The mistake is repeated for the following two terms. \n\n- Equations 3 and 4 suggest that despite their ordinal structure, sentiment labels are treated as unstructured at predict time.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]   This seems like a missed opportunity to capitalize on structure to bias predictions (neutral sentiment is closer to positive than is negative, after all, but the model does not know this as currently specified).[[MET-NEG], [EMP-NEG], [CRT], [MIN]]  \n\n- The authors write \"\\alpha_i W c_i does not explicitly depend on the target word t.[[MET-NEU], [null], [DIS], [MIN]]  \" I'm not sure I follow though, because the alpha terms do indeed depend on the word $t$, as per equation (1), which includes v_t, a vector representation of the target aspect.[[MET-NEU], [null], [DIS], [MIN]]  I think what the authors intend to note here is that the W parameters are independent of this, or as expounded upon in the concrete example that follows, that context words and weights W both are.[[MET-NEU], [null], [DIS], [MIN]]  In any case, this statement should be clarified.[[MET-NEG], [EMP-NEG], [SUG], [MIN]]  \n\n- It would seem to me at first glance that the most natural means of overcoming the problem discussed at length toward the end of Section 3 would be to add an additional layer, which would facilitate interactions between the attention-weighted word embedding (\\alpha_i c_i) and aspect embedding (v_t).[[MET-NEU], [null], [DIS], [MIN]]  However, the authors do not seem to have considered this straight-forward approach.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]  Why? \n\n- Surely \"d_t = Dt\" should be \"d_t = D v_t\" in the \"Interaction Term (IT)\" subsection?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]  \n\n- The authors write \"diWdt generates slightly better results and we adopt it in our experiments.[[EXP-NEU], [null], [DIS], [MIN]] \" -- please clarify; better results on a development set, I hope?[[RES-NEU], [EMP-NEU], [QSN], [MIN]]  \n\n- The evaluation is small.[[MET-NEG], [SUB-NEG], [CRT], [MIN]]  The authors use only two datasets comprising only a few thousand data points (and hence the test sets comprise 500-1000 instances).[[DAT-NEG], [SUB-NEG], [DFT], [MAJ]]  It is therefore hard to draw anything conclusive from these results, especially given how many moving parts there are here (from model initializations to training procedures).[[EXP-NEG,RES-NEG], [EMP-NEG], [CRT], [MIN]]  Regardless, the authors should report the micro-F1 in addition to the macro-F1.[[MET-NEU], [null], [DIS], [MIN]]  \n\nSmaller comments\n---\n- As a stylistic thing, I would suggest not pluralizing \"attention\" (i.e., remove \"Attentions\").[[MET-NEU], [EMP-NEU], [SUG], [MIN]]  \n\n- In Eq 3, please be explicit as to whether the '+' is here a concatentation or an actual (element-wise) sum?[[MET-NEU], [null], [QSN], [MIN]]  Eq. 5 clarifies this implicitly, but would be good to state outright. "[[MET-NEU], [EMP-NEU], [SUG], [MIN]] 