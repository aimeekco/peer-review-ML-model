"This paper proposed a novel framework Neuron as an Agent (NaaA) for training neural networks to perform various machine learning tasks, including classification (supervised learning) and sequential decision making (reinforcement learning).[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] The NaaA framework is based on the idea of treating all neural network units as self-interested agents and optimizes the neural network as a multi-agent RL problem.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] This paper also proposes adaptive dropconnect, which extends dropconnect (Wan et al., 2013) by using an adaptive algorithm for masking network topology.[[MET-NEU], [null], [SMY], [GEN]]\n \nThis work attempts to bring several fundamental principles in game theory to solve neural network optimization problems in deep learning.[[PDI-NEU], [null], [SMY], [GEN]] Although the ideas are interest and technically sound, and the proposed algorithms are demonstrated to outperform several baselines in various machine learning tasks,[[PDI-POS,MET-POS], [CMP-POS,EMP-POS], [APC], [MAJ]] there several major problems with this paper, including lacking clarity of presentation, insights and substantiations of many claims.[[OAL-NEG], [CLA-NEG,PNF-NEG], [CRT], [MIN]] These issues may need a significant amount of effort to fix as I will elaborate more below.[[OAL-NEU], [null], [DIS], [GEN]]\n \n1. Introduction\nThere are several important concepts, such as reward distribution, credit assignment, which are used (from the very beginning of the paper) without explanation until the final part of the paper.[[INT-NEG], [SUB-NEG], [DFT], [MIN]]\n \nThe motivation of the work is not very clear.[[PDI-NEG], [CLA-NEG], [CRT], [MIN]] There seems to be a gap between the first paragraph and the second paragraph.[[CNT], [PNF-NEG], [CRT], [MIN]]  The authors mentioned that \u201cFrom a micro perspective, the abstraction capability of each unit contribute to the return of the entire system.[[MET-NEU], [null], [DIS], [GEN]]  Therefore, we address the following questions.[[CNT], [null], [DIS], [GEN]]  Will reinforcement learning work even if we consider each unit as an autonomous agent \u201d\nIs there any citation for the claim \u201cFrom a micro perspective, the abstraction capability of each unit contribute to the return of the entire system\u201d ? [[MET-NEU], [EMP-NEU], [QSN], [MIN]] It seems to me this is a very general claim.[[MET-NEU], [EMP-NEU], [CRT], [MIN]] Even RL methods with linear function approximations use abstractions.[[MET-NEU], [null], [DIS], [GEN]]  Also, it is unclear to me why this is an interest question.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Does it have anything to do with existing issues in DRL?[[MET-NEU], [null], [DIS], [MIN]] Moreover, The definition of autonomous agent is not clear, do you mean learning agent or policy execution agent?[[MET-NEG], [EMP-NEG], [QSN], [MIN]]\n \n\u201cit uses \\epsilon-greedy as a policy, \u2026\u201d Do you mean exploration policy?[[MET-NEU], [null], [DIS], [MIN]]\nI also have some concerns regarding the claim that \u201cWe confirm that optimization with the framework of NaaA leads to better performance of RL\u201d.[[MET-NEU], [null], [DIS], [MIN]] Since there are only two baselines are compared to the proposed method, this claim seems too general to be true.[[RWK-NEG,MET-NEG], [SUB-NEG,NOV-NEG], [DFT,CRT], [MAJ]]\n \nIt is not clear to why the authors mention that \u201cnegative result that the return decreases if we naively consider units as agents\u201d.[[RES-NEG], [EMP-NEG], [CRT], [MIN]] What is the big picture behind this claim?[[RES-NEU], [EMP-NEU], [QSN], [MIN]]\n \n\u201cThe counterfactual return is that by which we extend reward \u2026\u201d need to be rewritten.[[MET-NEG], [CLA-NEG], [CRT], [MIN]]\n \nThe last paragraph of introduction discussed the possible applications of the proposed methods without any substantiation, especially neither citations nor any related experiments of the authors are provided.[[INT-NEG], [SUB-NEG], [DFT], [MIN]]\n \n2 Related Work\n \n\u201cPOSG, a class of reinforcement learning with multiple ..\u201d -> reinforcement learning framework[[RWK-NEG], [CLA-NEG], [CRT], [MIN]]\n \n\u201cAnother one is credit assignment.[[RWK-NEU], [null], [DIS], [GEN]] Instead of reward.. \u201d Two sentences are disconnected and need to be rewritten.[[RWK-NEU], [CLA-NEU], [CRT], [MIN]]\n \n\u201cThis paper unifies both issues\u201d sounds very weird.[[OAL-NEU], [null], [DIS], [MIN]] Do you mean \u201csolves/considers both issues in a principled way\u201d?[[OAL-NEU], [null], [QSN], [MIN]]\n \nThe introduction of GAN is very abrupt.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Rather than starting from introducing those new concepts directly, it might be better to mention that the proposed method is related to many important concepts in game theory and GANs.[[MET-NEU], [null], [SUG], [MIN]]\n \n\u201c,which we propose in a later part of this paper\u201d -> which we propose in this paper[[MET-NEU], [null], [DIS], [MIN]]\n \n \n3. Background\n \n\u201ca function from the state and the action of an agent to the real value\u201d -> a reward function[[MET-NEU], [null], [DIS], [MIN]]  \n \nShould provide a citation for DRQN[[BIB-NEU], [null], [SUG], [MIN]]\n \nThere is a big gap between the last two paragraphs of section 3.[[CNT], [PNF-NEG], [CRT], [MIN]]\n \n4. Neuro as an agent[[CNT], [CNT], [CNT], [GEN]]\n \n\u201cWe add the following assumption for characteristics of the v_i\u201d -> assumptions for characterizing v_i[[CNT], [CLA-NEG], [CRT], [MIN]]\n \n\u201cto maximize toward maximizing its own return\u201d -> to maximize its own return[[CNT], [CLA-NEG], [CRT], [MIN]]\n \nWe construct the framework of NaaA from the assumptions -> from these assumptions\n \n\u201cindicates that the unit gives additional value to the obtained data.[[CNT], [CLA-NEG], [CRT], [MIN]] \u2026\u201d I am not sure what this sentence means, given that \\rho_ijt is not clearly defined.[[CNT], [CLA-NEG], [CRT], [MIN]]\n \n5. Optimization\n \n\u201cNaaA assumes that all agents are not cooperative but selfish\u201d Why?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Is there any justification for such a claim?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n \n \nWhat is the relation between \\rho_jit and q_it ?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n \n\u201cA buyer which cannot receive the activation approximates x_i with \u2026\u201d It is unclear why a buyer need to do so given that it cannot receive the activation anyway.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n \n\u201cQ_it maximizing the equation is designated as the optimal price.\u201d Which equation?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n \ne_j and 0 are not defined in equation 8[[MET-NEG], [SUB-NEG], [DFT], [MIN]]\n \n \n6 Experiment\nsetare -> set are[[EXP-NEU], [null], [DIS], [GEN]]\n \nwhat is the std for CartPole in table 1[[TNF-NEU], [null], [QSN], [MIN]]\n \nIt is hard to judge the significance of the results on the left side of figure 2.[[RES-NEG,TNF-NEG], [IMP-NEG], [CRT], [MAJ]] It might be better to add errorbars to those curves[[TNF-NEU], [null], [SUG], [MIN]]\n \nMore description should be provided to explain the reward visualization on the right side of figure 2. What reward?[[TNF-NEU,ANA-NEU], [SUB-NEU], [SUG], [MIN]] External/internal?\n \n\u201cSpecifically, it is applicable to various methods as described below \u2026\u201d Related papers should be cited."[[RWK-NEU], [null], [SUG], [MIN]]