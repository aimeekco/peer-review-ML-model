"The paper studies the local optima of certain types of deep networks.[[INT-NEU], [null], [SMY], [GEN]] It uses the notion of a locally open map to draw equivalences between local optima and global optima.[[MET-NEU], [null], [SMY], [GEN]] The basic idea is that for fitting nonlinear models with a convex loss, if the mapping from the weights to the outputs is open, then every local optimum in weight space corresponds to a local optimum in output space; by convexity, in output space every local optimum is global.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] \n\nThis is mostly a \u201ctheory building\u201d work.[[MET-NEU], [null], [SMY], [MAJ]] With an appropriate fix, lemma 4 gives a cleaner set of assumptions than previous work in the same space (Nguyen + Hein \u201917), but yields essentially the same conclusions.[[RWK-NEU,RES-NEU], [CMP-NEU], [CRT], [MAJ]] \n\nThe notion of local openness seems very well adapted to deriving these type of results in a clean manner.[[MET-POS,RES-NEU], [EMP-POS], [APC], [MAJ]] The result in Section 3 on local openness of matrix multiplication on its range (which is substantially motivated by Behrends 2017) may be of independent interest.[[RWK-NEU,RES-NEU], [IMP-NEU], [DIS], [MAJ]] I did not check the proof of this result in detail, but it appears to be correct.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] For the linear, deep case, the paper corrects imprecisions in the previous work (Lu + Kawaguchi).[[RWK-NEU,MET-POS], [CMP-POS], [APC], [MAJ]] \n\nFor deep nonlinear networks, the results require the \u201cpyramidal\u201d assumption that the dimensionality is nonincreasing with respect to layer and (more restrictively) the feature dimension in the first layer is larger than the number of input points.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] This seems to differ from typical practice, in the sense that it does not allow for wide intermediate layers.[[MET-NEU], [EMP-NEU], [SMY], [MAJ]] This seems to be a limitation of the methodology: unless I'm missing something, this situation cannot be addressed using locally open maps.[[MET-NEG], [EMP-NEG], [DFT], [MAJ]] \n\n\n\nThere are some imprecisions in the writing.[[OAL-NEG], [CLA-NEG], [CRT], [MAJ]] For example, Lemma 4 is not correct as written \u2014 an invertible mapping \\sigma is not necessarily locally open. [[MET-NEG], [CLA-NEG], [CRT], [MIN]]Take $\\sigma_k(t) = t for t rational and -t for t irrational$ as an example.[[MET-NEG], [CLA-NEG], [CRT], [MIN]] This is easy to fix, but not correct as written.[[OAL-NEG], [CLA-NEG], [CRT], [MIN]] \n\nDespite mentioning matrix completion in the introduction and comparing to work of Ge et. al., the paper does not seem to have strong implications for matrix completion.[[INT-NEU,RWK-NEU], [CMP-NEU,EMP-NEG], [CRT], [MAJ]] It extends results of Ge and collaborators for the fully observed symmetric case to non-symmetric problems.[[RWK-NEU,MET-NEU], [EMP-NEU], [SMY], [MAJ]] But the main interest in matrix completion is in the undersampled case \u2014 in the full observed case, there is nothing to complete.[[EXP-NEG,MET-NEG], [EMP-NEU], [CRT], [MAJ]] \n\n\n"