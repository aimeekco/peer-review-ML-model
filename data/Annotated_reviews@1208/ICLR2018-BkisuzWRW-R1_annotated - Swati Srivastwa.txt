"The authors propose an approach for zero-shot visual learning. [[INT-NEU], [null], [SMY], [GEN]]The robot learns inverse and forward models through autonomous exploration.[[PDI-NEU], [null], [SMY], [GEN]] The robot then uses the learned parametric skill functions to reach goal states (images) provided by the demonstrator.[[PDI-NEU], [null], [SMY], [GEN]] The \u201czero-shot\u201d refers to the fact that all learning is performed before the human defines the task.[[PDI-NEU], [null], [SMY], [GEN]] The proposed method was evaluated on a mobile indoor navigation task and a knot tying task.[[MET-NEU], [null], [SMY], [GEN]] \n\nThe proposed approach is well founded and the experimental evaluations are promising. [[EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]]The paper is well written and easy to follow.[[OAL-POS], [CLA-POS], [APC], [MAJ]] \n\nI was expecting the authors to mention \u201cgoal emulation\u201d and \u201cdistal teacher learning\u201d in their related work.[[RWK-NEU], [CMP-NEU], [DIS], [MIN]] These topics seem sufficiently related to the proposed approach that the authors should include them in their related work section, and explain the similarities and differences.[[RWK-NEU], [CMP-NEU], [SUG], [MIN]] \n\nLearning both inverse and forward models is very effective.[[MET-POS], [EMP-POS], [APC], [MAJ]] How well does the framework scale to more complex scenarios, e.g., multiple types of manipulation together?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Do you have any intuition for what kind of features or information the networks are capturing?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] For the mobile robot, is the robot learning some form of traversability affordances, e.g., recognizing actions for crossings, corners, and obstacles?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] The authors should consider a test where the robot remains stationary with a fixed goal, but obstacles are move around it to  see how it affects the selected action distributions.[[EXP-NEU], [EMP-NEU], [SUG], [MIN]]\n\nHow much can change between the goal images and the environment before the system fails?[[RES-NEU], [EMP-NEU], [QSN], [MIN]] In the videos, it seems that the people and chairs are always in the same place.[[RES-NEU], [EMP-NEU], [DIS], [MIN]] I could imagine a network learning to ignore features of objects that tend to wander over time.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] The authors should consider exploring and discussing the effects of adding/moving/removing objects on the performance.[[ANA-NEU], [SUB-NEU], [SUG], [MIN]] \n\nI am very happy to see experimental evaluations on real robots, and even in two different application domains.[[EXP-POS], [EMP-POS], [APC], [MAJ]] Including videos of failure cases is also appreciated.[[EXP-POS], [EMP-POS], [APC], [MAJ]] The evaluation with the sequence of checkpoints was created by using every fifth image.[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [GEN]]  How does the performance change with the number of frames between checkpoints?[[EXP-NEU,RES-NEU], [EMP-NEU], [QSN], [MIN]] In the videos, it seems like the robot could get a slightly better view if it took another couple of steps.[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]] I assume this is an artifact of the way the goal recognizer is trained.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]] For the videos, it may be useful to indicate when the goal is detected, and then let it run a couple more steps and stop for a second.[[EXP-NEU,MET-NEU], [EMP-NEU], [SUG], [MIN]] It is difficult to compare the goal image and the video otherwise. "  [[MET-NEG], [CMP-NEU], [CRT], [MAJ]]