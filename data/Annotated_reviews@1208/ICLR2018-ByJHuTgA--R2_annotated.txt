"The authors did extensive tuning of the parameters for several recurrent neural architectures[[INT-POS], [null], [SMY], [GEN]]. The results are interesting.[[RES-POS], [null], [APC], [MAJ]] However the corpus the authors choose are quite small,[[OAL-NEG], [SUB-NEG], [DFT], [GEN]] the variance of the estimate will be quite high, I suspect whether the same conclusions could be drawn[[MET-NEG,ANA-NEU], [EMP-NEG,SUB-NEG], [DFT], [MIN]].\n\nIt would be more convincing if there are experiments on the billion word corpus or other larger datasets, or at least on a corpus with 50 million tokens.[[DAT-NEU,EXP-NEU], [SUB-NEU], [SUG], [MAJ]] This will use significant resources and is much more difficult,[[MET-NEU], [EMP-POS], [DIS], [GEN]] but it's also really valuable, because it's much more close to real world usage of language models.[[MET-POS], [IMP-POS,EMP-POS], [APC], [MAJ]] And less tuning is needed for these larger datasets.[[DAT-NEU], [EMP-NEU], [SMY], [GEN]] \n\nFinally it's better to do some experiments on machine translation or speech recognition and see how the improvement on BLEU or WER could get. "[[EXP-NEU,ANA-NEU], [REC-POS], [SUG], [MAJ]]