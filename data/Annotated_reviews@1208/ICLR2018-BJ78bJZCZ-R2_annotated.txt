"This paper extends the recurrent weight average (RWA, Ostmeyer and Cowell, 2017) in order to overcome the limitation of the original method while maintaining its advantage.[[RWK-NEU,MET-NEU,RES-NEU], [CMP-NEU], [SMY], [GEN]] The motivation of the paper and the approach taken by the authors are sensible, such as adding discounting was applied to introduce forget mechanism to the RWA and manipulating the attention and squash functions.[[EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]]\n\nThe proposed method is using Elman nets as the base RNN. [[EXP-NEU,MET-NEU], [null], [SMY], [GEN]]I think the same method can be applied to GRUs or LSTMs[[MET-NEU], [EMP-NEU], [DIS], [MIN]]. Some parameters might be redundant,[[MET-NEG], [EMP-NEG], [CRT], [MIN]] however, assuming that this kind of attention mechanism is helpful for learning long-term dependencies and can be computed efficiently, it would be nice to see the outcomes of this combination.[[MET-POS,RES-POS,ANA-POS], [IMP-POS], [APC], [MAJ]]\n\nIs there any explanation why LSTMs perform so badly compared to GRUs, the RWA and the RDA?[[MET-NEU,ANA-NEU], [EMP-NEU], [QSN], [MIN]]\nOverall, the proposed method seems to be very useful for the RWA."[[MET-POS], [EMP-POS], [APC], [MAJ]]