"This paper proposed a new parametrization scheme for weight matrices in neural network based on the Householder  reflectors to solve the gradient vanishing and exploding problems in training.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The proposed method improved two previous papers:\n1) stronger expressive power than Mahammedi et al. (2017),[[RWK-POS], [CMP-POS], [APC], [MAJ]]\n2) faster gradient update than Vorontsov et al. (2017)[[RWK-POS], [CMP-POS], [APC], [MAJ]].\nThe proposed parametrization scheme is natrual from numerical linear algebra point of view and authors did a good job in Section 3 in explaining the corresponding expressive power.[[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]] The experimental results also look promising.[[EXP-POS,RES-POS], [EMP-POS], [APC], [MAJ]] \n\nIt would be nice if the authors can analyze the spectral properties of the saddle points in linear RNN (nonlinear is better but it's too difficult I believe).[[ANA-NEU], [SUB-NEU], [SUG], [MIN]] If the authors can show the strict saddle properties then as a corollary, (stochastic) gradient descent finds a global minimum.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \n\nOverall this is a strong paper and I recommend to accept."