"This paper applies the boosting trick to deep learning.[[INT-NEU], [null], [SMY], [GEN]] The idea is quite straightforward, and the paper is relatively easy to follow.[[PDI-POS], [EMP-POS], [APC], [MAJ]] The proposed algorithm is validated on several image classification datasets.[[DAT-NEU,MET-NEU], [null], [SMY], [GEN]]\n\nThe paper is its current form has the following issues:\n1. There is hardly any baseline compared in the paper.[[RWK-NEG], [SUB-NEG,CMP-NEG], [DFT], [MIN]] The proposed algorithm is essentially an ensemble algorithm, there exist several works on deep model ensemble (e.g., Boosted convolutional neural networks, and Snapshot Ensemble) should be compared against.[[MET-NEG], [SUB-NEG,CMP-NEG], [DFT,CRT], [MIN]]\n2. I did not carefully check all the proofs, but seems most of the proof can be moved to supplementary to keep the paper more concise.[[MET-NEU], [EMP-NEU], [SUG], [MIN]]\n3. In Eq. (3), \\tilde{D} is not defined.[[MET-NEG], [EMP-NEG], [DFT], [MIN]]\n4. Under the assumption $\\epsilon_t(l) > \\frac{1}{2\\lambda}$, the definition of $\\beta_t$ in Eq.8 does not satisfy $0 < \\beta_t < 1$.[[MET-NEU], [EMP-NEU], [CRT], [MIN]]  \n5. How many layers is the DenseNet-BC used in this paper?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Why the error rate reported here is higher than that in the original paper?[[RWK-NEG,RES-NEG], [CMP-NEG], [QSN], [MIN]]\nTypo: \nIn Session 3 Line 7, there is a missing reference.[[RWK-NEG], [SUB-NEG], [DFT], [MIN]]\nIn Session 3 Line 10, \u201c1,00 object classes\u201d should be \u201c100 object classes\u201d.[[CNT], [CLA-NEG], [CRT], [MIN]]\nIn Line 3 of the paragraph below Equation 5, \u201cclasse\u201d should be \u201cclass\u201d.\n"[[CNT], [CLA-NEG], [CRT], [MIN]]