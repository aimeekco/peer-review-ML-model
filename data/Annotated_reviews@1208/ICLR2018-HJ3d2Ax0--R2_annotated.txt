"After reading the authors's rebuttal I increased my score from a 7 to a 6.[[EXT-POS], [null], [APC,FBK], [MAJ]]  I do think the paper would benefit from experimental results[[PDI-POS,RES-POS], [IMP-POS], [APC], [MAJ]], but agree with the authors that the theoretical results are non-trivial and interesting on their own merit.[[RWK-POS,EXP-POS,RES-POS], [IMP-POS,EMP-POS], [APC], [MAJ]]\n\n------------------------\nThe paper presents a theoretical analysis of depth in RNNs (technically a variant called RACs) i.e. stacking RNNs on top of one another, so that h_t^l (i.e. hidden state at time t and layer l is a function of h_t^{l-1} and h_{t-1}^{l}[[RWK-NEU,EXP-NEU,MET-NEU], [IMP-NEU,EMP-NEU], [SMY,DIS], [GEN]])\n\nThe work is inspired by previous results for feed forward nets and CNNs[[RWK-NEU,MET-NEU], [EMP-NEU], [SMY,DIS], [GEN]]. However, what is unique to RNNs is their ability to model long term dependencies across time.[[MET-NEU], [null], [QSN], [GEN]] \n\nTo analyze this specific property, the authors propose a concept called \"start-end rank\" that essentially models the richness of the dependency between two disjoint subsets of inputs[[PDI-NEU,DAT-NEU,EXP-NEU,MET-NEU,ANA-NEU], [IMP-NEU,EMP-NEU], [SMY,DIS], [GEN]]. Specifically, let S = {1, . . . , T/2} and E === {T/2 + 1, . . . , T}. sep_{S,E}(y) models the dependence between these two sets of time points.[[DAT-NEU,EXP-NEU], [PNF-NEU], [DIS], [GEN]] Specifically sep_{S,E}(y) = K means there exists g_s^k and g_e^k for k=1...K such that y(x) = \\sum_{k} g_s^k(x_S) g_e^k(x_E)[[RWK-NEU,DAT-NEU,EXP-NEU], [null], [SMY], [GEN]].\n\nTherefore sep_{S,E}(y) is the rank of a particular matricization of y (with respect to the partition S,E). [[EXP-NEU,RES-NEU,ANA-NEU], [EMP-NEU], [SUG], [GEN]]If sep_{S,E}=1 then it is rank 1 (and would correspond to independence if y(x) was a probability distribution)[[EXP-NEU,MET-NEU,ANA-NEU], [EMP-NEU], [SMY], [GEN]]. A higher rank would correspond to more dependence across time[[PDI-NEU,EXP-NEU,RES-NEU], [EMP-NEU], [SMY], [GEN]]. \n\n(Comment: I believe if I understood the above correctly, it would be easier to explain tensors/matricization first and then introduce separation rank,[[EXT-NEU], [null], [SUG,FBK], [GEN]] since I think it much makes it clearer to explain[[EXT-NEU], [CLA-NEU], [SUG,FBK], [GEN]]. Right now the authors explain separation rank first and then discuss tensors / matricization).[[RWK-NEU,EXP-NEU,ANA-NEU], [EMP-NEU], [DIS], [GEN]]\n\nUsing this concept, the authors prove that deep recurrent networks can express functions that have exponentially higher start/end ranks than shallow RNNs.[[RWK-NEU,PDI-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SMY,DIS], [GEN]]\n\nI overall like the paper's theoretical results,[[INT-POS,RWK-POS,EXP-POS], [EMP-POS], [APC], [MAJ]] but I have the following complaints:\n\n(1)  I have the same question as the other reviewer.[[RWK-NEG], [null], [DFT,QSN,FBK], [MIN]] Why is Theorem 1 not a function of L?[[RWK-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [GEN]]  Do the papers that prove similar theorems about ConvNets able to handle general L[[INT-NEU,RWK-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [GEN]]? What makes this more challenging?[[RWK-NEU], [null], [QSN], [GEN]] I feel if comparing L=2 vs L=3 is hard, the authors should be more up front about that in the introduction/abstract[[ABS-NEG,INT-NEG,RWK-NEG], [null], [SUG,DFT], [MIN]].\n\n(2) I think it would have been stronger if the authors would have provided some empirical results validating their claims. \n\n"[[RWK-NEG,EXP-NEG,MET-NEG,RES-NEG], [EMP-NEG], [SUG,DFT], [MIN]]