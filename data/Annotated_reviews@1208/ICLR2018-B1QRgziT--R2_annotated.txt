"This paper proposes \"spectral normalization\" -- constraining the spectral norm of the weights of each layer -- as a way to stabilize GAN training by in effect bounding the Lipschitz constant of the discriminator function.[[INT-NEU,PDI-NEU], [null], [SMY], [MIN]] The paper derives efficient approximations for the spectral norm, as well as an analysis of its gradient.[[MET-NEU,ANA-NEU], [EMP-POS], [APC], [MAJ]] Experimental results on CIFAR-10 and STL-10 show improved Inception scores and FID scores using this method compared to other baselines and other weight normalization methods.[[RWK-POS,DAT-POS,EXP-POS,MET-POS,RES-POS], [CMP-POS,EMP-NEU], [SUG], [MIN]]\n\nOverall, this is a well-written paper that tackles an important open problem in training GANs using a well-motivated and relatively simple approach.[[PDI-POS,MET-POS,OAL-POS], [CLA-POS,EMP-POS], [APC], [MAJ]] The experimental results seem solid and seem to support the authors' claims.[[EXP-POS,RES-POS], [EMP-POS], [APC], [MAJ]] I agree with the anonymous reviewer that connections (and differences) to related work should be made clearer.[[RWK-NEG], [EMP-NEG], [CRT], [MIN]] Like the anonymous commenter, I also initially thought that the proposed \"spectral normalization \" is basically the same as \"spectral norm regularization\", but given the authors' feedback on this I think the differences should be made more explicit in the paper.[[MET-NEU], [CMP-NEU], [SUG], [MIN]]\n\nOverall this seems to represent a strong step forward in improving the training of GANs, and I strongly recommend this paper for publication.[[OAL-POS], [REC-POS], [APC], [MAJ]]\n\nSmall Nits: \n\nSection 4: \"In order to evaluate the efficacy of our experiment\": I think you mean \"approach\".[[EXP-NEG,MET-NEG], [CLA-NEG], [CRT], [MIN]]\n\nThere are a few colloquial English usages which made me smile, e.g. \n * Sec 4.1.1. \"As we prophesied ...\", and in the paragraph below \n * \"... is a tad slower ...\"."[[CNT], [CLA-NEG], [CRT], [MIN]]