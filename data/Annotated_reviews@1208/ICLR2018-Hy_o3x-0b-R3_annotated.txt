"Update:  In light of Yoon Kim's retraction of replication, I've downgraded my score until the authors provide further validation (i.e. CIFAR and ImageNet samples).[[RWK-NEG,DAT-NEG], [CMP-NEG,REC-NEG], [FBK], [MAJ]]\n\nSummary\n\nThis paper proposes VAE modifications that allow for the use multiple layers of latent variables.[[INT-NEU,MET-NEU], [null], [SMY], [GEN]]  The modifications are: (1) a shared en/decoder parametrization as used in the Ladder VAE [1], [[MET-NEU], [null], [SMY], [GEN]](2) the latent variable parameters are functions of a CNN, [[MET-NEU], [null], [SMY], [GEN]]and (3) use of a PixelCNN decoder [2] that is fed both the last layer of stochastic variables and the input image, as done in [3].[[EXP-NEU,MET-NEU], [null], [SMY], [GEN]]  Negative log likelihood (NLL) results on CIFAR 10, binarized MNIST (dynamic and static), OMNIGLOT, and ImageNet (32x32) are reported.[[DAT-NEU,RES-NEU], [null], [SMY], [GEN]]  Samples are shown for CIFAR 10, MNIST, and OMNIGLOT. [[DAT-NEU,RES-NEU], [null], [SMY], [GEN]]       \n\n\nEvaluation\n\nPros:  The paper\u2019s primary contribution is experimental: SOTA results are achieved for nearly every benchmark image dataset (the exception being statically binarized MNIST, which is only .28 nats off).[[RWK-POS,DAT-POS,EXP-POS,MET-POS], [CMP-POS,EMP-POS], [APC], [MAJ]]  This experimental feat is quite impressive, and moreover, in the comments on OpenReview, Yoon Kim claims to have replicated the CIFAR result.[[DAT-NEU,EXP-NEU], [null], [SMY], [GEN]]  I commend the authors for making their code available already via DropBox.[[EXT-POS], [null], [APC], [MAJ]]  Lastly, I like how the authors isolated the effect of the concatenation via the \u2018FAME No Concatenation\u2019 results.[[RES-POS], [EMP-POS], [APC], [MAJ]]                 \n\nCons:  The paper provides little novelty in terms of model or algorithmic design, as using a CNN to parametrize the latent variables is the only model detail unique to this paper. [[MET-POS], [NOV-POS], [APC], [GEN]] In terms of experiments, the CIFAR samples look a bit blurry for the reported NLL (as others have mentioned in the OpenReview comments).[[DAT-NEG,RES-NEG], [EMP-NEG], [CRT], [GEN]]  I find the authors\u2019 claim that FAME is performing superior global modeling interesting. [[MET-NEU], [null], [SMY], [GEN]] Is there a way to support this experimentally?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]]  Also, I would have liked to see results w/o the CNN parametrization; how important was this choice?[[MET-NEU,RES-NEU], [SUB-NEU], [DIS], [MIN]]  \n\n\nConclusion\n\nWhile the paper's conceptual novelty is low,[[OAL-NEG], [NOV-NEG], [CRT], [MAJ]] the engineering and experimental work required (to combine the three ideas discussed in the summary and evaluate the model on every benchmark image dataset) is commendable. [[DAT-POS,EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]] I recommend the paper\u2019s acceptance for this reason.[[OAL-POS], [REC-POS], [FBK], [MAJ]]\n\n\n[1]  C. Sonderby et al., \u201cLadder Variational Autoencoders.\u201d  NIPS 2016.\n[2]  A. van den Oord et al., \u201cConditional Image Generation with PixelCNN Decoders.\u201d ArXiv 2016.\n[3]  I. Gulrajani et al., \u201cPixelVAE: A Latent Variable Model for Natural Images.\u201d  ICLR 2017.\n"[[BIB-NEU], [CNT], [DIS], [GEN]]