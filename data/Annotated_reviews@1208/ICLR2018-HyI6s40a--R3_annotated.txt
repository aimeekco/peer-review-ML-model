"This paper proposes an unsupervised method, called Parallel Checkpointing Learners (PCL), to detect and defend adversarial examples.[[INT-NEU,MET-NEU], [null], [SMY], [GEN]] The main idea is essentially learning the manifold of the data distribution and using Gaussian mixture models (GMMs) and dictionary learning to train a \"reformer\" (without seeing adversarial examples) to detect and correct adversarial examples.[[PDI-NEU], [null], [SMY], [GEN]] With PCL, one can use hypothesis testing framework to analyze the detection rate and false alarm of different neural networks against adversarial attacks.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] Although the motivation is well grounded,[[PDI-POS], [EMP-POS], [APC], [MAJ]] there are two major issues of this work: (i) limited  novelty - the idea of unsupervised manifold projection method has been proposed in the previous work;[[PDI-NEG], [NOV-NEG], [CRT], [MAJ]] and (ii) insufficient attack evaluations - the defender performance is evaluated against weak attacks or attacks with improper parameters.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] The details are as follows.\n\n1.  Limited novelty and performance comparison - the idea of unsupervised manifold projection method has been proposed and well-studied in \"MagNet: a Two-Pronged Defense against Adversarial Examples\", appeared in May 2017.[[RWK-NEG,RES-NEG], [NOV-NEG,SUB-NEG], [DFT,CRT], [MAJ]] Instead of GMMs and dictionary learning in PCL,  MagNet trains autoencoders for defense and provides sufficient experiments to claim its defense capability.[[RWK-NEU], [null], [DIS], [GEN]] On the other hand, the authors of this paper seem to be not aware of this pioneering work and claim \"To the best of our knowledge, our proposed PCL methodology is the first unsupervised countermeasure that is able to detect DL adversarial samples generated by the existing state-of-the-art attacks\", which is obviously not true.[[RWK-NEG,MET-NEG], [CMP-NEG,EMP-NEG], [CRT], [MAJ]] More importantly, MagNet is able to defend the adversarial examples very well (almost 100% success) no matter the adversarial examples are close to the information manifold or not.[[RWK-NEU], [CMP-NEU], [DIS], [GEN]] As a result, the resulting ROC and AUC score are expected be better than PCL.[[RWK-NEG], [EMP-NEG], [CRT], [MAJ]] In addition, the authors of MagNet also compared their performance in white-box (attacker knowing the reformer), gray-box (having multiple independent reformers), and black-box (attacker not knowing the reformer) scenarios, whereas this paper only considers the last case.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\n2. Insufficient attack evaluations - the attacks used in this paper to evaluate the performance of PCL are either weak (no longer state-of-the-art) or incorrectly implemented.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] For FGSM, the iterative version proposed by (Kurakin, ICLR 2017) should be used.[[RWK-NEU], [CMP-NEU], [DIS], [MIN]] JSMA and deep fool are not considered strong attacks now (see Carlini's bypassing 10 detection methods paper).[[RWK-NEG,MET-NEG], [EMP-NEG], [CRT], [MIN]] Carlini-Wagner attack is still strong, but the authors only use 40 iterations (should be at least 500) and setting the confidence=0, which is known to be producing non-transferable adversarial examples.[[RWK-NEG,MET-NEG], [EMP-NEG], [CRT], [MIN]]  In comparison, MagNet has shown to be effective against different confidence parameters.[[RWK-NEG,MET-NEG], [EMP-NEG], [CRT], [MIN]]  \n\nIn summary, this paper has limited novelty, incremental contributions, and lacks convincing experimental results due to weak attack implementation.  \n \n\n\n"[[EXP-NEG,RES-NEG], [NOV-NEG,EMP-NEG], [DFT,CRT], [MIN]] 