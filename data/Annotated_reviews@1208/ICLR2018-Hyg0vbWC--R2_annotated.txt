"This paper proposes an approach to generating the first section of Wikipedia articles (and potentially entire articles).[[INT-NEU], [null], [SMY], [GEN]] \nFirst relavant paragraphs are extracted from reference documents and documents retrieved through search engine queries through a TD-IDF-based ranking.[[INT-NEU,RWK-NEU], [null], [SMY], [GEN]] Then abstractive summarization is performed using a modification of Transformer networks (Vasvani et al 2017).[[INT-NEU,RWK-NEU], [null], [SMY], [GEN]] A mixture of experts layer further improves performance.[[MET-POS], [EMP-POS], [APC], [MAJ]] \nThe proposed transformer decoder defines a distribution over both the input and output sequences using the same self-attention-based network.[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]] On its own this modification improves perplexity (on longer sequences) but not the Rouge score; however the architecture enables memory-compressed attention which is more scalable to long input sequences.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] It is claimed that the transformer decoder makes optimization easier but no complete explanation or justification of this is given.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]  Computing self-attention and softmaxes over entire input sequences will significantly increase the computational cost of training.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] \n\nIn the task setup the information retrieval-based extractive stage is crucial to performance, but this contribution might be less important to the ICLR community.[[MET-NEG], [APR-NEG], [CRT], [MAJ]]  It willl also be hard to reproduce without significant computational resources, even if the URLs of the dataset are made available.[[DAT-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]]  The training data is significantly larger than the CNN/DailyMail single-document summarization dataset.[[DAT-NEG], [EMP-NEG], [CRT], [MAJ]] \n\nThe paper presents strong quantitative results and qualitative examples.[[RES-POS], [EMP-POS], [APC], [MAJ]]  Unfortunately it is hard to judge the effectiveness of the abstractive model due to the scale of the experiments, especially with regards to the quality of the generated output in comparison to the output of the extractive stage.[[RES-NEG], [EMP-NEG], [CRT], [MIN]] \nIn some of the examples the system output seems to be significantly shorter than the reference, so it would be helpful to quantify this, as well how much the quality degrades when the model is forced to generate outputs of a given minimum length.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MIN]]  While the proposed approach is more scalable, it is hard to judge the extend of this.[[MET-NEG,FWK-NEG], [IMP-NEG], [CRT], [MIN]] \n\nSo while the performance of the overall system is impressive,[[OAL-POS], [EMP-POS], [APC], [MAJ]]  it is hard to judge the significance of the technical contribution made by the paper.[[MET-NEG,OAL-NEG], [EMP-NEG], [CRT], [MIN]] \n\n---\nThe additional experiments and clarifications in the updated version give substantially more evidence in support of the claims made by the paper, and I would like to see the paper accepted. \n"[[EXP-POS], [EMP-POS], [APC], [MAJ]] 