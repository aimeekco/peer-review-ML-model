"This paper proposes a (new) semantic way for data augmentation problem, specifically targeted for one-shot learning setting, i.e. synthesizing training samples based on semantic similarity with a given sample .[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] Specifically, the authors propose to learn an autoencoder model, where the encoder translates image data into the lower dimensional subspace of semantic representation (word-to-vec representation of image classes), and the decoder translates semantic representation back to the original input space.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] For one-shot learning, in addition to a given input image, the following data augmentation is proposed: a) perturbed input image (Gaussian noise added to input image features);[[MET-NEU], [null], [DIS], [MIN]] b) perturbed decoded image;[[MET-NEU], [null], [SMY], [MIN]] c) perturbed decoded neighbour image, where neighbourhood is searched in the semantic space.[[MET-NEU], [null], [SMY], [MIN]]   \nThe idea is nice and simple,[[PDI-POS], [EMP-POS], [APC], [MAJ]] however the current framework has several weaknesses:\n1. The whole pipeline has three (neural network) components: a) input image features are extracted from VGG net pre-trained on auxiliary data;[[DAT-NEG,MET-NEG], [EMP-NEG], [CRT], [MIN]] 2) auto-encoder that is trained on data for one-shot learning;[[DAT-NEG,MET-NEG], [EMP-NEG], [CRT], [MIN]] 3) final classifier for one-shot learning is learned on augmented image space with two (if I am not mistaken) fully connected layers.[[MET-NEU], [null], [DIS], [MIN]] This three networks need to be clearly described; ideally combined into one end-to-end training pipeline.[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]]\n2. The empirical performance is very poor.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] If you look into literature for zero shot learning, work by Z. Akata in CVPR 2015, CVPR2016, the performance on AwA and on CUB-bird goes way above 50%, where in the current paper it is 30.57% and 8.21% at most (for the most recent survey on zero shot learning papers using attribute embeddings, please, refer to Zero-Shot Learning - The Good, the Bad and the Ugly by Xian et al, CVPR 2017).[[RWK-NEG,RES-NEG], [CMP-NEG], [CRT], [MAJ]] It is important to understand, why there is such a big drop in performance in one-shot learning comparing to zero-shot learning?[[MET-NEG,RES-NEG], [EMP-NEG], [QSN], [MIN]] One possible explanation is as follows: in the zero-shot learning, one has access to large training data to learn the semantic embedding (training classes).[[EXP-NEU,MET-NEU], [null], [DIS], [MIN]] In contrary, in the proposed approach, the auto-encoder model (with 10 hidden layers) is learned using 50 training samples in AwA, and 200 images of birds (or am I missing something?).[[EXP-NEU,MET-NEU], [null], [DIS], [MIN]] I am not sure, how can the auto-encoder model not overfit completely to the training data instances.[[DAT-NEG,EXP-NEG], [EMP-NEG], [CRT], [MIN]] Perhaps, one could try to explore the zero-shot learning setting, where there is a split between train and test classes: training the autoencoder model using large training dataset, and adapting the weights using single data points from test classes in one-shot learning setting.[[EXP-NEU], [EMP-NEU], [DIS], [MAJ]] \nOverall, I like the idea, so I am leaning towards accepting the paper,[[PDI-POS,OAL-POS], [EMP-POS], [APC], [MAJ]] but the empirical evaluations are not convincing. \n\n \n\n \n\n "[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]]