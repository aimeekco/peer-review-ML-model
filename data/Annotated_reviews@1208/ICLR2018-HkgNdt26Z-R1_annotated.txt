"This paper deals with improving language models on mobile equipments\nbased on small portion of text that the user has ever input.[[PDI-NEU,DAT-NEU,EXP-NEU,MET-NEU], [null], [SMY], [GEN]] For this\npurpose, authors employed a linearly interpolated objectives between user\nspecific text and general English, and investigated which method (learning\nwithout forgetting and random reheasal) and which interepolation works better.[[RWK-NEU,EXP-NEU,MET-NEU], [CMP-NEU,EMP-NEU], [SMY], [GEN]]\nMoreover, authors also look into privacy analysis to guarantee some level of\ndifferential privacy is preserved.[[RWK-NEU,PDI-NEU], [null], [SMY], [GEN]]\n\nBasically the motivation and method is good, the drawback of this paper is\nits narrow scope and lack of necessary explanations.[[RWK-NEG,PDI-POS], [CLA-NEG,EMP-NEG], [DFT,CRT], [MIN]] Reading the paper,\nmany questions arise in mind:\n\n- The paper implicitly assumes that the statistics from all the users must\n  be collected to improve \"general English\".[[RWK-NEU], [null], [QSN], [GEN]] Why is this necessary?[[RWK-NEU], [null], [QSN], [GEN]] Why not\n  just using better enough basic English and the text of the target user?[[RWK-NEU], [null], [QSN], [GEN]]\n\n- To achieve the goal above, huge data (not the \"portion of the general English\") should be communicated over the network.[[RWK-NEG,PDI-NEU], [SUB-NEG,EMP-NEG], [DFT], [MIN]] Is this really worth doing? [[RWK-NEU], [null], [QSN], [GEN]]If only\n  \"the portion of\" general English must be communicated, why is it validated?[[RWK-NEU], [null], [QSN], [GEN]]\n\n- For measuring performance, authors employ keystroke saving rate.[[RWK-NEU,RES-NEU], [EMP-NEU], [SMY], [GEN]] For the\n  purpose of mobile input, this is ok: but the use of language models will\n  cover much different situation where keystrokes are not necessarily \n  available, such as speech recognition or machine translation.[[RWK-NEU], [null], [SMY], [GEN]] Since this \n  paper is concerned with a general methodology of language modeling, \n  perplexity improvement (or other criteria generally applicable) is also\n  important.[[RWK-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]]\n\n- There are huge number of previous work on context dependent language models,\n  let alone a mixture of general English and specific models.[[RWK-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] Are there any\n  comparison with these previous efforts?[[RWK-NEU], [CMP-NEU], [QSN], [GEN]]\n\nFinally, this research only relates to ICLR in that the language model employed\nis LSTM: in other aspects, it easily and better fit to ordinary NLP conferences, such as EMNLP, NAACL or so.[[RWK-NEU], [IMP-NEU,REC-NEU,EMP-NEU], [FBK], [GEN]] I would like to advise the authors to submit\nthis work to such conferences where it will be reviewed by more NLP experts.[[OAL-NEU], [REC-NEU], [SUG], [GEN]]\n\nMinor:\n- t of $G_t$ in page 2 is not defined so far.\n- What is \"gr\" in Section 2.2[[RWK-NEG], [EMP-NEG], [DFT,QSN,CRT], [MIN]]?\n"
