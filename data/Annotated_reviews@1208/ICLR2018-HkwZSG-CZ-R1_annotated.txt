"The authors has addressed my concerns, so I raised my rating.[[EXT-POS], [CNT], [APC,FBK], [GEN]] \n\nThe paper is grounded on a solid theoretical motivation and the analysis is sound and quite interesting.[[PDI-POS,ANA-POS], [EMP-POS], [APC], [MAJ]]\n\nThere are no results on large corpora such as 1 billion tokens benchmark corpus, or at least medium level corpus with 50 million tokens.[[DAT-NEG,RES-NEG], [SUB-NEU], [DFT], [MAJ]] The corpora the authors choose are quite small, the variance of the estimates are high, and similar conclusions might not be valid on a large corpus.[[DAT-NEG,RES-NEG], [SUB-NEU,EMP-NEG], [DFT], [MAJ]] \n\n[1] provides the results of character level language models on Enwik8 dataset, which shows regularization doesn't have much effect and needs less tuning.[[DAT-NEG,RES-NEG], [SUB-NEU], [DFT], [MAJ]] Results on this data might be more convincing.[[DAT-NEU,RES-NEU], [SUB-NEU,EMP-NEU], [SUG], [MIN]]\n\nThe results of MOS is very good,[[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]] but the computation complexity is much higher than other baselines. In the experiments, the embedding dimension of MOS is slightly smaller, but the number of mixture is 15.[[EXP-NEG,MET-NEG], [SUB-NEU], [DFT], [MAJ]] This will make it less usable, I think it's necessary to provide the training time comparison.[[EXP-NEU], [EMP-NEU], [SUG], [MIN]]\n\nFinally experiments on machine translation or speech recognition should be done and to see what improvements the proposed method could bring for BLEU or WER.[[DAT-NEU,MET-NEU], [SUB-NEU], [SUG,DIS], [MIN]] \n\n[1] Melis, G\u00e1bor, Chris Dyer, and Phil Blunsom.[[BIB-NEU], [CNT], [DIS], [GEN]] \"On the state of the art of evaluation in neural language models.[[BIB-NEU], [CNT], [DIS], [GEN]]\" arXiv preprint arXiv:1707.05589 (2017).[[BIB-NEU], [CNT], [DIS], [GEN]]\n\n[2] Joris Pelemans, Noam Shazeer, Ciprian Chelba, Sparse Non-negative Matrix Language Modeling,  Transactions of the Association for Computational Linguistics, vol. 4 (2016), pp. 329-342[[BIB-NEU], [CNT], [DIS], [GEN]]\n\n[3] Shazeer et al. (2017). Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer. ICLR 2017\n"[[BIB-NEU], [CNT], [DIS], [GEN]]