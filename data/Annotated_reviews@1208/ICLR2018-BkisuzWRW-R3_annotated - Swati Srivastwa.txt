"One of the main problems with imitation learning in general is the expense of expert demonstration.[[EXT-NEU], [null], [DIS], [GEN]] The authors here propose a method for sidestepping this issue by using the random exploration of an agent to learn generalizable skills which can then be applied without any specific pretraining on any new task. [[MET-NEU], [null], [DIS], [GEN]]\n\nThe proposed method has at its core a method for learning a parametric skill function (PSF) that takes as input a description of the initial state, goal state, parameters of the skill and outputs a sequence of actions (could be of varying length) which take the agent from initial state to goal state.[[MET-NEU], [null], [DIS], [GEN]]\n\nThe skill function uses a RNN as function approximator and minimizes the sum of two losses i.e. the state mismatch loss over the trajectory (using an explicitly learnt forward model) and the action mismatch loss (using a model-free action prediction module) .[[MET-POS], [EMP-POS], [APC], [MAJ]] This is hard to do in practice due to jointly learning both the forward model as well as the state mismatches.[[MET-NEU], [null], [DIS], [GEN]] So first they are separately learnt and then fine-tuned together.[[MET-NEU], [null], [DIS], [GEN]] \n\nIn order to decide when to stop, an independent goal detector is trained which was found to be better than adding a 'goal-reached' action to the PSF.[[MET-NEU], [null], [DIS], [GEN]]\n\nExperiments on two domains are presented.[[EXP-NEU], [null], [SMY], [GEN]] 1. Visual navigation where images of start and goal states are given as input.[[EXP-NEU], [null], [SMY], [GEN]] 2. Robotic knot-tying with a loose rope where visual input of the initial and final rope states are given as input.[[EXP-NEU], [null], [SMY], [GEN]]\n\nComments:\n\n- In the visual navigation task no numbers are presented on the comparison to slam-based techniques used as baselines although it is mentioned that it will be revisited.[[MET-NEG], [EMP-NEG], [DFT], [MAJ]]\n\n- In the rope knot-tying task no slam-based or other classical baselines are mentioned.[[RWK-NEG], [SUB-NEG], [DFT], [MAJ]]\n\n- My main concern is that I am really trying to place this paper with respect to doing reinforcement learning first (either in simulation or in the real world itself, on-policy or off-policy) and then just using the learnt policy on test tasks.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Or in other words why should we call this zero-shot imitation instead of simply reinforcement learnt policy being learnt and then used.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] The nice part of doing RL is that it provides ways of actively controlling the exploration.[[MET-NEU], [null], [DIS], [MIN]] See this pretty relevant paper which attempts the same task and also claims to have the target state generalization ability.[[RWK-NEU], [CMP-NEU], [SUG], [MIN]] \n\nTarget-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning by Zhu et al.[[RWK-NEU], [CMP-NEU], [SUG], [MIN]] \n\nI am genuinely curious and would love the authors' comments on this.[[RWK-NEU], [SUB-NEU], [SUG], [MIN]]  It should help make it clearer in the paper as well.[[OAL-NEU], [CMP-NEU], [SUG], [MIN]] \n \nUpdate:\n\nAfter evaluating the response from the authors and ensuing discussion as well as the other reviews and their corresponding discussion, I am revising my rating for this paper up.[[OAL-POS], [REC-POS], [FBK], [MAJ]]  This will be an interesting paper to have at the conference and will spur more ideas and follow-on work."[[OAL-POS], [APR-POS], [APC], [MAJ]] 