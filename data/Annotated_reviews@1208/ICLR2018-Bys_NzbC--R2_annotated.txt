"The paper is well motivated and written.[[PDI-POS], [CLA-POS], [APC], [MAJ]] However, there are several issues.[[OAL-NEG], [null], [CRT], [MAJ]]\n1. As the regularization constant increases, the performance first increases and then falls down -- this specific aspect is well known for constrained optimization problems.[[EXP-NEU,ANA-NEU], [EMP-NEU], [DIS], [MAJ]] Further, the sudden drop in performance also follows from vanishing gradients problem in deep networks.[[MET-NEU], [EMP-NEU], [SMY], [MAJ]] The description for ReLUs in section 2.2 follows from these two arguments directly, hence not novel[[PDI-NEG], [NOV-NEG], [CRT], [MAJ]]. Several of the key aspects here not addressed are: \n1a. Is the time-delayed regularization equivalent to reducing the value (and there by bringing it back to the 'good' regime before the cliff in the example plots)? [[MET-NEU], [EMP-NEU], [QSN], [MAJ]]\n1b. Why should we keep increasing the regularization constant beyond a limit?[[MET-NEU], [EMP-NEU], [QSN], [MAJ]] Is this for compressing the networks (for which there are alternate procedures), or anything else.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] In other words, for a non-convex problem (about whose landscape we know barely anything), if there are regimes of regularizers that work well (see point 2) -- why should we ask for more stronger regularizers?[[MET-NEU], [EMP-NEU], [QSN], [MAJ]] Is there any optimization-related motivation here (beyond the single argument that networks are overparameterized)? [[RWK-NEU,MET-NEU], [EMP-NEU], [QSN], [MAJ]]\n2. The proposed experiments are not very conclusive.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Firstly, the authors need to test with modern state-of-the-art architectures including inception and residual networks.[[RWK-NEU,EXP-NEU], [EMP-NEU], [SUG], [MAJ]] Secondly, more datasets including imagenet needs to be tested.[[DAT-NEU], [SUB-NEU], [SUG], [MAJ]] Unless these two are done, we cannot assertively say that the proposal seems to do interesting things.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] Thirdly, it is not clear what Figure 5 means in terms of goodness of learning.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]] And lastly, although confidence intervals are reported for Figures 3,4 and Table 2, statistical tests needs to be performed to report p-values (so as to check if one model significantly beats the other).[[EXP-NEU,MET-NEU,TNF-NEU], [CMP-NEU,EMP-NEU], [SUG], [MAJ]]"