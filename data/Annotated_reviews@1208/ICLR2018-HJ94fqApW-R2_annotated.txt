"This paper is well written and it was easy to follow[[INT-POS], [CLA-POS], [APC,FBK], [MAJ]]. The authors propose prunning model technique by enforcing sparsity on the scaling parameter of batch normalization layers[[PDI-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]]. This is achieved by forcing the output of some channels being constant during training.[[RWK-NEU,EXP-NEU,RES-NEU], [EMP-NEU], [SMY,DIS], [GEN]] This is achieved an adaptation of ISTA algorithm to update the batch-norm parameter[[RWK-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]]. \n\nThe authors evaluate the performance of the proposed approach on different classification and segmentation tasks[[PDI-NEU,EXP-NEU,MET-NEU,RES-NEU], [IMP-NEU,EMP-NEU], [SMY,DIS], [GEN]]. The method seems to be relatively straightforward to train and achieve good performance (in terms of performance/parameter reduction) compared to other methods on Imagenet.[[MET-POS,RES-POS], [CMP-POS,EMP-POS], [APC], [MAJ]]\n\nSome of the hyperparameters used (alpha and specially rho) seem to be used very ad-hoc.[[PDI-NEU,EXP-NEG,MET-NEG,ANA-NEG], [EMP-NEG], [DFT], [MIN]] Could the authors explain their choices?[[RWK-NEU], [null], [QSN], [GEN]] How sensible is the algorithm to these hyperparameters?\[[DAT-NEU,EXP-NEU,MET-NEU], [null], [QSN], [GEN]]nIt would be nice to see empirically how much of computation the proposed approach takes during training. [[EXP-POS,MET-POS,ANA-POS], [CMP-POS,EMP-POS], [APC], [MAJ]]How much longer does it takes to train the model with the ISTA based constraint[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [GEN]]?\n\nOverall this is a good paper and I believe it should be accepted, given the authors are more clear on the details pointed above.[[INT-POS,OAL-POS], [CLA-POS,APR-POS,REC-POS], [APC,FBK], [MAJ]]\n"
