"This paper presents a reading comprehension model using convolutions and attention.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] This model does not use any recurrent operation but it is not per se simpler than a recurrent model.[[RWK-NEU], [CMP-NEU], [DIS], [GEN]] Furthermore, the authors proposed an interesting idea to augment additional training data by paraphrasing based on off-the-shelf neural machine translation.[[DAT-POS,MET-POS], [EMP-POS], [APC], [MAJ]]  On SQuAD dataset, their results show some small improvements using the proposed augmentation technique.[[DAT-NEU,MET-NEU,RES-NEU], [EMP-NEU], [DIS], [MIN]] Their best results, however, do not outperform the best results reported on the leader board.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nOverall, this is an interesting study on SQuAD dataset.[[DAT-POS,OAL-POS], [IMP-POS], [APC], [MAJ]] I would like to see results on more datasets and more discussion on the data augmentation technique.[[DAT-NEU,RES-NEU,ANA-NEU], [SUB-NEU], [DIS], [MIN]] At the moment, the description in section 3 is fuzzy in my opinion. [[CNT], [PNF-NEG], [CRT], [MIN]]Interesting information could be:\n- how is the performance of the NMT system?[[DAT-NEU,RES-NEU], [PNF-NEU], [QSN], [MIN]] \n- how many new data points are finally added into the training data set?[[DAT-NEU], [null], [QSN], [MIN]]\n- what do \u2018data aug\u2019 x 2 or x 3 exactly mean?\n"[[DAT-NEU], [PNF-NEU], [QSN], [MIN]]