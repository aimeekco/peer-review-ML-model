"Science is about reproducible results and it is very commendable from scientists to hold their peers accountable for their work by verifying their results.[[EXT-NEU], [null], [SMY], [GEN]] It is also necessary to inspect claims that are made by researchers to avoid the community straying in the wrong direction.[[EXT-NEU], [null], [SMY], [GEN]] However, any critique needs to be done properly, by 1) attending to the actual claims that were made in the first place,[[EXT-NEU], [null], [SMY], [GEN]] by 2) reproducing the results in the same way as in the original work,[[EXT-NEU], [null], [SMY], [GEN]] 3) by avoiding introducing false claims based on a misunderstanding of terminology [[EXT-NEU], [null], [SMY], [GEN]]and 4) by extensively researching the literature before trying to affirm that a general method (here, Deep RL) cannot solve certain tasks.[[EXT-NEU], [null], [SMY], [GEN]]\n\nThis paper is a critique of deep reinforcement learning methods for learning to navigate in 3D environments, and seems to focus intensively on one specific paper (Mirowski et al, 2016, \u201cLearning to Navigate in Complex Environments\u201d) and one of the architectures (NavA3C+D1D2L) from that paper.[[INT-NEU,RWK-NEU,PDI-NEU], [null], [SMY], [GEN]] It conducts an extensive assessment of the methods in the critiqued paper but does not introduce any alternative method.[[MET-NEU], [null], [SMY], [GEN]] For this reason, I had to carefully re-read the critiqued paper to be able to assess the validity of the arguments made in this submission and to evaluate its merit from the point of view of the quality of the critique.[[EXT-NEU], [null], [SMY], [GEN]] The (Mirowski et al, 2016) paper shows that a neural network-based agent with LSTM-based memory and auxiliary tasks such as depth map prediction can learn to navigate in fixed environments (3D mazes) with a fixed goal position (what they call \u201cstatic maze\u201d), and in fixed mazes with changing goal environments (what they call \u201cenvironments with dynamic elements\u201d or \u201crandom goal mazes\u201d).[[RWK-NEU], [null], [SMY], [GEN]]\n\nThis submission claims that:\n[a] \u201c[based on the critiqued paper] one might assume that DRL-based algorithms are able to 'learn to navigate' and are thus ready to replace classical mapping and path-planning algorithms\u201d,\n[b] \u201cfollowing training and testing on constant map structures, when trained and tested on the same maps, [the NavA3C+D1D2L algorithm] is able to choose the shorter paths to the goal\u201d,[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]]\n[c] \u201cwhen tested on unseen maps the algorithm utilizes a wall-following strategy to find the goal without doing any mapping or path planning\u201d,[[RWK-NEG,MET-NEG], [CMP-NEG], [CRT], [MAJ]]\n[d] \u201cthis state-of-the-art result is shown to be successful on only one map, which brings into question the repeatability of the results\u201d,[[RWK-NEG,RES-NEG], [CMP-NEG], [CRT], [MAJ]]\n[e] \u201cDo DRL-based navigation algorithms really 'learn to navigate'?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Our results answer this question negatively.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]]\u201d\n[f] \u201cwe are the first to evaluate any DRL-based navigation method on maps with unseen structures[[MET-NEU], [null], [DIS], [GEN]]\u201d\n\nThe paper also conducts an extensive analysis of the performance of a different version of the NavA3C+D1D2L algorithm (without velocity inputs, which probably makes learning path integration much more difficult), in the same environments but by introducing unjustified changes (e.g., with constant velocities and a different action space) and with a different reward structure (incorporating a negative reward for wall collisions).[[MET-NEG,ANA-NEG], [EMP-NEG], [CRT], [MAJ]] While the experimental setup does not match (Mirowski et al, 2016), thereby invalidating claim [d], the experiments are thorough and do show that that architecture does not generalize to unseen mazes.[[RWK-NEG,EXP-NEG], [CMP-NEG], [CRT], [MAJ]] The use of attention heat maps is interesting.[[MET-POS], [EMP-POS], [APC], [MAJ]]\n\nThe main problem however is that it seems that this submission completely misrepresents the intent of (Mirowski et al, 2016) by using a straw man argument, and makes a rather unacademic and unsubstantiated accusation of lack of repeatability of the results.[[RWK-NEG,RES-NEG], [CMP-NEG,EMP-NEG], [CRT], [MAJ]]\n\nRegarding the former, I could not find any claim that the methods in (Mirowski et al, 2017) learn mapping and path planning in unseen environments, that could support claim [a].[[RWK-NEG,MET-NEG], [CMP-NEG], [CRT], [MIN]] More worryingly, when observing that the method of (Mirowski et al, 2017) may not generalize to unseen environments in claim [c], the authors of this submission seem to confuse navigation, cartography and SLAM, and attribute to that work claims that were never made in the first place, using a straw man argument.[[RWK-NEG,RES-NEG], [CMP-NEG], [CRT], [MAJ]] Navigation is commonly defined as the goal driven control of an agent, following localization, and is a broad skill that involves the determination of position and direction, with or without a map of the environment (Fox 1998, \u201d Markov Localization: A Probabilistic Framework for Mobile Robot Localization and Navigation\u201d).[[MET-NEU], [null], [DIS], [GEN]] This widely accepted definition of navigation does not preclude being limited to known environments only.[[MET-NEU], [null], [DIS], [GEN]]\n\nRegarding repeatability, the claim [d] is contradicted in section 5 when the authors demonstrate that the NavA3C+D1D2L algorithm does achieve a reduction in latency to goal in 8 out of 10 experiments on random goal, static map and random or static spawns.[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]] The experiments in section 5.3 are conducted in simple but previously unseen maps and cannot logically contradict results (Mirowski et al, 2016) achieved by training on static maps such as their \u201cI-maze\u201d.[[RWK-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] Moreover, claim [d] about repeatability is also invalidated by the fact that\u00a0the experiments described in the paper use different observations (no velocity inputs), different action space, different reward structure, with no empirical evidence to support these changes.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]] It seems, as the authors also claim in [b], that the work of (Mirowski et al, 2017), which was about navigation in known environments, actually is repeatable.[[RWK-NEU], [null], [DIS], [GEN]]\n\nAdditionally, some statements made by the authors are demonstrably untrue.[[CNT], [null], [CRT], [MIN]] First, the authors claim that they are the first to train DRL agents in all random mazes [f], but this has been already shown in at least two publications (Mnih et al, 2016 and Jaderberg et al, 2016).[[MET-NEG], [NOV-NEG], [CRT], [MAJ]]\n\nSecond, the title of the submission, \u201cDo Deep Reinforcement Learning Algorithms Really Learn to Navigate\u201d makes a broad statement [e] that cannot be logically invalidated by only one particular set of experiments on a particular model and environment, particularly since it directly targets one specific paper (out of several recent papers that have addressed navigation) and one specific architecture from that paper, NavA3C+D1D2L (incidentally, not the best-performing one, according to table 1 in that paper).[[INT-NEU,RWK-NEU], [EMP-NEG], [CRT], [MAJ]] Why did the authors not cite and consider (Parisotto et al, 2017, \u201cNeural Map: Structured Memory for Deep Reinforcement Learning\u201d), which explicitly claims that their method is \u201ccapable of generalizing to environments that were not seen during training\u201d?[[RWK-NEG,EXP-NEG,MET-NEG], [CMP-NEG], [DFT,QSN], [MIN]] It seems that the authors need to revise both their bibliography and their logical reasoning: one cannot invalidate a broad set of algorithms for a broad goal, simply by taking a specific example and showing that it does not fit a particular interpretation of navigation *in previously unseen environments*.\n"[[MET-NEG,BIB-NEG], [CNT], [CRT], [MIN]]