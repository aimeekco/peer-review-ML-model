"This paper analyzes the effect of image features on image captioning.[[PDI-NEU], [null], [SMY], [GEN]] The authors propose to use a model similar to that of Vinyals et al., 2015 and change the image features it is conditioned on.[[PDI-NEU,RWK-NEU], [null], [SMY], [GEN]] The MSCOCO captioning and Flickr30K datasets are used for evaluation.[[DAT-NEU], [null], [SMY], [GEN]]\n\nIntroduction\n- The introduction to the paper could be made clearer - the authors talk about the language of captioning datasets being repetitive, but that fact is neither used or discussed later.[[INT-NEU], [CLA-NEU], [SUG], [MIN]]\n- The introduction also states that the authors will propose ways to improve image captioning.[[INT-NEU], [PNF-NEU], [SUG], [MIN]] This is never discussed.[[INT-NEG], [CNT], [DFT], [MIN]]\n\nCaptioning Model and Table 1\n- The authors use greedy (argmax) decoding which is known to result in repetitive captions.[[TNF-NEU], [PNF-NEU], [DIS], [MIN]] In fact, Vinyals et al. note this very point in their paper.[[RWK-NEU], [null], [DIS], [GEN]] I understand this design choice was made to focus more on the image side, rather than the decoding (language) side, but I find it to be very limiting.[[OAL-NEG], [SUB-NEG], [CRT], [MAJ]]  In this regime of greedy decoding it is hard to see any difference between the different ConvNet features used for captioning - Table 1 shows meteor scores within 0.19 - 0.22 for all methods.[[MET-NEG,TNF-NEG], [EMP-NEG], [CRT], [MAJ]] \n- Another effect (possibly due to greedy decoding + choice of model), is that the numbers in Table 1 are rather low compared to the COCO leaderboard.[[MET-NEG,TNF-NEG], [EMP-NEG], [CRT], [MAJ]] The top 50 entries have METEOR scores >= 0.25, while the maximum METEOR score reported by the authors is 0.22.[[RES-NEU], [EMP-NEU], [DIS], [MIN]] Similar trend holds for other metrics like BLEU-4.[[DAT-NEU], [null], [DIS], [GEN]]\n- The results of Table 5 need to be presented and interpreted in the light of this caveat of greedy decoding.[[RES-NEU], [EMP-NEU], [SUG], [MAJ]]\n\nExperimental Setup and Training Details\n- How was the model optimized?[[EXP-NEU], [EMP-NEU], [QSN], [MAJ]] No training details are provided.[[EXP-NEG,ANA-NEG], [SUB-NEG], [DFT,CRT], [MAJ]] Did you use dropout?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]] Were hyperparamters fixed for training across different feature sizes of VGG19 and ResNet-152?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]] What is the variance in the numbers for Table 1?[[TNF-NEU], [null], [QSN], [MIN]]\n\nMain claim of the paper\nDevlin et al., 2015 show a simple nearest neighbor baseline which in my opinion shows this more convincingly.[[RWK-NEG], [CMP-NEG], [CRT], [MAJ]] Two more papers from the same group which use also make similar observations - tweaking the image representation makes image captioning better: (1) Fang et al., 2015: Multiple-instance Learning using bag-of-objects helps captioning (2) Misra et al. 2016 (not cited): label noise can be modeled which helps captioning. This claim has been both made and empirically demonstrated earlier.\n\nMetrics for evaluation\n- Anderson et al., 2016 (not cited) proposed the SPICE metric and also showed how current metrics including CiDER may not be suitable for evaluating image captions.[[RWK-NEU], [CMP-NEU], [DIS], [MAJ]] The COCO leaderboard also uses this metric as one of its evaluation metrics.[[MET-NEU], [null], [DIS], [GEN]] If the authors are evaluating on the test set and reporting numbers, then it is odd that they `skipped' reporting SPICE numbers.[[DAT-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nChoice of Datasets\n- If we are thoroughly evaluating the effect of image features, doing so on other datasets is very important.[[DAT-NEU], [EMP-NEU], [DIS], [MAJ]] Visual Genome (Krishnan et al., not cited) and SIND (Huang et al., not cited) are two datasets which are both larger than Flickr30k and have different image distributions from MSCOCO.[[RWK-NEU,DAT-NEU], [CMP-NEU], [DIS], [GEN]] These datasets should show whether using more general features (YOLO-9k) helps.[[DAT-NEU], [EMP-NEU], [SUG], [MIN]]\nThe authors should evaluate on these datasets to make their findings stronger and more valuable.[[DAT-NEU], [EMP-NEU], [SUG], [MIN]]\n\nMinor comments\n- Figure 1 is hard to read on paper. Please improve it.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]]\n- Figure 2 is hard to read even on screen.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]] It is really interesting, so improving the quality of this figure will really help."[[TNF-NEG], [PNF-NEG], [SUG,CRT], [MIN]]