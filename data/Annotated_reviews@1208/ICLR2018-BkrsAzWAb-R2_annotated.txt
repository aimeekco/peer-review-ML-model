"The authors consider a method (which they trace back to 1998, but may have a longer history) of learning the learning rate of a first-order algorithm at the same time as the underlying model is being optimized, using a stochastic multiplicative update.[[INT-NEU,MET-NEU], [null], [SMY], [GEN]] The basic observation (for SGD) is that if \\theta_{t+1} = \\theta_t - \\alpha \\nabla f(\\theta_t), then \\partial/\\partial\\alpha f(\\theta_{t+1}) = -<\\nabla f(\\theta_t), \\nabla f(\\theta_{t+1})>, i.e. that the negative inner product of two successive stochastic gradients is equal in expectation to the derivative of the tth update w.r.t. the learning rate \\alpha.[[MET-NEU], [EMP-NEU], [SMY], [GEN]]\n\nI have seen this before for SGD (the authors do not claim that the basic idea is novel), but I believe that the application to other algorithms (the authors explicitly consider Nesterov momentum and ADAM) are novel, as is the use of the multiplicative and normalized update of equation 8 (particularly the normalization).[[MET-POS], [NOV-POS], [DIS], [MAJ]]\n\nThe experiments are well-presented, and appear to convincingly show a benefit.[[EXP-POS], [PNF-POS,EMP-POS], [APC], [MAJ]] Figure 3, which explores the robustness of the algorithms to the choice of \\alpha_0 and \\beta, is particularly nicely-done, and addresses the most natural criticism of this approach (that it replaces one hyperparameter with two).[[MET-NEU,TNF-POS], [PNF-POS,EMP-POS], [APC], [MAJ]]\n\nThe authors highlight theoretical convergence guarantees as an important future work item, and the lack of them here (aside from Theorem 5.1, which just shows asymptotic convergence if the learning rates become sufficiently small) is a weakness, but not, I think, a critical one.[[MET-NEU,FWK-POS], [EMP-NEU], [DIS], [MAJ]] This appears to be a promising approach, and bringing it back to the attention of the machine learning community is valuable."[[MET-POS], [EMP-POS], [APC], [MAJ]]