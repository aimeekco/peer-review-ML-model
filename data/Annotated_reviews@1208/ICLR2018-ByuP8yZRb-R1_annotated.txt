"The below review addresses the first revision of the paper[[EXT-NEU], [null], [CNT], [GEN]]. The revised version does address my concerns.[[OAL-POS], [null], [APC], [GEN]] The fact that the paper does not come with substantial theoretical contributions/justification still stands out.[[MET-NEU], [EMP-NEU], [APC], [MAJ]]\n\n---\n\nThe authors present a variant of the adversarial feature learning (AFL) approach by Edwards & Storkey.[[RWK-NEU], [null], [SMY], [MAJ]] AFL aims to find a data representation that allows to construct a predictive model for target variable Y, and at the same time prevents to build a predictor for sensitive variable S.[[RWK-NEU], [null], [DIS], [GEN]] The key idea is to solve a minimax problem where the log-likelihood of a model predicting Y is maximized, and the log-likelihood of an adversarial model predicting S is minimized.[[RWK-NEU], [null], [DIS], [GEN]] The authors suggest the use of multiple adversarial models, which can be interpreted as using an ensemble model instead of a single model.[[MET-NEU], [null], [SMY], [GEN]]\n\nThe way the log-likelihoods of the multiple adversarial models are aggregated does not yield a probability distribution as stated in Eq. 2.[[EXP-NEG,MET-NEU], [EMP-NEG], [DFT], [MAJ]] While there is no requirement to have a distribution here - a simple loss term is sufficient - the scale of this term differs compared to calibrated log-likelihoods coming from a single adversary.[[MET-NEU], [EMP-NEU], [DFT,DIS], [GEN]] Hence, lambda in Eq. 3 may need to be chosen differently depending on the adversarial model. Without tuning lambda for each method, the empirical experiments seem unfair.[[MET-NEU], [EMP-NEG], [SUG,DFT], [MAJ]] This may also explain why, for example, the baseline method with one adversary effectively fails for Opp-L.[[RWK-POS], [null], [SUG,DIS], [GEN]] A better comparison would be to plot the performance of the predictor of S against the performance of Y for varying lambdas. The area under this curve allows much better to compare the various methods.[[EXP-NEU,MET-NEU], [CMP-NEU], [SUG], [MIN]]\n\nThere are little theoretical contributions. Basically, instead of a single adversarial model - e.g., a single-layer NN or a multi-layer NN - the authors propose to train multiple adversarial models on different views of the data.[[MET-NEU], [EMP-NEG], [DFT], [MAJ]] An alternative interpretation is to use an ensemble learner where each learner is trained on a different (overlapping) feature set.[[MET-NEU], [null], [DIS], [GEN]] Though, there is no theoretical justification why ensemble learning is expected to better trade-off model capacity and robustness against an adversary.[[MET-NEG], [EMP-NEG], [DFT], [MIN]] Tuning the architecture of the single multi-layer NN adversary might be as good?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nIn short, in the current experiments, the trade-off of the predictive performance and the effectiveness of obtaining anonymized representations effectively differs between the compared methods. This renders the comparison unfair.[[RWK-NEU,EXP-NEG], [CMP-NEG], [DIS], [MAJ]] Given that there is also no theoretical argument why an ensemble approach is expected to perform better,[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] I recommend to reject the paper."[[OAL-NEG], [REC-NEG], [FBK], [MAJ]]