"The paper shows that several recently proposed interpretation techniques for neural network are performing similar processing and yield similar results.[[INT-NEU,MET-NEU,RES-NEU], [null], [SMY], [GEN]] The authors show that these techniques can all be seen as a product of input activations and a modified gradient, where the local derivative of the activation function at each neuron is replaced by some fixed function.[[MET-NEU], [null], [SMY], [GEN]]\n\nA second part of the paper looks at whether explanations are global or local.[[CNT], [CNT], [CNT], [MIN]] The authors propose a metric called sensitivity-n for that purpose, and make some observations about the optimality of some interpretation techniques with respect to this metric in the linear case.[[MET-NEU], [null], [SMY], [GEN]] The behavior of each explanation w.r.t. these properties is then tested on multiple DNN models tested on real-world datasets.[[DAT-NEU,EXP-NEU,MET-NEU], [null], [SMY], [GEN]] Results further outline the resemblance between the compared methods.[[MET-NEU,RES-NEU], [null], [DIS], [GEN]]\n\nIn the appendix, the last step of the proof below Eq. 7 is unclear.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] As far as I can see, the variable g_i^LRP wasn\u2019t defined, and the use of Eq. 5 to achieve this last could be better explained.[[MET-NEG], [EMP-NEG], [SUG,CRT], [MIN]] There also seems to be some issues with the ordering i,j, where these indices alternatively describe the lower/higher layers, or the higher/lower layers."[[MET-NEG], [EMP-NEG], [CRT], [MIN]]