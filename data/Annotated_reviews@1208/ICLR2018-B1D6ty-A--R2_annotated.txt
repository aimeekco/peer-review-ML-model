"The authors propose an alternating minimization framework for training autoencoders and encoder-decoder networks.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The central idea is that a single encoder-decoder network can be cast as an alternating minimization problem.[[PDI-NEU], [null], [SMY], [GEN]] Each minimization problem is not convex but is quasi-convex and hence one can use stochastic normalized gradient descent to minimize w.r.t. each variable.[[PDI-NEU], [null], [SMY], [GEN]] This leads to the proposed algorithm called DANTE which simply minimizes w.r.t. each variable using stochastic normalized gradient algorithm to minimize w.r.t. each variable The authors start with this idea and introduce a generalized ReLU which is specified via a subgradient function only whose local quasi-convexity properties are established.[[MET-NEU], [null], [SMY], [GEN]] They then extend these idea to multi-layer encoder-decoder networks by performing greedy layer-wise training and using the proposed algorithms for training each layer.[[MET-NEU,EXP-NEU], [null], [DIS], [GEN]] The ideas are interesting,[[PDI-POS], [EMP-POS], [APC], [MAJ]] but I have some concerns regarding this work.[[OAL-NEU], [CNT], [CRT], [MAJ]]\n\nMajor comments:\n\n1. When dealing with a 2 layer network where there are 2 matrices W_1, W_2 to optimize over, It is not clear to me why optimizing over W_1 is a quasi-convex optimization problem?[[MET-NEG], [EMP-NEG], [QSN], [MAJ]] The authors seem to use the idea that solving a GLM problem is a quasi-convex optimization problem.[[PDI-NEU], [null], [DIS], [GEN]] However, optimizing w.r.t. W_1 is definitely not a GLM problem, since W_1 undergoes two non-linear transformations one via \\phi_1 and another via \\phi_2.[[PDI-NEG], [EMP-NEG], [CRT], [MAJ]] Could the authors justify why minimizing w.r.t. W_1 is still a quasi-convex optimization problem?[[PDI-NEU], [EMP-NEU], [QSN], [MIN]]\n\n2. Theorem 3.4, 3.5 establish  SLQC properties with generalized RELU activations.[[MET-NEU], [null], [DIS], [GEN]] This is an interesting result, and useful in its own right.[[RES-POS], [EMP-POS], [APC], [MAJ]] However, it is not clear to me why this result is even relevant here. The main application of this paper is autoencoders, which are functions from R^d -> R^d. However, GLMs are functions from R^d ---> R. So, it is not at all clear to me how Theorem 3.4, 3.5 and eventually 3.6 are useful for the autoencoder problem that the authors care about.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Yes they are useful if one was doing 2-layer neural networks for binary classification, but it is not clear to me how they are useful for autoencoder problems.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n\n3. Experimental results for classification are not convincing enough.[[EXP-NEG,RES-NEG], [EMP-NEG], [CRT], [MIN]] If, one looks at Table 1. SGD outperforms DANTE on ionosphere dataset and is competent with DANTE on MNIST and USPS.[[DAT-NEU,MET-NEG], [CMP-NEG,EMP-NEG], [CRT], [MAJ]] \n\n4. The results on reconstruction do not show any benefits for DANTE over SGD (Figure 3).[[RES-NEG], [CMP-NEG], [CRT], [MAJ]] I would recommend the authors to rerun these experiments but truncate the iterations early enough.[[RES-NEG], [EMP-NEG], [SUG], [MAJ]] If DANTE has better reconstruction performance than SGD with fewer iterations then that would be a positive result."[[MET-NEU,RES-NEU], [EMP-NEU], [SUG], [MIN]]