"** post-rebuttal revision **\n\nI thank the authors for running the baseline experiments, especially for running the TwinNet to learn an agreement between two RNNs going forward in time.[[RWK-POS,EXP-NEU], [EMP-POS], [APC], [MAj]] This raises my confidence that what is reported is better than mere distillation of an ensemble of rnns.[[EXP-POS], [CMP-POS], [APC], [MAJ]] I am raising the score.[[OAL-POS], [EMP-POS], [APC], [MAJ]]\n\n** original review **\n\n\nThe paper presents a way to regularize a sequence generator by making the hidden states also predict the hidden states of an RNN working backward.[[PDI-NEU], [null], [SMY], [GEN]]\n\nApplied to sequence-to-sequence networks, the approach requires training one encoder, and two separate decoders, that generate the target sequence in forward and reversed orders. [[EXP-NEU,MET-NEU], [null], [SMY], [GEN]]A penalty term is added that forces an agreement between the hidden states of the two decoders.[[MET-NEU], [null], [SMY], [GEN]] During model evaluation only the forward decoder is used, with the backward operating decoder discarded.[[MET-NEU], [null], [DIS], [MIN]] The method can be interpreted to generalize other recurrent network regularizers, such as putting an L2 loss on the hidden states.[[MET-NEU], [EMP-NEU], [SUG], [MIN]]\n\nExperiments indicate that the  approach is most successful when the regularized RNNs are conditional generators, which emit sequences of low entropy, such as decoders of a seq2seq speech recognition network.[[EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]] Negative results were reported when the proposed regularization technique was applied to language models, whose output distribution has more entropy.[[MET-NEU,RES-NEU], [null], [DIS], [GEN]]\n\nThe proposed regularization is evaluated with positive results on a speech recognition task and on an  image captioning task, and with negative results (no improvement, but also no deterioration) on a language modeling and sequential MNIST digit generation tasks.[[DAT-NEU,MET-NEU,RES-NEU], [null], [DIS], [GEN]]\n\nI have one question about baselines: is the proposed approach better than training to forward generators and force an agreement between them (in the spirit of the concurrent ICLR submission https://openreview.net/forum?id=rkr1UDeC-)?[[RWK-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]] \n\nAlso, would using the backward RNN, e.g. for rescoring, bring another advantage?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] In other words, what is (and is there) a gap between an ensemble of a forward and backward rnn and the forward-rnn only, but trained with the state-matching penalty?[[MET-NEU,EXP-NEU], [EMP-NEU], [QSN], [MIN]]\n\nQuality:\nThe proposed approach is well motivated and the experiments show the limits of applicability range of the technique.[[EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]]\n\nClarity:\nThe paper is clearly written[[OAL-POS], [CLA-POS], [APC], [MAJ]].\n\nOriginality:\nThe presented idea seems novel.[[PDI-POS], [NOV-POS], [APC], [MAJ]]\n\nSignificance:\nThe method may prove to be useful to regularize recurrent networks, however I would like to see a comparison with ensemble methods.[[MET-POS], [EMP-POS], [APC], [MAJ]] Also, as the authors note the method seems to be limited to conditional sequence generators.[[MET-POS], [EMP-POS], [APC], [MAJ]]\n\nPros and cons:\nPros: the method is simple to implement, the paper lists for what kind of datasets it can be used.[[DAT-NEU,MET-POS], [EMP-POS], [APC], [MAJ]]\nCons: the method needs to be compared with typical ensembles of models going only forward in time, it may turn that it using the backward RNN is not necessary\n"[[MET-NEG], [CMP-NEG], [CRT], [MAJ]]