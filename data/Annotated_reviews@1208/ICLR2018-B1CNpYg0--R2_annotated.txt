"\nThis paper illustrates a method to compute produce word embeddings on the fly for rare words, using a pragmatic combination of existing ideas:[[INT-NEU,RWK-NEU,PDI-NEU], [null], [SMY], [GEN]]\n\n* Backing off to a separate decoder for rare words a la Luong and Manning (https://arxiv.org/pdf/1604.00788.pdf, should be cited, though the idea might be older).[[RWK-NEU,BIB-NEU], [null], [DFT], [MIN]]\n\n* Using character-level models a la Ling et al.[[RWK-NEU,BIB-NEU], [null], [SUG], [MIN]]\n\n* Using dictionary embeddings a la Hill et al.[[RWK-NEU,BIB-NEU], [null], [SUG], [MIN]]\n\nNone of these ideas are new before but I haven\u2019t seen them combined in this way before.[[RWK-NEG], [NOV-NEG], [DIS], [MIN]] This is a very practical idea, well-explained with a thorough set of experiments across three different tasks.[[PDI-POS,EXP-POS], [EMP-POS], [APC], [MAJ]] The paper is not surprising[[OAL-NEG], [NOV-NEG], [CRT], [MIN]] but this seems like an effective technique for people who want to build effective systems with whatever data they\u2019ve got. \n"[[MET-POS], [EMP-POS], [APC], [MAJ]]