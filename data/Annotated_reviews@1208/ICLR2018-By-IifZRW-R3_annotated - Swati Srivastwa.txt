"This paper investigates probabilistic activation functions that can be structured in a manner similar to traditional neural networks whilst deriving an efficient implementation and training regime that allows them to scale to arbitrarily sized datasets.[[INT-NEU,MET-NEU], [null], [SMY], [GEN]] \n\nThe extension of Gaussian Processes to Gaussian Process Neurons is reasonably straight forward, with the crux of the paper being the path taken to extend GPNs from intractable to tractable.[[MET-POS], [EMP-POS], [APC], [MAj]] \nThe first step, virtual observations, are used to provide stand ins for inputs and outputs of the GPN.[[MET-NEU], [null], [DIS], [GEN]] \nThese are temporary and are later made redundant.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] \nTo avoid the intractable marginalization over latent variables, the paper applies variational inference to approximate the posterior within the context of given training data.[[DAT-NEU,EXP-NEU], [EMP-NEU], [DIS], [MIN]] \nOverall the process by which GPNs are made tractable to train leverages many recent and not so recent techniques.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \n\nThe resulting model is theoretically scalable to arbitrary datasets as the total model parameters are independent of the number of training samples.[[DAT-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]] \nIt is unfortunate but understandable that the GPN model experiments are confined to another paper."[[EXP-NEG], [EMP-NEG], [CRT], [MIN]] 