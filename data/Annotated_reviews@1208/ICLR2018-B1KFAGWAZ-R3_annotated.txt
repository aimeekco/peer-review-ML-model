"The paper presents results across a range of cooperative multi-agent tasks, including a simple traffic simulation and StarCraft micro-management.[[RES-NEU], [null], [DIS], [MIN]] The architecture used is a fully centralized actor (Master) which observes the central state in combination with agents that receive local observation, MS-MARL.[[MET-NEU], [null], [DIS], [MIN]] \nA gating mechanism is used in order to produce the contribution from the hidden state of the master to the logits of each agent.[[MET-NEU], [null], [DIS], [MIN]]  This contribution is added to the logits coming from each agent.[[MET-NEU], [null], [DIS], [MIN]] \n\nPros: \n-The results on StarCraft are encouraging and present state of the art performance if reproducible.[[RES-POS], [EMP-POS], [APC], [MAJ]]\n\nCons:\n-The experimental evaluation is not very thorough:[[EXP-NEG], [SUB-NEG], [CRT], [MIN]]\nNo uncertainty of the mean is stated for any of the results.[[RES-NEG], [EMP-NEG], [CRT], [MIN]] 100 evaluation runs is very low.[[RES-NEG], [EMP-NEG], [CRT], [MIN]] It is furthermore not clear whether training was carried out on multiple seeds or whether these are individual runs.[[EXP-NEG], [EMP-NEG], [CRT], [MIN]] \n\n-BiCNet and CommNet are both aiming to learn communication protocols which allow decentralized execution.[[MET-NEU], [EMP-NEU], [CRT], [MIN]] Thus they represent weak baselines for a fully centralized method such as MS-MARL.[[RWK-NEG,MET-NEG], [CMP-NEG], [CRT], [MAJ]] \nThe only fully centralized baseline in the paper is GMEZO, however results stated are much lower than what is reported in the original paper (eg. 63% vs 79% for M15v16). [[RWK-NEG,RES-NEG], [CMP-NEG], [CRT], [MAJ]]The paper is missing further centralized baselines.[[RWK-NEG,ANA-NEG], [SUB-NEG], [DFT], [MAJ]] \n\n-It is unclear to what extends the novelty of the paper (specific architecture choices) are required.[[OAL-NEG], [NOV-NEG], [CRT], [MAJ]] For example, the gating mechanism for producing the action logits is rather complex and seems to only help in a subset of settings (if at all).[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nDetailed comments:\n\"For all tasks, the number of batch per training epoch is set to 100.[[DAT-NEU,EXP-NEU], [null], [DIS], [MIN]]\"\nWhat does this mean?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]]\n\nFigure 1: \nThis figure is very helpful, however the colour for M->S is wrong in the legend.[[TNF-NEG], [CLA-NEG], [CRT], [MIN]] \n\nTable 2:\nGMEZO win rates are low compared to the original publication.[[RWK-NEG], [CMP-NEG], [CRT], [MAJ]] \nWhat many independent seeds where used for training?[[DAT-NEU,EXP-NEU], [EMP-NEU], [QSN], [MIN]] What are the confidence intervals? [[DAT-NEU,EXP-NEU], [EMP-NEU], [QSN], [MIN]]How many runs for evaluation?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]] \n\n\nFigure 4:\nB) What does it mean to feed two vectors into a Tanh?[[EXP-NEU,TNF-NEU], [EMP-NEU], [QSN], [MIN]] This figure currently very unclear.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]] What was the rational for choosing a vanilla RNN for the slave modules?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]]\n\nFigure 5:\na) What was the rational for stopping training of CommNet after 100 epochs?[[DAT-NEU,EXP-NEU,TNF-NEU], [EMP-NEU], [QSN], [MIN]] The plot looks like CommNet is still improving.[[TNF-NEU], [PNF-NEU], [DIS], [MIN]] \nc) This plot is disconcerting.[[EXP-NEU], [EMP-NEU], [QSN], [MIN]] Training in this plot is very unstable.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]] The final performance of the method ('ours') does not match what is stated in 'Table 2'.[[MET-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] I wonder if this is due to the very small batch size used (\"a small batch size of 4 \").\n\n\n"[[MET-NEU], [EMP-NEU], [DIS], [MIN]]