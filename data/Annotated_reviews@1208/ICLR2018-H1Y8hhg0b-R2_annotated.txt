"Learning sparse neural networks through L0 regularisation\[[MET-NEU], [null], [DIS], [GEN]]n\nSummary: \n\nThe authors introduce a gradient-based approach to minimise an objective function with an L0 sparse penalty.[[INT-NEU,MET-NEU], [EMP-POS], [APC], [MAJ]] The problem is relaxed onto a continuous optimisation by changing an expectation over discrete variables (representing whether a variable is present or not) to an expectation over continuous variables, inspired by earlier work from Maddison et al (ICLR 2017) where a similar transformation was used to learn over discrete variable prediction tasks with neural networks[[RWK-NEU,PDI-NEU,MET-NEU], [CMP-NEU,EMP-NEU], [SMY,DIS], [GEN]]. Here the application is to learn sparse feedforward networks in standard classification tasks, although the framework described is quite general and could be used to impose L0 sparsity to any objective function in principal.[[MET-NEU], [EMP-NEU], [SMY], [GEN]] The method provides equivalent accuracy and sparsity to published state-of-the-art results on these datasets [[DAT-POS,MET-POS,RES-POS], [NOV-POS], [SMY], [MAJ]]but it is argue that learning sparsity during the training process will lead to significant speed-ups - this is demonstrated by comparing to a theoretical benchmark (standard training with dropout) rather than through empirical testing against other implementations[[EXP-NEU,MET-NEU,ANA-NEU], [CMP-NEU,EMP-NEU], [SMY], [GEN]]. \n\nPros:\n\nThe paper is well written and the derivation of the method is easy to follow with a good explanation of the underlying theory. [[INT-POS,MET-POS], [CLA-POS,IMP-POS,PNF-POS,EMP-POS], [APC], [MAJ]]\n\nOptimisation under L0 regularisation is a difficult and generally important topic and certainly has advantages over other sparse inference objective functions that impose shrinkage on non-sparse parameters.[[PDI-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] \n\nThe work is put in context and related to some previous relaxation approaches to sparsity.[[RWK-NEU], [CMP-NEU], [DIS], [GEN]] \n\nThe method allows for sparsity to be learned during training rather than after training (as in standard dropout approaches) and this allows the algorithm to obtain significant per-iteration speed-ups, which improves through training. [[PDI-POS,MET-POS], [CMP-POS,EMP-POS], [APC], [MAJ]]\n\nCons:\n\nThe method is applied to standard neural network architectures and performance in terms of accuracy and final achieved sparsity is comparable to the state-of-the-art methods[[MET-NEU,RES-NEU], [NOV-NEU], [SMY], [GEN]]. Therefore the main advance is in terms of learning speed to obtain this similar performance.[[MET-POS], [IMP-POS], [APC], [MAJ]] However, the learning speed-up is presented against a theoretical FLOPs estimate per iteration for a similar network with dropout[[MET-POS], [EMP-POS], [APC], [MAJ]]. It would be useful to know whether the number of iterations to achieve a particular performance is equivalent for all the different architectures considered,e.g. does the proposed sparse learning method converge at the same rate as the others?[[PDI-NEU,MET-NEU,ANA-NEU], [CMP-NEU,EMP-NEU], [DIS], [GEN]] I felt a more thorough experimental section would have greatly improved the work, focussing on this learning speed aspect. [[PDI-NEG,EXP-NEG], [IMP-NEG,EMP-NEG], [DFT], [MIN]]\n\nIt was unclear how much tuning of the lambda hyper-parameter, which tunes the sparsity, would be required in a practical application since tuning this parameter would increase computation time.[[MET-NEG], [EMP-NEG], [DFT], [MIN]] It might be useful to provide a full Bayesian treatment so that the optimal sparsity can be chosen through hyper-parameter learning.[[MET-POS,FWK-POS], [IMP-POS,EMP-POS], [APC], [MAJ]] \n\nMinor point: it wasn\u2019t completely clear to me why the fact (3) is a variational approximation to a spike-and-slab is important (Appendix).[[PDI-POS], [CLA-POS], [APC], [MAJ]] I don\u2019t see why the spike-and-slab is any more fundamental than the L0 norm prior in (2),[[RWK-NEG,EXP-NEG], [CMP-NEG,EMP-NEG], [DFT], [MIN]] it is just more convenient in Bayesian inference because it is an iid prior and potentially allows an informative prior over each parameter. [[MET-POS], [EMP-POS], [APC], [MAJ]]In the context here this didn\u2019t seem a particularly relevant addition to the paper. [[RWK-POS], [REC-POS], [APC], [MAJ]]\n\n"