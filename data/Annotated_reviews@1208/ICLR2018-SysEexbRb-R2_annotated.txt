"This paper studies the critical points of shallow and deep linear networks.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The authors give a (necessary and sufficient) characterization of the form of critical points and use this to derive necessary and sufficient conditions for which critical points are global optima.[[DAT-POS,RES-POS], [EMP-POS], [APC], [MAJ]] Essentially this paper revisits a classic paper by Baldi and Hornik (1989) and relaxes a few requires assumptions on the matrices.[[RWK-NEU], [CMP-NEU], [DIS], [MIN]] I have not checked the proofs in detail but the general strategy seems sound.[[MET-POS], [EMP-POS], [APC], [MIN]] While the exposition of the paper can be improved in my view this is a neat and concise result and merits publication in ICLR.[[OAL-POS], [APR-POS], [APC], [MAJ]] The authors also study the analytic form of critical points of a single-hidden layer ReLU network.[[ANA-NEU], [null], [SMY,DIS], [GEN]] However, given the form of the necessary and sufficient conditions the usefulness of of these results is less clear.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]]\n\n\nDetailed comments:\n\n- I think in the title/abstract/intro the use of Neural nets is somewhat misleading as neural nets are typically nonlinear.[[ABS-NEG,INT-NEG], [CNT], [CRT], [MAJ]] This paper is mostly about linear networks.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] While a result has been stated for single-hidden ReLU networks.[[RES-NEU], [null], [DIS], [GEN]] In my view this particular result is an immediate corollary of the result for linear networks.[[RES-NEU], [EMP-NEU], [DIS], [GEN]] As I explain further below given the combinatorial form of the result, the usefulness of this particular extension to ReLU network is not very clear.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] I would suggest rewording title/abstract/intro[[ABS-NEG,INT-NEG], [PNF-NEG], [SUG], [MIN]]\n\n- Theorem 1 is neat, well done![[MET-POS], [EMP-POS], [APC], [MAJ]]\n\n- Page 4 p_i\u2019s in proposition 1\nFrom my understanding the p_i have been introduced in Theorem 1 but given their prominent role in this proposition they merit a separate definition (and ideally in terms of the A_i directly).[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \n\n- Theorems 1, prop 1, prop 2, prop 3, Theorem 3, prop 4 and 5\n\tAre these characterizations computable i.e. given X and Y can one run an algorithm to find all the critical points or at least the parameters used in the characterization p_i, V_i etc?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\n- Theorems 1, prop 1, prop 2, prop 3, Theorem 3, prop 4 and 5\n\tWould recommend a better exposition why these theorems are useful.[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]] What insights do you gain by knowing these theorems etc.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Are less sufficient conditions that is more intuitive or useful(an insightful sufficient condition in some cases is much more valuable than an unintuitive necessary and sufficient one).[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\n- Page 5 Theorem 2\n\tDoes this theorem have any computational implications?[[MET-NEU, RES-NEU], [EMP-NEU], [QSN], [MIN]] Does it imply that the global optima can be found efficiently, e.g. are saddles strict with a quantifiable bound?[[MET-NEU, RES-NEU], [EMP-NEU], [QSN], [MIN]]\n\n- Page 7 proposition 6 seems like an immediate consequence of Theorem 1 however given the combinatorial nature of the K_{I,J} it is not clear why this theorem is useful.[[MET-NEG, RES-NEU], [EMP-NEG], [QSN], [MIN]] e.g . back to my earlier comment w.r.t. Linear networks given Y and X can you find the parameters of this characterization with a computationally efficient algorithm? \n"[[MET-NEU], [EMP-NEU], [QSN], [MIN]]