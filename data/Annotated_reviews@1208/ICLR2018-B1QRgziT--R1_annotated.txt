"This paper borrows the classic idea of spectral regularization, recently applied to deep learning by Yoshida and Miyato (2017) and use it to normalize GAN objectives.[[RWK-NEU,PDI-NEU], [null], [SMY], [GEN]] The ensuing GAN, coined SN-GAN, essentially ensures the Lipschitz property of the discriminator.[[PDI-NEU], [null], [SMY], [GEN]] This Lipschitz property has already been proposed by recent methods and has showed some success.[[RWK-NEU], [null], [SMY], [GEN]] However,  the authors here argue that spectral normalization is more powerful; it allows for models of higher rank (more non-zero singular values) which implies a more powerful discriminator and eventually more accurate generator.[[MET-POS], [EMP-POS], [APC], [MAJ]] This is demonstrated in comparison to weight normalization in Figure 4.[[TNF-NEU], [null], [SMY], [GEN]] The experimental results are very good and give strong support for the proposed normalization.[[EXP-POS,MEt-POS,RES-POS], [EMP-POS], [APC], [MAJ]]\n\n\nWhile the main idea is not new to machine learning (or deep learning), to the best of my knowledge it has not been applied on GANs.[[MET-NEG], [NOV-NEG], [CRT], [MAJ]] The paper is overall well written (though check Comment 3 below), it covers the related work well and it includes an insightful discussion about the importance of high rank models.[[RWK-POS,OAL-POS], [CLA-POS,EMP-POS], [APC], [MAJ]] I am recommending acceptance,[[OAL-POS], [REC-POS], [FBK], [MAJ]] though I anticipate to see a more rounded evaluation of the exact mechanism under which SN improves over the state of the art.[[RWK-NEU,MET-NEU], [SUB-NEU], [SUG], [MIN]] More details in the comments below.[[OAL-NEU], [null], [DIS], [GEN]]\n\nComments:\n1. One concern about this paper is that it doesn\u2019t fully answer the reasons why this normalization works better.[[MET-NEG], [SUB-NEG], [DFT,CRT], [MIN]] I found the discussion about rank to be very intuitive,[[MET-POS], [EMP-POS], [APC], [MAJ]] however this intuition is not fully tested.[[MET-NEG], [SUB-NEG], [DFT], [MAJ]]  Figure 4 reports layer spectra for SN and WN.[[TNF-NEU], [null], [DIS], [GEN]] The authors claim that other methods, like (Arjovsky et al. 2017) also suffer from the same rank deficiency.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MIN]] I would like to see the same spectra included.[[MET-NEU], [null], [SMY], [GEN]] \n2. Continuing on the previous point: maybe there is another mechanism at play beyond just rank that give SN its apparent edge?[[RWK-NEU,MET-NEU], [CMP-NEU], [QSN], [MIN]] One way to test the rank hypothesis and better explain this method is to run a couple of truncated-SN experiments.[[EXP-NEU,MET-NEU], [EMP-NEU], [SUG], [MIN]] What happens if you run your SN but truncate its spectrum after every iteration in order to make it comparable to the rank of WN? Do you get comparable inception scores? Or does SN still win?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]]\n3. Section 4 needs some careful editing for language and grammar.\n"[[CNT], [CLA-NEG], [SUG], [MIN]]