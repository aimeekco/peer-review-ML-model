"The authors propose first applying dependency parsing to documents, then using pairs of words connected via dependency as features in a similarity metric.[[INT-NEU], [NULL], [SMY], [GEN]]\n\nWhile intriguing, a lot more work would be required to publish this at ICLR.[[OAL-NEG], [REC-NEG], [DFT], [MAJ]] Namely, the following questions need to be answered:\n\n1. Does using linked-word-pairs truly raise the state of the art?[[PDI-NEU], [EMP-NEU], [QSN], [MAJ]] Unlike what is stated in the abstract, the experimental results only compare RBMs with and without this feature.[[ABS-NEU,EXP-NEU], [CMP-NEU], [SMY], [GEN]] RBMs are not state-of-the-art in topic modeling, therefore it\u2019s difficult to assess whether this is helpful.[[MET-NEG], [IMP-NEU], [DFT], [MIN]]\n2. If linked words does improve topic modeling, why does it do so?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] There needs to be some sort of error analysis to show why this idea improves, rather than simply stating metrics.[[RES-NEU,ANA-NEU],[EMP-NEU], [SUG], [MIN]]\n3. Are words that are linked via a dependency better than commonly co-occuring words?[[MET-NEU], [NULL], [QSN], [MIN]] Experiments need to be done to show that a full dependency parse is actually required, rather than simply looking for co-occuring words.[[EXP-NEU], [EMP-NEU], [SUG], [MIN]]\n4. How is this work related to the extensive work in NLP in applying parsing to various tasks?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] A quick search reveals [1] (probabilistic modeling of dependency parses to create Bayesian topic models directly) and [2] (creating a semantic vector space from a dependency parse) I suspect there are others[[ANA-NEU],[CMP-NEU], [DIS], [GEN]]. Citations in [2] could be a good place to start.[[RWK-NEU], [CMP-NEU], [SMY,DIS], [MIN]]\n5. Can the selection of word pairs be done automatically, from data, rather than pre-computed with a known dependency parser?[[DAT-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]] After all, this is submitted to the International Conference on Learning Representations --- feature engineering papers can easily be published at EMNLP, ICML, etc. An excellent ICLR paper would show some way to either (a) use dependency parsing only at training time (to provide a hint), or (b) not require dependency parsing at all.[[MET-NEU], [EMP-NEU], [SUG], [MIN]]\n\nA few suggestions for experiments:\nA. I would recommend first doing comparisons between bag-of-words representation and the dependency-bigram representation, just using log(tf)-idf as a distance metric.[[DAT-NEU,EXP-NEU], [CMP-NEU], [SUG], [MIN]] By stripping away more advanced modeling, that could reveal whether the dependency bi-gram has utility[[EXP-NEU,MET-NEU], [EMP-NEU], [SUG], [MIN]].\nB. The authors may wish to consider applying LSA to both bag of words and dependency-bigrams, using log(tf)-idf weighting for both[[DAT-NEU,EXP-NEU,MET-NEU], [SUB-NEU], [SUG], [MIN]]. From what I\u2019ve seen, log(tf)-idf LSA seems to perform about as well as LDA[[EXP-NEU,RES-POS], [EMP-POS], [SMY], [GEN]]. Plain LSA takes into account correlations between words  --- it would be interesting to see whether dependency-bigrams can improve on LSA at all[[DAT-NEU,EXP-NEU], [EMP-NEU], [SUG], [MIN]].\nC. Reiterating point (3) above, to really show whether the power of the dependency parse is being used, I would strongly suggest doing a null experiment with co-occuring nearby words.[[DAT-NEU,EXP-NEU], [SUB-NEU], [SUG], [MIN]]\n\n\nReferences:\n[1] Boyd-Graber, J. L., & Blei, D. M. (2009). Syntactic topic models. In Advances in neural information processing systems(pp. 185-192).\n[2] Pad\u00f3, S. and Lapata, M., 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33(2), pp.161-199.[[BIB-NEU], [null], [DIS], [GEN]]