"Update (original review below):\nThe authors have addressed several of the reviewers' comments and improved the paper.[[OAL-POS], [null], [APC], [MAJ]]\nThe motivation has certainly been clarified, but in my opinion it is still hazy.[[OAL-NEG], [CLA-NEG], [CRT], [MIN]] The paper does use skip connections, but the difference is that they are phased out over training.[[EXP-NEU], [null], [SMY], [GEN]] So I think that the motivation behind introducing this specific difference should be clear.[[PDI-NEU], [EMP-NEU], [DIS], [MAJ]] Is it to save the additional (small) overhead of using skip connections?[[PDI-NEU], [EMP-NEU], [QSN], [MIN]]\nNevertheless, the additional experiments and clarifications are very welcome.[[EXP-NEU], [CLA-NEU], [SUG], [MIN]]\n\nFor the newly added case of VAN(lambda=0), please note the strong similarity to https://arxiv.org/abs/1611.01260 (ICLR2017 reviews at https://openreview.net/forum?id=Sywh5KYex).[[RWK-NEU], [null], [DIS], [MIN]] In that report \\alpha_l is a scalar instead of a vector.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MIN]] \n\nAlthough it is interesting, the above case case also calls into question the additional value brought by the use of constrained optimization, a main contribution of the paper.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n \nIn light of the above, I have increased my score since I find this to be an interesting approach,[[MET-POS], [EMP-POS], [APC], [MAJ]] but in my opinion the significance of the results as they stand is low.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] The paper demonstrates that it is possible to obtain very deep plain networks (without skip connections) with improved performance  through the use of constrained optimization that gradually removes skip connections, but the value of this demonstration is unclear because a) consistent improvements over past work or the \\lambda=0 case were not found,[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] and b) The technique still relies on skip connections in a sense so it's not clear that it suggests a truly different method of addressing the degradation problem.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] \n\nOriginal Review\n=============\nSummary:\nThe contribution of this paper is a method for training deep networks such that skip connections are present at initialization, but gradually removed during training, resulting in a final network without any skip connections.[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]]\nThe paper first proposes an approach based on a formulation of deep networks with (non-parameterized, non-gated) skip connections with an equality constraint that effectively removes the skip connections when satisfied.[[EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]] It is proposed to optimize the formulation using the method of Lagrange multipliers.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\nA toy model with a single unit is used to illustrate the basic ideas behind the method.[[MET-NEU], [null], [DIS], [MIN]] Finally, experimental results for the task of image classification are reported using the MNIST, Fashion-MNIST, and CIFAR datasets.[[DAT-NEU,EXP-NEU,RES-NEU], [null], [DIS], [MIN]]\n\nQuality and significance:\nThe proposed methodology is simple and straightforward.[[MET-POS], [EMP-POS], [APC], [MAJ]] The analysis with the toy network is interesting and helps illustrate the method.[[MET-POS,ANA-POS], [EMP-POS], [APC], [MAJ]] However, my main concerns with this paper are related to motivation and experiments.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nThe motivation of the work is not clear at all.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] The stated goal is to address some of the issues related to the role of depth in deep networks, but I think it should be clarified which specific issues in particular are relevant to this method and how they are addressed.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] One could additionally consider that removing the skip connections at the end of training reduces the computational expense (slightly), but beyond that the expected utility of this investigation is very hazy from the description in the paper.[[EXP-NEG], [CLA-NEG,EMP-NEG], [CRT], [MAJ]]\n\nFor MNIST and MNIST-Fashion experiments, the motivation is mentioned to be similar to Srivastava et al. (2015), but in that study the corresponding experiment was designed to test if deeper networks could be optimized.[[RWK-NEU], [CMP-NEU], [DIS], [MIN]] Here, the generalization error is measured instead, which is heavily influenced by regularization.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Moreover, only some architectures appear to employ batch normalization, which is a potent regularizer.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] The general difference between plain and non-plain networks is very likely due to optimization difficulties alone, and due to the above issues further comparisons can not be made from the results.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] \n\nFor the CIFAR experiments, the experiment design is reasonable for a general comparison.[[DAT-POS,EXP-POS], [CMP-POS,EMP-POS], [APC], [MAJ]] Similar experimental setups have been used in previous papers to report that a proposed method can achieve good results, but there is no doubt that this does not make a rigorous comparison without employing expensive hyper-parameter searches.[[MET-NEG,RES-NEG], [CMP-NEG], [CRT], [MAJ]] This is not the fault of the present paper but an unfortunate tradition in the field.[[EXT-NEU], [null], [DIS], [MIN]] Nevertheless, it is important to note that direct comparison should not be made among approaches with key differences.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] For the reported results, Fitnets and Highway Networks did not use Batch Normalization (which is a powerful regularizer) while VANs and Resnets do.[[MET-NEU,RES-NEU], [EMP-NEU], [DIS], [MIN]] Moreover, it is important to report the training performance of deeper VANs (which have a worse generalization error) to clarify if the VANs suffered difficulties in optimization or generalization.[[EXP-NEU,ANA-NEU], [SUB-NEU], [DIS], [MIN]]\n\nClarity:\nThe paper is generally well-written and easy to read.[[OAL-POS], [CLA-POS], [APC], [MAJ]] There are some clarity issues related to the use of the term \"activation function\" and a typo in an equation but the authors are already aware of these."[[MET-NEG], [CLA-NEG], [CRT], [MIN]]