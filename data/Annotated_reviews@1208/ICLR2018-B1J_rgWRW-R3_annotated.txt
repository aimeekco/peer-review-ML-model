"The paper presents an analysis and characterization of ReLU networks (with a linear final layer) via the set of functions these networks can model, especially focusing on the set of \u201chard\u201d functions that are not easily representable by shallower networks.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]]  It makes several important contributions, including extending the previously published bounds by Telgarsky et al. to tighter bounds for the special case of ReLU DNNs, giving a construction for a family of hard functions whose affine pieces scale exponentially with the dimensionality of the inputs, and giving a procedure for searching for globally optimal solution of a 1-hidden layer ReLU DNN with linear output layer and convex loss.[[RWK-POS,MET-POS], [EMP-POS], [APC], [MAJ]]  I think these contributions warrant publishing the paper at ICLR 2018.[[OAL-POS], [APR-POS,REC-POS], [FBK], [MAJ]]  The paper is also well written, a bit dense in places, but overall well organized and easy to follow.[[OAL-POS], [CLA-POS,PNF-POS], [APC], [MAJ]] \n\nA key limitation of the paper in my opinion is that typically DNNs do not contain a linear final layer.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]  It will be valuable to note what, if any, of the representation analysis and global convergence results carry over to networks with non-linear (Softmax, e.g.) final layer.[[ANA-NEU], [EMP-NEU], [DIS], [MIN]]  I also think that the global convergence algorithm is practically unfeasible for all but trivial use cases due to terms like D^nw, would like hearing authors\u2019 comments in case I\u2019m missing some simplification.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nOne minor suggestion for improving readability is to explicitly state, whenever applicable, that functions under consideration are PWL.[[MET-NEU], [SUB-NEU,EMP-NEU], [SUG], [MIN]]  For example, adding PWL to Theorems and Corollaries in Section 3.1 will help. [[MET-NEU], [SUB-NEU,EMP-NEU], [SUG], [MIN]]  Similarly would be good to state, wherever applicable, the DNN being discussed is a ReLU DNN."[[MET-NEU], [SUB-NEU], [SUG], [MIN]] 