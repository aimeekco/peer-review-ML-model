"Summary: The authors show that using visual modality as a pivot they can train a model to translate from L1 to L2.[[INT-NEU,MET-NEU], [null], [SMY], [GEN]] \n\nPlease find my detailed comments/questions/suggestions below:\n\n1) IMO, the paper could have been written much better.[[CNT], [CLA-NEG], [SUG,CRT], [MIN]] At the core, this is simply a model which uses images as a pivot for learning to translate between L1 and L2 by learning a common representation space for {L1, image} or {L2, image}.[[MET-NEU], [null], [SMY], [GEN]] There are several works on such multimodal representation learning but the authors present their work in a way which makes it look very different from these works.[[MET-NEU], [null], [DIS], [GEN]]IMO, this leads to unnecessary confusion and does more harm than good.[[RWK-NEG,ANA-NEG], [null], [CRT], [MIN]] For example, the abstract gives an impression that the authors have designed a game to collect data (and it took me a while to set this confusion aside).\[[ABS-NEU], [null], [SMY], [GEN]]n\n2) Continuing on the above point, this is essentially about learning a common multimodal representation and then decode from this common representation.[[PDI-NEU], [null], [DIS], [GEN]] However, the authors do not cite enough work on such multimodal representation learning (for example, look at Spandana et. al.[[INT-NEU,ANA-NEU], [SUB-NEU], [DFT,APC], [MIN]]: Image Pivoting for Learning Multilingual Multimodal Representations, EMNLP 2017 for a good set of references)[[RWK-NEU,ANA-NEU], [CMP-NEU], [DIS], [GEN]]\n\n3) This omission of related work also weakens the experimental section.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]] At least for the word translation task many of these common representation learning frameworks could have been easily evaluated.[[RWK-NEU,ANA-NEU], [CMP-NEU], [DIS], [MIN]] For example, find the nearest german neighbour of the word \"dog\" in the common representation space. The authors instead compare with very simple baselines.[[RWK-NEU,ANA-NEU], [CMP-NEU], [DIS], [MIN]]\n\n4) Even when comparing with simple baselines, the proposed model does not convincingly outperform them. [[RWK-NEG,ANA-NEG], [CMP-NEG], [CRT], [MAJ]]In particular,  the P@5 and P@20 numbers are only slightly better[[RWK-POS], [CMP-POS], [APC], [MAJ]]. \n\n5) Some of the choices made in the Experimental setup seem questionable to me:[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]]\n   - Why  use a NMT model without attention? That is not standard and does not make sense to use when a better baseline model (with attention) is available ?[[RWK-NEU,MET-NEU], [CMP-NEU], [QSN], [MIN]]\n   - It is mentioned that \"While their model unit-normalizes the output of every encoder, we found this to consistently hurt performance, so do not use normalization for fair comparison with our models.[[EXP-NEG,RES-NEG], [EMP-NEG], [SUG], [MIN]]\" I don't think this is a fair comparison.[[RWK-NEG], [CMP-NEG], [CRT], [MAJ]] The authors can mention their results without normalization if that works well for them but it is not fair to drop normalization from the model of N&N if that gives better performance.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] Please mention the numbers with unit normalization to give a better picture.[[MET-NEU], [PNF-NEU], [SUG], [MIN]] It does not make sense to weaken an existing baseline and then compare with it.[[RWK-NEG,ANA-NEG], [CMP-NEG], [CRT], [MAJ]]\n\n6) It would be good to mention the results of the NMT model in Table 1 itself instead of mentioning them separately in a paragraph[[TNF-NEU], [PNF-NEU], [SUG], [MIN]]. This again leads to poor readability and it is hard to read and compare the corresponding numbers from Table 1.[[TNF-NEG], [CLA-NEG], [CRT], [MIN]]  I am not sure why this cannot be accommodated in the Table itself.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]]\n\n7) In Figure 2, what exactly do you mean by \"Results are averaged over 30 translation scenarios\". Can you please elaborate ?"[[TNF-NEG,RES-NEG], [CLA-NEG], [CRT], [MIN]]