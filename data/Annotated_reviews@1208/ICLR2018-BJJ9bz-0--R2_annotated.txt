"This paper proposes a method to learn a control policy from both interactions with an environment and demonstrations.[[INT-NEU], [null], [SMY], [GEN]] The method is inspired by the recent work on max entropy reinforcement learning and links between Q-learning and policy gradient methods.[[RWK-NEU,MET-NEU], [null], [SMY], [GEN]] Especially the work builds upon the recent work by Haarnoja et al (2017) and Schulman et al (2017) (both unpublished Arxiv papers).[[RWK-NEU], [null], [SMY], [GEN]] \n\nI'm also not sur to see much differences with the previous work by Haarnoja et al and Schulman et al.[[OAL-NEG], [CMP-NEG], [CRT], [MAJ]] It uses demonstrations to learn in an off-policy manner as in these papers.[[RWK-NEU], [CMP-NEU], [DIS], [GEN]] Also, the fact that the importance sampling ration is always cut at 1 (or not used at all) is inherited from these papers too.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [GEN]] \n\nThe authors say they compare to DQfD but the last version of this method makes use of prioritized replay so as to avoid reusing too much the expert transitions and overfit (L2 regularization is also used).[[MET-NEU], [EMP-NEU], [DIS], [MIN]] It seems this has not been implemented for comparison and that overfitting may come from this method missing.[[MET-NEG], [SUB-NEG,CMP-NEU], [DFT], [MIN]] \n\nI'm also uncomfortable with the way most of the expert data are generated for experiments.[[DAT-NEG,EXP-NEG], [EMP-NEG], [CRT], [MIN]] Using data generated by a pre-trained network is usually not representative of what will happen in real life.[[DAT-NEU], [EMP-NEU], [DIS], [MIN]] Also, corrupting actions with noise in the replay buffer is not simulating correctly what would happen in reality.[[MET-NEG,RES-NEG], [EMP-NEU], [DIS], [MIN]] Indeed, a single error in some given state will often generate totally different trajectories and not affect a single transition.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] So imperfect demonstration have very typical distributions.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] I acknowledge that some real human demonstrations are used but there is not much about them and the experiment is very shortly described. "[[EXP-NEG], [SUB-NEG], [CRT], [MIN]]