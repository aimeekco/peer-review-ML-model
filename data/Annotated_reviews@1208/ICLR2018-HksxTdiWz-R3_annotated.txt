"Overall strength:\nIn this paper, the authors proposed target-aware memory networks to model sentiment interactions between target aspects and the context words with attentions.[[OAL-POS], [CNT], [APC], [MAJ]]  This work has a well-established motivation: traditional attention for target-dependent sentiment classification cannot model the interaction between target term and context words when making predictions.[[PDI-POS], [EMP-POS], [APC], [MAJ]]  To solve this problem, the authors proposed five formulations in the final prediction layer.[[MET-NEU], [null], [SMY], [GEN]]  The illustration about the problem is clear, as well as the explanation for the formulations.[[PDI-POS,MET-POS], [EMP-POS], [APC], [MAJ]] \n\nMajor concerns:\n1.\tThis work brings some modifications to the prediction layer, which is a bit trivial.[[OAL-NEU], [EMP-NEU], [DIS], [MIN]]  Although the effect has been shown, the model is too specific to a narrow area, and is not general to be applied in a broad sense.[[MET-NEG], [SUB-NEG], [CRT], [MIN]]  It could have more contribution if the authors model the interactions within the attention model itself, instead of a simple prediction layer, which is problem-dependent.[[MET-NEU], [EMP-NEU], [SUG], [MIN]] \n2.\tThe experiments are insufficient to show the effectiveness.[[EXP-NEG], [EMP-NEG], [CRT], [MIN]]  It would be better to provide some statistics showing how the target-context interaction model outperforms the traditional ones in the special cases like the one shown in Table 4.[[TNF-NEU,ANA-NEU], [SUB-NEU], [SUG], [MIN]]  Only two examples are not convincing.[[CNT], [EMP-NEG], [CRT], [MAJ]] \n3.\tIn section 3, the authors claimed that (5) models the target and context independently.[[MET-NEU], [null], [DIS], [MIN]]  However, in section 4, in (7), the authors claimed the target vector v_t will affect the context shifting their representation to c\u2019_i.[[MET-NEU], [null], [DIS], [MIN]]  This should also work for (5).[[MET-NEU], [null], [DIS], [MIN]]  \n4.\tThere are too many typos in the paper, e.g., \\alpha is replaced by a, etc.[[OAL-NEG], [CLA-NEG], [CRT], [MIN]] \n\nOther concerns:\n1.\tIt seems that one needs to train at least three embedding matrices: A, C, D which represent input embeddings, output embeddings, and interactive embeddings, respectively.[[MET-NEU], [null], [DIS], [MIN]]  I wonder if this brings redundant parameters that do not guarantee convergence.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]]  Why not use one matrix instead?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]  Did the authors try experiments with less embedding matrices?[[EXP-NEU], [null], [QSN], [MIN]] \n2.\tThere is another work that also considers the target-context interaction using interactive attention model.[[MET-NEU], [null], [DIS], [MIN]]  Please refer to this paper \u201cInteractive Attention Networks for Aspect-Level Sentiment Classification\u201d.[[RWK-NEU], [null], [SUG], [MIN]]  A comparison is needed.[[RWK-NEU], [CMP-NEU], [SUG], [MIN]] \n3.\tIt is better to provide results in terms of accuracy for both datasets, as previous methods usually use accuracy for comparison.[[DAT-NEU,RES-NEU], [SUB-NEU], [SUG], [MIN]]  How\u2019s the score of the proposed model compared with the above paper as well as [Tang et al. 2016]?\n"[[RWK-NEU], [null], [DIS], [MIN]] 