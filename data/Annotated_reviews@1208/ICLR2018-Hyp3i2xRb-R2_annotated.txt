"Here are my main critics of the papers:\n\n1. Equation (1), (2), (3) are those expectations w.r.t. the data distribution (otherwise I can't think of any other stochasticity)?[[MET-NEG], [EMP-NEU], [CRT], [MAJ]] If so your phrase \"is zero given a sequence of inputs X1, ...,T\" is misleading.[[MET-NEG], [EMP-NEU], [CRT], [MAJ]] \n2. Lack of motivation for IE or UIE.[[MET-NEG], [EMP-NEU], [CRT], [MAJ]] Where is your background material?[[CNT], [EMP-NEU], [CRT], [MAJ]] I do not understand why we would like to assume (1), (2), (3).[[MET-NEG], [EMP-NEU], [CRT], [MAJ]] Why the same intuition of UIE can be applied to RNNs?[[MET-NEG], [EMP-NEG], [QSN,CRT], [MAJ]] \n3. The paper proposed the new architecture RIN, but it is not much different than a simple RNN with identity initialization.[[MET-NEG], [CMP-NEG], [CRT], [MIN]] Not much novelty.[[OAL-NEG], [NOV-NEG], [CRT], [MAJ]]\n4. The experimental results are not convincing.[[EXP-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] It's not compared against any previous published results.[[RWK-NEG,RES-NEG], [CMP-NEG], [CRT], [MAJ]] E.g. the addition tasks and sMNIST tasks are not as good as those reported in [1].[[DAT-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] Also it only has been tested on very simple datasets.[[DAT-NEG], [EMP-NEG], [CRT], [MAJ]]\n\n\n[1] Path-Normalized Optimization of Recurrent Neural Networks with ReLU Activations.[[RWK-NEU,BIB-NEU], [SUB-NEU], [DIS], [MIN]] Behnam Neyshabur, Yuhuai Wu, Ruslan Salakhutdinov, Nathan Srebro."[[RWK-NEU,BIB-NEU], [SUB-NEU], [DIS], [MIN]]