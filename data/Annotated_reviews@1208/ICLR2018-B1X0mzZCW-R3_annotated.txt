"The authors propose an approach for training deep learning models for situation where there is not enough reliable annotated data.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]]  This algorithm can be useful because correct annotation of enough cases to train a deep model in many domains is not affordable.[[MET-POS,FWK-POS], [IMP-POS], [APC], [MAJ]]  The authors propose to combine a huge number of weakly annotated data with a small set of strongly annotated cases to train a model in a student-teacher framework.[[DAT-NEU,MET-NEU], [null], [SMY], [GEN]]  The authors evaluate their proposed methods on one toy problem and two real-world problems.[[MET-NEU], [null], [SMY], [GEN]]  The paper is well written, easy to follow, and have good experimental study.[[EXP-POS,OAL-POS], [CLA-POS,EMP-POS], [APC], [MAJ]]   My main problem with the paper is the lack of enough motivation and justification for the proposed method; the methodology seems pretty ad-hoc to me and there is a need for more experimental study to show how the methodology work[[EXP-NEG,MET-NEG], [SUB-NEG], [DFT], [MAJ]] . Here are some questions that comes to my mind:  (1) Why first building a student model only using the weak data and why not all the data together to train the student model?[[DAT-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]]  To me, it seems that the algorithm first tries to learn a good representation for which lots of data is needed and the weak training data can be useful but why not combing with the strong data?[[DAT-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]]  (2) What are the sensitivity of the procedure to how weakly the weak data are annotated (this could be studied using both toy example and real-world examples)?[[DAT-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]]  (3) The authors explicitly suggest using an unsupervised method (check Baseline no.1) to annotate data weakly?[[RWK-NEU,DAT-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]]  Why not learning the representation using an unsupervised learning method (unsupervised pre training)?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]]  This should be at least one of the baselines.[[EXP-NEU], [EMP-NEU], [QSN], [MIN]] \n(4) the idea of using surrogate labels to learn representation is also not new.[[MET-NEU], [NOV-NEG], [CRT], [MAJ]]  One example work is \"Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks\". The authors didn't compare their method with this one."[[RWK-NEG,MET-NEU], [CMP-NEU], [CRT], [MIN]] 