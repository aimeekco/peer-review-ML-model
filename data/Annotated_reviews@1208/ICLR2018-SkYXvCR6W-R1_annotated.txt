"The paper proposed to encode text into a binary matrix by using a compressing code for each word in each matrix row.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The idea is interesting, and overall introduction is clear.[[INT-POS,PDI-POS], [null], [APC], [GEN]]\n\nHowever, the work lacks justification for this particular way of encoding, and no comparison for any other encoding mechanism is provided except for the one-hot encoding used in Zhang & LeCun 2015.[[MET-NEG], [SUB-NEG], [DFT], [MAJ]] The results using this particular encoding are not better than any previous work.[[RWK-NEG,RES-NEG], [CMP-NEG,EMP-NEG], [CRT], [MAJ]]\n\nThe network architecture seems to be arbitrary and unusual.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] It was designed with 4 convolutional layers stacked together for the first layer, while a common choice is to just make it one convolutional layer with 4 times the output channels.[[MET-NEU], [null], [SMY,DIS], [GEN]] The depth of the network is only 5, even with many layers listed in table 5.[[MET-NEG,RES-NEG,TNF-NEG], [EMP-NEG], [CRT], [MIN]]\n\nIt uses 1-D convolution across the word dimension (inferred from the feature size in table 5), which means the convolutional layers learn intra-word features for the entire text but not any character-level features.[[MET-NEG], [EMP-NEG], [DFT], [MIN]] This does not seem to be reasonable.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nOverall, the lack of comparisons and the questionable choices for the networks render this work lacking significance to be published in ICLR 2018."[[OAL-NEG], [APR-NEG, SUB-NEG], [CRT], [MAJ]]