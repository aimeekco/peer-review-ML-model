"**I am happy to see some good responses from the authors to my questions.[[EXT-NEU], [null], [SMY], [GEN]] I am raising my score a bit higher.[[OAL-POS], [REC-POS], [FBK], [MAJ]] \n\nSummary: \nA new stochastic method based on trust region (TR) is proposed.[[MET-NEU], [null], [SMY], [GEN]] Experiments show improved generalization over mini-batch SGD, which is the main positive aspect of this paper. [[EXP-POS], [EMP-POS], [APC], [MAJ]]The main algorithm has not been properly developed; there is too much focus on the convergence aspects of the inner iterations, for which there are many good algorithms already in the optimization literature.[[MET-NEG], [CMP-NEG,EMP-NEG], [CRT], [MAJ]] There are no good explanations for why the method yields better generalization.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Overall, TR seems like an interesting idea,[[PDI-POS], [EMP-POS], [APC], [MAJ]]  but it has neither been carefully expanded or investigated.[[PDI-NEG], [EMP-NEG], [CRT], [MAJ]] \n\nLet me state the main interesting results before going into criticisms:\n1. TR method seems to generalize better than mini-batch SGD.[[RES-POS], [EMP-POS], [APC], [MAJ]]  \n2. TR seems to lose generalization more gracefully than SGD when batch size is increased.[[RES-POS], [EMP-POS], [APC], [MAJ]] [But note here that mini-batch SGD is not a closed chapter.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] With better ways of adjusting the noise level via step-size control (larger step sizes mean more noise) the loss of generalization associated with large mini-batch sizes can be brought down.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] See, for example: https://arxiv.org/pdf/1711.00489.pdf.]\n3. Hybrid method is even better.[[MET-NEU], [null], [DIS], [MIN]] This only means that more understanding is needed as to how TR can be combined with SGD.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\nTrust region methods are generally batch methods.[[MET-NEU], [null], [DIS], [GEN]] Algorithm 1 is also stated from that thinking and it is a well-known optimization algorithm.[[MET-NEU], [null], [DIS], [GEN]] The authors never mention mini-batch when Algorithm 1 is introduced.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] But the authors clearly have only the stochastic min-batch implementation of the algorithm in mind.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \n\nOne has to wait till we go into the experiments section to read something like:\n\"Lastly, although in theory, we need full gradient and full Hessian to guarantee convergence, calculating them in each iteration is not practical, so we calculate both Hessian and gradient on subsampled data to replace the whole dataset\"\nfor readers to realize that the authors are talking about a stochastic mini-batch method.[[DAT-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]] This is a bad way of introducing the main method.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] This stochastic version obviously requires a step size; so it would have been proper to state the stochastic version of the algorithm instead of the batch algorithm in Algorithm 1.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] \n\nInstead of saying that in passing why not explicitly state it in key places, including the abstract and title?[[ABS-NEU,INT-NEU], [PNF-NEU], [QSN], [MIN]]  I suggest TR be replaced by \"Stochastic TR\" everywhere.[[OAL-NEU], [PNF-NEU], [SUG], [MIN]]  Also, what does \"step size\" mean in the TR method?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]  I suggest that all these are fully clarified as parts of Algorithm 1 itself.[[MET-NEU], [EMP-NEU], [SUG], [MIN]]  \n\nTrust region subproblem (TRS) has been analyzed and developed so much in the optimization literature.[[EXT-NEU], [null], [DIS], [GEN]]  For example, the conjugate gradient-based method leading to the Steihaug-Toint point is so much used. [Note: Here, the gradient refers to the gradient of the quadratic model, and it uses only Hessian-vector products.] http://www.ii.uib.no/~trond/publications/papers/trust.pdf.[[EXT-NEU], [null], [DIS], [GEN]] The authors spend so much effort developing their own algorithm! Also, in actual implementation, they only use a crude version of the inner algorithm for reasons of efficiency.[[MET-POS], [EMP-POS], [APC], [MAJ]]\n\nThe paper does not say anything about the convergence of the full algorithm.[[MET-NEG], [SUB-NEG], [DFT], [MAJ]] How good are the trust region updates based on q_t given the huge variability associated with the mini-batch operation? [[MET-NEU], [EMP-NEU], [QSN], [MIN]]The authors should look at several existing papers on stochastic trust region and stochastic quasi-Newton methods, e.g., papers from Katya Scheinberg (Lehigh) and Richard Byrd (Colorado)'s groups.[[RWK-NEU], [CMP-NEU,SUB-NEU], [SUG], [MIN]]\n\nThe best-claimed method of the method, called \"Hybrid method\" is also mentioned only in passing, and that too in a scratchy fashion (see end of subsec 4.3):[[MET-POS], [EMP-POS], [APC], [MAJ]]\n\"To enjoy the best of both worlds, we also introduce a \u201chybrid\u201d method in the Figure 3, that is, first run TR method for several epochs to get coarse solution and then run SGD for a while until fully converge.[[MET-NEU,TNF-NEU], [null], [DIS], [GEN]] Our rule of thumb is, when the training accuracy raises slowly, run SGD for 10 epochs (because it\u2019s already close to minimum).[[EXP-NEU,RES-NEU], [EMP-NEU], [SUG], [MAJ]] We find this \u201chybrid\u201d method is both fast and accurate, for both small batch and large batch.[[MET-POS], [EMP-POS], [APC], [MAJ]]\"\n\nExplanations of better generalization properties of TR over SGD are important.[[MET-NEU,ANA-NEU], [CMP-NEU], [SUG], [MAJ]] I feel this part is badly done in the paper.[[ANA-NEG], [EMP-NEG], [CRT], [MAJ]] For example, there is this statement:\n\"We observe that our method (TR) converges to solutions with much better test error but\nworse training error when batch size is larger than 128.[[EXP-NEG,ANA-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] We postulate this is because SGD is easy to overfit training data and \u201cstick\u201d to a solution that has a high loss in testing data, especially with the large batch case as the inherent noise cannot push the iterate out of loss valley while our TR method can.[[DAT-NEG,EXP-NEG,MET-NEU], [EMP-NEG], [CRT], [MAJ]]\"\nFrankly, I am unable to decipher what is being said here.[[ANA-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nThere is an explanation indicating that switching from SGD to TR causes an uphill movement (which I presume, is due to the trust region radius r being large); but statements such as - this will lead to climbing over to a wide minimum etc. are too strong; no evidence is given for this.[[MET-NEG,ANA-NEG], [SUB-NEG,EMP-NEG], [DFT,CRT], [MAJ]]\n\nThere is a statement - \"even if the exact local minima is reached, the subsampled Hessian may still have negative curvature\" - again, there is no evidence.[[ANA-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nOverall, the paper only has a few interesting observations,[[RES-POS], [EMP-POS], [APC], [MAJ]] but there is no good and detailed experimental analysis that help explain these observations.[[EXP-NEG,ANA-NEG], [SUB-NEG,EMP-NEG], [DFT,CRT], [MAJ]]\n\nThe writing of the paper needs a lot of improvement.\n\n\n\n\n\n\n"[[OAL-NEG], [CLA-NEG], [SUG,CRT], [MIN]]