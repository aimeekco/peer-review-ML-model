"This paper presents a nearest-neighbor based continuous control policy.[[INT-NEU], [null], [SMY], [GEN]]  Two algorithms are presented: NN-1 runs open-loop trajectories from the beginning state, and NN-2 runs a state-condition policy that retrieves nearest state-action tuples for each state. [[MET-NEU], [null], [SMY], [GEN]] \n\nThe overall algorithm is very simple to implement and can do reasonably well on some simple control tasks, but quickly gets overwhelmed by higher-dimensional and stochastic environments.[[MET-POS], [EMP-NEU], [APC], [MAJ]]  It is very similar to \"Learning to Steer on Winding Tracks Using Semi-Parametric Control Policies\" and is effectively an indirect form of tile coding (each could be seen as a fixed voronoi cell).[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MAJ]]  I am sure this idea has been tried before in the 90s but I am not familiar enough with all the literature to find it (A quick google search brings this up: Reinforcement Learning of Active Recognition Behaviors, with a chapter on nearest-neighbor lookup for policies: https://people.eecs.berkeley.edu/~trevor/papers/1997-045/node3.html).[[RWK-NEU,PDI-NEG,BIB-NEU], [NOV-NEG], [CRT], [MAJ]]\n\nAlthough I believe there is work to be done in the current round of RL research using nearest neighbor policies, I don't believe this paper delves very far into pushing new ideas (even a simple adaptive distance metric could have provided some interesting results, nevermind doing a learned metric in a latent space to allow for rapid retrainig of a policy on new domains....),[[RWK-NEU,PDI-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] and for that reason I don't think it has a place as a conference paper at ICLR.[[OAL-NEG], [REC-NEG], [FBK], [MAJ]]  I would suggest its submission to a workshop where it might have more use triggering discussion of further work in this area."[[EXT-POS], [null], [SUG], [MIN]]