"Pros:\nThe paper is easy to read.[[OAL-POS], [CLA-POS,PNF-POS], [APC], [MAJ]] Logic flows naturally within the paper.\n\n[[OAL-POS], [EMP-POS], [APC], [MAJ]]Cons:\n\n1. Experimental results are neither enough nor convincing.[[EXP-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] \n\nOnly one set of data is used throughout the paper: the Cifar10 dataset, and the architecture used is only a 100 layered MLP.[[DAT-NEG], [SUB-NEG], [CRT], [MIN]] Even though LCW performs better than others in this circumstance,[[MET-POS], [EMP-POS], [APC], [MAJ]] it does not prove its effectiveness in general or its elimination of the gradient vanishing problem.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] For the 100 layer MLP, it's very hard to train a simple MLP and the training/testing accuracy is very low for all the methods.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] More experiments with different number of layers and different architecture like ResNet should be tried to show better results.[[EXP-NEU,RES-NEU], [EMP-NEU], [SUG], [MIN]] \n\nIn Figure (7), LCW seems to avoid gradient vanishing but introduces gradient exploding problem.[[TNF-NEU], [CNT], [DIS], [MIN]]\n\nThe proposed concept is only analyzed in MLP with Sigmoid activation function.[[MET-NEU,ANA-NEU], [null], [SMY], [GEN]] In the experimental parts, the authors claim they use both ReLU and Sigmoid function, but no comparisons are reflected in the figures.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]] \n\n2. The whole standpoint of the paper is quite vague and not very convincing.[[OAL-NEG], [CNT], [CRT], [MAJ]]\nIn section 2, the authors introduce angle bias and suggest its effect in MLPs that with random weights, showing that different samples may result in similar output in the second and deeper layers.[[MET-NEU], [null], [SMY], [GEN]] However, the connection between angle bias and the issue of gradient vanishing lacks a clear analytical connection.[[ANA-NEG], [EMP-NEG], [CRT], [MAJ]] The whole analysis of the connection is built solely on this one sentence \"At the same time, the output does not change if we adjust the weight vectors in Layer 1\", which is nowhere verified.[[ANA-NEG], [SUB-NEG,EMP-NEG], [CRT], [MAJ]] \n\nFurther, the phenomenon is only tested on random initialization. When the network is trained for several iterations and becomes more settled, it is not clear how \"angle affect\" affects gradient vanishing problem.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]]\n\n\nMinors:\n1. Theorem 1,2,3 are direct conclusions from the definitions and are mis-stated as Theorems.[[MET-NEG], [PNF-NEG], [CRT], [MIN]]\n\n2. 'patters' -> 'patterns'\n\n[[MET-NEG,TNF-NEG], [CLA-NEG], [CRT], [MIN]]3. In section 2.3, reasons 1 and 2 state the similar thing that output of MLP has relatively small change with different input data when angle bias occurs.[[RES-NEU], [EMP-NEU], [CRT], [MIN]] Only reason 1 mentions the gradient vanishing problem, even though the title of this section is \"Relation to Vanishing Gradient Problem\". \n"[[INT-NEG], [PNF-NEG], [CRT], [MIN]]