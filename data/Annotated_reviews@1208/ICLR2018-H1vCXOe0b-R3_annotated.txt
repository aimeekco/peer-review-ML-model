"The paper develops a technique to understand what nodes in a neural network are important\nfor prediction.[[INT-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] The approach they develop consists of using an Indian Buffet Process \nto model a binary activation matrix with number of rows equal to the number of examples.[[RWK-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] \nThe binary variables are estimated by taking a relaxed version of the \nasymptotic MAP objective for this problem.[[PDI-NEU,EXP-NEU], [EMP-NEU], [SMY], [GEN]] One question from the use of the \nIndian Buffet Process: how do the asymptotics of the feature allocation determine \nthe number of hidden units selected?[[RWK-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SMY,QSN], [GEN]] \n\nOverall, the results didn't warrant the complexity of the method.[[MET-NEU,RES-NEG,OAL-NEU], [IMP-NEG,EMP-NEU], [DFT], [MIN]] The results are neat, but \nI couldn't tell why this approach was better than others.[[PDI-NEU,RES-NEU], [IMP-NEU,CMP-NEU], [DIS], [GEN]]\n\nLastly, can you intuitively explain the additivity assumption in the distribution for p(y')"[[PDI-NEU,EXP-NEU], [EMP-NEU], [QSN], [GEN]]