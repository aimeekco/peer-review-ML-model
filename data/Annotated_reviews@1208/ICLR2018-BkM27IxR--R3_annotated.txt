"Summary of the paper\n---------------------------\nThe paper derives a scheme for learning optimization algorithm for high-dimensional stochastic problems as the one involved in shallow neural nets training.[[INT-NEU], [null], [SMY], [GEN]] The main motivation is to learn to optimize with the goal to design a meta-learner able to generalize across optimization problems (related to machine learning applications as learning a neural network) sharing the same properties.[[MET-NEU], [null], [SMY], [GEN]] For this sake, the paper casts the problem into reinforcement learning framework and relies on guided policy search (GPS) to explore the space of states and actions.[[MET-NEU], [null], [SMY], [GEN]] The states are represented by the iterates, the gradients, the objective function values, derived statistics and features, the actions are the update directions of parameters to be learned. [[MET-NEU], [null], [SMY], [GEN]]To make the formulated problem tractable, some simplifications are introduced (the policies are restricted to gaussian distributions family, block diagonal structure is imposed on the involved parameters).[[MET-NEU], [null], [SMY], [GEN]] The mean of the stationary non-linear policy of GPS is modeled as a recurrent network with parameters to be learned.[[MET-NEU], [null], [SMY], [GEN]] A hatch of how to learn the overall process is presented.[[MET-NEU], [null], [SMY], [GEN]] Finally experimental evaluations on synthetic or real datasets are conducted to show the effectiveness of the approach[[DAT-NEU,EXP-NEU], [null], [SMY], [GEN]].\n\nComments\n-------------\n- The overall idea of the paper, learning how to optimize, is very seducing and the experimental evaluations (comparison to normal optimizers and other meta-learners) tend to conclude the proposed method is able to learn the behavior of an optimizer and to generalize to unseen problems.[[PDI-POS,EXP-POS], [EMP-POS], [APC], [MAJ]]\n- Materials of the paper sometimes appear tedious to follow, mainly in sub-sections 3.4 and 3.5.[[OAL-NEG], [CLA-NEG], [CRT], [MAJ]] It would be desirable to sum up the overall procedure in an algorithm.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] Page 5, the term $\\omega$ intervening in the definition of the policy $\\pi$ is not defined.[[MET-NEG], [EMP-NEU], [CRT], [MIN]]\n- The definitions of the statistics and features (state and observation features) look highly elaborated.[[MET-NEU], [EMP-NEU], [CRT], [MIN]] Can authors provide more intuition on these precise definitions?[[MET-NEU], [EMP-NEU], [SUG], [MIN]] How do they impact for instance changing the time range in the definition of $\\Phi$) in the performance of the meta-learner?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n- Figures 3 and 4 illustrate some oscillations of the proposed approach.[[TNF-NEU], [PNF-NEU], [SMY], [GEN]] Which guarantees do we have that the algorithm will not diverge as L2LBGDBGD does?[[MET-NEU], [EMP-NEU], [QSN], [MAJ]] How long should be the training to ensure a good and stable convergence of the method?[[MET-NEU], [EMP-NEU], [QSN], [MAJ]]\n- An interesting experience to be conducted and shown is to train the meta-learner on another dataset (CIFAR for example) and to evaluate its generalization ability on the other sets to emphasize the effectiveness of the method.[[DAT-NEU,EXP-POS], [EMP-POS], [APC], [MAJ]] "