"Summary:\n\nThis paper proposes an adversarial learning framework for machine comprehension task.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] Specifically, authors consider a reader network which learns to answer the question by reading the passage and a narrator network which learns to obfuscate the passage so that the reader can fail in its task.[[PDI-NEU], [null], [SMY], [GEN]] Authors report results in 3 different reading comprehension datasets and the proposed learning framework results in improving the performance of GMemN2N.[[DAT-NEU,RES-NEU], [null], [SMY], [GEN]]\n\n\nMy Comments:\n\nThis paper is a direct application of adversarial learning to the task of reading comprehension.[[OAL-NEU], [null], [DIS], [GEN]] It is a reasonable idea and authors indeed show that it works.[[PDI-NEU], [null], [SMY], [GEN]]\n\n1. The paper needs a lot of editing.[[OAL-NEG], [PNF-NEG], [CRT], [MIN]] Please check the minor comments.[[OAL-NEG], [PNF-NEG], [CRT], [MIN]]\n\n2. Why is the adversary called narrator network?[[MET-NEU], [null], [QSN], [MIN]] It is bit confusing because the job of that network is to obfuscate the passage.[[MET-NEG], [PNF-NEG], [CRT], [MIN]]\n\n3. Why do you motivate the learning method using self-play?[[MET-NEU], [EMP-NEG], [CRT], [MIN]] This is just using the idea of adversarial learning (like GAN) and it is not related to self-play.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n\n4 In section 2, first paragraph, authors mention that the narrator prevents catastrophic forgetting.[[CNT], [null], [DIS], [MIN]] How is this happening?[[CNT], [null], [QSN], [MIN]] Can you elaborate more?[[CNT], [SUB-NEU], [QSN], [MIN]]\n\n5. The learning framework is not explained in a precise way.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] What do you mean by re-initializing and retraining the narrator? Isn\u2019t it costly to reinitialize the network and retrain it for every turn?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] How many such epochs are done?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] You say that test set also contains obfuscated documents.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Is it only for the validation set?[[MET-NEG], [EMP-NEG], [QSN], [MIN]] Can you please explain if you use obfuscation when you report the final test performance too?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] It would be more clear if you can provide a complete pseudo-code of the learning procedure.[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\n6. How does the narrator choose which word to obfuscate?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Do you run the narrator model with all possible obfuscations and pick the best choice?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]]\n\n7. Why don\u2019t you treat number of hops as a hyper-parameter and choose it based on validation set?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] I would like to see the results in Table 1 where you choose number of hops for each of the three models based on validation set.[[TNF-NEU,RES-NEU], [EMP-NEU], [DIS], [MIN]]\n\n8. In figure 2, how are rounds constructed?[[TNF-NEU], [EMP-NEU], [QSN], [MIN]] Does the model sees the same document again and again for 100 times or each time it sees a random document and you sample documents with replacement?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]] This will be clear if you provide the pseudo-code for learning.[[MET-NEU], [EMP-NEU], [SUG], [MIN]]\n\n9. I do not understand author's\u2019 justification for figure-3.[[TNF-NEG], [EMP-NEG], [CRT], [MIN]] Is it the case that the model learns to attend to last sentences for all the questions?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Or where it attends varies across examples?\n\n10. Are you willing to release the code for reproducing the results?\n\nMinor comments:\n\nPage 1, \u201cexploit his own decision\u201d should be \u201cexploit its own decision[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\u201d\nIn page 2, section 2.1, sentence starting with \u201cIndeed, a too low percentage \u2026\u201d needs to be fixed.[[RES-NEG], [EMP-NEG], [CRT], [MIN]]\nPage 3, \u201cforgetting is compensate\u201d should be \u201cforgetting is compensated\u201d.[[CNT], [null], [DIS], [MIN]]\nPage 4, \u201cfor one sentences\u201d needs to be fixed.[[CNT], [null], [CRT], [MIN]]\nPage 4, \u201cunknow\u201d should be \u201cunknown\u201d.\nPage 4, \u201c??[[CNT], [null], [QSN], [MIN]]\u201d needs to be fixed.[[CNT], [null], [CRT], [MIN]]\nPage 5, \u201cfor the two first datasets\u201d needs to be fixed.[[DAT-NEG], [EMP-NEG], [CRT], [MIN]]\nTable 1, \u201cGMenN2N\u201d should be \u201cGMemN2N\u201d.[[TNF-NEG], [null], [CRT], [MIN]] In caption, is it mean accuracy or maximum accuracy?[[CNT], [null], [QSN], [MIN]]\nPage 6, \u201cdataset was achieves\u201d needs to be fixed.[[DAT-NEG], [EMP-NEG], [QSN], [MIN]]\nPage 7, \u201cdocument by obfuscated this word\u201d needs to be fixed.\[[CNT], [null], [CRT], [MIN]]nPage 7, \u201coverall aspect of the two first readers\u201d needs to be fixed.[[CNT], [null], [CRT], [MIN]]\nPage 8, last para, references needs to be fixed.[[BIB-NEG], [null], [CRT], [MIN]]\nPage 9, first sentence, please check grammar.[[CNT], [CLA-NEG], [CRT], [MIN]]\nSection 6.2, last sentence is irrelevant.\n"[[CNT], [null], [CRT], [MIN]]