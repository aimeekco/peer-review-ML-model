"This paper analyzes the loss function and properties of CNNs with one \"wide\" layer, i.e., a layer with number of neurons greater than the train sample size.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] Under this and some additional technique conditions, the paper shows that this layer can extract linearly independent features and all critical points are local minimums.[[MET-NEU,RES-NEU], [null], [SMY], [MIN]] I like the presentation and writing of this paper.[[OAL-POS], [CLA-POS,PNF-POS], [APC], [MAJ]] However, I find it uneasy to fully evaluate the merit of this paper, mainly because the \"wide\"-layer assumption seems somewhat artificial and makes the corresponding results somewhat expected.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] The mathematical intuition is that the severe overfitting induced by the wide layer essentially lifts the loss surface to be extremely flat so training to zero/small error becomes easy.[[MET-NEU,RES-NEU], [null], [SMY,DIS], [GEN]] This is not surprising.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MIN]] It would be interesting to make the results more quantitive, e.g., to quantify the tradeoff between having local minimums and having nonzero training error. "[[RES-NEU], [EMP-NEU], [SUG], [MIN]]