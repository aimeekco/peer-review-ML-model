"The paper studies the effect of reduced precision weights and activations on the performance, memory and computation cost of deep networks and proposes a quantization scheme and wide filters to offset the accuracy lost due to the reduced precision.[[INT-NEU,PDI-NEU], [null], [DIS], [MIN]] The study is performed on AlexNet, ResNet and Inception on the Imagenet datasets and results show that accuracy matching the full precision baselines can be obtained by widening the filters on the networks.[[DAT-NEU,EXP-NEU], [null], [SMY], [GEN]] \n\nPositives\n- Using lower precision activations to save memory and compute seems new and widening the filter sizes seems to recover the accuracy lost due to the lower precision.[[RES-POS], [EMP-POS], [APC], [MAJ]]\n\nNegatives\n- While the exhaustive analysis is extremely useful[[ANA-POS], [EMP-POS], [APC], [MAJ]] the overall technical contribution of the paper that of widening the networks is fairly small.[[EXP-NEG,MET-NEG], [SUB-NEG], [CRT], [MIN]] \n- The paper motivates the need for reduced precision weights from the perspective of saving memory footprint when using large batches.[[MET-NEU], [null], [DIS], [MIN]] However, the results are more focused on compute cost.[[RES-NEU], [EMP-NEU], [DIS], [MIN]] Also large batches are used mainly during training where memory is generally not a huge issue.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]] Memory critical situations such as inference on mobile phones can be largely mitigated by using smaller batch sizes.[[DAT-NEU,RES-NEU], [null], [DIS], [MIN]] It might help to emphasize the speed-up in compute more in the contributions.  "[[RES-NEU], [EMP-NEU], [SUG], [MIN]]