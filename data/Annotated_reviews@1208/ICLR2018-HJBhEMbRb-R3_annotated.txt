"This paper studies the generalization properties of 2-layer neural networks based on Fourier analysis.[[INT-NEU,MET-NEU], [null], [SMY], [GEN]] Studying the generalization property of neural network is an important problem and Fourier-based analysis is a promising direction, as shown in (Lee et al., 2017).[[RWK-NEU,MET-NEU,BIB-NEU], [null], [SMY], [GEN]] However, I am not satisfied with the results in the current version.[[RES-NEG,OAL-NEG], [IMP-NEG], [DFT], [MIN]]\n\n1) The main theoretical results are on the sin activation functions instead of commonly used ReLU functions. [[RWK-NEU,MET-NEU,RES-NEU], [null], [SMY], [GEN]]\n\n2) Even if for sin activation functions, the analysis is NOT complete. [[RWK-NEU,MET-NEG,ANA-NEG], [SUB-NEG,EMP-NEG], [DFT], [MIN]]The authors claimed in the abstract that gradient-based methods will converge to generalizable local minima.[[RWK-NEU,EXP-NEU,MET-NEU], [null], [SMY], [GEN]] However, Corollary 3 is only a concentration bound on the gradient.[[RWK-NEU], [null], [SMY], [GEN]] There is a gap that how this corollary implies generalization.[[RWK-NEU], [null], [SMY], [GEN]] The paragraph below this corollary is only a high level intuition. [[RWK-NEU], [EMP-NEU], [SMY], [GEN]]\n\n\n"