"This paper proposes a method for parameter space noise in exploration.[[INT-NEU], [null], [SMY], [GEN]] \nRather than the \"baseline\" epsilon-greedy (that sometimes takes a single action at random)... this paper presents an method for perturbations to the policy.[[MET-NEU], [null], [SMY], [GEN]] \nIn some domains this can be a much better approach and this is supported by experimentation.[[EXP-NEU,MET-POS], [EMP-POS], [APC], [MAJ]] \n\nThere are several things to like about the paper:[[OAL-POS], [null], [APC], [MAJ]] \n- Efficient exploration is a big problem for deep reinforcement learning (epsilon-greedy or Boltzmann is the de-facto baseline) and there are clearly some examples where this approach does much better.[[MET-POS], [EMP-POS], [APC], [MAJ]] \n- The noise-scaling approach is (to my knowledge) novel, good and in my view the most valuable part of the paper.[[MET-POS], [NOV-POS], [APC], [MAJ]]\n- This is clearly a very practical and extensible idea... the authors present good results on a whole suite of tasks.[[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]]\n- The paper is clear and well written, it has a narrative and the plots/experiments tend to back this up.[[EXP-POS,OAL-POS], [CLA-POS,EMP-POS], [APC], [MAJ]]\n- I like the algorithm, it's pretty simple/clean and there's something obviously *right* about it (in SOME circumstances).[[MET-POS], [EMP-POS], [APC], [MAJ]]\n\nHowever, there are also a few things to be cautious of... and some of them serious:[[OAL-NEU], [EMP-NEU], [DIS], [MIN]]\n- At many points in the paper the claims are quite overstated.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Parameter noise on the policy won't necessarily get you efficient exploration... and in some cases it can even be *worse* than epsilon-greedy... if you just read this paper you might think that this was a truly general \"statistically efficient\" method for exploration (in the style of UCRL or even E^3/Rmax etc).[[RWK-NEG,MET-NEG], [CMP-NEG], [CRT], [MIN]]\n- For instance, the example in 4.2 only works because the optimal solution is to go \"right\" in every timestep... if you had the network parameterized in a different way (or the actions left/right were relabelled) then this parameter noise approach would *not* work...[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] By contrast, methods such as UCRL/PSRL and RLSVI https://arxiv.org/abs/1402.0635 *are* able to learn polynomially in this type of environment.[[MET-NEU], [CMP-NEU], [DIS], [MIN]] I think the claim/motivation for this example in the bootstrapped DQN paper is more along the lines of \"deep exploration\" and you should be clear that your parameter noise does *not* address this issue.[[MET-NEU], [CMP-NEU], [DIS], [MIN]]\n- That said I think that the example in 4.2 is *great* to include... you just need to be more upfront about how/why it works and  what you are banking on with the parameter-space exploration.[[MET-POS], [EMP-POS], [APC], [MAJ]] Essentially you perform a local exploration rule in parameter space... and sometimes this is great -[[MET-POS], [EMP-POS], [APC], [MAJ]] but you should be careful to distinguish this type of method from other approaches.[[MET-NEU], [CMP-NEU], [DIS], [MIN]] This must be mentioned in section 4.2 \"does parameter space noise explore efficiently\" because the answer you seem to imply is \"yes\" ... when the answer is clearly NOT IN GENERAL... but it can still be good sometimes ;D[[MET-NEU], [PNF-NEU], [DIS], [MIN]]\n- The demarcation of \"RL\" and \"evolutionary strategies\" suggests a pretty poor understanding of the literature and associated concepts.[[RWK-NEG], [CNT], [CRT], [MAJ]] I can't really support the conclusion \"RL with parameter noise exploration learns more efficiently than both RL and evolutionary strategies individually\".[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] This sort of sentence is clearly wrong and for many separate reasons:\n    - Parameter noise exploration is not a separate/new thing from RL... it's even been around for ages! It feels like you are talking about DQN/A3C/(whatever algorithm got good scores in Atari last year) as \"RL\" and that's just really not a good way to think about it.[[RWK-NEG,MET-NEG], [CMP-NEG,EMP-NEG], [CRT], [MAJ]]\n    - Parameter noise exploration can be *extremely* bad relative to efficient exploration methods (see section 2.4.3 https://searchworks.stanford.edu/view/11891201)[[RWK-NEG,MET-NEG], [CMP-NEG], [CRT], [MAJ]]\\n\n\nOverall, I like the paper, I like the algorithm and I think it is a valuable contribution.[[MET-POS,OAL-POS], [EMP-POS], [APC], [MAJ]]\\nI think the value in this paper comes from a practical/simple way to do policy randomization in deep RL.[[MET-NEU], [null], [DIS], [GEN]]\nIn some (maybe even many of the ones you actually care about) settings this can be a really great approach, especially when compared to epsilon-greedy.[[MET-POS], [CMP-POS,EMP-POS], [APC], [MAJ]]\n\nHowever, I hope that you address some of the concerns I have raised in this review.[[EXT-NEU], [null], [DIS], [GEN]]\nYou shouldn't claim such a universal revolution to exploration / RL / evolution because I don't think that it's correct.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\nFurther, I don't think that clarifying that this method is *not* universal/general really hurts the paper... you could just add a section in 4.2 pointing out that the \"chain\" example wouldn't work if you needed to do different actions at each timestep (this algorithm does *not* perform \"deep exploration\").[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n\nI vote accept."[[OAL-POS], [REC-POS], [APC], [MAJ]]