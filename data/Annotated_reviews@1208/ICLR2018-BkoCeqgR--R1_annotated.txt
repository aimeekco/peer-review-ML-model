"The paper proposes and evaluates a method to make neural networks for image recognition color invariant.[[INT-NEU], [null], [SMY], [GEN]]\n\nThe contribution of the paper is: \n - some proposed methods to extract a color-invariant representation[[MET-NEU], [null], [SMY], [GEN]]\n - an experimental evaluation of the methods on the cifar 10 dataset[[DAT-NEU,EXP-NEU], [null], [SMY], [GEN]]\n - a new dataset \"crashed cars[[DAT-POS], [null], [SMY], [GEN]]\"\n - evaluation of the best method from the cifar10 experiments on the new dataset[[DAT-NEU,EXP-NEU,MET-NEU], [null], [SMY], [GEN]]\n\nPros: \n - the crashed cars dataset is interesting.[[DAT-POS], [null], [APC], [MAJ]] The authors have definitely found an interesting untapped source of interesting images.[[DAT-POS], [SUB-POS], [APC], [MAJ]]\n\n\nCons: \n- The authors name their method order network but the method they propose is not really parts of the network but simple preprocessing steps to the input of the network.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] \n- The paper is incomplete without the appendices.[[CNT], [SUB-NEG], [DFT], [MIN]] In fact the paper is referring to specific figures in the appendix in the main text.[[TNF-NEG], [SUB-NEG], [DFT], [MIN]]\n - the authors define color invariance as a being invariant to which specific color an object in an image does have, e.g. whether a car is red or green, but they don't think about color invariance in the broader context - color changes because of lighting, shades, .....[[MET-NEG], [EMP-NEG], [DIS], [MAJ]] Also, the proposed methods aim to preserve the \"colorfullness\" of a color.[[MET-NEU], [null], [SMY], [GEN]] This is also problematic, because while the proposed method works for a car that is green or a car that is red, it will fail for a car that is black (or white) - because in both cases the \"colorfulness\" is not relevant.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] Note that this is specifically interesting in the context of the task at hand (cars) and many cars being, white, grey (silver), or black.[[MET-NEU], [EMP-NEU], [SUG], [MAJ]] \n- the difference in the results in table 1 could well come from the fact that in all of the invariant methods except for \"ord\" the input is a WxHx1 matrix, but for \"ord\" and \"cifar\" the input is a \"WxHx3\" matrix.[[MET-NEU,TNF-NEU], [EMP-NEU], [DIS], [MAJ]] This probably leads to more parameters in the convolutions.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] \n- the results in the  figure 4: it's very unlikely that the differences reported are actually significant.[[RES-NEG,TNF-NEU], [IMP-NEG], [CRT], [MAJ]] It appears that all methods perform approximately the same - and the authors pick a specific line (25k steps) as the relevant one in which the RGB-input space performs best.[[MET-NEU], [EMP-NEU], [CRT], [MAJ]] The proposed method does not lead to any relevant improvement.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\nFigure 6/7: are very hard to read. I am still not sure what exactly they are trying to say.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]]\n\nMinor comments: \n - section 1: \"called for is network\" -> called for is a network[[CNT], [CLA-NEG], [SUG], [MIN]]\n - section 1.1: And and -> And[[CNT], [CLA-NEG], [SUG], [MIN]]\n - section 1.1: Appendix -> Appendix C\n - section 2: Their exists many -> There exist many\n - section 2: these transformation -> these transformations\n - section 2: what does \"the wallpaper groups\" refer to? \n - section 2: are a groups -> are groups\n - section 3.2: reference to a non-existing figure\n - section 3.2/Training: 2499999 iterations = steps? \n - section 3.2/Training: longer as suggested -> longer than suggested[[CNT], [CLA-NEG], [SUG], [MIN]]\n\n"