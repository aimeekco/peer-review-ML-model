"This paper presents a method for choosing a subset of examples on which to run a constraint solver\nin order to solve program synthesis problems.[[INT-NEU,MET-NEU], [null], [SMY], [GEN]] This problem is basically active learning for\nprogramming by example, but the considerations are slightly different than in standard active\nlearning.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] The assumption here is that labels (aka outputs) are easily available for all possible\ninputs, but we don't want to give a constraint solver all the input-output examples, because it will\nslow down the solver's execution.[[PDI-NEU], [null], [SMY], [GEN]]\n\nThe main baseline technique CEGIS (counterexample-guided inductive synthesis) addresses this problem\nby starting with a small set of examples, solving a constraint problem to get a hypothesis program,\nthen looking for \"counterexamples\" where the hypothesis program is incorrect.[[RWK-NEU,PDI-NEU], [null], [DIS], [MIN]]\n\nThis paper instead proposes to learn a surrogate function for choosing which examples to select.[[PDI-NEU], [null], [DIS], [GEN]] The\npaper isn't presented in exactly these terms, but the idea is to consider a uniform distribution\nover programs and a zero-one likelihood for input-output examples (so observations of I/O examples\njust eliminate inconsistent programs).[[PDI-NEU], [null], [DIS], [GEN]] We can then compute a posterior distribution over programs[[PDI-NEU], [null], [DIS], [GEN]]\nand form a predictive distribution over the output for all the remaining possible inputs.[[PDI-NEU], [null], [DIS], [GEN]] The paper\nsuggests always adding the I/O example that is least likely under this predictive distribution\n(i.e., the one that is most \"surprising\").[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]]\n\nForming the predictive distribution explicitly is intractable, so the paper suggests training a\nneural net to map from a subset of inputs to the predictive distribution over outputs.[[EXP-NEU], [null], [SMY], [GEN]]  Results show\nthat the approach is a bit faster than CEGIS in a synthetic drawing domain.[[MET-NEU], [null], [SMY], [GEN]]\n\nThe paper starts off strong.[[OAL-POS], [CNT], [APC], [MAJ]] There is a start at an interesting idea here, and I appreciate the\nthorough treatment of the background, including CEGIS and submodularity as a motivation for doing\ngreedy active learning, although I'd also appreciate a discussion of relationships between this approach \nand what is done in the active learning literature.[[PDI-POS,MET-POS], [EMP-POS], [APC], [MAJ]]Once getting into the details of the proposed approach, \nthe quality takes a downturn, unfortunately.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] \n\nMain issues:\n- It's not generally scalable to build a neural network whose size scales with the number\nof possible inputs.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] I can't see how this approach would be tractable in more standard program\nsynthesis domains where inputs might be lists of arrays or strings, for example.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]  It seems that this\napproach only works due to the peculiarities of the formulation of the only task that is considered,\nin which the program maps a pixel location in 32x32 images to a binary value.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\n- It's odd to write \"we do not suggest a specific neural network architecture for the\nmiddle layers, one should seelect whichever architecture that is appropriate for the domain at\nhand.[[MET-NEU], [CNT], [DIS], [MIN]]\" Not only is it impossible to reproduce a paper without any architectural details, but the\nresult is then that Fig 3 essentially says inputs -> \"magic\" -> outputs.[[RES-NEU,TNF-NEU], [null], [DIS], [MIN]] Given that I don't even\nthink the representation of inputs and outputs is practical in general, I don't see what the \ncontribution is here.[[RES-NEU], [EMP-NEU], [DIS], [MIN]]\n\n- This paper is poor in the reproducibility category.[[OAL-NEG], [IMP-NEG], [CRT], [MAJ]] The architecture is never described,\nit is light on details of the training objective, it's not entirely clear what the DSL used in the\nexperiments is (is Figure 1 the DSL used in experiments), and it's not totally clear how the random\nimages were generated (I assume values for the holes in Figure 1 were sampled from some\ndistribution, and then the program was executed to generate the data?).[[EXP-NEG,MET-NEG,RES-NEG,TNF-NEG], [SUB-NEG,EMP-NEG], [CRT], [MIN]]\n\n- Experiments are only presented in one domain, and it has some peculiarities relative to \nmore standard program synthesis tasks (e.g., it's tractable to enumerate all possible inputs).[[EXP-NEG], [SUB-NEG,EMP-NEG], [CRT], [MIN]]  It'd\nbe stronger if the approach could also be demonstrated in another domain.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\n- Technical point: it's not clear to me that the training procedure as described is consistent\nwith the desired objective in sec 3.3.[[EXP-NEG], [EMP-NEG], [CRT], [MIN]] Question for the authors: in the limit of infinite training\ndata and model capacity, will the neural network training lead to a model that will reproduce the\nprobabilities in 3.3?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]]\n\nTypos:\n- The paper needs a cleanup pass for grammar, typos, and remnants like \"Figure blah shows our \nneural network architecture\" on page 5.[[OAL-NEG], [CLA-NEG], [CRT], [MIN]]\n\nOverall: There's the start of an interesting idea here,[[PDI-POS], [EMP-POS], [APC], [MAJ]] but I don't think the quality is high enough\nto warrant publication at this time.\n"[[OAL-NEG], [REC-NEG], [FBK], [MAJ]]