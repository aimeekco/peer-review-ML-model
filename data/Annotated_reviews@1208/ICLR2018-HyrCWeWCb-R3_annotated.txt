"The paper extends softmax consistency by adding in a relative entropy term to the entropy regularization and applying trust region policy optimization instead of gradient descent.[[INT-NEU,PDI-NEU,MET-NEU], [null], [SMY], [GEN]]  I am not an expert in this area.[[EXT-NEU], [CNT], [DIS], [GEN]] It is hard to judge the significance of this extension.[[OAL-NEU], [IMP-NEU], [DIS], [MIN]]\n\nThe paper largely follows the work of Nachum et al 2017.[[RWK-NEU], [null], [DIS], [MIN]] The differences (i.e., the claimed novelty) from that work are the relative entropy and trust region method for training.[[MET-POS], [NOV-POS], [APC], [MAJ]] However, the relative entropy term added seems like a marginal modification.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Authors claimed that it satisfies the multi-step path consistency but the derivation is missing.[[MET-NEG], [SUB-NEG], [DFT,CRT], [MIN]\n\nI am a bit confused about the way trust region method is used in the paper.[[MET-NEG], [EMP-NEG], [CRT], [MIN] Initially,  problem is written as a constrained optimization problem (12).[[MET-NEU], [EMP-NEU], [DIS], [MIN] It is then converted into a penalty form for softmax consistency.[[MET-NEU], [EMP-NEU], [DIS], [MIN] Finally, the Lagrange parameter is estimated from the trust region method.[[MET-NEU], [EMP-NEU], [DIS], [MIN] In addition, how do you get the Lagrange parameter from epsilon?[[MET-NEU], [EMP-NEU], [QSN], [MIN]\n\nThe pseudo code of the algorithm is missing.[[MET-NEG], [SUB-NEU], [DFT,CRT], [MIN] It would be much clearer if a detailed description of the algorithmic procedure is given.[[MET-NEU], [SUB-NEU], [SUG], [MIN]\n\nHow is the performance of Trust-PCL compared to PCL? "[[MET-NEU], [CMP-NEU], [QSN], [MIN]