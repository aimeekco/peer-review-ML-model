"This authors proposed to use an implicit weight normalization approach to replace the explicit weight normalization used in training of neural networks.[[INT-NEU,PDI-NEU,MET-NEU], [null], [SMY], [GEN]] The authors claimed to obtain efficiency improvement and better numerical stability.[[RES-NEU], [null], [SMY], [GEN]]\n\nThis is a short paper that contains five pages.[[OAL-NEU], [IMP-NEU], [DIS], [MIN]] The idea of the proposed implicit weight normalization is to apply the normalization to scaling the input rather than the rows of the matrices.[[PDI-NEU,MET-NEU], [null], [SMY], [MIN]] In terms of the overall time complexity, the improvement seems quite limited considering that the normalization is not the bottleneck operations in the training.[[RES-NEG], [SUB-NEG], [DFT], [MIN]] In addition, it is not very clear how the proposed approach benefits the mini-batch training of the network.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] In terms of numerical stability, though experimental results were reported, there is no theoretical analysis.[[EXP-NEU,RES-NEU,ANA-NEG], [IMP-NEG], [DFT,CRT], [MAJ]] The experiments are quite limited.\n"[[EXP-NEG], [SUB-NEG], [DFT,CRT], [MIN]]