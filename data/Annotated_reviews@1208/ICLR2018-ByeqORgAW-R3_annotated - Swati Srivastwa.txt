"The paper uses a lesser-known interpretation of the gradient step of a composite function (i.e., via reverse mode automatic differentiation or backpropagation), and then replaces one of the steps with a proximal step.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The proximal step requries the solution of a positive-definite linear system, so it is approximated using a few iterations of CG.[[PDI-NEU], [null], [SMY], [GEN]] The paper provides theory to show that their proximal variant (even with the CG approximations) can lead to convergent algorithms (and since practical algorithms are not necessarily globally convergent, most of the theory shows that the proximal variant has similar guarantees to a standard gradient step).[[INT-NEU], [null], [SMY], [GEN]]\n\nOn reading the abstract and knowing quite a bit about proximal methods, I was initially skeptical, but I think the authors have done a good job of making their case. [[ABS-POS,MET-POS], [CLA-POS], [APC], [MAJ]]It is a well-written, very clear paper, and it has a good understanding of the literature, and does not overstate the results.[[RES-POS,OAL-POS], [EMP-POS], [APC], [MAJ]] The experiments are serious, and done using standard state-of-the-art tools and architectures.[[EXP-POS], [EMP-POS], [APC], [MAJ]] Overall, it is an interesting idea, and due to the current focus on neural nets, it is of interest even though it is not yet providing substantial improvements.[[MET-POS], [EMP-POS], [APC], [MAJ]]\n\nThe main drawback of this paper is that there is no theory to suggest the ProxProp algorithm has better worst-case convergence guarantees, and that the experiments do not show a consistent benefit (in terms of time) of the method.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] On the one hand, I somewhat agree with the authors that \"while the running time is higher... we expect that it can be improved through further engineering efforts\", but on the other hand, the idea of nested algorithms (\"matrix-free\" or \"truncated Newton\") always has this issue.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] A very similar type of ideas comes up in constrained or proximal quasi-Newton methods, and I have seen many papers (or paper submissions) on this style of method (e.g., see the 2017 SIAM Review paper on FWI by Metivier et al. at https://doi.org/10.1137/16M1093239). In every case, the answer seems to be that it can work on *some problems* and for a few well chosen parameters, so I don't yet buy that ProxProp is going to make a huge savings on a wide-range of problems.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MIN]]\n\nIn brief: quality is high, clarity is high, originality is high, and significance is medium.[[OAL-POS], [CLA-POS,IMP-NEU], [APC], [MAJ]]\nPros: interesting idea, relevant theory provided, high-quality experiments[[PDI-POS,EXP-POS], [EMP-POS], [APC], [MAJ]]\nCons: no evidence that this is a \"break-through\" idea[[PDI-NEU,ANA-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nMinor comments:\n\n- Theorems seemed reasonable and I have no reason to doubt their accuracy[[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]]\n\n- No typos at all, which I find very unusual. Nice job![[OAL-POS], [CLA-POS], [APC], [MAJ]]\n\n- In Algo 1, it would help to be more explicit about the updates (a), (b), (c), e.g., for (a), give a reference to eq (8), and for (b), reference equations (9,10).[[MET-NEU], [PNF-NEU], [SUG], [MIN]]  It's nice to have it very clear, since \"gradient step\" doesn't make it clear what the stepsize is, and if this is done in a \"Jacob-like\" or \"Gauss-Seidel-like\" fashion.[[MET-NEU], [CMP-NEU], [SUG], [MIN]] (c) has no reference equation, does it?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\n- Similarly, for Algo 2, add references.[[MET-NEU], [SUB-NEU], [SUG], [MIN]] In particular, tie in the stepsizes tau and tau_theta here.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\n- Motivation in section 4.1 was a bit iffy.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] A larger stepsize is not always better, and smaller is not worse. [[MET-NEU], [EMP-NEU], [DIS], [MIN]]Minimizing a quadratic f(x) = .5||x||^2 will converge in one step with a step-size of 1 because this is well-conditioned; on the flip side, slow convergence comes from lack of strong convexity, or with strong convexity, ill-conditioning of the Hessian (like a stiff ODE).[[MET-POS], [EMP-POS], [APC], [MAJ]]\n\n- The form of equation (6) was very nice, and you could also point out the connection with backward Euler for finite-difference methods.[[MET-POS], [CMP-POS], [APC], [MAJ]] This was the initial setting of analysis for most of original results that rely on the proximal operator (e.g., Lions and Mercier 1970s).[[RES-NEU,ANA-NEU], [EMP-NEU], [DIS], [MIN]]\n\n- Eq (9), this is done component-wise, i.e., Hadamard product, right?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\n- About eq (12), even if softmax cross-entropy doesn't have a closed-form prox (and check the tables of Combettes and Pesquet), because it is separable (if I understand correctly) then it ought to be amenable to solving with a handful of Newton iterations which would be quite cheap.[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nProx tables (see also the new edition of Bauschke and Combettes' book): P. L. Combettes and J.-C. Pesquet, \"Proximal splitting methods in signal processing,\" in: Fixed-Point Algorithms for Inverse Problems in Science and Engineering (2011) http://www4.ncsu.edu/~pcombet/prox.pdf[[BIB-NEU], [null], [DIS], [GEN]]\n\n- Below prop 4, discussing why not to make step (b) proximal, this was a bit vague to me.[[CNT], [CNT], [CRT], [MIN]] It would be nice to expand this[[CNT], [null], [DIS], [MIN]].\n\n- Page 6 near the top, to apply the operator, in the fully-connected case, this is just a matrix multiply, right?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] and in a conv net, just a convolution?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] It would help the reader to be more explicit here.[[MET-NEU], [EMP-NEU], [SUG], [MIN]]\n\n- Section 5.1, 2nd paragraph, did you swap tau_theta and tau, or am I just confused?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]] The wording here was confusing.[[EXP-NEU,MET-NEG], [EMP-NEG], [CRT], [MIN]]\n\n- Fig 2 was not that convincing since the figure with time showed that either usual BackProp or the exact ProxProp were best, so why care about the approximate ProxProp with a few CG iterations? [[TNF-NEG,MET-NEU], [EMP-NEG], [QSN], [MIN]]The argument of better generalization is based on very limited experiments and without any explanation, so I find that a weak argument (and it just seems weird that inexact CG gives better generalization).[[MET-NEG], [EMP-NEG], [CRT], [MIN]]  The right figure would be nice to see with time on the x-axis as well.[[TNF-NEU], [CNT], [DIS], [MIN]]\n\n- Section 5.2, this was nice and contributed to my favorable opinion about the work.[[CNT], [CNT], [APC], [MAJ]] However, any kind of standard convergence theory for usual SGD requires the stepsize to change per iteration and decrease toward zero.[[EXT-NEU], [null], [DIS], [GEN]] I've heard of heuristics saying that a fixed stepsize is best and then you just make sure to stop the algorithm a bit early before it diverges or behaves wildly -- is that true here?[[EXT-NEU], [null], [QSN], [GEN]] \n\n- Final section of 5.3, about the validation accuracy, and the accuracy on the test set after 50 epochs.[[CNT], [null], [DIS], [MIN]]  I am confused why these are different numbers.[[CNT], [null], [DIS], [GEN]]  Is it just because 50 epochs wasn't enough to reach convergence, while 300 seconds was?[[RES-NEU], [EMP-NEU], [QSN], [MIN]]  And why limit to 50 epochs then?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]  Basically, what's the difference between the bottom two plots in Fig 3 (other than scaling the x-axis by time/epoch), and why does ProxProp achieve better accuracy only in the right figure?"[[TNF-NEU], [PNF-NEU], [DIS], [MIN]] 