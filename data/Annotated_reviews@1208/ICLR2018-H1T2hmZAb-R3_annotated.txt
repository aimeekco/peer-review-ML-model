"Authors present complex valued analogues of real-valued convolution, ReLU and batch normalization functions.[[INT-NEU], [null], [SMY], [GEN]] Their \"related work section\" brings up uses of complex valued computation such as discrete Fourier transforms and Holographic Reduced Representations.[[RWK-NEU], [null], [SMY], [GEN]] However their application don't seem to connect to any of those uses and simply reimplement existing real-valued networks as complex valued.[[MET-NEU], [EMP-NEU], [DIS], [GEN]]\n\nTheir contributions are:\n\n1. Formulate complex valued convolution[[MET-NEU], [null], [SMY], [GEN]]\n2. Formulate two complex-valued alternatives to ReLU and compare them[[MET-NEU], [CMP-NEU], [SMY], [GEN]]\n3. Formulate complex batch normalization as a \"whitening\" operation on complex domain[[MET-NEU], [null], [SMY], [GEN]]\n4. Formulate complex analogue of Glorot weight normalization scheme[[MET-NEU], [null], [SMY], [GEN]]\n\nSince any complex valued computation can be done with a real-valued arithmetic, switching to complex arithmetic needs a compelling use-case.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] For instance, some existing algorithm may be formulated in terms of complex values, and reformulating it in terms of real-valued computation may be awkward.[[MET-NEU], [EMP-NEG], [DIS], [GEN]] However, cases the authors address, which are training batch-norm ReLU networks on standard datasets, are already formulated in terms of real valued arithmetic.[[RWK-NEU,DAT-NEU,MET-NEU], [EMP-NEU], [DIS], [MAJ]] Switching these networks to complex values doesn't seem to bring any benefit, either in simplicity, or in classification performance."[[MET-NEU], [null], [CRT], [MAJ]]