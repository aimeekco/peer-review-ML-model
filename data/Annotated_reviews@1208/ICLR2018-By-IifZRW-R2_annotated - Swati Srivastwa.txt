"In Bayesian neural networks, a deterministic or parametric activation is typically used.[[EXT-NEU], [null], [DIS], [GEN]] In this work, activation functions are considered random functions with a GP prior and are inferred from data.[[PDI-NEU], [null], [SMY], [GEN]]\n\n\n- Unnecessary complexity[[OAL-NEG], [null], [CRT], [MIN]]\n\nThe presentation of the paper is unnecessarily complex.[[OAL-NEG], [PNF-NEG], [CRT], [MIN]] It seems that authors spend extra space creating problems and then solving them.[[OAL-NEG], [PNF-NEG], [CRT], [MIN]] Although some of the derivations in Section 3.2.2 are a bit involved, most of the derivations up to that point (which is already in page 6) follow preexisting literature.[[RWK-NEU], [CMP-NEU], [DIS], [MIN]]\n\nFor instance, eq. (3) proposes one model for p(F|X). Eq. (8) proposes a different model for p(F|X), which is an approximation to the previous one.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [GEN]] Instead, the second model could have been proposed directly, with the appropriate citation from the literature, since it isn't new.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MIN]] Eq. (13) is introduced as a \"solution\" to a non-existent problem, because the virtual observations are drawn from the same prior as the real ones, so it is not that we are \"coming up\" with a convenient GP prior that turns out to produce a computationally tractable solution, we are just using the prior on the observations consistently.\n\nIn general, the authors seem to use \"approximately equal\" and \"equal\" interchangeably, which is incorrect.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] There should be a single definition for p(F|X).[[MET-NEU], [EMP-NEU], [DIS], [MIN]] And there should be a single definition for L_pred.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] The expression for L_pred given in eq. (20) (exact) and eq. (41) (approximate) do not match and yet both are connected with an equality (or proportionality), which they shouldn't.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nQ(A) is sometimes taken to mean the true posterior (i.e., eq. (31)), sometimes a Gaussian approximation (i.e., eq (32) inside the integral), and both are used interchangeably.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\n\n- Incorrect references to the literature[[RWK-NEG], [CNT], [CRT], [MAJ]]\n\nPage 3: \"using virtual observations (originally proposed by Qui\u00f1onero-Candela & Rasmussen (2005) for sparse approximations of GPs)\"\n\nThe authors are citing as the origin of virtual observations a survey paper on the topic.[[RWK-NEU], [CNT], [DIS], [MIN]] Of course, that survey paper correctly attributes the origin to [1].[[RWK-NEU], [CNT], [DIS], [MIN]]\n\nPage 4: \"we apply the technique of variational inference Wainwright et al. (2008)\".[[RWK-NEU,MET-NEU], [CNT], [DIS], [MIN]]\n\nHow can variational inference be attributed to (again) a survey paper on the topic from 2008, when for instance [2] appeared in 2003?[[CNT], [CNT], [QSN], [MIN]]\n\n\n- Correctness of the approach[[MET-NEU], [null], [DIS], [MIN]]\n\nCan the authors guarantee that the variational bound that they are introducing (as defined in eqs. (19) and (41)) is actually a variational bound?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] It seems to me that the approximations made to Q(A) to propagate the uncertainty are breaking the bounding guarantee.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]  If it is no longer a lower bound, what is the rationale behind maximizing it?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] \n\nThe mathematical basis for this paper is actually introduced in [3] and a single-layer version of the current model is developed in [4].[[MET-NEU], [EMP-NEU], [DIS], [MIN]] However, in [4] the authors manage to avoid the additional Q(A) approximation that breaks the variational bound.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] The authors should contrast their approach with [4] and discuss if and why that additional central limit theorem application is necessary.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\n\n- No experiments[[EXP-NEG], [EMP-NEG], [DFT], [MAJ]]\n\nThe use of a non-parametric definition for the activation function should be contrasted with the use of a parametric one.[[MET-NEU], [CMP-NEU], [DIS], [MIN]] With enough data, both might produce similar results.[[DAT-NEU,RES-NEU], [EMP-NEU], [DIS], [MIN]] And the parameter sharing in the parametric one might actually be beneficial.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] With no experiments at all showing the benefit of this proposal, this paper cannot be considered complete.[[EXP-NEG,OAL-NEG], [SUB-NEG], [CRT], [MIN]]\n\n\n- Minor errors:\n\nEq. (4), for consistency, should use the identity matrix for the covariance matrix definition.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\nEq. (10) uses subscript d where it should be using subscript n\nEq.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] (17) includes p(X^L|F^L) in the definition of Q(...), but it shouldn't.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] That was particularly misleading, since if we take eq. (17) to be correct (which I did at first), then p(X^L|F^L) cancels out and should not appear in eq. (20).[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\nEq. (23) uses Q(F|A) to mean the same as P(F|A) as far as I understand. Then why use Q?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\n\n- References\n\n[1] Edward Snelson and Zoubin Ghahramani.[[BIB-NEU], [null], [DIS], [GEN]] Sparse Gaussian processes using pseudo-inputs.[[BIB-NEU], [null], [DIS], [GEN]]\n[2] Beal, M.J. Variational Algorithms for Approximate Bayesian Inference.[[BIB-NEU], [null], [DIS], [GEN]]\n[3] M.K. Titsias and N.D. Lawrence. Bayesian Gaussian process latent variable model. [[BIB-NEU], [null], [DIS], [GEN]]\n[4] M. L\u00e1zaro-Gredilla. Bayesian warped Gaussian processes.\n"[[BIB-NEU], [null], [DIS], [GEN]]