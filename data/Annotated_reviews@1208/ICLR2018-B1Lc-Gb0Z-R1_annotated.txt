"The paper studies learning in deep neural networks with hard activation functions, e.g. step functions like sign(x).[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] Of course, backpropagation is difficult to adapt to such networks, so prior work has considered different approaches.[[PDI-NEU], [null], [SMY], [GEN]] Arguably the most popular is straight-through estimation (Hinton 2012, Bengio et al. 2013), in which the activation functions are simply treated as identity functions during backpropagation.[[RWK-NEU,PDI-NEU], [null], [SMY], [GEN]] More recently, a new type of straight-through estimation, saturated STE (Hubara et al., 2016) uses 1[|z|<1] as the derivative of sign(z).[[RWK-NEU,MET-NEU], [null], [SMY], [GEN]]\n\nThe paper generalizes saturated STE by recognizing that other discrete targets of each activation layer can be chosen.[[MET-NEU], [null], [SMY], [GEN]] Deciding on these targets is formulated as a combinatorial optimization problem.[[PDI-NEU], [null], [SMY], [GEN]] Once the targets are chosen, updating the weights of each layer to minimize the loss on those targets is a convex optimization.[[PDI-NEU], [null], [DIS], [GEN]] The targets are heuristically updated through the layers, starting out the output using the proposed feasibility target propagation.[[PDI-NEU], [null], [DIS], [GEN]] At each layer, the targets can be chosen using a variety of search algorithms such as beam search.[[MET-NEU], [null], [SMY], [GEN]]\n\nExperiments show that FTP often outperforms saturated STE on CIFAR and ImageNet with sign and quantized activation functions, reaching levels of performance closer to the full-precision activation networks.[[DAT-POS,EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]]\n\nThis paper's ideas are very interesting, exploring an alternative training method to backpropagation that supports hard-threshold activation functions.[[PDI-POS,MET-POS], [EMP-POS], [APC], [MAJ]] The experimental results are encouraging,[[EXP-POS], [EMP-POS], [APC], [MAJ]] though I have a few questions below that prevent me for now from rating the paper higher.[[OAL-NEU], [null], [DIS], [GEN]]\n\nComments and questions:\n\n1) How computationally expensive is FTP?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] The experiments using ResNet indicate it is not prohibitively expensive, but I am eager for more details.[[DAT-POS,EXP-POS], [SUB-NEU,EMP-POS], [APC], [MAJ]]\n\n2) Does (Hubara et al., 2016) actually compare their proposed saturated STE with the orignal STE on any tasks?[[MET-NEU], [CMP-NEU], [QSN], [MIN]] I do not see a comparison.[[MET-NEG], [CMP-NEG], [DFT], [MIN]] If that is so, should this paper also compare with STE?[[MET-NEU], [CMP-NEU], [QSN], [MIN]] How do we know if generalizing saturated STE is more worthwhile than generalizing STE?[[MET-NEU], [CMP-NEU], [QSN], [MIN]]\n\n3) It took me a while to understand the authors' subtle comparison with target propagation, where they say \"Our framework can be viewed as an instance of target propagation that uses combinatorial optimization to set discrete targets, whereas previous approaches employed continuous optimization.[[MET-NEG], [CMP-NEG], [CRT], [MIN]]\" It seems that the difference is greater than explicitly stated, that prior target propagation used continuous optimization to set *continuous targets*. (One could imagine using continuous optimization to set discrete targets such as a convex relaxation of a constraint satisfaction problem.)[[MET-NEG,RES-NEG], [CMP-NEG], [CRT], [MIN]] Focusing on discrete targets gains the benefits of quantized networks.[[RES-NEU], [EMP-NEU], [DIS], [MIN]] If I am understanding the novelty correctly, it would strengthen the paper to make this difference clear.[[MET-NEU], [NOV-NEU], [DIS], [MIN]]\n\n4) On a related note, if feasible target propagation generalizes saturated straight through estimation, is there a connection between (continuous) target propagation and the original type of straight through estimation?[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\n5) In Table 1, the significance of the last two columns is unclear.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]] It seems that ReLU and Saturated ReLU are included to show the performance of networks with full-precision activation functions (which is good).[[MET-POS], [EMP-POS], [APC], [MAJ]] I am unclear though on why they are compared against each other (bolding one or the other) and if there is some correspondence between those two columns and the other pairs, i.e., is ReLU some kind of analog of SSTE and Saturated ReLU corresponds to FTP-SH somehow?"[[MET-POS], [CMP-POS], [QSN], [MAJ]]