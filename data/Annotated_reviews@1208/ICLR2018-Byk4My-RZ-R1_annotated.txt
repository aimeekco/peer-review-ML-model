"Summary:\n\nThe paper proposes to learn new priors for latent codes z  for GAN training.[[INT-NEU], [null], [SMY], [GEN]]  for this the paper shows that there is a mismatch between the gaussian prior and an estimated of the latent codes of real data by reversal of the generator .[[RWK-NEU,MET-NEU], [CMP-NEU], [SMY], [GEN]] To fix this the paper proposes to learn a second GAN to learn the prior distributions of \"real latent code\" of the first GAN.[[MET-NEU], [null], [SMY], [GEN]] The first GAN then uses the second GAN as prior to generate the z codes. [[MET-NEU], [null], [SMY], [GEN]]\n \nQuality/clarity:\n\nThe paper is well written and easy to follow.[[OAL-POS], [CLA-POS], [APC], [MAJ]]\n\nOriginality:\n\npros:\n-The paper while simple sheds some light on important problem with the prior distribution used in GAN.[[PDI-POS], [NOV-NEU], [SMY], [MAJ]]\n- the second GAN solution trained on reverse codes from real data is interesting [[MET-POS], [EMP-POS], [APC], [MAJ]]\n- In general the topic is interesting, the solution presented is simple but needs more study[[PDI-POS,MET-NEU], [SUB-NEU,EMP-NEU], [APC], [MAJ]]\n\ncons:\n\n- It related to adversarial learned inference and BiGAN, in term of learning the mapping  z ->x, x->z and seeking the agreement. [[MET-NEU], [null], [SMY], [GEN]]\n- The solution presented is not end to end (learning a prior generator on learned models have been done in many previous works on encoder/decoder)[[MET-NEU], [NOV-NEG], [CRT], [MAJ]]\n\nGeneral Review:\n\nMore experimentation with the latent codes will be interesting:[[EXP-NEU], [SUB-NEU], [SUG], [MAJ]]\n\n- Have you looked at the decay of the singular values of the latent codes obtained from reversing the generator? [[MET-NEU], [EMP-NEU], [QSN], [MAJ]]Is this data low rank? [[DAT-NEU], [null], [QSN], [MIN]]how does this change depending on the dimensionality of the latent codes?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Maybe adding plots to the paper can help.[[TNF-NEU], [PNF-NEU], [SUG], [MAJ]]\n\n- the prior agreement score is interesting but assuming gaussian prior also for the learned latent codes from real data is maybe not adequate.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]]  Maybe computing the entropy of the codes using a nearest neighbor estimate of the entropy  can help understanding the entropy difference wrt to the isotropic gaussian prior?[[MET-NEU], [EMP-NEU], [QSN], [MAJ]]\n\n- Have you tried to multiply the isotropic normal noise with the learned singular values and generate images from  this new prior  and compute inceptions scores etc?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Maybe also rotating the codes with the singular vector matrix V or \\Sigma^{0.5} V?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\n- What architecture did you use for the prior generator GAN?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\n- Have you thought of an end to end way to learn the prior generator GAN?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] \n\n****** I read the authors reply. Thank you for your answers and for the SVD plots this is  helpful.[[EXT-POS], [null], [DIS], [GEN]]  *****\n\n"