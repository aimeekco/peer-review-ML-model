"Typical recurrent neural networks suffer from over-paramterization.[[EXT-NEU], [null], [DIS], [GEN]] Additionally, standard RNNs (non-gated versions) have an ill-conditioned recurrent weight matrix, leading to vanishing/exploding gradients during training.[[EXP-NEU], [null], [DIS], [GEN]] This paper suggests to factorize the recurrent weight matrix as a Kronecker product of matrices.[[PDI-NEU], [null], [SMY], [GEN]] Additionally, in order to avoid vanishing/exploding gradients in standard RNNs, a soft unitary constraint is used.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]]  The regularizer is specifically nice in this setting, as it suffices to have the Kronecker factors be unitary.[[MET-POS], [EMP-POS], [APC], [MAJ]]  In the empirical section, several RNNs are trained using this approach, using only ~ 100 recurrent parameters, and still achieve comparable results to state-of-the-art approaches.[[RWK-NEU,EXP-NEU,RES-NEU], [CMP-NEU], [DIS], [MAJ]]  The paper argues that the recurrent state should be high-dimensional (in order to be able to encode the input and extract predictive information) but the recurrent dynamic should be realized by a low-capacity model.[[PDI-NEU], [null], [SMY], [GEN]] \n\nQuality: The paper is well written.[[OAL-POS], [CLA-POS], [APC], [MAJ]] \n\nClarity: Main ideas are clearly presented.[[PDI-POS], [CLA-POS,PNF-POS], [APC], [MAJ]] \n\nOriginality/Significance: Kronecker factorization was introduced for Convolutional networks (citation is in the paper).[[RWK-NEU], [NOV-NEU,IMP-NEU], [DIS], [GEN]]  Soft unitary constraints also have been introduced in earlier work (citations are also in the paper).[[RWK-NEU], [NOV-NEU,IMP-NEU], [DIS], [GEN]]  Nevertheless, showing that these two ideas work also for RNNs in combination (and seeing, e.g. the nice relationship between Kronecker factors and unitary) is a relevant contribution.[[RWK-NEU], [CMP-NEU], [DIS], [MIN]]  Additionally, this approach allows a significant reduction of training time it seems.\n\n"[[EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]] 