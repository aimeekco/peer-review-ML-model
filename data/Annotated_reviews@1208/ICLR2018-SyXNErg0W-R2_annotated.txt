"The paper studies the problem of DNN loss function design for reducing intra-class variance in the output feature space.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The key contribution is proposing an isotropic variant of the softmax loss that can balance the accuracy of classification and compactness of individual class.[[MET-NEU], [null], [SMY], [GEN]] The proposed loss has been compared extensively against a number of closely related approaches in methodology.[[MET-NEU], [null], [SMY], [GEN]] Numerical results on benchmark datasets show some improvement of the proposed loss over softmax loss and center loss (Wen et al., 2016), when applied to distance-based classifiers such as k-NN and k-means.[[RWK-POS,DAT-POS,RES-POS], [CMP-POS,EMP-POS], [SMY], [MIN]] \n\nPros: \n\n- The idea of isotropic normalization for enhancing compactness of class is well motivated[[PDI-POS], [EMP-POS], [APC], [MAJ]]\n\n- The paper is mostly clearly organized and presented.[[OAL-POS], [CLA-POS,PNF-POS], [APC], [MAJ]]\n\n- Numerical study shows some promise of the proposed method.[[MET-POS], [EMP-POS], [APC], [MAJ]]\n\nCons:\n\n-  The novelty of method is mostly incremental given the prior work of (Wen et al., 2016) which has provided a slightly different isotropic variant of softmax loss.[[MET-NEG], [NOV-NEG,CMP-NEG], [CRT], [MIN]]\n\n- The training procedure of the proposed method remains unclear in this paper. \n\n\n"[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]]