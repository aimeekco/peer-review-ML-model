"Deep neural networks have found great success in various applications.[[MET-POS], [EMP-POS], [APC], [MAJ]] This paper presents a theoretical analysis for 2-layer neural networks (NNs) through a spectral approach.[[RWK-NEU,EXP-NEU,MET-NEU,ANA-NEU], [EMP-NEU], [SMY], [GEN]] Specifically, the authors develop a Fourier-based generalization bound.[[RWK-NEU,PDI-NEU], [EMP-NEU], [SMY], [GEN]] Based on this, the authors show that the bandwidth, Fourier l_1 norm and the gradient for local minima of the population risk can be controlled for 2-layer NNs with SINE activation functions.[[RWK-NEU,PDI-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] Numerical experimental results are also presented to verify the theory.[[ANA-NEU], [EMP-NEU], [SMY], [GEN]]\n\n(1) The scope is a bit limited. [[OAL-NEG], [SUB-NEG], [DFT], [MIN]]The paper only considers 2-layer NNs. Is there an essential difficulty in extending the result here to NNs with more layers?[[RWK-NEG], [SUB-NEG], [QSN], [MIN]] Also, the analysis for gradient-based method in section 6  is only for squared-error loss, SINE activation and a deterministic target variable. [[RWK-NEG,EXP-NEG,ANA-NEG], [SUB-NEG], [DFT], [MIN]]What would happen if Y is random or the activation is ReLU?[[RWK-NEU,EXP-NEU], [null], [QSN], [GEN]]\n(2) The generalization bound in Corollary 3 is only for the gradient w.r.t. \\alpha_j. Perhaps, an object of more interest is the gradient w.r.t. W.[[RWK-NEU,EXP-POS], [EMP-POS], [APC], [MAJ]] It would be intersting to present some analysis regarding the gradient w.r.t. W.[[RWK-NEU,ANA-POS], [SUB-NEU], [SUG,DFT], [GEN]]\n(3) It is claimed that the bound is tighter than that obtained using only the Lipschitz property of the activation function.[[RWK-NEU,EXP-NEU], [null], [SMY], [GEN]] However, no comparison is clearly made.[[OAL-NEG], [CLA-NEG,CMP-NEG], [DFT,CRT], [MIN]] It would be better if the authors could explain this more?[[RWK-NEG], [SUB-NEG], [DFT], [MIN]]\n\nIn summary, the application domain of the theoretical results seems a bit restricted.[[RWK-NEG,RES-NEG,OAL-NEU], [SUB-NEG], [DFT], [MIN]]\n\nMinor comments:\nEq. (1): d\\xi should be dx\nLemma 2: one \\hat{g} should be \\hat{f}"[[EXT-NEU], [CNT], [CNT], [CNT]]
