"The authors propose an extension to the Neural Statistician which can model contexts with multiple partially overlapping features.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] This model can explain datasets by taking into account covariate structure needed to explain away factors of variation and it can also share this structure partially between datasets.[[PDI-NEU,DAT-NEU], [null], [SMY], [GEN]]\n\nA particularly interesting aspect of this model is the fact that it can learn these context c as features conditioned on meta-context a, which leads to a disentangled representation.[[MET-POS], [EMP-POS], [APC], [MAJ]]\nThis is also not dissimilar to ideas used in 'Bayesian Representation Learning With Oracle Constraints' Karaletsos et al 2016 where similar contextual features c are learned to disentangle representations over observations and implicit supervision.[[PDI-NEU,RWK-NEU], [CMP-NEU], [DIS], [MIN]]\n\nThe authors provide a clean variational inference algorithm to learn their model.[[MET-NEU], [null], [SMY], [GEN]] However, a key problem is the following: the nature of the discrete variables being used makes them hard to be inferred with variational inference.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] The authors mention categorical reparametrization as their trick of choice, but do not go into empirical details int heir experiments regarding the success of this approach.[[EXP-NEG,MET-NEG], [SUB-NEG], [CRT], [MIN]] In fact, it would be interesting to study which level of these variables could be analytically collapsed (such as done in the Semi-Supervised learning work by Kingma et al 2014) and which ones can be sampled effectively using a form of reparametrization.[[RWK-NEU,ANA-NEU], [SUB-NEU], [SUG,DIS], [MIN]]\n\nThis also touches on the main criticism of the paper: While the model technically makes sense and is cleanly described and derived,[[MET-POS], [EMP-POS], [APC], [MAJ]]  the empirical evaluation is on the weak side and the rich properties of the model are not really shown off.[[MET-NEG,ANA-NEG], [SUB-NEG,EMP-NEG], [DFT,CRT], [MAJ]] It would be interesting if the authors could consider adding a more illustrative experiment and some more empirical results regarding inference in this model and the marginal structures that can be learned with this model in controlled toy settings.[[EXP-NEG,RES-NEG], [SUB-NEG,EMP-NEG], [SUG], [MIN]]\nCan the model recover richer structure that was imposed during data generation?[[MET-NEU,RES-NEU], [EMP-NEU], [QSN], [MIN]] How limiting is the learning of a?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\nHow does the likelihood of the model behave under the circumstances?[[RES-NEU], [EMP-NEU], [QSN], [MIN]]\nThe experiments do not really convey how well this all will work in practice.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]]\n\n"