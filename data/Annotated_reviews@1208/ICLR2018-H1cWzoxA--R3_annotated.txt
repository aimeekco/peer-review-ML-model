"This paper introduces bi-directional block self-attention model (Bi-BioSAN) as a general-purpose encoder for sequence modeling tasks in NLP.[[INT-NEU], [null], [SMY], [GEN]] The experiments include tasks like natural language inference, reading comprehension (SquAD), semantic relatedness and sentence classifications.[[EXP-NEU], [EMP-NEU], [SMY], [GEN]] The new model shows decent performance when comparing with Bi-LSTM, CNN and other baselines while running at a reasonably fast speed.[[RWK-POS], [CMP-POS], [APC], [GEN]]\n\nThe advantage of this model is that we can use little memory (as in RNNs) and enjoy the parallelizable computation as in (SANs), and achieve similar (or better) performance.[[EXP-POS,MET-POS], [EMP-POS], [APC,DIS], [GEN]]\n\nWhile I do appreciate the solid experiment section, I don't think the model itself is sufficient contribution for a publication at ICLR.[[EXP-POS,MET-NEG], [APR-NEG], [CRT], [MAJ]] First, there is not much innovation in the model architecture.[[MET-NEG], [EMP-NEG], [DFT], [GEN]] The idea of the Bi-BioSAN model simply to split the sentence into blocks and compute self-attention for each of them, and then using the same mechanisms as a pooling operation followed by a fusion level.[[MET-NEU], [null], [DIS], [GEN]] I think this more counts as careful engineering of the SAN model rather than a main innovation.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] Second, the model introduces much more parameters. In the experiments, it can easily use 2 times parameters than the commonly used encoders.[[EXP-NEU], [null], [SUG,DIS], [GEN]] What if we use the same amount of parameters for Bi-LSTM encoders? Will the gap between the new model and the commonly used ones be smaller?[[MET-NEU], [null], [QSN], [MIN]]\n\n====\n\nI appreciate the answers the authors added and I change the score to 6."[[OAL-POS], [null], [APC,FBK], [GEN]]