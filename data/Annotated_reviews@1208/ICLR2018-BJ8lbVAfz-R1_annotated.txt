"The paper discusses learning in a neural network with three layers, where the middle layer is topographically organized.[[INT-NEU], [null], [SMY], [GEN]] The learning dynamics defined for the network results in specific update equations of the weights W (Eqn. 14), which combine elements of supervised learning and self-organizing maps (SOMs).[[PDI-NEU], [null], [SMY], [GEN]] The weights thus change according to the imposed neighborhood relationship and depending on the class labels.[[PDI-NEU], [null], [SMY], [GEN]]\n\nWhat I like about the approach is the investigation of the interplay between unsupervised and hierarchical supervised learning in a biological context.[[MET-POS], [EMP-POS], [APC], [MAJ]] I agree with the authors that an integrated view of self-organization and learning across layers is presumably required to better understand biological learning.[[MET-POS], [EMP-POS], [APC], [MAJ]] The general methodology also makes sense to me.[[MET-POS], [EMP-POS], [APC], [MAJ]] However, I do have concerns including two major concerns: (A) delimitation of results from earlier work; (B) numerical results (especially Tab. 1).[[RWK-NEG,RES-NEG], [CMP-NEG], [CRT], [MAJ]]\n\n(A) The paper derives the main update equation of W which combines self-organization and label-sensitive learning - Eqn. 15.[[MET-NEG], [CMP-NEG], [CRT], [MIN]] This equation is then discussed and the SOM-like updates and the differences to previous pure SOMs are highlighted.[[RWK-NEG,MET-NEG], [CMP-NEG], [CRT], [MIN]] The paper also states (Secs. 1 and 2) that the the network studied here is based on Hartono et al, 2015, with the main difference of the sigmoidal ouput layer being replaced by a softmax layer.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MIN]] What is missing is a discussion of the differences regarding the later numerical experiments, and a clear delimitation to Hartono et al., 2015, when Eqn. 15 is discussed.[[EXP-NEG], [SUB-NEG], [DFT], [MAJ]] What is the major structural difference to their Eqn. 13 which is discussed along very similar lines as Eqn. 15 of this paper.[[MET-NEG], [CMP-NEG], [QSN], [MIN]] Also after reading the abstract of this paper, one may think that this is the first paper discussing the SOM / supervised learning combination.[[ABS-NEG], [CLA-NEG], [CRT], [MIN]]\n\n(B) A further difference to Hartono et al, 2015, are comparisons with multi-layer networks, and the presentation and discussion of this comparison is my strongest concern.[[RWK-NEG], [CMP-NEG,PNF-NEG], [CRT], [MAJ]] In the first paragraph of Sec. 3 the competing deep networks are introduced.[[MET-NEU], [null], [DIS], [GEN]] Then it is stated: \"the number of hidden neurons, as well as the structures for the deep neural networks\nwere empirically tried, and the results of the best settings were registered for comparison\" (1st paragraph Sec. 3).[[EXP-NEU,RES-NEU], [CMP-NEU], [DIS], [MIN]] What I do not understand are then the high classification errors reported in Tab. 1.[[TNF-NEG], [EMP-NEG], [CRT], [MIN]] It is known that even basic multi-layer perceptrons (MLPs) result in much lower classification errors, e.g., for MNIST. LeCun et al., 1998, is a classical example with less then 3% error on MNIST with many later examples that improve on these.[[DAT-NEU,RWK-NEU], [CMP-NEU], [DIS], [MIN]] Also the well-known original DBN paper has MNIST as main example (and main selling point) with close to 1% error.[[RWK-NEU], [CMP-NEU], [DIS], [MIN]] Why are the classification errors for DBN and MLP in the Tab 1 so high?[[RES-NEG,TNF-NEG], [EMP-NEG], [CRT], [MAJ]] And if they are in reality much lower, then competitiveness of s-rRBF in terms of classification results to these systems is questionable.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] The table makes me having doubts regarding the competitiveness of S-rRBF.[[RES-NEG,TNF-NEG], [EMP-NEG], [CRT], [MAJ]] I therefore disagree with the conclusion that this paper has shown that S-rRBFs are \"comparable to the best performer for most of the diverse benchmark applications\" (last paragraph in Conclusion).[[RES-NEG], [CMP-NEG], [CRT], [MAJ]] The feature of providing auxiliary visual information (also conclusion) is much more convincing (but also a feature of Hartono et al, 2015).[[RWK-NEG,RES-NEG], [CMP-NEG], [CRT], [MIN]]\n\nMore generally, putting the biological arguments aside, why would a 2D neighborhood relationship be helpful?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] I see a benefit in interpretation which can help.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Also, if there is an intrinsic 2D hidden structure in the data, then imposing a 2D representation can help (as a sort of a prior).[[MET-NEU], [EMP-NEU], [CRT], [MIN]] But in general there may not be a 2D intrinsic property, or there is a higher dimensional hidden structure - so why not 3D or more? Related to this, why not using an objective that would result in a dynamics similar to a growing neural gas instead of an SOM?[[MET-NEU], [EMP-NEG], [QSN], [MIN]]\n\n\nMinor:\n\nThe work is first introduced as multi-layer but only the single hidden layer case is actually discussed.[[MET-NEU], [EMP-NEU], [CRT], [MIN]] I would suggest to either really add multi-hidden-layer results (which is not really doable in a conference revision), or state multi-layer work as outlook.[[RES-NEU], [EMP-NEG], [CRT], [MIN]]\n\nFig. 5, bad readability of axes labels.[[TNF-NEU], [CLA-NEG], [CRT], [MIN]]\n\nis a hierarchical -> are hierarchical\n\nyields -> yield\n\ntwice \"otherwise\" after Eqn. 7\n\nare can be viewed\n\nthey occurs\n\ncan can readily expanded\n\ntransfer transform\n"[[TNF-NEU], [CLA-NEG], [CRT], [MIN]]]