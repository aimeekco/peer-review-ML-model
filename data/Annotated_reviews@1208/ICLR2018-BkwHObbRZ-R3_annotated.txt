"This paper proposes a tensor factorization-type method for learning one hidden-layer neural network.[[INT-NEU], [null], [SMY], [GEN]] The most interesting part is the Hermite polynomial expansion of the activation function.[[MET-NEU], [null], [SMY], [GEN]] Such a decomposition allows them to convert the population risk function as a fourth-order orthogonal tensor factorization problem.[[MET-NEU], [null], [SMY], [GEN]] They further redesign a new formulation for the tensor decomposition problem, and show that the new formulation enjoys the nice strict saddle properties as shown in Ge et al. 2015. [[RWK-NEU,MET-NEU], [null], [SMY], [GEN]]At last, they also establish the sample complexity for recovery.[[MET-NEU], [null], [SMY], [GEN]]\n\nThe organization and presentation of the paper need some improvement.[[OAL-NEU], [PNF-NEU], [SUG], [MAJ]] For example, the authors defer many technical details.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] To make the paper accessible to the readers, they could provide more intuitions in the first 9 pages.[[OAL-NEU], [CLA-NEU], [SUG], [MAJ]]\n\nThere are also some typos: For example, the dimension of a is inconsistent.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] In the abstract, a is an m-dimensional vector, and on Page 2, a is a d-dimensional vector.[[ABS-NEU], [CLA-NEG], [CRT], [MIN]] On Page 8, P(B) should be a degree-4 polynomial of B.[[CNT], [CLA-NEG], [SUG], [MIN]]\n\nThe paper does not contains any experimental results on real data."[[EXP-NEU,RES-NEG], [EMP-NEG], [CRT], [MAJ]]