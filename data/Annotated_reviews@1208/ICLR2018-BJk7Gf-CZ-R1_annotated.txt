"The paper gives sufficient and necessary conditions for the global optimality of the loss function of deep linear neural networks.[[INT-POS,PDI-POS], [null], [SMY], [GEN]] The paper is an extension of Kawaguchi'16.[[RWK-NEU], [NOV-NEU], [SMY], [GEN]] It also provides some sufficient conditions for the non-linear cases.[[PDI-POS], [CNT], [SMY], [GEN]] \n\nI think the main technical concerns with the paper is that the technique only applies to a linear model, and it doesn't sound the techniques are much beyond Kawaguchi'16.[[RWK-NEG,MET-NEG], [SUB-NEG,CMP-NEG], [DFT,CRT], [MAJ]] I am happy to see more papers on linear models, but I would expect there are more conceptual or technical ingredients in it.[[MET-NEU,OAL-NEU], [SUB-NEU], [SUG,DIS], [MIN]] As far as I can see, the same technique here will fail for non-linear models for the same reason as Kawaguchi's technique.[[RWK-NEG,MET-NEG], [CMP-NEG], [CRT], [MAJ]] Also, I think a more interesting question might be turning the landscape results into an algorithmic result --- have an algorithm that can guarantee to converge a global minimum.[[MET-NEU,RES-NEU,FWK-NEU], [SUB-NEU,IMP-NEU,EMP-NEU], [SUG], [MIN]] This won't be trivial because the deep linear networks do have a lot of very flat saddle points and therefore it's unclear whether one can avoid those saddle points. "[[OAL-NEU], [CNT], [DIS], [GEN]]