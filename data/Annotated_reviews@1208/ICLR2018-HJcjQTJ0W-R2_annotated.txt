"Summary: The paper studies the problem of effectively training Deep NN under the constraint of privacy.[[PDI-NEU], [null], [SMY], [GEN]] The paper first argues that achieving privacy guarantees like differential privacy is hard, and then provides frameworks and algorithms that quantify the privacy loss via Signal-to-noise ratio.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]]  In my opinion, one of the main features of this work is to split the NN computation to local computation and cloud computation, which ensures that unnecessary amount of data is never released to the cloud.[[PDI-NEU,MET-NEU], [null], [DIS], [GEN]]\n\nComments: I have my concerns about the effectiveness of the notion of privacy introduced in this paper.[[CNT], [null], [SMY], [GEN]] The definition of privacy loss in Equation 5 is an average notion, where the averaging is performed over all the sensitive training data samples.[[EXP-NEU], [null], [SMY], [GEN]] This notion does not seem to protect the privacy of every individual training example, in contrast to notions like differential privacy.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]] Average case notions of privacy are usually not appreciated in the privacy community because of their vulnerability to a suite of attacks.[[EXP-NEG], [EMP-NEG], [CRT], [MIN]]\n\nThe paper may have a valid point that differential privacy is hard to work with, in the case of Deep NN.[[PDI-NEU], [null], [DIS], [MAJ]] However, the paper needs to make a much stronger argument to defend this claim."[[CNT], [CNT], [SUG], [MAJ]]