"\n-I think title is misleading, as the more concise results in this paper is about linear networks I recommend adding linear in the title i.e. changing the title to \u2026 deep LINEAR networks[[INT-NEG,RES-NEU], [APR-NEG,PNF-NEG], [SUG,CRT], [MAJ]]\n\n- Theorems 2.1, 2.2 and the observation (2) are nice![[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]]\n \n- Theorem 2.2 there is no discussion about the nature of the saddle point is it strict?[[MET-NEG], [SUB-NEG], [DFT,QSN], [MAJ]] Does this theorem imply that the global optima can be reached from a random initialization?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Regardless of if this theorem can deal with these issues, a discussion of the computational implications of this theorem is necessary.[[MET-NEU,ANA-NEU], [SUB-NEU], [SUG], [MIN]]\n\n- I\u2019m a bit puzzled by Theorems 4.1 and 4.2 and why they are useful.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Since these results do not seem to have any computational implications about training the neural nets what insights do we gain about the problem by knowing this result? [[RES-NEG], [IMP-NEG], [QSN,CRT], [MAJ]]Further discussion would be helpful.\n"[[ANA-NEU], [SUB-NEU], [SUG], [MIN]]