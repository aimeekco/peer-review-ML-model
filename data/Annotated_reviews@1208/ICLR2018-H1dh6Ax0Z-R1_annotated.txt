"# Update after the rebuttal\nThank you for the rebuttal.[[EXT-POS], [null], [FBK], [GEN]]\nThe authors claim that the source of objective mismatch comes from n-step Q-learning, and their method is well-justified in 1-step Q-learning[[MET-POS], [EMP-POS], [SMY], [GEN]]. However, there is still a mismatch even with 1-step Q-learning because the bootstrapped target is also computed from the TreeQN.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] More specifically, there can be a mismatch between the optimal action sequences computed from TreeQN at time t and t+1 if the depth of TreeQN is equal or greater than 2.[[MET-NEU], [EMP-NEG], [CRT], [MAJ]] Thus, the author's response is still not convincing to me.[[MET-NEU], [EMP-NEG], [CRT], [MAJ]]\nI like the overall idea of using a tree-structured neural network which internally performs planning as an abstraction of Q-function, which makes implementation simpler compared to VPN.[[PDI-POS,MET-POS], [EMP-POS], [APC], [MAJ]] However, the particular method (TreeQN) proposed in this paper introduces a mismatch in the model learning as mentioned above.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] One could argue that TreeQN is learning an \"abstract\" planning rather than \"grounded\" planning.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] However, the fact that reward prediction loss is used to train TreeQN significantly weakens this claim, and there is no such an evidence in the paper.[[MET-NEU], [EMP-NEG], [CRT], [MAJ]] \nIn conclusion, I think the research direction is worth pursuing, but the proposed modification from VPN is not well-justifie[[PDI-POS,MET-NEG], [EMP-NEG], [CRT], [MAJ]]d.\n\n# Summary\nThis paper proposes TreeQN and ATreeC which perform look-ahead planning using neural networks.[[INT-NEU], [null], [SMY], [GEN]] TreeQN simulates the future by predicting rewards/values of the future states and performs tree backup to construct Q-values.[[MET-NEU], [null], [SMY], [GEN]] ATreeC is an actor-critic architecture that uses a softmax over TreeQN.[[MET-NEU], [null], [SMY], [GEN]] The architecture is trained through n-step Q-learning with reward prediction loss.[[EXP-NEU], [null], [SMY], [GEN]] The proposed methods outperform DQN baseline on 2D Box Pushing domain and outperforms VPN on Atari games.[[RWK-NEU,MET-POS], [CMP-POS], [SMY], [GEN]]\n\n[Pros]\n- The paper is easy to follow.[[OAL-POS], [CLA-POS], [APC], [MAJ]]\n- The application to actor-critic setting (ATreeC) is novel, though the underlying idea was proposed by [O'Donoghue et al., Schulman et al.].[[RWK-NEU,PDI-NEU], [NOV-POS], [APC], [MAJ]]\n\n[Cons]\n- The proposed method has a technical issue.[[MET-NEG], [null], [CRT], [MAJ]]\n- The proposed idea (TreeQN) and underlying motivation are almost same as those of VPN [Oh et al.], but there is no in-depth discussion that shows why TreeQN is potentially better than VPN.[[RWK-NEU,PDI-NEU], [CMP-NEG], [CRT], [MAJ]] \n- Comparison to VPN on Atari is not much convincing.[[DAT-NEU,EXP-NEU], [CMP-NEG], [CRT], [MAJ]] \n\n# Novelty and Significance\n- The underlying motivation (planning without predicting observations), the architecture (transition/reward/value functions applied to the latent state space), and the algorithm (n-step Q-learning with reward prediction loss) are same as those of VPN. [[MET-NEU], [EMP-NEU], [SMY], [GEN]]But, the paper does not provide in-depth discussion on this.[[MET-NEU], [EMP-NEG], [CRT], [MAJ]] The following is the differences that I found from this paper, so it would be important to discuss why such differences are important.[[OAL-NEU], [null], [DIS], [GEN]]\n\n1) The paper emphasizes the \"fully-differentiable tree planning\" aspect in contrast to VPN that back-propagates only through \"non-branching\" trajectories during training.[[MET-NEU], [EMP-NEU], [SMY], [GEN]] However, differentiating TreeQN also amounts to back-propagating through a \"single\" trajectory in the tree that gives the maximum Q-value. [[MET-NEU], [EMP-NEU], [DIS], [GEN]]Thus, the only difference between TreeQN and VPN is that TreeQN follows the best (estimated) action sequence, while VPN follows the chosen action sequence in retrospect during back-propagation.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [GEN]] Can you justify why following the best estimated action sequence is better than following the chosen action sequence during back-propagation (see Technical Soundness section for discussion)[[MET-NEU], [EMP-NEU], [QSN], [MAJ]]?\n\n2) TreeQN only sets targets for the final Q-value after tree backup, whereas VPN sets targets for all intermediate value predictions in the tree.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [GEN]] Why is TreeQN's approach better than VPN's approach?[[MET-NEU], [CMP-NEU], [QSN], [MAJ]] \n\n- The application to actor-critic setting (ATreeC) is novel, though the underlying idea of combining Q-learning with policy gradient was proposed by [O'Donoghue et al.] and [Schulman et al.].[[RWK-NEU,PDI-POS], [NOV-POS], [SMY], [MAJ]]\n\n# Technical Soundness\n- The proposed idea of setting targets for the final Q-value after tree backup can potentially make the temporal credit assignment difficult, because the best estimated actions during tree planning does not necessarily match with the chosen actions.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Suppose that TreeQN estimated \"up-right-right\" as the best future action sequence the during 3-step tree planning, while the agent actually ended up with choosing \"up-left-left\" (this is possible because the agent re-plans at every step and follows epsilon-greedy policy).[[MET-NEU], [EMP-NEU], [DIS], [GEN]] Following n-step Q-learning procedure, we end up with setting target Q-value based on on-policy action sequence \"up-left-left\", while back-propagating through \"up-right-right\" action sequence in the TreeQN's plan.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] This causes a wrong temporal credit assignment, because TreeQN can potentially increase/decrease value estimates in the wrong direction due to the mismatch between the planned actions and chosen actions.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] So, it is unclear why the proposed algorithm is technically correct or better than VPN's approach (i.e., back-propagating through the chosen actions in the search tree).[[RWK-NEU,MET-NEU], [CMP-NEG], [CRT], [MAJ]]\n \n# Quality\n- Comparison to VPN on Atari is not convincing because TreeQN-1 is actually (almost) equivalent to VPN-1, but the results show that TreeQN-1 performs much better than VPN on many games.[[RWK-NEU,MET-NEU], [CMP-NEG], [CRT], [MAJ]] Since the authors took the numbers from [Oh et al.] rather than replicating VPN, it is possible that the gap comes from implementation details (e.g., hyperparameter).[[DAT-NEG,EXP-NEU], [EMP-NEU], [DIS], [MAJ]] \n\n# Clarity\n- The paper is overall easy to follow and the description of the proposed method is clear.[[OAL-POS], [CLA-POS], [APC], [MAJ]]"