"This high-quality paper tackles the quadratic dependency of memory on sequence length in attention-based models, and presents strong empirical results across multiple evaluation tasks.[[INT-POS], [null], [SMY,APC], [GEN]] The approach is basically to apply self-attention at two levels, such that each level only has a small, fixed number of items, thereby limiting the memory requirement while having negligible impact on speed.[[MET-NEU], [EMP-NEU], [SMY], [GEN]] It captures local information into so-called blocks using self-attention, and then applies a second level of self-attention over the blocks themselves.[[MET-NEU], [EMP-NEU], [SMY], [GEN]]\n\nThe paper is well organized and clearly written, modulo minor language mistakes that should be easy to fix with further proof-reading.[[OAL-POS], [CLA-POS], [SUG,APC], [GEN]] The contextualization of the method relative to CNNs/RNNs/Transformers is good, and the beneficial trade-offs between memory, runtime and accuracy are thoroughly investigated, and they're compelling.[[MET-POS], [EMP-POS], [APC], [GEN]]\n\nI am curious how the story would look if one tried to push beyond two levels...?[[MET-NEU], [null], [QSN], [MIN]] For example, how effective might a further inter-sentence attention level be for obtaining representations for long documents?[[MET-NEU], [null], [QSN], [MIN]] \n\nMinor points:\n- Text between Eq 4 & 5: W^{(1)} appears twice; one instance should probably be W^{(2)}[[MET-NEU], [CLA-NEU], [FBK], [MIN]].\n- Multiple locations, e.g. S4.1: for NLI, the word is *premise*, not *promise*.\n- Missing word in first sentence of S4.1: ... reason __ the ..."[[OAL-NEU], [CLA-NEU], [FBK], [MIN]]