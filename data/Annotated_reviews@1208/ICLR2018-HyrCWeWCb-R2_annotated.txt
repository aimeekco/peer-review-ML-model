"This paper presents a policy gradient method that employs entropy regularization and entropy constraint at the same time.[[INT-NEU,MET-NEU], [null], [SMY], [GEN]] The entropy regularization on action probability is to encourage the exploration of the policy, while the entropy constraint is to stabilize the gradient.[[MET-NEU], [null], [SMY], [GEN]]\n\nThe major weakness of this paper is the unclear presentation.[[OAL-NEG], [PNF-NEG], [CRT], [MIN]] For example, the algorithm is never fully described, though a handful variants are discussed.[[MET-NEG], [SUB-NEG,PNF-NEG], [CRT], [MIN]]  How the off-policy version is implemented is missing.[[MET-NEG], [SUB-NEG], [DFT], [MIN]] \n\nIn experiments, why the off-policy version of TRPO is not compared.[[EXP-NEG], [SUB-NEG], [DFT,QSN], [MIN]] Comparing the on-policy results, PCL does not show a significant advantage over TRPO.[[RES-NEG], [CMP-NEG], [CRT], [MAJ]] Moreover, the curves of TRPO is so unstable, which is a bit uncommon.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] \n\nWhat is the exploration strategy in the experiments?[[EXP-NEU], [EMP-NEG], [QSN], [MIN]] I guess it was softmax probability.[[EXP-NEU], [null], [DIS], [MIN]] However, in many cases, softmax does not perform a good exploration, even if the entropy regularization is added.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nAnother issue is the discussion of the entropy regularization in the objective function.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] This regularization, while helping exploration, do changes the original objective.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] When a policy is required to pass through a very narrow tunnel of states, the regularization that forces a wide action distribution could not have a good performance.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] Thus it would be more interesting to see experiments on more complex benchmark problems like humanoids."[[EXP-NEG], [SUB-NEU], [SUG], [MIN]]