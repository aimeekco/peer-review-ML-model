"This paper presents an embedding algorithm for text corpora that allows known\ncovariates, e.g. author information, to modify a shared embedding to take context\ninto account.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The method is an extension of the GloVe method and in the case of\na single covariate value the proposed method reduces to GloVe.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] The covariate-dependent\nembeddings are diagonal scalings of the shared embedding.[[MET-NEU], [null], [DIS], [GEN]] The authors demonstrate\nthe method on a corpus of books by various authors and on a corpus of subreddits.[[MET-NEU], [null], [SMY], [GEN]]\nThough not technically difficult, the extension of GloVe to covariate-dependent\nembeddings is very interesting and well motivated.[[MET-POS], [EMP-POS], [APC], [MAJ]] Some of the experimental results\ndo a good job of demonstrating the advantages of the models.[[MET-POS], [EMP-POS], [APC], [MAJ]] However, some of the\nexperiments are not obvious that the model is really doing a good job.[[EXP-NEG,RES-NEG], [EMP-NEG], [CRT], [MIN]]\n\nI have some small qualms with the presentation of the method.[[MET-NEU], [EMP-NEU], [CRT], [MIN]] First, using the term\n\"size m\" for the number of values that the covariate can take is a bit misleading.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\nUsually the size of a covariate would be the dimensionality.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] These would be the same\nif the covariate is one hot coded, however, this isn't obvious in the paper right now.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\nAdditionally, v_i and c_k live in R^d, however, it's not really explained what\n'd' is, is it the number of 'topics', or something else?[[MET-NEG], [EMP-NEG], [QSN], [MAJ]] Additionally, the functional\nform chosen for f() in the objective was chosen to match previous work but with no\nexplanation as to why that's a reasonable form to choose.[[MET-NEG], [CMP-NEG], [CRT], [MIN]] Finally, the authors\nsay toward the end of Section 2 that \"A careful comparision shows that this\napproximation is precisely that which is implied by equation 4, as desired\". [[MET-NEU], [EMP-NEU], [DIS], [MIN]]This is\ncryptic, just show us that this is the case.[[CNT], [null], [DIS], [GEN]]\n\nRegarding the experiments there needs to be more discussion about how the\ndifferent model parameters were determined.[[EXP-NEU], [null], [DIS], [MIN]] The authors say \"... and after tuning\nour algorithm to emged this dataset, ...\", but this isn't enough.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] What type of\ntuning did you do to choose in particular the latent dimensionality and the\nlearning rate?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] I will detail concerns for the specific experiments below.\n\nSection 4.1:\n- How does held-out data fit into the plot?[[EXP-NEU], [EMP-NEU], [QSN], [GEN]]\n\nSection 4.2:\n- For the second embedding, what exactly was the algorithm trained on?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]] Just the\n  book, or the whole corpus?[[DAT-NEU], [null], [QSN], [MIN]]\n- What is the reader supposed to take away from Table 1?[[TNF-NEU], [null], [QSN], [MIN]] Are higher or lower\n  values better? Maybe highlight the best scores for each column.[[RES-NEU], [EMP-NEU], [QSN], [MIN]]\n\n\nSection 4.3:\n- Many of these distributions don't look sparse.[[RES-NEG], [EMP-NEG], [CRT], [MIN]]\n- There is a terminology problem in this section.[[CNT], [null], [DIS], [GEN]] Coordinates in a vector are\n  not sparse, the vector itself is sparse if there are many zeros, but\n  coordinates are either zero or not zero.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] The authors' use of 'sparse' when\n  they mean 'zero' is really confusing.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n- Due to the weird sparsity terminology Table 1 is very confusing.[[TNF-NEG], [null], [CRT], [MIN]] Based on how\n  the authors use 'sparse' I think that Table 1 shows the fraction of zeros in\n  the learned embedding vectors.[[TNF-NEG], [null], [CRT], [MIN]] But if so, then these vectors aren't sparse at all\n  as most values are non-zero.[[TNF-NEU], [null], [DIS], [MIN]]\n\nSection 5.1:\n- I don't agree with the authors that the topics in Table 3 are interpretable.[[TNF-NEU], [EMP-NEU], [DIS], [MIN]]\n  As such, I think it's a reach to claim the model is learning interpretable topics.[[MET-NEU], [null], [DIS], [MIN]]\n  This isn't necessarily a problem, it's fine for models to not do everything well,\n  but it's a stretch for the authors to claim that these results are a positive\n  aspect of the model.[[RES-POS], [EMP-POS], [APC], [MAJ]] The results in Section 5.2 seem to make a lot of sense and\n  show the big contribution of the model.[[RES-POS], [EMP-POS], [APC], [MAJ]] \n\nSection 5.3:\n- What is the \"a : b :: c : d\" notation?\n"[[MET-NEU], [EMP-NEU], [QSN], [MIN]] 