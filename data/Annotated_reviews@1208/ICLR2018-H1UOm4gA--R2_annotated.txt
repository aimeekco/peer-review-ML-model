"[Overview]\nIn this paper, the authors proposed a unified model for combining vision, language, and action. It is aimed at controlling an agent in a virtual environment to move to a specified location in a 2D map, and answer user's questions as well.[[PDI-NEU], [null], [SMY], [GEN]] To address this problem, the authors proposed an explicit grounding way to connect the words in a sentence and spatial regions in the images.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]]  Specifically, By this way, the model could exploit the outputs of concept detection module to perform the actions and question answering as well jointly.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]]  In the experiments, the authors compared with several previous attention methods to show the effectiveness of the proposed concept detection module and demonstrated its superiority on several configurations, including in-domain and out-of-domain cases.[[EXP-NEU,MET-NEU], [CMP-NEU], [SMY], [GEN]] \n\n[Strengths]\n\n1. I think this paper proposed interesting tasks to combine the vision, language, and actions.[[PDI-POS,MET-POS], [EMP-POS], [APC], [MAJ]]  As we know, in a realistic environment, all three components are necessary to complete a complex tasks which need the interactions with the physical environments.[[MET-NEU], [null], [DIS], [GEN]]  The authors should release the dataset to prompt the research in this area.[[DAT-NEU,FWK-NEU], [IMP-NEU], [SUG], [MAJ]] \n\n2. The authors proposed a simple method to ground the language on visual input.[[MET-POS], [EMP-POS], [APC], [MAJ]]  Specifically, the authors grounded each word in a sentence to all locations of the visual map, and then perform a simple concept detection upon it.[[MET-POS], [EMP-POS], [APC], [MAJ]] Then, the model used this intermediate representation to guide the navigation of agent in the 2D map and visual question answering as well.[[MET-NEU], [null], [DIS], [GEN]]\n\n3. From the experiments, it is shown that the proposed model outperforms several baseline methods in both normal tasks and out-of-domain ones.[[EXP-NEU,MET-NEU], [CMP-NEU], [SMY], [GEN]] According to the visualizations, the interpreter could generate meaningful attention map given a textual query.[[EXP-NEU], [null], [DIS], [GEN]]\n\n[Weakness]\n\n1. The definition of explicit grounding is a bit misleading.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Though the grounding or attention is performed for each word at each location of the visual map.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] It is a still kind of soft-attention, except that is performed for each word in a sentence.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] As far as I know, this has been done in several previous works, such as: (a). Hierarchical question-image co-attention for visual question answering (https://scholar.google.com/scholar?oi=bibs&cluster=15146345852176060026&btnI=1&hl=en). Lu et al. NIPS 2016.[[RWK-NEU], [CMP-NEU], [DIS], [MIN]] (b). Graph-Structured Representations for Visual Question Answering. Teney et al. arXiv 2016.[[RWK-NEU], [CMP-NEU], [DIS], [MIN]] At most recent, we have seen some more explicit way for visual grounding like: (c). Bottom-up and top-down attention for image captioning and VQA (https://arxiv.org/abs/1707.07998). Anderson et al. arXiv 2017.[[RWK-NEU], [CMP-NEU], [DIS], [MIN]]\n\n2. Since the model is aimed at grounding the language on the vision based on interactions, it is worth to show how well the final model could ground the text words to each of the visual objects.[[MET-NEU,ANA-NEU], [SUB-NEU], [SUG], [MIN]] Say, show the affinity matrix between the words and the objects to indicate the correlations.[[MET-NEU], [null], [DIS], [GEN]]\n\n[Summary]\n\nI think this is a good paper which integrates vision, language, and actions in a virtual environment.[[OAL-POS], [CNT], [APC], [MAJ]] I would foresee more and more works will be devoted to this area, considering its close connection to our daily life.[[FWK-POS], [IMP-POS], [APC], [MAJ]] To address this problem, the authors proposed a simple model to ground words on visual signals, which prove to outperform previous methods, such as CA, SAN, etc.[[MET-POS], [CMP-POS,EMP-POS], [APC], [MAJ]] According to the visualization, the model could attend the right region of the image for finishing a navigation and QA task.[[MET-POS], [EMP-POS], [APC], [MAJ]] As I said, the authors should rephrase the definition of explicit grounding, to make it clearly distinguished with the previous work I listed above.[[MET-NEU], [CMP-NEU], [SUG], [MIN]] Also, the authors should definitely show the grounding attention results of words and visual signal jointly, i.e., showing them together in one figure instead of separately in Figure 9 and Figure 10.\n"[[RES-NEU,TNF-NEU], [PNF-NEU], [SUG], [MAJ]]