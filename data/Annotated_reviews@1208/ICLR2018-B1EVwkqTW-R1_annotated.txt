"After reading the rebuttal:\n\nThis paper does have encouraging results.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]]  But as mentioned earlier, it still lacks systematic comparisons with existing (and strongest) baselines, and perhaps a better understanding the differences between approaches and the pros and cons.[[RWK-NEG,MET-NEG], [CMP-NEG], [CRT], [MAJ]]  The writing also needs to be improved.[[OAL-NEG], [CLA-NEG], [CRT], [MIN]]  So I think the paper is not ready for publication and my opinion remains.[[OAL-NEG], [REC-NEG], [FBK], [MAJ]] \n===========================================================\n\nThis paper presents an algorithm for few shot learning.[[MET-NEU], [null], [SMY], [GEN]]  The idea is to first learn representation of data using the siamese networks architecture, which predicts if a pair of two samples are similar (e.g., from the same class) or not using a SVM hinge loss, and then finetune the classifier using few labeled examples (with possibly a different set of labels).[[MET-NEU], [null], [SMY], [GEN]]  I think the idea of representation learning using a somewhat artificial task makes sense in this setting. \n\nI have several concerns for this submission.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] \n1. I am not very familiar with the literature of few shot learning.[[RWK-NEU], [null], [DIS], [GEN]]  I think a very related approach that learns the representation using pretty much the same information is the contrastive loss:\n-- Hermann and Blunsom.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [GEN]]  Multilingual Distributed Representations without Word Alignment. ICLR 2014.[[RWK-NEU], [null], [SMY], [GEN]]\nThe intuition is similar: similar pairs shall have higher similarity in the learned representation, than dissimilar pairs, by a large margin.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MIN]] This approach is useful even when there is only weak supervision to provide the \"similarity/dissimilarity\" information.[[MET-POS], [EMP-POS], [APC], [MAJ]] I wonder how does this approach compare with the proposed method.[[MET-NEU], [CMP-NEU], [DIS], [GEN]]\n\n2. The experiments are conducted on a small dataset OMNIGLOT and TIMIT.[[DAT-NEU,EXP-NEU], [SUB-NEU,EMP-NEU], [DIS], [MIN]] I do not understand why the compared methods are not consistently used in both experiments.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MIN]] Also, the experiment of speaker classification on TIMIT (where the inputs are audio segments with different durations and sampling frequency) is a quite nonstandard task; I do not have a sense of how challenging it is.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] It is not clear why CNN transfer learning (the authors did not give details about how it works) performs even worse than the non-deep baseline, yet the proposed method achieves very high accuracy.[[RWK-POS,MET-POS], [EMP-POS], [APC], [MAJ]] It would be nice to understand/visualize what information have been extracted in the representation learning phase.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \n\n3. Relatively minor: The writing of this paper is readable,[[OAL-POS], [CLA-POS], [APC], [MAJ]] but could be improved.[[OAL-NEG], [CLA-NEG], [SUG], [MIN]] It sometimes uses vague/nonstandard terminology (\"parameterless\") and statement.[[OAL-NEG], [CLA-NEG], [CRT], [MAJ]] The term \"siamese kernel\" is not very informative: yes, you are learning new representations of data using DNNs, but this feature mapping does not have the properties of RKHS; also you are not solving the SVM dual problem as one typically does for kernel SVMs.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] In my opinion the introduction of SVM can be shortened, and more focuses can be put on related deep learning methods and few shot learning."[[MET-NEU], [SUB-NEU], [SUG], [MIN]]