"This paper proposes a neural clustering model following the \"Noise as Target\" technique.[[INT-NEU], [null], [SMY], [GEN]]  Combining with an reconstruction objective and \"delete-and-copy\" trick, it is able to cluster the data points into different groups and is shown to give competitive results on different benchmarks.[[DAT-NEU,MET-NEU], [null], [SMY], [GEN]] \n\nIt is nice that the authors tried to extend the \"noise as target\" to the clustering problem, and proposed the simple \"delete-and-copy\" technique to group different data points into clusters.[[DAT-NEU,MET-NEU], [null], [SMY], [GEN]]  Even tough a little bit ad-hoc, it seems promising based on the experiment results.[[EXP-POS,RES-POS], [EMP-POS], [APC], [MAJ]]  However, it is unclear to me why it is necessary to have the optimal matching here and why the simple nearest target would not work.[[EXP-NEG], [EMP-NEG], [CRT], [MIN]]  After all, the cluster membership is found based on the nearest target in the test stage.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]] \n\nAlso, the authors should provide more detailed description regarding the scheduling of the alpha and lambda values during training, and how sensitive it is to the final clustering performance.[[EXP-NEU,ANA-NEU], [SUB-NEU], [SUG], [MIN]] The authors cited the no requirement of \"a predefined number of clusters\" as one of the contributions, but the tuning of alpha seems more concerning.[[EXP-POS], [EMP-POS], [APC], [MAJ]]\n\nI like the authors experimented with different benchmarks, but lack of comparisons with existing deep clustering techniques is definitely a weakness.[[EXP-NEG], [CMP-NEG], [DFT], [MAJ]] The only baseline comparison provided is the k-means clustering, but the comparisons were somewhat unfair.[[EXP-NEG], [CMP-NEG], [CRT], [MAJ]]  For all the text datasets, there were no comparisons with k-means on the features learned from the auto-encoders or clusterings learned from similar number of clusters.[[DAT-NEG,EXP-NEG], [CMP-NEG], [DFT], [MAJ]]  The comparisons for the Twitter dataset were even based on character-level with word-level.[[DAT-NEU], [CMP-NEU], [DIS], [MIN]]  It is more convincing to show the superiority of the proposed method than existing ones on the same ground.[[MET-NEU], [CMP-NEU], [DIS], [MIN]] \n\nSome other issues regarding quantitative results:\n- In Table 1, there are 152 clusters for 10-d latent space after convergence, but there are 61 clusters for 10-d latent space in Table 2 for the same MNIST dataset.[[DAT-NEU,RES-NEG,TNF-NEG], [EMP-NEG], [CRT], [MAJ]]  Are they based on different alpha and lambda values?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]  \n- Why does NATAC perform much better than NATAC-k?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Would NATAC-k need a different number of clusters than the one from NATAC?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] The number of centroids learned from NATAC may not be good for k-means clustering.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n- It seems like the performance of AE-k is increasing with increase of dimensionality of latent space for Fashion-MNIST.[[MET-NEU,RES-NEU], [EMP-NEU], [DIS], [MIN]] Would AE-k beat NATAC with a different dimensionality of latent space and k?"[[MET-NEU], [EMP-NEU], [QSN], [MIN]]