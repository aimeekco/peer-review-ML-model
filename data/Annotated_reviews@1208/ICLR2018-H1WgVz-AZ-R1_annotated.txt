"= Quality = \nOverall, the authors do a good job of placing their work in the context of related research, and employ a variety of non-trivial technical details to get their methods to work well.[[RWK-POS,ANA-POS], [EMP-POS], [APC], [MAJ]] \n\n= Clarity = \n\nOverall, the exposition regarding the method is good.[[RWK-POS,MET-POS], [CMP-POS], [APC], [MAJ]] I found the setup for the sequence tagging experiments confusing, tough.[[RWK-NEG,EXP-NEG], [CMP-NEG], [CRT], [MIN]] See more comments below.\n\n= Originality / Significance = \n\nThe paper presents a clever idea that could help make SPENs more practical.[[PDI-POS], [EMP-POS], [APC], [MAJ]] The paper's results also suggest that we should be thinking more broadly about how to using complicated structured distributions as teachers for model compression.[[PDI-POS,RES-POS], [EMP-POS], [APC], [MAJ]]\n\n= Major Comment =\n\nI'm concerned by the quality of your results and the overall setup of your experiments.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] In particular, the principal contribution of the sequence tagging experiments seems top be different than what is advertised earlier on in the paper. [[RWK-NEU,EXP-NEU,ANA-NEU], [CMP-NEU], [DIS], [GEN]]\n\nMost of your empirical success is obtained by taking a pretrained CRF energy function and using this as a teacher model to train a feed-forward inference network. You have have very few experiments using a SPEN energy function parametrization that doesn't correspond to a CRF, even though you could have used an arbitrary convnet, RNN, etc.[[RWK-NEU,EXP-NEU,ANA-NEU], [CMP-NEU], [DIS], [GEN]] The one exception is when you use the tag language model. This is a good idea, but it is pretrained, not trained using the saddle-point objective you introduce.[[PDI-NEU,ANA-NEU], [null], [DIS], [GEN]] In fact, you don't have any results demonstrating that the saddle-point approach is better than simpler alternatives.[[RWK-NEU,RES-NEU], [null], [DIS], [GEN]]\n\nIt seems that you could have written a very different paper about model compression with CRFs that would have been very interesting and you could've have used many of the same experiments. It's unclear why SPENs are so important. [[RWK-NEU,EXP-NEU,ANA-NEU], [EMP-NEU], [SUG], [MIN]]The idea of amortizing inference is perhaps more general. [[PDI-NEU,ANA-NEU], [EMP-NEU], [DIS], [MIN]]My recommendation is that you either rebrand the paper to be more about general methods for amortizing structured prediction inference using model compression or do more fine-grained experiments with SPENs that demonstrate empirical gains that leverage their flexible deep-network-based energy functions[[EXP-NEU,MET-NEU,ANA-NEU], [EMP-NEU], [DIS], [MIN]].\n\n\n= Minor Comments = \n\n* You should mention 'Energy Based GANs[[PDI-NEG], [EMP-NEU], [SUG], [MIN]]\"\n\n* I don't understand[[PDI-NEG,ANA-NEG], [EMP-NEU], [DIS], [MIN]] \"This approach performs backpropagation through each step of gradient descent, permitting more stable training but also evidently more overfitting.[[RWK-NEU,ANA-NEU], [EMP-NEU], [DIS], [MIN]]\" Why would it overfit more? Simply because training was more stable? Couldn't you prevent overfitting by regularizing more?\n\n* [[RWK-NEU,DAT-NEU], [EMP-NEU], [QSN], [MIN]]You spend too much space talking about specific hyperparameter ranges, etc.[[RWK-NEU,ANA-NEU], [EMP-NEU], [DIS], [GEN]] This should be moved to the appendix. You should also add a short summary of the TLM architecture to the main paper body.\[[RWK-NEU,BIB-NEG], [EMP-NEU], [DIS], [MIN]]n\n* Regarding your footnote discussing using a positive vs. negative sign on the entropy regularization term, I recommend checking out \"Regularizing neural networks by penalizing confident output distributions.[[MET-NEU,ANA-NEU], [EMP-NEU], [SUG], [MIN]]\"\n\n* You should add citations for the statement \"In these and related settings, gradient descent has started to be replaced by inference networks.[[RWK-NEU,BIB-NEG], [EMP-NEU], [DIS], [MAJ]]\"\n\n* I didn't find Table 1 particularly illuminating.[[RWK-NEU,TNF-NEG], [EMP-NEG], [CRT], [MIN]] All of the approaches seem to perform about the same[[RWK-NEU,TNF-NEG], [EMP-NEG], [CRT], [MIN]]. What conclusions should I make from it?\n\n* Why not use KL divergence as your \\Delta function?\n\n* Why are the results in Table 5 on the dev data?\n\n* I was confused by Table 4[[RWK-NEU,TNF-NEG], [EMP-NEG], [CRT], [MIN]]. First of all, it took me a very long time to figure out that the middle block of results corresponds to taking a pretrained CRF energy and amortizing inference by training an inference network.[[RWK-NEU,TNF-NEG], [EMP-NEG], [CRT], [MIN]] This idea of training with a standard loss (conditional log lik.) and then amortizing inference post-hoc was not explicitly introduced as an alternative to the saddle point objective you put forth earlier in the paper.[[PDI-NEG,ANA-NEG], [EMP-NEG], [CRT], [MIN]] Second, I was very surprised that the inference network outperformed Viterbi (89.7 vs. 89.1 for the same CRF energy).[[RWK-NEU], [null], [DIS], [GEN]] Why is this?\n\n* I'm confused by the difference between Table 6 and Table 4? Why not just include the TLM results in Table 4?\n\n\n\n\n\n\n"[[RES-NEU,TNF-NEU], [null], [DIS], [GEN]]
