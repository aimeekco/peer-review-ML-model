"Neal (1994) showed that a one hidden layer Bayesian neural network, under certain conditions, converges to a Gaussian process as the number of hidden units approaches infinity.[[EXT-NEU], [null], [DIS], [GEN]] Neal (1994) and Williams (1997) derive the resulting kernel functions for such Gaussian processes when the neural networks have certain transfer functions.[[EXT-NEU], [null], [DIS], [GEN]]\n\nSimilarly, the authors show an analogous result for deep neural networks with multiple hidden layers and an infinite number of hidden units per layer, and show the form of the resulting kernel functions.[[RES-NEU], [CMP-NEU], [SMY], [GEN]] For certain transfer functions, the authors perform a numerical integration to compute the resulting kernels.[[MET-NEU,RES-NEU], [null], [SMY], [GEN]] They perform experiments on MNIST and CIFAR-10, doing classification by scaled regression.[[DAT-NEU,EXP-NEU], [null], [SMY], [GEN]] \n\nOverall, the work is an interesting read, and a nice follow-up to Neal\u2019s earlier observations about 1 hidden layer neural networks.[[RWK-POS,OAL-POS], [CMP-POS], [APC], [MAJ]] It combines several insights into a nice narrative about infinite Bayesian deep networks.[[MET-POS], [EMP-POS], [APC], [MAJ]] However, the practical utility, significance, and novelty of this work -- in its current form -- are questionable, and the related work sections, analysis, and experiments should be significantly extended.[[RWK-NEG,EXP-NEG,ANA-NEG], [NOV-NEG,IMP-NEG], [CRT], [MAJ]] \n\n\nIn detail:\n\n(1) This paper misses some obvious connections and references, such as \n* Krauth et. al (2017): \u201cExploring the capabilities and limitations of Gaussian process models\u201d for recursive kernels with GPs.[[RWK-NEG], [SUB-NEG,CMP-NEG], [DFT], [MIN]]\n* Hazzan & Jakkola (2015): \u201cSteps Toward Deep Kernel Methods from Infinite Neural Networks\u201d for GPs corresponding to NNs with more than one hidden layer.[[RWK-NEG], [SUB-NEG,CMP-NEG], [DFT], [MIN]]\n* The growing body of work on deep kernel learning, which \u201ccombines the inductive biases and representation learning abilities of deep neural networks with the non-parametric flexibility of Gaussian processes\u201d. E.g.: (i) \u201cDeep Kernel Learning\u201d (AISTATS 2016);[[RWK-NEU], [CMP-NEU], [DIS], [MIN]] (ii) \u201cStochastic Variational Deep Kernel Learning\u201d (NIPS 2016);[[RWK-NEU], [CMP-NEU], [DIS], [MIN]] (iii) \u201cLearning Scalable Deep Kernels with Recurrent Structure\u201d (JMLR 2017).[[RWK-NEU], [CMP-NEU], [DIS], [MIN]] \n\nThese works should be discussed in the text.[[RWK-NEG], [SUB-NEG,CMP-NEG], [SUG], [MIN]]\n\n(2) Moreover, as the authors rightly point out, covariance functions of the form used in (4) have already been proposed.[[RWK-NEU], [NOV-NEU], [DIS], [MIN]] It seems the novelty here is mainly the empirical exploration (will return to this later), and numerical integration for various activation functions.[[EXP-POS,MET-POS], [NOV-POS], [APC], [MAJ]] That is perfectly fine -- and this work is still valuable.[[OAL-POS], [IMP-POS], [APC], [MAJ]] However, the statement \u201crecently, kernel functions for multi-layer random neural networks have been developed, but only outside of a Bayesian framework\u201d is incorrect.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] For example, Hazzan & Jakkola (2015) in \u201cSteps Toward Deep Kernel Methods from Infinite Neural Networks\u201d consider GP constructions with more than one hidden layer.[[RWK-NEU], [CMP-NEU], [DIS], [MIN]] Thus the novelty of this aspect of the paper is overstated.[[OAL-POS], [NOV-POS], [APC], [MAJ]] \n\nSee also comment [*] later on the presentation.[[OAL-NEU], [PNF-NEU], [DIS], [GEN]] In any case, the derivation for computing the covariance function (4) of a multi-layer network is a very simple reapplication of the procedure in Neal (1994).[[RWK-NEG,MET-NEG], [PNF-NEG,EMP-NEG], [CRT], [MIN]] What is less trivial is estimating (4) for various activations, and that seems to the major methodological contribution.[[MET-POS], [EMP-POS], [APC], [MAJ]] \n\nAlso note that multidimensional CLT here is glossed over.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] It\u2019s actually really unclear whether the final limit will converge to a multidimensional Gaussian with that kernel without stronger conditions.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]]  This derivation should be treated more thoroughly and carefully.[[MET-NEG], [EMP-NEG], [SUG,CRT], [MAJ]]\n\n(3) Most importantly, in this derivation, we see that the kernels lose the interesting representations that come from depth in deep neural networks.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Indeed, Neal himself says that in the multi-output settings, all the outputs become uncorrelated.[[RWK-NEU], [CMP-NEU], [DIS], [MIN]] Multi-layer representations are mostly interesting because each layer shares hidden basis functions.[[MET-NEU], [null], [DIS], [MIN]] Here, the sharing is essentially meaningless, because the variance of the weights in this derivation shrinks to zero.[[MET-NEU], [null], [DIS], [MIN]] \nIn Neal\u2019s case, the method was explored for single output regression, where the fact that we lose this sharing of basis functions may not be so restrictive.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] However, these assumptions are very constraining for multi-output classification and also interesting multi-output regressions.[[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]]\n\n[*]: Generally, in reading the abstract and introduction, we get the impression that this work somehow allows us to use really deep and infinitely wide neural networks as Gaussian processes, and even without the pain of training these networks.[[ABS-POS,INT-POS,EXP-POS], [EMP-POS], [APC], [MAJ]] \u201cDeep neural networks without training deep networks\u201d.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]] This is not an accurate portrayal.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]] The very title \u201cDeep neural networks as Gaussian processes\u201d is misleading, since it\u2019s not really the deep neural networks that we know and love.[[INT-NEG], [CNT], [CRT], [MAJ]] In fact, you lose valuable structure when you take these limits, and what you get is very different than a standard deep neural network.[[INT-NEG], [CNT], [CRT], [MAJ]]  In this sense, the presentation should be re-worked.[[INT-NEG], [PNF-NEG], [CRT], [MIN]]\n\n(4) Moreover, neural networks are mostly interesting because they learn the representation.[[MET-POS], [EMP-POS], [APC], [MAJ]] To do something similar with GPs, we would need to learn the kernel.[[MET-NEU], [null], [DIS], [GEN]] But here, essentially no kernel learning is happening.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] The kernel is fixed.[[MET-NEU], [null], [DIS], [GEN]] \n\n(5) Given the above considerations, there is great importance in understanding the practical utility of the proposed approach through a detailed empirical evaluation.[[EXP-NEU,MET-NEU], [EMP-NEU], [SUG], [MIN]] In other words, how structured is this prior and does it really give us some of the interesting properties of deep neural networks, or is it mostly a cute mathematical trick?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]]  \n\nUnfortunately, the empirical evaluation is very preliminary, and provides no reassurance that this approach will have any practical relevance:[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n(i) Directly performing regression on classification problems is very heuristic and unnecessary.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MIN]]\n(ii) Given the loss of dependence between neurons in this approach, it makes sense to first explore this method on single output regression, where we will likely get the best idea of its useful properties and advantages.[[EXP-NEU,MET-NEU], [EMP-NEU], [SUG], [GEN]] \n(iii) The results on CIFAR10 are very poor.[[DAT-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] We don\u2019t need to see SOTA performance to get some useful insights in comparing for example parametric vs non-parametric, but 40% more error than SOTA makes it very hard to say whether any of the observed patterns hold weight for more competitive architectural choices.[[RWK-NEG,RES-NEG], [CMP-NEG,EMP-NEG], [CRT], [MAJ]] \n\nA few more minor comments:\n(i) How are you training a GP exactly on 50k training points?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]] Even storing a 50k x 50k matrix requires about 20GB of RAM.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]] Even with the best hardware, computing the marginal likelihood dozens of times to learn hyperparameters would be near impossible.[[EXP-NEG], [EMP-NEG], [DIS], [MIN]] What are the runtimes?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]]\n(ii) \"One benefit in using the GP is due to its Bayesian nature, so that predictions have uncertainty estimates (Equation (9)).[[MET-POS], [EMP-POS], [APC], [MIN]]\u201d  The main benefit of the GP is not the uncertainty in the predictions, but the marginal likelihood which is useful for kernel learning."[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [GEN]]