"This work fits well into a growing body of research concerning the encoding of network topologies and training of topology via evolution or RL.[[OAL-POS], [IMP-POS], [APC], [MAJ]] The experimentation and basic results are probably sufficient for acceptance, but to this reviewer, the paper spins the actual experiments and results a too strongly.[[EXP-POS,RES-POS], [APR-POS], [APC], [MAJ]]\n\nThe biggest two nitpicks:\n\n> In our work we pursue an alternative approach: instead of restricting the search space directly, we allow the architectures to have flexible network topologies (arbitrary directed acyclic graphs)[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\nThis is a gross overstatement. The architectures considered in this paper are heavily restricted to be a stack of cells of uniform content interspersed with specifically and manually designed convolution, separable convolution, and pooling layers.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Only the topology of the cells themselves are designed.[[MET-NEU], [null], [DIS], [MIN]] The work is still great,[[OAL-POS], [null], [APC], [MAJ]] but this misleading statement in the beginning of the paper left the rest of the paper with a dishonest aftertaste.[[OAL-NEG], [PNF-NEG], [CRT], [MAJ]] As an exercise to the authors, count the hyperparameters used just to set up the learning problem in this paper and compare them to those used in describing the entire VGG-16 network.[[RWK-NEU,PDI-NEU], [CMP-NEU], [SUG], [MIN]] It seems fewer hyperparameters are needed to describe VGG-16, making this paper hardly an alternative to the \"[common solution] to restrict the search space to reduce complexity and increase efficiency of architecture search.[[MET-NEU], [EMP-NEU], [CRT], [MIN]]\"\n\n> Table 1\n\nWhy is the second best method on CIFAR (\u201cHier. repr-n, random search (7000 samples)\u201d) never tested on ImageNet? [[DAT-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]]The omission is conspicuous.[[MET-NEG], [SUB-NEG], [CRT], [MIN]] Just test it and report.[[ANA-NEU], [SUB-NEU], [DIS], [MIN]]\n\nSmaller nitpicks:\n\n> \u201cNew state of the art for evolutionary strategies on this task[[MET-NEU], [null], [DIS], [GEN]]\u201d\n\n\u201cEvolutionary Strategies\u201d, at least as used in Salimans 2017, has a specific connotation of estimating and then following a gradient using random perturbations which this paper does not do.[[RWK-NEG,ANA-NEG], [SUB-NEG,CMP-NEG], [QSN], [MIN]] It may be more clear to change this phrase to \u201cevolutionary methods\u201d or similar.[[MET-NEU], [EMp-NEU], [DIS], [MIN]]\n\n> Our evolution algorithm is similar but more generic than the binary tournament selection (K = 2) used in a recent large-scale evolutionary method (Real et al., 2017).[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MIN]]\n\nA K=5% tournament does not seem more generic than a binary K=2 tournament. They\u2019re just different."[[MET-NEU], [CMP-NEU], [DIS], [MIN]]