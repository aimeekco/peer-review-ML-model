"This paper formulates GAN as a Lagrangian of a primal convex constrained optimization problem.[[INT-NEU], [null], [SMY], [GEN]] They then suggest to modify the updates used in the standard GAN training to be similar to the primal-dual updates typically used by primal-dual subgradient methods.[[PDI-NEU], [null], [SMY], [GEN]]\n\nTechnically, the paper is sound.[[OAL-POS], [CNT], [APC], [MAJ]] It mostly leverages the existing literature on primal-dual subgradient methods to modify the GAN training procedure.[[RWK-NEU], [null], [SMY], [GEN]] I think this is a nice contribution that does yield to some interesting insights.[[RES-POS,OAL-POS], [EMP-POS], [APC], [MAJ]] However I do have some concerns about the way the paper is currently written and I find some claims misleading.[[OAL-NEG], [CLA-NEG], [CRT], [MAJ]]\n\nPrior convergence proofs: I think the way the paper is currently written is misleading.[[OAL-NEG], [CLA-NEG], [CRT], [MAJ]] The authors quote the paper from Ian Goodfellow: \u201cFor GANs, there is no theoretical prediction as to\nwhether simultaneous gradient descent should converge or not.[[RWK-NEG], [CMP-NEG], [CRT], [MAJ]]\u201d. However, the f-GAN paper gave a proof of convergence, see Theorem 2 here: https://arxiv.org/pdf/1606.00709.pdf.[[EXT-NEU], [null], [DIS], [GEN]] A recent NIPS paper by (Nagarajan and Kolter, 2017) also study the convergence properties of simultaneous gradient descent.[[EXT-NEU], [null], [DIS], [GEN]] Another problem is of course the assumptions required for the proof that typically don\u2019t hold in practice (see comment below).[[EXP-NEG], [EMP-NEG], [CRT], [MIN]]\n\nConvex-concave assumption: In practice the GAN objective is optimized over the parameters of the neural network rather than the generative distribution.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] This typically yields a non-convex non-concave optimization problem.[[CNT], [null], [DIS], [GEN]] This should be mentioned in the paper and I would like to see a discussion concerning the gap between the theory and the practical algorithm.[[MET-NEU], [SUB-NEU], [SMY], [GEN]]\n\nRelation to existing regularization techniques: Combining Equations 11 and 13, the second terms acts as a regularizer that minimizes [\\lapha f_1(D(x_i))]^2.[[MET-NEU], [EMP-NEU], [SMY], [GEN]] This looks rather similar to some of the recent regularization techniques such as\nImproved Training of Wasserstein GANs, https://arxiv.org/pdf/1704.00028.pdf[[RWK-NEU], [CMP-NEU], [DIS], [GEN]]\nStabilizing Training of Generative Adversarial Networks through Regularization, https://arxiv.org/pdf/1705.09367.pdf\nCan the authors comment on this?[[RWK-NEU], [CMP-NEU], [QSN], [GEN]] I think this would also shed some light as to why this approach alleviates the problem of mode collapse.[[MET-NEU], [CMP-NEU], [QSN], [GEN]]\n\nCurse of dimensionality: Nonparametric density estimators such as the KDE technique used in this paper suffer from the well-known curse of dimensionality.[[MET-NEU], [EMP-NEU], [QSN], [GEN]] For the synthetic data, the empirical evidence seem to indicate that the technique proposed by the authors does work[[MET-POS],[EMP-POS], [APC], [MAJ]] but I\u2019m not sure the empirical evidence provided for the MNIST and CIFAR-10 datasets is sufficient to judge whether or not the method does help with mode collapse.[[DAT-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] The inception score fails to capture this property. [[RES-NEG], [EMP-NEG], [CRT], [MAJ]]Could the authors explore other quantitative measure?[[MET-NEG],[EMP-NEU], [QSN], [MIN]] Have you considered trying your approach on the augmented version of the MNIST dataset used in Metz et al. (2016) and Che et al. (2016)?[[RWK-NEG,MET-NEG], [EMP-NEU], [QSN], [MAJ]]\n\nExperiments\nTypo: Should say \u201cThe data distribution is p_d(x) = 1{x=1}\u201d.\n"[[MET-NEG], [CLA-NEG], [CRT], [MIN]]