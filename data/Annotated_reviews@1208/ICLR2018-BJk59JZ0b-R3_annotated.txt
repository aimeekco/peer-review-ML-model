"The paper presents a clever trick for updating the actor in an actor-critic setting: computing a guide actor that diverges from the actor to improve critic value, then updating the actor parameters towards the guide actor.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] This can be done since, when the parametrized actor is Gaussian and the critic value can be well-approximated as quadratic in the action, the guide actor can be optimized in closed form.[[PDI-NEU], [null], [SMY], [GEN]]\n\nThe paper is mostly clear and well-presented,[[OAL-POS], [CLA-POS,PNF-POS], [APC], [MAJ]] except for two issues: 1) there is virtually nothing novel presented in the first half of the paper (before Section 3.3);[[OAL-NEG], [NOV-NEG], [CRT], [MAJ]] and 2) the actual learning step is only presented on page 6, making it hard to understand the motivation behind the guide actor until very late through the paper.\n\nThe presented method itself seems to be an important contribution, even if the results are not overwhelmingly positive.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] It'd be interesting to see a more elaborate analysis of why it works well in some domains but not in others.[[ANA-NEG], [SUB-NEG], [SUG], [MIN]] More trials are also needed to alleviate any suspicion of lucky trials.[[ANA-NEG], [SUB-NEG], [SUG], [MIN]] \n\nThere are some other issues with the presentation of the method, but these don't affect the merit of the method:[[MET-NEG], [PNF-NEG], [CRT], [MIN]] \n\n1. Returns are defined from an initial distribution that is stationary for the policy.[[MET-NEU], [null], [DIS], [MIN]]  While this makes sense in well-mixing domains, the experiment domains are not well-mixing for most policies during training, for example a fallen humanoid will not get up on its own, and must be reset.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]] \n\n2. The definition of beta(a|s) as a mixture of past actors is inconsistent with the sampling method, which seems to be a mixture of past trajectories.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] \n\n3. In the first paragraph of Section 3.3: \"[...] the quality of a guide actor mostly depends on the accuracy of Taylor's approximation.[[MET-NEU], [null], [DIS], [MIN]] \" What else does it depend on?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Then: \"[...] the action a_0 should be in a local vicinity of a.[[MET-NEU], [null], [DIS], [MIN]]\"; and \"[...] the action a_0 should be similar to actions sampled from pi_theta(a|s).\" What do you mean \"should\"?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] In order for the Taylor approximation to be good?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\n4. The line before (19) is confusing, since (19) is exact and not an approximation.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] For the approximation (20), it isn't clear if this is a good approximation.[[RES-NEG], [EMP-NEG], [CRT], [MIN]] Why/when is the 2nd term in (19) small?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\n5. The parametrization nu of \\hat{Q} is never specified in Section 3.6.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] This is important in order to evaluate the complexities involved in computing its Hessian.\n"[[MET-NEU], [null], [DIS], [MIN]]