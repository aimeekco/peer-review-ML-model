"This paper introduces a neural network architecture for generating sketch drawings.[[INT-NEU], [null], [SMY], [GEN]] The authors propose that this is particularly interesting over generating pixel data as it emphasises more human concepts I agree.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The contribution of this paper of this paper is two-fold.[[OAL-NEU], [CNT], [DIS], [GEN]] I Firstly, the paper introduces a large sketch dataset that future papers can rely on.[[DAT-POS,FWK-POS], [SUB-POS,IMP-POS], [APC], [MAJ]] Secondly, the paper introduces the model for generating sketch drawings.[[MET-NEU], [null], [SMY], [GEN]] I\n\nThe model is inspired by the variational autoencoder.[[MET-NEU], [null], [SMY], [GEN]] I However, the proposed method departs from the theory that justifies the variational autoencoder.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] I believe the following things would be interesting points to discuss / follow up:[[MET-NEU], [SUB-NEU], [SUG], [MIN]]\n- The paper preliminarily investigates the influence of the KL regularisation term on a validation data likelihood.[[MET-NEU], [null], [SMY], [GEN]] It seems to have a negative impact for the range of values that are discussed.[[DAT-NEU,RES-NEG], [EMP-NEG], [CRT], [MAJ]] However, I would expect there to be an optimum.[[RES-NEU], [EMP-NEU], [DIS], [MIN]] Does the KL term help prevent overfitting at some stage?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Answering this question may help understand what influence variational inference has on this model.[[MET-NEU,RES-NEU], [EMP-NEU], [DIS], [MIN]]\n- The decoder model has randomness injected in it at every stage of the RNN.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Because of this, the latent state actually encodes a distribution over drawings, rather than a single drawing.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] It seems plausible that this is one of the reasons that the model cannot obtain a high likelihood with a high KL regularisation term.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] Would it help to rephrase the model to make the mapping from latent representation to drawing more deterministic?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] This definitely would bring it closer to the way the VAE was originally introduced.[[MET-NEU], [EMP-NEU], [DIS], [GEN]]\n- The unconditional generative model *only* relies on the \"injected randomness\" for generating drawings, as the initial state is initialised to 0.[[MET-NEU,RES-NEU], [EMP-NEU], [DIS], [MIN]] This also is not in the spirit of the original VAE, where unconditional generation involves sampling from the prior over the latent space.[[RWK-NEG,MET-NEG], [CMP-NEG], [CRT], [MIN]]\n\nI believe the design choices made by the authors to be valid in order to get things to work.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] But it would be interesting to see why a more straightforward application of theory perhaps *doesn't* work as well (or whether it works better).[[ANA-NEU], [SUB-NEU,EMP-NEU], [SUG,DIS], [MIN]] This would help interesting applications inform what is wrong with current theoretical views.[[EXP-NEU,MET-NEU], [EMP-NEU], [SUG,DIS], [MIN]]\n\nOverall, I would argue that this paper is a clear accept."[[OAL-POS], [REC-POS], [FBK], [MAJ]]