"This paper presented a Generative entity networks (GEN).[[INT-NEU], [null], [SMY], [GEN]] It is a multi-view extension of variational autoencoder (VAE) for disentangled representation.[[EXP-NEU], [null], [DIS], [GEN]] It uses the image and its attributes.[[EXP-NEU], [null], [DIS], [GEN]] The paper is very well motivated and tackles an important problem.[[PDI-POS], [EMP-POS], [APC], [MAJ]] However, the presentation of the method is not clear, the experiment is not sufficient, and the paper is not polished.[[EXP-NEG,MET-NEG,OAL-NEG], [CLA-NEG,EMP-NEG], [CRT], [MAJ]] \n\nPros:\n1. This paper tackles an important research question. [[PDI-POS], [EMP-POS], [APC], [MAJ]] \nLearning a meaningful representation is needed in general.[[PDI-POS], [EMP-POS], [APC], [MAJ]]  For the application of images, using text description to refine the representation is a natural and important research question.[[PDI-POS,TNF-POS], [EMP-POS], [APC], [MAJ]]  \n\n2. The proposed idea is very well motivated, and the proposed model seems correct.[[PDI-POS,MET-POS], [EMP-POS], [APC], [MAJ]]  \n\nCons and questions:\n1. The presentation of the model is not clear.[[MET-NEG], [PNF-NEG], [CRT], [MIN]]  \nFigure 2 which is the graphic representation of the model is hard to read.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]]  There is no meaningful caption for this important figure.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]]   Which notation in the figure corresponds to which variable is not clear at all.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]]  This also leads to unclarity of the text presentation of the model, for example, section 3.2. Which latent variable is used to decode which part?[[MET-NEG], [EMP-NEG], [QSN,CRT], [MIN]]\n\n2. Missing important related works.[[RWK-NEG], [SUB-NEG], [DFT], [MAJ]]\nThere are a couple of highly related work with multi-view VAE tracking similar problem have been proposed in the past.[[RWK-NEU], [null], [DIS], [GEN]] The paper did not discuss these related work and did not compare the performances.[[RWK-NEG,MET-NEG], [SUB-NEG,EMP-NEG], [DFT,CRT], [MAJ]] Examples of these related work include [1] and [2] (at the end of the review).[[RWK-NEG], [SUB-NEG], [DFT], [MAJ]]\nAdditionally, the idea of factorized representation idea (describable component and indescribable component) has a long history.[[EXT-NEU], [null], [DIS], [GEN]]  It can be traced back to [3], used in PGM setting in [4] and used in VAE setting in [1].[[MET-NEU], [null], [DIS], [GEN]] This group of related work should also be discussed.[[RWK-NEU], [null], [DIS], [MIN]] \n\n3. Experiment evaluation is not sufficient. [[EXP-NEG,MET-NEU], [EMP-NEG], [CRT], [MAJ]]\nFirstly, only one toy dataset is used for experimental evaluations.[[DAT-NEU,EXP-NEU], [EMP-NEU], [DIS], [GEN]] More evaluations are needed to verify the method, especially with natural images.[[MET-NEU], [SUB-NEU], [DIS], [GEN]] \nSecondly, there are no other state-of-the-art baselines are used.[[RWK-NEG], [SUB-NEG], [CRT], [MIN]] The baselines are various simiplied versions of the proposed model.[[RWK-NEU], [CMP-NEU], [CRT], [MIN]] More state-of-the-art baselines are needed, e.g. [1] and [2].[[MET-NEU], [SUB-NEU], [CRT], [MIN]]\n\n4. Maybe overclaiming.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\nIn the paper, only attributes of objects are used which is not semi-natural languages.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\n5. The paper, in general, needs to be polished.[[OAL-NEG], [null], [CRT], [MAJ]] \nThere are missing links and references in the paper and un-explained notations, and non-informative captions.[[BIB-NEU], [null], [CRT], [MIN]]\n\n6. Possibility to apply to natural images.[[BIB-NEU], [null], [CRT], [MIN]] \nThis method does not model spatial information.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] How can the method make sure that  simple adding generated images with each component will lead to a meaningful image in the end?[[MET-NEU], [EMP-NEU], [CRT], [MIN]] Especially with natural images,  the spacial location and the scale should be critical. [[MET-NEU], [EMP-NEU], [DIS], [GEN]]\n\n[1] Wang, Weiran, Honglak Lee, and Karen Livescu. \"Deep variational canonical correlation analysis.\" arXiv preprint arXiv:1610.03454 (2016).[[BIB-NEU], [null], [DIS], [GEN]]\n[2] Suzuki, Masahiro, Kotaro Nakayama, and Yutaka Matsuo. \"Joint Multimodal Learning with Deep Generative Models.\" arXiv preprint arXiv:1611.01891 (2016).\n[3] Tucker, Ledyard R. \"An inter-battery method of factor analysis.[[BIB-NEU], [null], [DIS], [GEN]]\" Psychometrika 23.2 (1958): 111-136.\n[4] Zhang, Cheng, Hedvig Kjellstr\u00f6m, and Carl Henrik Ek. \"Inter-battery topic representation learning.\" European Conference on Computer Vision. Springer International Publishing, 2016.\n\n"[[BIB-NEU], [null], [DIS], [GEN]]