"This paper provides the analysis of empirical risk landscape for GENERAL deep neural networks (DNNs).[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] Assumptions are comparable to existing results for OVERSIMPLIFED shallow neural networks.[[RWK-NEU,MET-NEU], [CMP-NEU], [SMY], [GEN]] The main results analyzed: 1) Correspondence of non-degenerate stationary points between empirical risk and the population counterparts.[[RES-NEU,ANA-NEU], [EMP-NEU], [SMY], [GEN]] 2) Uniform convergence of the empirical risk to population risk.[[RES-NEU,ANA-NEU], [EMP-NEU], [SMY], [GEN]] 3) Generalization bound based on stability.[[RES-NEU,ANA-NEU], [EMP-NEU], [SMY], [GEN]] The theory is first developed for linear DNNs and then generalized to nonlinear DNNs with sigmoid activations.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]]\n\nHere are two detailed comments:\n\n1) For deep linear networks with squared loss, Kawaguchi 2016 has shown that the global optima are the only non-degerenate stationary points.[[RWK-NEU], [CNT], [DIS], [GEN]] Thus, the obtained non-degerenate stationary deep linear network should be equivalent to the linear regression model Y=XW.[[RES-NEU], [EMP-NEU], [DIS], [MIN]] Should the risk bound only depends on the dimensions of the matrix W?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\n2) The comparison with Bartlett & Maass\u2019s (BM) work is a bit unfair, because their result holds for polynomial activations while this paper handles linear activations.[[RWK-NEG,RES-NEU], [CMP-NEG], [CRT], [MIN]] Thus, the authors need to refine BM's result for comparison."[[RWK-NEU,RES-NEU], [CMP-NEU], [SUG], [MIN]]