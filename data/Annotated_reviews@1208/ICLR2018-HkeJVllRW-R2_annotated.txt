"This paper introduces a new design of kernels in convolutional neural networks.[[INT-NEU,MET-NEU], [null], [SMY], [GEN]] The idea is to have sparse but complementary kernels with predefined patterns, which altogether cover the same receptive field as dense kernels.[[PDI-POS,EXP-NEU,MET-NEU], [null], [SMY], [GEN]] Because of the sparsity of such kernels, deeper or wider networks can be designed at the same computational cost as networks with dense kernels.[[RWK-NEU,EXP-NEU,MET-NEU], [null], [SMY], [GEN]]\n\nStrengths:\n- The complementary kernels come at no loss compare to standard ones\n- The resulting wider networks can achieve better accuracies than the original ones\n\nWeaknesses:\n- The proposed patterns are clear for 3x3 kernels, but no solution is proposed for other dimensions\n- The improvement over the baseline is not very impressive\n- There is no comparison against other strategies, such as 1xk and kx1 kernels (e.g., Ioannou et al. 2016)\n\nDetailed comments:\n- The separation into + and x patterns is quite clear for 3x3 kernels.[[RWK-NEU,PDI-NEU,EXP-NEU,MET-NEU], [null], [SMY], [GEN]] However, two such patterns would not be sufficient for 5x5 or 7x7 kernels.[[RWK-NEG], [SUB-NEG], [DFT], [MIN]] This idea would have more impact if it generalized to arbitrary kernel dimensions.[[RWK-NEU], [EMP-NEU], [SMY], [GEN]]\n\n- The improvement over the original models are of the order of less than 1 percent.[[RWK-NEU,MET-NEU], [null], [SMY], [GEN]] I understand that such improvements are not easy to achieve, but one could wonder if they are not due to the randomness of initialization/mini-batches.[[RWK-NEU], [null], [SMY], [GEN]] It would be more meaningful to report average accuracies and standard deviations over several runs of each experiment.[[RWK-NEU,MET-NEU], [null], [SMY], [GEN]]\n\n- Section 4.4 briefly discusses the comparison with using 3x1 and 1x3 kernels, mentioning that an empirical comparison is beyond the scope of this paper.[[RWK-NEU], [CMP-NEU,EMP-NEU], [SMY], [GEN]] To me, this comparison is a must. In fact, the discussion in this section is not very clear to me, as it mentions additional experiments that I could not find (maybe I misunderstood the authors).[[RWK-NEU], [CLA-NEG,CMP-NEU], [DFT], [MIN]] What I would like to see is the results of a model based on the method of Ioannou et al, 2016 with the same number of FLOPS.[[RWK-NEU,MET-NEU,BIB-NEU], [null], [SMY], [GEN]]\n\n- In Section 2, the authors review ideas of so-called random kernel sparsity.[[PDI-NEU], [EMP-NEU], [SMY], [GEN]] Note that the work of Wen et al., 2016, and that of Alvarez & Salzmann, NIPS 2016, do not really impose random sparsity, but rather aim to cancel out entire kernels, thus reducing the size of the model and not requiring implementation overhead.[[PDI-NEU,BIB-NEU], [EMP-NEU], [SMY], [GEN]] They also do not require pre-training and re-training, but just a single training procedure.[[RWK-NEU,EXP-NEU], [EMP-NEU], [SMY], [GEN]] Note also that these methods often tend not to decrease accuracy, but rather even increase it (by a similar magnitude to that in this paper), for a more compact model.[[RWK-NEU,PDI-NEU], [null], [SMY], [GEN]]\n\n- In the context of random sparsity, it would be worth citing the work of Collins & Kohli, 2014, Memory Bounded Deep Convolutional Networks.[[RWK-NEU,PDI-NEU], [null], [SMY], [GEN]]\n\n- I am not entirely convinced by the discussion of the grouped sparsity method in Section 3.1. In fact, the order of the channels is arbitrary, since the kernels are learnt.[[RWK-NEG,OAL-NEG], [EMP-NEG], [DFT], [MIN]] Therefore, it seems to me that they could achieve the same result. Maybe the authors can clarify this?[[RWK-NEU], [null], [QSN], [GEN]]\n\n- Is there a particular reason why the central points appears in both complementary kernels (+ and x)?[[RWK-NEU], [null], [QSN], [GEN]]\n\n- Why did the authors change the training procedure of ResNets slightly compared to the original paper, i.e., 50k training images instead of 45k training + 5k validation?[[RWK-NEU], [null], [SMY], [GEN]] Did the baseline (original model) reported here also use 50k? [[RWK-NEU], [null], [SMY], [GEN]]What would the results be with 45k?[[RWK-NEU,RES-NEU], [null], [QSN], [GEN]]\n\n- Fig. 5 is not entirely clear to me.[[TNF-NEG], [CLA-NEG], [DFT], [MIN]] What was the width of each layer? [[RWK-NEU,MET-NEU], [null], [QSN], [GEN]]The original one or the modified one?[[RWK-NEU], [null], [SMY], [GEN]]\n\n- It would be interesting to report the accuracy of a standard ResNet with 1.325*width as a comparison, as well as the runtime of such a model.[[RWK-POS], [null], [APC], [MAJ]]\n\n- In Table 4, I find it surprising that there is an actual speedup for the model with larger width.[[TNF-NEG], [PNF-NEG], [DFT], [MIN]] I would have expected the same runtime.[[RWK-NEU], [null], [SMY], [GEN]] How do the authors explain this?[[RWK-NEU], [null], [SMY], [GEN]] \n"