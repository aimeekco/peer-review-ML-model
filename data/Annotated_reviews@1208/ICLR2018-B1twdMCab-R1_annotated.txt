"This paper proposes a model for adding background knowledge to natural language understanding tasks.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The model reads the relevant text and then more assertions gathered from background knowledge before determining the final prediction.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The authors show this leads to some improvement on multiple tasks like question answering and natural language inference (they do not obtain state of the art but improve over a base model, which is fine in my opinion).[[RES-NEU], [null], [SMY], [GEN]]\n\nI think the paper does a fairly good job at doing what it does,[[OAL-POS], [EMP-POS], [APC], [MAJ]] it is just hard to get excited by it.[[OAL-NEU], [CNT], [DIS], [GEN]] \nHere are my major comments:\n\n* The authors explains that the motivation for the work is that one cannot really capture all of the knowledge necessary for doing natural language understanding because the knowledge is very dynamic.[[PDI-NEU], [null], [SMY], [GEN]] But then they just concept net to augment text.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] This is quite a static strategy, I was assuming the authors are going to use some IR method over the web to back up their motivation.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] As is, I don't really see how this motivation has anything to do with getting things out of a KB.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] A KB is usually a pretty static entity, and things are added to it at a slow pace.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\n* The author's main claim is that retrieving background knowledge and adding it when reading text can improve performance a little when doing QA and NLI.[[PDI-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] Specifically they take text and add common sense knowledge from concept net.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] The authors do a good job of showing that indeed the knowledge is important to gain this improvement through analysis.[[ANA-POS], [EMP-POS], [APC], [MAJ]] However, is this statement enough to cross the acceptance threshold of ICLR?[[OAL-NEU], [APR-NEU], [QSN], [MAJ]] Seems a bit marginal to me.[[OAL-NEG], [APR-NEG], [CRT], [MAJ]]\n\n* The author's propose a specific way of incorporating knowledge into a machine reading algorithm through re-embeddings that have some unique properties of sharing embeddings across lemmas and also having some residual connections that connect embeddings and some processed versions of them.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] To me it is unclear why we should use this method for incorporating background knowledge and not some simpler way.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] For example, have another RNN read the assertions and somehow integrate that.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] The process of re-creating embeddings seems like one choice in a space of many, not the simplest, and not very well motivated.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] There are no comparisons to other possibilities.[[MET-NEG], [CMP-NEG], [CRT], [MAJ]] As a result, it is very hard for me to say anything about whether this particular architecture is interesting or is it just in general that background knowledge from concept net is useful.[[MET-NEG], [IMP-NEG], [CRT], [MAJ]] As is, I would guess the second is more likely and so I am not convinced the architecture itself is a significant contribution.[[MET-NEG], [IMP-NEG], [CRT], [MAJ]]\n\nSo to conclude, the paper is well-written, clear, and has nice results and analysis.[[RES-POS,ANA-POS,OAL-POS], [CLA-POS,EMP-NEG], [APC], [MAJ]] The conclusion is that reading background knowledge from concept net boost performance using some architecture.[[RES-NEU], [EMP-NEU], [DIS], [MIN]] This is nice to know but I think does not cross the acceptance threshold.\n\n"[[OAL-NEG], [REC-NEG], [FBK], [MAJ]]