"This paper paper uses an ensemble of networks to represent the uncertainty in deep reinforcement learning.[[INT-NEU], [null], [SMY], [GEN]]\nThe algorithm then chooses optimistically over the distribution induced by the ensemble.[[MET-POS], [null], [SMY], [GEN]]\nThis leads to improved learning / exploration, notably better than the similar approach bootstrapped DQN.[[MET-POS], [EMP-POS], [SMY], [GEN]]\n\nThere are several things to like about this paper:\n- It is a clear paper, with a simple message and experiments that back up the claims.[[EXP-POS], [CLA-POS], [APC], [MAJ]]\n- The proposed algorithm is simple and could be practical in a lot of settings and even non-DQN variants.[[MET-NEU], [EMP-NEU], [DFT], [GEN]]\n- It is interesting that Bootstrapped DQN gets such poor performance, this suggests that it is very important in the original paper https://arxiv.org/abs/1602.04621 that \"ensemble voting\" is applied to the test evaluation... (why do you think this is by the way, do you think it has something to do with the data being *more* off-policy / diverse under a TS vs UCB scheme?)[[RWK-NEU,MET-POS], [CMP-POS,EMP-POS], [DFT,QSN], [MAJ]]\n\nOn the other hand:\n- The novelty/scope of this work is somewhat limited... this is more likely (valuable) incremental work than a game-changer.[[PDI-NEU], [NOV-NEU], [DFT], [MAJ]]\n- Something feels wrong/hacky/incomplete about just doing \"ensemble\" for uncertainty without bootstrapping/randomization... if we had access to more powerful optimization techniques then this certainly wouldn't be sensible - I think that you should mention that you are heavily reliant on \"random initialization + SGD/Adam + specific network architecture\" to maintain this idea of uncertainty.[[MET-NEG], [EMP-NEG], [DFT,DIS], [MAJ]] For example, this wouldn't work for linear value functions!\n- I think the original bootstrapped DQN used \"ensemble voting\" at test time, so maybe you should change the labels or the way this is introduced/discussed. [[MET-NEG], [EMP-NEG], [DFT], [MAJ]]It's definitely very interesting that *essentially* the learning benefit is coming from ensembling (rather than \"raw\" bootstrapped DQN) and UCB still looks like it does better.[[MET-NEU], [EMP-NEU], [DFT], [MAJ]]\n- I'm not convinced that page 4 and the \"Bayesian\" derivation really add too much value to this paper... alternatively, maybe you could introduce the actual algorithm first (train K models in parallel) and then say \"this is similar to particle filter\" and add the mathematical derivation after, rather than as if it was some complex formula derived.[[RWK-NEU,MET-NEU], [CMP-NEG], [CRT], [MAJ]] If you want to reference some justification/theory for ensemble-based uncertainty approximation you might consider https://arxiv.org/pdf/1705.07347.pdf instead.[[RWK-NEU,BIB-NEU], [CMP-NEU], [SUG], [MAJ]]\n- I think this paper might miss the point of the \"bigger\" problem of efficient exploration in RL... or even how to get \"deep\" exploration with deep RL.[[PDI-NEG], [IMP-NEG], [CRT], [MAJ]] Yes this algorithm sees improvements across Atari, but it's not clear why/if this is a step change versus simply increasing the amount of replay or tuning the learning rate.  (Actually I do believe this algorithm can demonstrate deep exploration... but it looks like we're not seeing the big improvements on the \"sub-human\" games you might hope.)[[RWK-NEU,MET-NEG], [CMP-POS,EMP-NEG], [CRT], [MAJ]]\n\nOverall I do think this is a pretty good short paper/evaluation of UCB-ensembles on Atari.[[MET-POS,OAL-POS], [null], [APC], [MAJ]]\nThe scope/insight of the paper isn't groundbreaking, but I think it delivers a clear short message on the Atari benchmark.[[RWK-POS,MET-POS], [IMP-NEU,CMP-POS,EMP-POS], [DIS], [MAJ]]\nPerhaps this will encourage people to dig deeper into some of these issues... I vote accept.\n"[[OAL-POS], [REC-POS], [FBK], [MAJ]]