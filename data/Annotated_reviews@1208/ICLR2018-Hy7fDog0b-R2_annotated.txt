"The paper explores GAN training under a linear measurement model in which one assumes that the underlying state vector $x$ is not directly observed but we do have access to measurements $y$ under a linear measurement model plus noise.[[INT-NEU,PDI-NEU,MET-NEU], [null], [SMY], [GEN]] The paper explores in detail several practically useful versions of the linear measurement model, such as blurring, linear projection, masking etc. and establishes identifiability conditions/theorems for the underlying models.[[INT-NEU,MET-NEU], [null], [SMY], [GEN]]\nThe AmbientGAN approach advocated in the paper amounts to learning end-to-end differentiable Generator/Discriminator networks that operate in the measurement space.[[MET-NEU], [null], [SMY], [GEN]] The experimental results in the paper show that this works much better than reasonable baselines, such as trying to invert the measurement model for each individual training sample, followed by standard GAN training.[[MET-NEU,RES-POS], [EMP-POS], [APC], [MAJ]]\nThe theoretical analysis is satisfactory.[[ANA-POS], [EMP-POS], [APC], [MAJ]] However, it would be great if the theoretical results in the paper were able to associate the difficulty of the inversion process with the difficulty of AmbientGAN training.[[EXP-NEU,RES-NEU], [EMP-NEU], [SUG,DIS], [MIN]] For example, if the condition number for the linear measurement model is high, one would expect that recovering the target real distribution is more difficult.[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]] The condition in Theorem 5.4 is a step in this direction, showing that the required number of samples for correct recovery increases with the probability of missing data.[[EXP-NEU,MET-NEU,RES-NEU], [EMP-NEU], [DIS], [MIN]] It would be great if Theorems 5.2 and 5.3 also came with similar quantitative bounds."[[MET-NEU], [EMP-NEU], [SUG], [MIN]]