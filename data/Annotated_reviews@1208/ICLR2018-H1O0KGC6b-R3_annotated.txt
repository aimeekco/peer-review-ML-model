"This paper demonstrate that by freezing all the penultimate layers at the end of regular training improves generalization. [[INT-NEU], [null], [SMY], [GEN]]However, the results do not convince this reviewer to switch to using 'post-training'.[[RES-NEG], [null], [DFT], [MAJ]]\n\nLearning features and then use a classifier such as a softmax or SVM is not new and were actually widely used 10 years ago.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] However, freezing the layers and continue to train the last layer is of a minor novelty.[[MET-NEU], [NOV-NEU], [DFT], [MAJ]] The results of the paper show a generalization gain in terms of better test time performance, however, it seems like the gain could be due to the \\lambda term which is added for post-training but not added for the baseline.[[MET-NEU,RES-NEU], [EMP-NEG], [DFT,DIS], [MAJ]] c.f. Eq 3 and Eq 4.\nTherefore, it's unclear whether the gain in generalization is due to an additional \\lambda term or from the post-training training itself.[[MET-NEG,RES-NEG], [EMP-NEG], [DFT], [MAJ]]\n\nA way to improve the paper and be more convincing would be to obtain the state-of-the-art results with post-training that's not possible otherwise.[[EXP-NEU,MET-NEU], [EMP-NEU], [SUG], [MAJ]]\n\nOther notes, \n\nRemark 1: While it is true that dropout would change the feature function, to say that dropout 'should not be' applied, it would be good to support that statement with some experiments.[[EXP-NEU,MET-POS], [SUB-NEU], [SUG], [MIN]]\n\nFor table 1, please use decimal points instead of commas.\n"[[TNF-NEG], [CLA-NEG], [SUG], [MIN]]