"This paper discusses the phenomenon of a fast convergence rate for training resnet with cyclical learning rates under a few particular setting.[[INT-NEU], [null], [SMY], [GEN]] It tries to provide an explanation for the phenomenon and a procedure to test when it happens.[[MET-NEU], [null], [SMY], [GEN]] However, I don't find the paper of high significance or the proposed method solid for publication at ICLR.[[OAL-NEG], [APR-NEG], [CRT], [MAJ]]\n\nThe paper is based on the cyclical learning rates proposed by Smith (2015, 2017). I don't understand what is offered beyond the original papers.[[RWK-NEU], [null], [SMY], [GEN]] The \"super-convergence\" occurs under special settings of hyper-parameters for resnet only and therefore I am concerned if it is of general interest for deep learning models.[[EXP-NEU,MET-NEU], [IMP-NEU], [DFT], [MIN]] Also, the authors do not give a conclusive analysis under what condition it may happen.[[ANA-NEG], [null], [DFT], [MIN]]\n\nThe explanation of the cause of \"super-convergence\" from the perspective of  transversing the loss function topology in section 3 is rather illustrative at the best without convincing support of arguments.[[MET-NEU], [EMP-NEU], [DFT], [MIN]] I feel most content of this paper (section 3, 4, 5) is observational results, and there is lack of solid analysis or discussion behind these observations."[[RES-NEU,ANA-NEG], [EMP-NEG], [DFT,DIS], [MAJ]]