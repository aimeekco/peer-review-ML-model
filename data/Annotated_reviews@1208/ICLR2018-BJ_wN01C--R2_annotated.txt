"The authors provide a novel, interesting, and simple algorithm capable of training with limited memory.[[MET-POS], [NOV-POS], [APC], [MAJ]]  The algorithm is well-motivated and clearly explained, and empirical evidence suggests that the algorithm works well. [[MET-POS], [EMP-POS], [APC], [MAJ]] However, the paper needs additional examination in how the algorithm can deal with larger data inputs and outputs.[[DAT-NEU,MET-NEU,ANA-NEG], [SUB-NEG], [DFT], [MAJ]]  Second, the relationship to existing work needs to be explained better.\n\n[[RWK-NEG], [CMP-NEG], [CRT], [MAJ]]Pro:\nThe algorithm is clearly explained, well-motivated, and empirically supported.\n\n[[MET-POS], [EMP-POS], [APC], [MAJ]]Con:\nThe relationship to stochastic gradient markov chain monte carlo needs to be explained better.[[RWK-NEG], [CMP-NEG], [DFT], [MIN]]  In particular, the update form was first introduced in [1], the annealing scheme was analyzed in [2], and the reflection step was introduced in [3].  These relationships need to be explained clearly.[[RWK-NEG,ANA-NEG], [CMP-NEG,PNF-NEG], [DFT], [MIN]]\nThe evidence is presented on very small input data. [[DAT-NEG], [SUB-NEG], [CRT], [MIN]] With something like natural images, the parameterization is much larger and with more data, the number of total parameters is much larger.[[DAT-NEG,MET-NEG], [SUB-NEG], [CRT], [MIN]]  Is there any evidence that the proposed algorithm could continue performing comparatively as the total number of parameters in state-of-the-art networks increases? [[DAT-NEG,MET-NEU], [SUB-NEG], [QSN], [MIN]]This would require a smaller ratio of included parameters.[[DAT-NEG], [SUB-NEU], [DIS], [MIN]]\n\n[1] Welling, M. and Teh, Y.W., 2011. Bayesian learning via stochastic gradient Langevin dynamics.[[BIB-NEU], [CMP-NEG], [DFT], [MIN]] In Proceedings of the 28th International Conference on Machine Learning (ICML-11)(pp. 681-688).[[BIB-NEU], [CMP-NEG], [DFT], [MIN]]\n\n[2] Chen, C., Carlson, D., Gan, Z., Li, C. and Carin, L., 2016, May.[[BIB-NEU], [CMP-NEG], [DFT], [MIN]] Bridging the gap between stochastic gradient MCMC and stochastic optimization.[[BIB-NEU], [CMP-NEG], [DFT], [MIN]] In Artificial Intelligence and Statistics(pp. 1051-1060).\n\n [[BIB-NEU], [CMP-NEG], [DFT], [MIN]][3] Patterson, S. and Teh, Y.W., 2013.[[BIB-NEU], [CMP-NEG], [DFT], [MIN]] Stochastic gradient Riemannian Langevin dynamics on the probability simplex.[[BIB-NEU], [CMP-NEG], [DFT], [MIN]] In Advances in Neural Information Processing Systems (pp. 3102-3110).\n \n" [[BIB-NEU], [CMP-NEG], [DFT], [MIN]]