"The paper studies the global convergence for policy gradient methods for linear control problems.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] \n(1) The topic of this paper seems to have minimal connection with ICRL.[[INT-NEU,PDI-NEG], [APR-NEG], [CRT], [MAJ]] It might be more appropriate for this paper to be reviewed at a control/optimization conference, so that all the technical analysis can be evaluated carefully.[[OAL-NEG], [APR-NEG], [CRT], [MAJ]] \n\n(2) I am not convinced if the main results are novel.[[RES-NEG], [NOV-NEG], [CRT], [MAJ]] The convergence of policy gradient does not rely on the convexity of the loss function, which is known in the community of control and dynamic programming.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] The convergence of policy gradient is related to the convergence of actor-critic, which is essentially a form of policy iteration. [[MET-NEU], [null], [DIS], [GEN]]I am not sure if it is a good idea to examine the convergence purely from an optimization perspective.[[PDI-NEG], [EMP-NEG], [CRT], [MIN]]\n\n(3) The main results of this paper seem technical sound.[[RES-POS], [EMP-POS], [APC], [MAJ]] However, the results seem a bit limited because it does not apply to neural-network function approximator. [[RES-NEG], [SUB-NEG], [CRT], [MAJ]]It does not apply to the more general control problem rather than quadratic cost function, which is quite restricted.[[PDI-NEG], [EMP-NEG], [CRT], [MAJ]] I might have missed something here.[[EXT-NEU], [null], [DIS], [GEN]] I strongly suggest that these results be submitted to a more suitable venue.\n\n"[[RES-NEG], [APR-NEG], [CRT], [MAJ]]