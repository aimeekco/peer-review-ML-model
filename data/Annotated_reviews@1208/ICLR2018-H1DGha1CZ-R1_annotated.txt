"The key argument authors present against ReLU+BN is the fact that using ReLU after BN skews the values resulting in non-normalized activations.[[INT-NEU], [null], [SMY], [GEN]] Although the BN paper suggests using BN before non-linearity many articles have been using BN after non-linearity which then gives normalized activations (https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md) and also better overall performance.[[RWK-NEU], [CMP-NEU], [DIS], [GEN]] The approach of using BN after non-linearity is termed \"standardization layer\" (https://arxiv.org/pdf/1301.4083.pdf).[[RWK-NEU], [null], [DIS], [GEN]] I encourage the authors to validate their claims against simple approach of using BN after non-linearity.  "[[MET-NEU], [REC-POS], [FBK], [MAJ]]