"This  paper is  on ab important topic : unsupervised learning on unaligned data.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] \n\nThe paper shows that is possible to learn the between domains mapping using GAN only without a reconstruction (cyclic) loss[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]]. The paper postulates that learning should happen on shallower networks first, then on a deeper network that uses the GAN cost function and regularizing discrepancy between the deeper and the small network. [[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] I did not get the time to go through the proofs, but they handle the fully connected case as far as I understand.[[PDI-NEU], [EMP-NEU], [DIS], [GEN]] Please find my comments are below.\n\nOverall it is an interesting  but long paper,[[OAL-POS], [SUB-NEU], [APC], [MAJ]] the claims are a bit strong for CNN and need further theoretical and experimental verification.[[EXP-POS,ANA-POS], [EMP-POS], [APC], [MAJ]] The number of layer as a complexity is not appropriate , as we need to take in account many parameters:  the pooling or the striding for the resolution, the presence or the absence of residual connections (for content preservation), the number of feature maps. More experimentation is needed.[[RWK-NEU,ANA-NEU], [EMP-NEU], [DIS], [MIN]]  \n\n\n\nPros:\n\nImportant and challenging topic to analyze and any progress on unsupervised learning is interesting.[[PDI-POS,RES-POS], [EMP-POS], [APC], [MAJ]]\n\nCons:\n\nI have some questions on the shallow/deep in the context of CNN, and to what extent the cyclic cost is not needed, or it is just distilled from the shallow training: \n\n- Arguably the shallow to deep distillation can be understood as a reconstruction cost , since the shallow network will keep a lot of the spatial information. [[MET-NEG], [EMP-NEG], [QSN], [MAJ]]If the deep network match the shallow one , this can be understood as a form of \u201cdistilled content \u201c loss? and the disc of the deep one will take care of the texture , style content? is this intuition correct? \n\n- original cyclic reconstruction constraint is in the pixel space using l1 norm usually, the regularizer introduced matches in a feature space , which is known to produce better results as a \u201cperceptual loss\u201d, can the author comment on this? is this what is really happening here, moving from cyclic constraint on pixels to a  cyclic constraint in a feature space  (shallow network)?\n\n-  *[[RWK-NEU,ANA-NEU,RES-NEU], [CMP-NEU], [DIS], [MIN]]Spatial resolution*: 1) The analysis seems to be done with respect to DNN not to a  CNN. [[RWK-NEU,MET-NEU], [CMP-NEU], [CRT], [MIN]]did you study the effect of the architectures in terms of striding and pooling how it affects the results?[[RES-NEU], [EMP-NEU], [QSN], [MIN]]  I think just counting number of layers as a complexity is not reasonable when we deal with images, with respect to  what preserves contents and what matches texture or style[[MET-NEU], [EMP-NEU], [DIS], [MIN]] . \n\n2) - Have you tried resnets generators and discriminators  at various depths , with padding so that the spatial resolution is preserved?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\n- Depth versus width: Another measure that is missing is also the number of feature maps how wide is the network , how does this interplays with the depth?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\n3) Regularizing deeper networks: in the experiments of varying the length did you see if the results can be stabilized using dropout with deep networks and small feature maps?\n\n4) between training g and h ?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] how do you initialize h? fully at random ?\n\n5) seems the paper is following implementation by Kim et al. what happens if the discriminator is like in cycle GAN acting on pixels.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Pixel GAN rather then only giving a global score for the whole image? "[[MET-NEU], [EMP-NEU], [QSN], [MIN]]