"The paper studies the problem of inputting a screenshot of a user interface and outputting code that can be used to generate the interface[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]]. Similar to image captioning systems, the image is processed with a CNN and an LSTM is used to output tokens one at a time[[PDI-NEU,MET-NEU], [EMP-NEU], [DIS], [GEN]]. Experiments are performed on three new synthetic datasets of user interfaces for iOS, Android, and HTML/CSS, which will be publicly released.[[PDI-NEU,DAT-NEU,EXP-NEU], [EMP-NEU], [SMY,DIS], [GEN]]\n\nPros:\n- Generating programs with neural networks is an exciting direction[[PDI-NEU,MET-NEU], [IMP-POS], [APC], [MAJ]]\n- Novel task of generating UI code from UI screenshots[[PDI-POS], [NOV-POS], [APC], [MAJ]]\n- Three new datasets of UI images and corresponding code\[[DAT-POS], [EMP-POS], [APC], [MAJ]]n- Paper is clearly written\[[INT-POS], [CLA-POS], [APC], [MAJ]]n\nCons:\n- Limited technical novelt[[PDI-NEG], [NOV-NEG], [DFT], [MIN]]y\n- Limited experiments[[PDI-NEG,EXP-NEG], [IMP-NEG,EMP-NEG], [DFT], [MIN]]\n\nI agree that the general direction of automatically generating programs with neural networks is a very exciting direction of research.[[PDI-POS,MET-POS], [IMP-POS], [APC], [MAJ]] Generating code for user interfaces from images of user interfaces is a novel and potentially useful task within this general area of interest.[[RWK-POS], [NOV-POS,IMP-POS], [APC], [MAJ]] The main novelty of the paper is the task itself, and the three synthetic datasets created to study the task[[INT-POS,RWK-POS,DAT-POS], [NOV-POS,EMP-POS], [APC], [MAJ]].\n\nMy main concern with this paper is a lack of technical novelty.[[INT-NEG], [EMP-NEG], [DFT], [MIN]] The model combines a CNN with an LSTM, and as such looks nearly identical to baseline models for image captioning that have been in widespread use for a few years now.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] Ideally I would have liked to see CNN+LSTM as a baseline, together with some technical innovations that specialize this general model to the particular task at hand.[[MET-NEG], [SUB-NEG], [DFT], [MIN]]\n\nThe experiments in this paper are also lacking.[[INT-NEG,EXP-NEG], [EMP-NEG], [DFT], [MIN]] Given that the main contribution of the paper is the pix2code task and datasets, I would have liked to see more thorough experiments[[INT-POS,DAT-POS,EXP-NEG,MET-POS], [EMP-NEG], [DFT], [MIN]]. The only model tested is CNN+LSTM with various beam sizes, and performance is only demonstrated through overall accuracy and qualitative examples.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] I would have liked to see comparisons with other methods, such as nearest neighbor or other retrieval-based methods.[[MET-NEG], [CMP-NEG,EMP-NEG], [DFT], [MIN]] I would have also liked to see more innovation in evaluation.[[MET-NEG], [IMP-NEG], [DFT], [MIN]] Are there metrics other than overall accuracy that could be used to measure performance? [[MET-NEU], [EMP-NEU], [QSN], [GEN]]Compared to other tasks like image captioning, can you design metrics that capture the particular challenges involved in the pix2code task?[[PDI-NEU,ANA-NEU], [CMP-NEU], [QSN], [GEN]] In general, in what types of circumstances does your model succeed or fail, and can you capture this quantitatively through carefully designed metrics?[[PDI-NEU,MET-NEG], [EMP-NEU], [QSN], [GEN]] Since the data is synthetic, could you generate different datasets of increasing complexity and measure performance as complexity increases?[[DAT-NEU], [IMP-NEU], [QSN], [GEN]] How does performance change with different amounts of training data[[DAT-NEU,RES-NEU], [IMP-NEU], [QSN], [GEN]]? Would it be possible to somehow transfer knowledge of UI across datasets, where you pretrain on one dataset and somehow finetune on another[[DAT-NEU], [IMP-NEU], [QSN], [GEN]]? I don\u2019t expect the authors to answer any of these questions in particular;[[INT-NEG,FWK-NEG], [REC-NEG], [DFT], [MIN]] I list them to emphasize that there are a lot of interesting experiments that could have been done with this task and dataset.[[PDI-POS,DAT-POS,EXP-POS], [IMP-POS,EMP-POS], [APC], [MAJ]]\n\nOn the whole I appreciate the novelty of the task and dataset,[[INT-POS,RWK-POS,DAT-POS], [NOV-POS], [APC], [MAJ]] but the paper suffers from a lack of technical novelty in the model and limited experimental validation."[[INT-NEG,RWK-NEG,EXP-NEG], [EMP-NEG], [DFT], [MIN]]