"This paper produces word embedding tensors where the third order gives covariate information, via venue or author.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]]  The model is simple: tensor factorization, where the covariate can be viewed as warping the cosine distance to favor that covariate's more commonly cooccuring vocabulary (e.g. trump on hillary and crooked)[[PDI-NEU], [null], [SMY], [GEN]] \n\n\nThere is a nice variety of authors and words, though I question if even with all those books, the corpus is big enough to produce meaningful vectors.[[DAT-POS], [EMP-POS], [APC], [MAJ]]  From my own experience, even if I spend several hours copy-pasting from project gutenberg, it is not enough for even good matrix factorization embeddings, much less tensor embeddings.[[EXT-NEU], [null], [DIS], [GEN]]  It is hard to believe that meaningful results are achieved using such a small dataset with random initialization.[[DAT-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]]  \n\nI think table 5 is also a bit strange.[[TNF-NEG], [null], [CRT], [MIN]]  If the rank is > 1000 I wonder how meaningful it actually is.[[CNT], [null], [DIS], [MIN]]  For the usual analogies task, you can usually find what you are looking for in the top 5 or less.[[CNT], [null], [DIS], [MIN]]  \n\nIt seems that table 1 is the only evaluation of the proposed method against any other type of method (glove, which is not a tensor-based method).[[MET-NEG,TNF-NEU], [EMP-NEG], [CRT], [MAJ]] I think this is not sufficient.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] \n\nOverall, I believe the idea is nice, and the initial analysis is good,[[PDI-POS,ANA-POS], [EMP-POS], [APC], [MAJ]] but I think the evaluation, especially against other methods, needs to be stronger.[[MET-NEG], [CMP-NEG], [CRT], [MIN]] Methods like neelakantan et al's multisense embedding, for example, which the work cites, can be used in some of these evaluations, specifically on those where covariate information clearly contributes (like contextual tasks).[[RWK-NEU], [EMP-NEU], [DIS], [MIN]] The addition of one or two tables with either a standard task against reported results or created tasks against downloadable contextual / tensor embeddings would be enough for me to change my vote. [[MET-NEU,TNF-NEU], [REC-NEU], [FBK], [MAJ]]"