"The problem of numerical instability in applying SGD to soft-max minimization is the motivation.[[INT-NEU], [null], [SMY], [GEN]] It would have been helpful if the author(s) could have made a formal statement. [[MET-NEU], [null], [SUG], [MIN]]\nSince the main contributions are two algorithms for stable SGD it is not clear how one can formally say that they are stable. For this a formal problem statement is necessary. [[MET-NEG], [EMP-NEG], [SUG,CRT], [MAJ]]The discussion around eq (7) is helpful but is intuitive and it is difficult to get a formal problem which we can use later to examine the proposed algorithms.[[MET-NEU], [EMP-NEU], [DFT], [MAJ]]\n\nThe proposed algorithms are variants of SGD but it is not clear why they should converge faster than existing strategies.[[MET-NEG], [EMP-NEG], [DFT], [MIN]]\nSome parts of the text are badly written, see for example the following line(see paragraph before Sec 3)\n\n\"Since the converge of SGD is\ninversely proportional to the magnitude of its gradients (Lacoste-Julien et al., 2012), we expect the\nformulation to converge faster.\"\n \nwhich could have shed more light on the matter.[[MET-NEU], [CLA-NEG], [CRT], [MIN]] \n\nThe title is also misleading in using the word \"exact\"[[CNT], [null], [CRT], [MAJ]]. I have understand it correct the proposed SGD method solves the optimization problem to an additive error.\[[MET-NEU], [EMP-NEU], [DIS], [MAJ]]n\nIn summary the algorithms are novel variants of SGD[[MET-POS], [NOV-POS], [SMY], [MAJ]] but the associated claims of numerical stability and speed of convergence vis-a-vis existing methods are missing.[[MET-NEG], [EMP-NEG], [DFT], [MAJ]] The choice of word exact is also not clear.[[CNT], [null], [DFT], [MAJ]]\n"