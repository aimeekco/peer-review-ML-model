"After reading rebuttals from the authors: The authors have addressed all of my concerns.[[EXT-POS], [null], [APC], [GEN]] THe additional experiments are a good addition.[[EXP-POS], [null], [APC], [GEN]]\n\n************************\nThe authors provide an algorithm-agnostic active learning algorithm for multi-class classification.[[MET-NEU], [null], [SMY], [GEN]] The core technique is to construct a coreset of points whose labels inform the labels of other points.[[MET-NEU], [EMP-NEU], [SMY], [GEN]]  The coreset construction requires one to construct a set of  points which can cover the entire dataset. [[DAT-NEU,MET-NEU], [null], [SMY], [GEN]]While this is NP-hard problem in general, the greedy algorithm is 2-approximate[[MET-NEU], [null], [SMY], [GEN]]. The authors use a variant of the greedy algorithm along with bisection search to solve a series of feasibility problems to obtain a good cover of the dataset each time.  This cover tells us which points are to be queried.[[DAT-NEU,MET-NEU], [EMP-POS], [SMY], [GEN]] The reason why choosing the cover is a good idea is because under suitable Lipschitz continuity assumption the generalization error can be controlled via an appropriate value of the covering radius in the data space.[[MET-POS], [EMP-POS], [SMY,DIS], [GEN]]  The authors use the coreset construction with a CNN to demonstrate an active learning algorithm for multi-class classification.[[MET-NEU], [null], [SMY], [GEN]] \nThe experimental results are convincing enough to show that it outperforms other active learning algorithms.[[EXP-POS,RES-POS], [CMP-POS], [SMY], [GEN]] However, I have a few major and minor comments.\n\nMajor comments:\n\n1. The proof of Lemma 1 is incomplete.[[MET-NEG], [EMP-NEG], [DFT], [MIN]] We need the Lipschitz constant of the loss function. The loss function is a function of the CNN function and the true label.[[MET-NEU], [EMP-NEU], [SUG,DIS], [MAJ]] The proof of lemma 1 only establishes the Lipschitz constant of the CNN function.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] Some more extra work is needed to derive the lipschitz constant of the loss function from the CNN function.[[MET-NEU], [EMP-NEU], [SUG], [MAJ]] \n\n2. The statement of Prop 1 seems a bit confusing to me. the hypothsis says that the loss on the coreset = 0. But the equation in proposition 1 also includes the loss on the coreset.[[MET-NEU], [EMP-NEU], [DFT], [MAJ]] Why is this term included. Is this term not equal to 0?\[[MET-NEU], [null], [QSN], [MAJ]]n\n3. Some important works are missing.  Especially works related to pool based active learning, and landmark results on labell complexity of agnostic active learning[[MET-NEG], [SUB-NEG], [CRT], [MAJ]].\nUPAL: Unbiased Pool based active learning by Ganti & Gray. http://proceedings.mlr.press/v22/ganti12/ganti12.pdf\nEfficient active learning of half-spaces by Gonen et al. http://www.jmlr.org/papers/volume14/gonen13a/gonen13a.pdf\nA bound on the label complexity of agnostic active learning. http://www.machinelearning.org/proceedings/icml2007/papers/375.pdf\n\n4.[[BIB-NEU], [null], [DIS], [GEN]]  The authors use L_2 loss as their objective function. This is a bit of a weird choice given that they are dealing with multi-class classification and the output layer is a sigmoid layer, making it a natural fit to work with something like a cross-entropy loss function.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] I guess the theoretical results do not extend to cross-entropy loss, but the authors do not mention these points anywhere in the paper.[[MET-NEG], [SUB-NEG], [DFT], [MAJ]] For example, the ladder network, which is one of the networks used by the authors is a network that uses cross-entropy for training.[[MET-NEU], [EMP-NEU], [SMY], [GEN]]\n\nMinor-comment: \n1. The feasibility program in (6) is an MILP. However, the way it is written it does not look like an MILP. [[MET-NEG], [CLA-NEG], [CRT], [MIN]]It would have been great had the authors mentioned that u_j \\in {0,1}. [[MET-NEU], [null], [SUG], [MIN]]\n\n2. The authors write on page 4, \"Moreover, zero training error can be enforced by converting average loss into maximal loss\". It is not clear to me what the authors mean here.[[MET-NEG], [CLA-NEG], [DFT], [MIN]] For example, can I replace the average error in proposition 1, by maximal loss? Why can I do that?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Why would that result in zero training error?[[MET-NEU,RES-NEU], [null], [QSN], [MIN]]\n\nOn the whole this is interesting work and the results are very nice[[OAL-POS], [IMP-POS], [APC], [MAJ]]. But, the proof for Lemma 1 seems incomplete to me, and some choices (such as choice of loss function) are unjustified.[[MET-NEG], [EMP-NEG], [DFT], [MIN]] Also, important references in active learning literature are missing."[[BIB-NEG], [null], [DFT], [MIN]]