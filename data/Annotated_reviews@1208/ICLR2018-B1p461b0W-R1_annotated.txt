"The paper makes a bold claim, that deep neural networks are robust to arbitrary level of noise.[[INT-NEU], [null], [SMY], [GEN]] It also implies that this would be true for any type of noise, and support this later claim using experiments on CIFAR and MNIST with three noise types: (1) uniform label noise (2) non-uniform but image-independent label noise, which is named \"structured noise\", and (3) Samples from out-of-dataset classes.[[PDI-NEU], [EMP-NEU], [SMY], [MIN]] The experiments show robustness to these types of noise.[[EXP-NEU], [EMP-NEU], [SMY], [MIN]] \n\nReview: \nThe claim made by the paper is overly general, and in my own experience incorrect when considering real-world-noise.[[EXP-NEG], [EMP-NEG], [CRT], [MIN]] This is supported by the literature on \"data cleaning\" (partially by the authors), a procedure which is widely acknowledged as critical for good object recognition.[[MET-NEU], [EMP-NEU], [SUG], [MIN]]  While it is true that some image-independent label noise can be alleviated in some datasets, incorrect labels in real world datasets can substantially harm classification accuracy.[[DAT-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nIt would be interesting to understand the source of the difference between the results in this paper and the more common results (where label noise damages recognition quality).[[RES-NEU], [EMP-NEU], [DIS], [MIN]] The paper did not get a chance to test these differences, and I can only raise a few hypotheses.[[RES-NEG], [CMP-NEG], [DFT], [MIN]] First, real-world noise depends on the image and classes in a more structured way. For instance, raters may confuse one bird species from a similar one, when the bird is photographed from a particular angle.[[EXT-NEU], [null], [DIS], [GEN]] This could be tested experimentally, for example by adding incorrect labels for close species using the CUB data for fine-grained bird species recognition. [[EXT-NEU], [null], [DIS], [GEN]]  Another possible reason is that classes in MNIST and CIFAR10 are already very distinctive, so are more robust to noise.[[DAT-POS], [EMP-POS], [APC], [MAJ]]  Once again, it would be interesting for the paper to study why they achieve robustness to noise while the effect does not hold in general.[[ANA-NEU], [SUB-NEU], [SUG], [MIN]]  \n\nWithout such an analysis, I feel the paper should not be accepted to ICLR because the way it states its claim may mislead readers.[[ANA-NEG,OAL-NEG], [SUB-NEG,REC-NEG], [FBK], [MAJ]]  \n\nOther specific comments: \n-- Section 3.4 the experimental setup, should clearly state details of the optimization, architecture and hyper parameter search.[[EXP-NEU], [EMP-NEU], [DIS], [GEN]]  For example, for Conv4, how many channels at each layer?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]  how was the net initialized? [[EXP-NEU], [EMP-NEU], [QSN], [MIN]]which hyper parameters were tuned and with which values?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]] were hyper parameters tuned on a separate validation set?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] How was the train/val/test split done, etc.[[EXP-NEU], [EMP-NEU], [QSN], [MIN]] These details are useful for judging technical correctness.[[MET-NEU], [null], [DIS], [MIN]]\n-- Section 4, importance of large datasets.[[DAT-POS], [EMP-POS], [APC], [MAJ]] The recent paper by Chen et al (2017) would be relevant here.[[RWK-NEU], [SUB-NEU], [SUG], [MIN]]\n-- Figure 8 failed to show for me.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]] \n-- Figure 9,10, need to specify which noise model was used.\n\n\n\n\n\n"[[TNF-NEG], [EMP-NEG], [CRT], [MIN]] 