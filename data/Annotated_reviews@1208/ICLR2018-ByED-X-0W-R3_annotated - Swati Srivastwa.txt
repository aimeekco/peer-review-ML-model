"# Paper overview:\nThis paper views the learning process for stochastic feedforward networks through the lens of an\niterative information bottleneck process; at each layer an attempt is made to minimise the mutual\ninformation (MI) with the feed-in layer while maximising the MI between that layer and the presumed-endogenous variable, 'Y'.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]]\n\nTwo propositions are made, (although I would argue that their derivations are trivially the consequence\nof the model structure and inference scheme defined), and experiments are run which compare the approach to maximum likelihood estimation for 'Y' using an equivalent stochastic network architecture.[[PDI-NEU,EXP-NEU,MET-NEU], [null], [SMY], [GEN]]\n\n# Paper discussion:\nIn general I like the idea of looking further into the effect of adding network structure on the original\ninformation bottleneck results (empirical and theoretical).[[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]]  I would be interested to see if layerwise\ninput skip connections (i.e. between each network layer L_i and the original input variable 'X') hastened the 'compression' stage of learning e.g. (i.e. the time during which the intermediate layers minimise MI with 'X').[[MET-NEU,ANA-NEU], [EMP-NEU], [DIS], [MIN]]  I'm also interested that clear examples of the information bottleneck principle in practice (e.g. CCA) are rarely mentioned.[[ANA-NEG], [SUB-NEG], [DFT], [MIN]]\n\nOn the other hand, I think this paper is not quite ready: it reads like work written in a hurry, and is at times hard to follow as a result.[[RES-NEG,OAL-NEG], [CLA-NEG], [CRT], [MIN]]  There are several places where I think the terminology does not quite reflect what the authors perhaps hoped to express, or was otherwise slightly clumsy e.g:[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\n* \"...self-consistent equations are highly non-linear and still too abstract to be used for many...\", presumably what was implied was that the original solution to the information bottleneck as expressed by Tishby et al is non-analytic for most practical cases of interest?[[RWK-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\n* \"Furthermore, we exploit the existing network architecture as variational decoders rather than resort to variational decoders that are not part of the neural network architecture.[[RWK-NEG,MET-NEG], [CMP-NEG], [CRT], [MIN]]\" -> The existing network architecture is used to provide a variational inference framework for I(Z,Y).[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MIN]]\n\n* \"On average, 2H(X|Z) elements of X are mapped to the same code in Z.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\" In an ideal world I would like the assumptions required for this to hold true to be a fleshed out a little here.[[ANA-NEU], [SUB-NEU], [DIS], [MIN]]\n\n* \"The generated bottleneck samples are then used to estimate mutual information\" -> an empirical estimation of I(Z,X) would seem a very high variance estimator; the dimensionality of X is typically large in modern deep-learning problems---do you have any thoughts on how the learning process fares as this varies?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]  Further on you cite that L_PIB is intractable due to the high dimensionality of the bottleneck variables, I imagine that this still yields a high var MC estimator in your approximation (in practice)?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]   Was the performance significantly worse without the Raiko estimator?[[RES-NEU], [EMP-NEU], [QSN], [MIN]] \n\n* \"In this experiment, we compare PIBs with ....\" -> I find this whole section hard to read, the description of how the models relate to each other is a little difficult to follow at first sight.[[EXP-NEG], [CMP-NEG], [CRT], [MIN]] \n\n* Information dynamics of learning process (Figures 3, 6, 7, 8) -> I am curious as to why you did not run the PIB for the same number of epochs as the SFNN?[[MET-NEU,TNF-NEU], [EMP-NEU], [QSN], [MIN]]   I would also argue that you did not run either method as long as you should have (both approaches lack the longer term 'compression' stage whereby layers near the input reduce I(X,Z_i) as compared to their starting condition)?[[MET-NEG], [CMP-NEG,EMP-NEG], [DFT,QSN], [MIN]]   This property is visible in I(Z_2,X) for PIB in Figure 3, but otherwise absent.[[TNF-NEU], [PNF-NEU], [DIS], [MIN]] \n\n# Conclusion:\nIn conclusion, while interesting, for me the paper is not yet ready for publication.[[OAL-NEG], [REC-NEG], [FBK], [MAJ]]   I would recommend this work for a workshop presentation at this stage.\n"[[OAL-NEU], [APR-NEU], [FBK], [MAJ]] 