"The paper proposes a model for abstractive document summarization using a self-critical policy gradient training algorithm, which is mixed with maximum likelihood objective.[[PDI-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] The Seq2seq architecture incorporates both intra-temporal and intra-decoder attention, and a pointer copying mechanism.[[RWK-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] A hard constraint is imposed during decoding to avoid trigram repetition.[[RWK-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] Most of the modelling ideas already exists, but this paper show how they can be applied as a strong summarization model.[[RWK-NEU,PDI-NEU], [EMP-NEU], [SMY], [GEN]]\n\nThe approach obtains strong results on the CNN/Daily Mail and NYT datasets.[[RWK-NEU,DAT-NEU,RES-NEU], [EMP-NEU], [SMY], [GEN]] Results show that intra-attention improves performance for only one of the datasets.[[RWK-NEU,DAT-NEU,RES-NEU], [IMP-NEU,EMP-NEU], [SMY,APC], [GEN]] RL results are reported with only the best-performing attention setup for each dataset.[[RWK-NEU,DAT-NEU,RES-NEU], [IMP-NEU,EMP-NEU], [SMY], [GEN]] My concern with that is that the authors might be using the test set for model selection; It is not a priori clear that the setup that works better for ML should also be better for RL, especially as it is not the same across datasets.[[RWK-NEG,DAT-NEG,EXP-NEG,MET-NEG], [CLA-NEG,EMP-NEG], [DFT], [MIN]] So I suggest that results for RL should be reported with and without intra-attention on both datasets, at least on the validation set.[[RWK-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SMY,SUG], [GEN]]\n\nIt is shown that intra-decoder attention decoder improves performance on longer sentences.[[RWK-POS,RES-POS], [IMP-POS,EMP-POS], [APC], [MAJ]] It would be interesting to see more analysis on this, especially analyzing what the mechanism is attending to, as it is less clear what its interpretation should be than for intra-temporal attention.[[EXP-NEG,ANA-NEG,OAL-NEG], [CLA-NEG], [DFT], [MIN]] Further ablations such as the effect of the trigram repetition constraint will also help to analyse the contribution of different modelling choices to the performance.[[RWK-NEU,EXP-NEU,ANA-NEU], [EMP-NEU], [SMY], [GEN]] \n\nFor the mixed decoding objective, how is the mixing weight chosen and what is its effect on performance?[[RWK-NEU,EXP-NEU], [null], [SMY,QSN], [GEN]] If it is purely a scaling factor, how is the scale quantified? [[RWK-NEU], [null], [QSN], [GEN]]It is claimed that readability correlates with perplexity, so it would be interesting to see perplexity results for the models.[[EXP-POS,MET-POS,RES-POS,OAL-POS], [IMP-POS,EMP-POS], [APC], [MAJ]] The lack of correlation between automatic and human evaluation raises interesting questions about the evaluation of abstractive summarization that should be investigated further in future work.[[OAL-NEU], [null], [SMY], [GEN]]\n\nThis is a strong paper that presents a significant improvement in document summarization.[[OAL-POS], [IMP-POS], [APC], [MAJ]]\n"


\
