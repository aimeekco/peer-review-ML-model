"This paper proposes a novel regularization scheme for Wasserstein GAN based on a relaxation of the constraints on the Lipschitz constant of 1.[[PDI-NEU,MET-POS], [NOV-POS], [APC], [MAJ]] The proposed regularization penalize the critic function only when its gradient has a norm larger than one using some kind of squared hinge loss.[[MET-NEU], [null], [SMY], [GEN]] The reasons for this choice are discussed and linked to theoretical properties of OT.[[MET-NEU], [null], [SMY], [GEN]]Numerical experiments suggests that the proposed regularization leads to better posed optimization problem and even a slight advantage in terms of inception score on the CIFAR-10 dataset.[[DAT-POS,EXP-POS], [EMP-POS], [APC], [MAJ]]\n\nThe paper is interesting and well written, the proposed regularization makes sens since it is basically a relaxation of the constraints and the numerical experiments also suggest it's a good idea.[[EXP-POS,OAL-POS], [CLA-POS,EMP-POS], [APC], [MAJ]] Still as discussed below the justification do not address a lots of interesting developments and implications of the method and should better discuss the relation with regularized optimal transport.[[MET-NEG], [SUB-NEG], [DFT], [MAJ]]\n\nDiscussion:\n\n+ The paper spends a lot of time justifying the proposed method by discussing the limits of the \"Improved training of Wasserstein GAN\" from Gulrajani et al. (2017).[[MET-NEU,RWK-NEU], [CMP-NEU], [DIS], [GEN]] The two limits (sampling from marginals instead of optimal coupling and differentiability of the critic) are interesting and indeed suggest that one can do better but the examples and observations are well known in OT and do not require proof in appendix.[[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]] The reviewer believes that this space could be better spend discussing the theoretical implication of the proposed regularization (see next).[[ANA-NEU], [NOV-NEU], [SUG], [MIN]]\n\n+ The proposed approach is a relaxation of the constraints on the dual variable for the OT problem.[[PDI-POS,MET-POS], [EMP-POS], [APC], [MAJ]] As a matter of fact we can clearly recognize a squared hinge loss is the proposed loss.[[MET-NEU], [EMP-POS], [APC], [MAJ]] This approach (relaxing a strong constraint) has been used for years when learning support vector machines and ranking and a small discussion or at least reference to those venerable methods would position the paper on a bigger picture.[[MET-POS,FWK-POS], [IMP-POS,EMP-POS], [APC], [MAJ]]\n\n+ The paper is rather vague on the reason to go from Eq. (6) to Eq. (7). (gradient approximation between samples to gradient on samples).[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Does it lead to better stability to choose one or the other?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] \n How is it implemented in practice? [[MET-NEU], [EMP-NEU], [QSN], [MIN]]recent NN toolbox can easily compute the exact gradient and use it for the penalization but this is not clearly discussed even in appendix.[[MET-NEG], [SUB-NEG], [DFT,CRT], [MIN]] Numerical experiments comparing the two implementation or at least a discussion is necessary.[[EXP-NEU], [SUB-NEU], [SUG], [MIN]]\n\n+ The proposed approach has a very strong relations to the recently proposed regularized OT (see [1] for a long list of regularizations) and more precisely to the euclidean regularization.[[MET-POS], [EMP-POS], [APC], [MAJ]] I understand that GANS (and Wasserstein GAN) is a relatively young community and that references list can be short but their is a large number of papers discussing regularized optimal transport and how the resulting problems are easier to solve.[[RWK-NEU], [CMP-NEU], [DIS], [MIN]] A discussion of the links is necessary and will clearly bring more theoretical ground to the method.[[RWK-NEU,MET-NEU], [CMP-NEU], [SUG], [MIN]] Note that a square euclidean regularization leads to a regularization term in the dual of the form max(0,f(x)+f(y)-|x-y|)^2 that is very similar to the proposed regularization.[[MET-NEU], [CMP-NEU], [DIS], [MIN]] In other words the authors propose to do regularized OT (possibly with a new regularization term) and should discuss that.[[RWK-NEU], [SUB-NEU], [DFT], [MIN]]\n\n+ The numerical experiments are encouraging[[EXP-POS], [EMP-POS], [APC], [MAJ]] but a bit short.[[EXP-NEG], [SUB-NEG], [CRT], [MIN]] The 2D example seem to work very well and the convergence curves are far better with the proposed regularization.[[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]] But the real data CIFAR experiments are much less detailed with only a final inception score (very similar to the competing method) and no images even in appendix.[[DAT-NEG,EXP-NEG], [SUB-NEG], [DFT], [MIN]] The authors should also define (maybe in appendix) the conditional and unconditional inception scores and why they are important (and why only some of them are computed in Table 1).[[EXP-NEG,TNF-NEG], [SUB-NEG], [SUG], [MIN]]\n\n+ This is more of a suggestion.[[EXP-NEU,TNF-NEU], [SUB-NEU], [SUG], [MIN]] The comparison of the dual critic to the true Wasserstein distance is very interesting.[[MET-POS,ANA-POS], [CMP-POS], [APC], [MAJ]] It would be nice to see the behavior for different values of lambda.[[ANA-POS], [SUB-NEG], [SUG], [MIN]]\n\n\n[1] Dessein, A., Papadakis, N., & Rouas, J. L. (2016). Regularized Optimal Transport and the Rot Mover's Distance. arXiv preprint arXiv:1610.06447.[[BIB-NEU], [null], [DIS], [MIN]]\n\n\nReview update after reply:\n\nThe authors have responded to most of my concerns and I think the paper is much stronger now and discuss the relation with regularized OT. I change the rating to Accept. \n" 001 101[[OAL-POS], [REC-POS], [FBK], [MAJ]]