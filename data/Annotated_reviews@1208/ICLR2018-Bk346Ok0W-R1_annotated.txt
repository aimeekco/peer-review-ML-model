"This paper proposes sensor transformation attention network (STAN), which dynamically select appropriate sequential sensor inputs based on an attention mechanism.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] \n\nPros:\nOne of the main focuses of this paper is to apply this method to a real task, multichannel speech recognition based on CHiME-3, by providing its reasonable sensor selection function in real data especially to avoid audio data corruptions.[[EXP-POS,MET-POS], [EMP-POS], [APC], [MIN]] This analysis is quite intuitive, and also shows the effectiveness of the proposed method in this practical setup.[[MET-POS,ANA-POS], [EMP-POS], [APC], [MAJ]] \n\nCons:\nThe idea seems to be simple and does not have significant originality.[[PDI-NEG], [NOV-NEG], [CRT], [MAJ]] Also, the paper does not clearly mention the attention mechanism part, and needs some improvement.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] \n\nComments:\n-\tThe paper mainly focuses on the soft sensor selection.[[MET-NEU], [SUB-NEU], [SMY], [MIN]] However, in an array signal processing context (and its application to multichannel speech recognition), it would be better to mention beamforming techniques, where the compensation of the delays of sensors is quite important.[[MET-NEU], [SUB-NEG], [SUG], [MIN]]\n-\tIn addition, there is a related study of using multichannel speech recognition based on sequence-to-sequence modeling and attention mechanism by Ochiai et al, \"A Unified Architecture for Multichannel End-to-End Speech Recognition with Neural Beamforming,\" IEEE Journal of Selected Topics in Signal Processing. [[BIB-NEU,EXT-NEU], [CMP-NEU], [DIS], [GEN]]This paper uses the same CHiME-3 database, and also showing a similar analysis of channel selection.[[RWK-NEU,DAT-NEU,ANA-NEU], [CMP-NEU], [DIS], [MIN]] It\u2019s better to discuss about this paper as well as a reference.[[BIB-NEG], [CMP-NEG], [DFT], [MIN]]\n-\tSection 2: better to explain about how to obtain attention scores z in more details.[[RES-NEG], [SUB-NEG], [SUG,DFT], [MIN]]\n-\tFigure 3, experiments of Double audio/video clean conditions: I cannot understand why they are improved from single audio/video clean conditions.Need some explanations.[[EXP-NEG,TNF-NEG], [PNF-NEG], [CRT], [MAJ]] \n-\tSection 3.1: 39-dimensional Mel-frequency cepstral coefficients (MFCCs) -> 13 -dimensional Mel-frequency cepstral coefficients (MFCCs) with 1st and 2nd order delta features.[[DAT-NEU,EXP-NEU], [CNT], [DIS], [MIN]]\n-\tSection 3.2 Dataset \u201cAs for TIDIGIT\u201d: \u201cAs for GRID\u201d(?)[[DAT-NEU], [CNT], [DIS], [MIN]]\n-\tSection 4 Models \u201cThe parameters of the attention modules are either shared across sensors (STAN-shared) or not shared across sensors (STAN- default).[[MET-NEG], [SUB-NEG,PNF-NEG], [CRT], [MIN]]\u201d: It\u2019s better to explain this part in more details, possibly with some equations. It is hard to understand the difference.\n\n"[[MET-NEG], [SUB-NEG, PNF-NEG], [SUG,CRT], [MIN]]