"This paper interprets deep residual network as a dynamic system, and proposes a novel training algorithm to train it in a constructive way.[[EXP-POS,MET-POS], [NOV-POS], [APC], [MAJ]] On three image classification datasets, the proposed algorithm speeds up the training process without sacrificing accuracy.[[DAT-NEU,EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]] The paper is interesting and easy to follow.[[OAL-POS], [EMP-POS], [APC], [MAJ]] \n\nI have several comments:\n1.\tIt would be interesting to see a comparison with Stochastic Depth, which is also able to speed up the training process, and gives better generalization performance.[[RWK-NEU,EXP-NEU,MET-NEU,RES-NEU], [SUB-NEU,EMP-NEU], [SUG,DIS], [MIN]] Moreover, is it possible to combine the proposed method with Stochastic Depth to obtain further improved efficiency?[[MET-NEU,RES-NEU], [EMP-NEU], [QSN], [MIN]]\n2.\tThe mollifying networks [1] is related to the proposed method as it also starts with shorter networks, and ends with deeper models.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] It would be interesting to see a comparison or discussion. \n[1] C Gulcehre, Mollifying Networks, 2016\n3.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MIN]]\tCould you show the curves (on Figure 6 or another plot) for training a short ResNet (same depth as your starting model) and a deep ResNet (same depth as your final model) without using your approach?"[[EXP-NEU,MET-NEU,TNF-NEU], [SUB-NEU], [QSN], [MIN]]