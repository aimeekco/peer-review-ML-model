"This paper proposes a ranking-based similarity metric for distributional semantic models.[[PDI-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] The main idea is to learn \"baseline\" word embeddings, retrofitting those and applying localized centering, to then calculate similarity using a measure called \"Ranking-based Exponential Similarity Measure\" (RESM), which is based on the recently proposed APSyn measure.[[PDI-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]]\n\nI think the work has several important issues:\n\n1.[[RWK-NEG], [SUB-NEG,EMP-NEG], [DFT], [MIN]] The work is very light on references.[[RWK-NEU,BIB-NEG], [null], [DFT], [MIN]] There is a lot of previous work on evaluating similarity in word embeddings (e.g. Hill et al, a lot of the papers in RepEval workshops, etc.); specialization for similarity of word embeddings (e.g. Kiela et al., Mrksic et al., and many others); multi-sense embeddings (e.g. from Navigli's group); and the hubness problem (e.g. Dinu et al.). For the localized centering approach, Hara et al.'s introduced that method. [[RWK-NEG,BIB-NEG], [IMP-NEG,EMP-NEG], [DFT,CRT], [MIN]]None of this work is cited, which I find inexcusable.\u2028\n\n2. [[RWK-NEG], [null], [DFT], [MIN]]The evaluation is limited, in that the standard evaluations (e.g. SimLex would be a good one to add, as well as many others, please refer to the literature) are not used and there is no comparison to previous work.[[RWK-NEG,ANA-NEG], [IMP-NEG,CMP-NEG,EMP-NEG], [DFT,CRT], [MIN]] The results are also presented in a confusing way, with the current state of the art results separate from the main results of the paper.[[RES-NEG,OAL-NEG], [CLA-NEG,IMP-NEG], [DFT,CRT], [MIN]] It is unclear what exactly helps, in which case, and why.\u2028\n\n3.[[RWK-NEG,BIB-NEG], [EMP-NEG], [CRT], [MIN]] There are technical issues with what is presented, with some seemingly factual errors.[[PDI-NEG,EXP-NEG,MET-NEG], [IMP-NEG,PNF-NEG,EMP-NEG], [DFT], [MIN]] For example, \"In this case we could apply the inversion, however it is much more convinient [sic] to take the negative of distance.[[RWK-NEG], [null], [SUG], [MIN]] Number 1 in the equation stands for the normalizing, hence the similarity is defined as follows\" - the 1 does not stand for normalizing, that is the way to invert the cosine distance (put differently, cosine distance is 1-cosine similarity, which is a metric in Euclidean space due to the properties of the dot product).[[RWK-NEU,PDI-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] Another example, \"are obtained using the GloVe vector, not using PPMI\" - there are close relationships between what GloVe learns and PPMI, which the authors seem unaware of (see e.g. the GloVe paper and Omer Levy's work).\u2028\n\n4.[[RWK-NEG,EXP-NEG,MET-NEG], [SUB-NEG,IMP-NEG,EMP-NEG], [DFT], [MIN]] Then there is the additional question, why should we care?[[RWK-NEU], [null], [QSN], [GEN]] The paper does not really motivate why it is important to score well on these tests: these kinds of tests are often used as ways to measure the quality of word embeddings, but in this case the main contribution is the similarity metric used *on top* of the word embeddings.[[RWK-NEU,PDI-NEG,OAL-NEG], [IMP-NEG], [DFT], [MIN]] In other words, what is supposed to be the take-away, and why should we care?[[RWK-NEU], [null], [SMY], [GEN]]\n\nAs such, I do not recommend it for acceptance - it needs significant work before it can be accepted at a conference.[[RWK-NEG,OAL-NEG], [SUB-NEG,REC-NEG], [SUG,DFT], [MIN]]\n\nMinor points:\n- Typo in Eq 10\n- Typo on page 6 (/cite instead of \\cite)"[[OAL-NEG], [REC-NEG], [DFT,CRT], [MIN]]