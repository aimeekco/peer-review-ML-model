"The authors propose techniques for multitask and few shot learning, where the number of tasks is potentially very large, and the different tasks might have different output spaces.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] Prior techniques which can address some of these aspects do not necessarily work with deep learning, which is a key focus of the paper.[[RWK-NEG,PDI-NEG], [CMP-NEG], [CRT], [MAJ]] The authors suggest computing a similarity matrix amongst the tasks.[[PDI-NEU], [null], [SMY], [GEN]] Given such a matrix, they propose to do multitask learning by clustering the similarity matrix, and learning a single model for each cluster.[[PDI-NEU], [null], [SMY], [GEN]] If the tasks in a cluster have different output spaces, then a separate output layer is learned for each task in the cluster following a common encoding module.[[MET-NEU], [null], [SMY], [GEN]]\nTo deal with the large number of tasks, the authors further propose computing a few randomly sampled entries of the similarity matrix, and then using ideas from robust matrix completion to induce the full matrix.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] The resulting algorithm is evaluated on a standard amazon reviews benchmark from multitask learning, as well as two datasets from intent classification in dialog systems.[[DAT-NEU,MET-NEU], [null], [SMY], [GEN]]\n\nI think there are some interesting ideas in this paper, and the use of matrix completion techniques to deal with a large number of tasks is nice.[[PDI-POS,MET-POS], [EMP-POS], [APC], [MAJ]] But I believe there are important drawbacks in the framing and basic methodology and evaluation which make the paper unfit for publication in its current form.[[OAL-NEG], [EMP-NEG], [CRT], [MIN]]\n\n1. The prior works which do task clustering and multitask learning typically focus on how one might induce clusters which work well with the multitask learning methods used (see e.g. Kang et al. which is cited, as well as Kshirsagar et al. in ECML 2017 as two examples).[[RWK-NEU,MET-NEU], [CMP-NEU], [SMY], [GEN]] In this paper, on the other hand, the clusters are obtained in a manner which only accounts for pairwise similarities of tasks, using a pairwise similarity metric which is quite different from how the cluster is eventually used.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] This seems quite suboptimal.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n2. The pairwise similarity measure appears to be one that might have a high false negative rate.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] That is, it might rate many tasks as dissimilar even when they are not.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] This is because you train individual model on i and apply it to j.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]] It is possible that this model does not do well, but there is an equally good model for i which also does well on j.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Such a model would indeed be found if i and j are put in the same cluster, but the method would fail to do so, leading to high fragmentation.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n3. I do not see how you apply the model from task i to task j when the two have different output spaces.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Since this is a major motivation of the paper, I actually do not see how the setup makes sense![[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n4. It seems odd to put absolute errors on task j instead of regret to the model trained on j in the similarity matrix.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n5. The inducing of edges in the Y matrix by comparing to a mean and standard deviation is completely baseless. [[MET-NEG], [CMP-NEG], [CRT], [MAJ]]Without good reasoning from the authors, I see no reason why the entries in the row of a matrix should have a normal-like distribution.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Furthermore, in the matrix completion scenario, you have O(log^2n) entries per row on average, which means with high probability few rows should have a constant number of entries.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] In this case, the means are standard deviations do not even make sense to me.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] At the very least, I would consider using regret to the model of the task, and compute some quantiles on that which is still suspect in the matrix completion setting.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n6 .In the evaluation, why are just 12 tasks used in the Amazon dataset?[[DAT-NEU], [EMP-NEU], [QSN], [MAJ]] Why don't you present evaluation results on all tasks in the multitask setting?[[DAT-NEU,MET-NEU], [EMP-NEU], [QSN], [MAJ]]\n7. Why is average accuracy the right thing?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] If the error rates are different for different tasks, it is not sensible to measure raw accuracies.[[RES-NEU], [EMP-NEU], [DIS], [MIN]]\n\nThe authors also seem to miss a potentially relevant baseline in Cross-Stitch Networks (https://arxiv.org/abs/1604.03539)[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\nBesides these major issues, there are also a few minor issues I have with the paper. [[OAL-NEG], [null], [CRT], [MIN]]I do not see why there's need for a proof for the matrix completion result. [[RES-NEG], [EMP-NEG], [CRT], [MAJ]]This appears to be a direct application of Chandrasekaran et al, and in fact matrix completion has been used for clustering before (https://arxiv.org/abs/1104.4803).[[RWK-NEU], [CMP-NEU], [DIS], [MIN]] Given this, the presentation in the paper makes the idea look more novel than it is.[[OAL-NEU], [NOV-POS,PNF-POS], [APC], [MAJ]] I also think that the authors might benefit from dropping the whole few-shot learning angle here, and instead do a more thorough job of evaluating their multitask learning method."[[MET-POS], [EMP-POS], [APC], [MAJ]]