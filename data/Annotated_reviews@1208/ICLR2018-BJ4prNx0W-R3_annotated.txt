"Previous work by Cai et al. (2017) shows how to use Neural Programmer-Interpreter (NPI) framework to prove correctness of a learned neural network program by introducing recursion. [[RWK-NEU,EXT-NEU], [CMP-NEU], [DIS], [GEN]]It requires generation of a diverse training set consisting of execution traces which describe in detail the role of each function in solving a given input problem.[[RWK-NEU,EXT-NEU], [null], [DIS], [GEN]] Moreover, the traces need to be recursive: each function only takes a finite, bounded number of actions.[[MET-NEU], [null], [DIS], [GEN]] In this paper, the authors show how training set can be generated automatically satisfying the conditions of Cai et al.'s paper.[[RWK-NEU,PDI-NEU,DAT-NEU,MET-NEU], [CMP-NEU], [SMY], [GEN]] They iteratively explore all\npossible behaviors of the oracle in a breadth-first manner, and the bounded nature of the recursive\noracle ensures that the procedure converges.[[MET-NEU], [null], [SMY], [GEN]] As a running example, they show how this can be be done for bubblesort.[[MET-NEU], [null], [SMY], [GEN]] The training set generated in this Fprocess may have a lot of duplicates, and the authors show how these duplicates can possibly be removed.[[DAT-NEU,MET-NEU], [null], [SMY], [GEN]] It indeeds shows a dramatic reduction in the number of training samples for the three experiments that have been shown in the paper.[[RES-POS], [EMP-POS], [APC], [MAJ]] \n\nI am not an expert in this area, so it is difficult for me to judge the technical merit of the work.[[EXT-NEU], [null], [DIS], [GEN]] My feeling from reading the paper is that it is rather incremental over Cai et al.[[RWK-POS], [CMP-POS], [APC], [MAJ]] I am impressed by the results of the three experiments that have been shown here, specifically, the reduction in the training samples once they have been generated is significant.[[DAT-POS,EXP-POS,RES-POS], [EMP-POS], [APC], [MAJ]] But these are also the same set of experiments performed by Cai et al.[[RWK-NEU,EXP-NEU], [CMP-NEU], [DIS], [GEN]] \n\nGiven the original number of traces generated is huge, I do not understand, why this method is at all practical.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] This also explains why the authors have just tested the performance on extremely small sized data. It will not scale.[[DAT-NEG], [EMP-NEG], [CRT], [MAJ]] So, I am hesitant accepting the paper.[[OAL-NEG], [REC-NEG], [FBK], [MAJ]] I would have been more enthusiastic if the authors had proposed a way to combine the training space exploration as well as removing redundant traces together to make the whole process more scalable and done experiments on reasonably sized data. "[[DAT-NEG,EXP-NEG,ANA-NEG], [SUB-NEG,EMP-NEG], [DFT,CRT], [MAJ]]