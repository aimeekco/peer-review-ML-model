"The paper proposes to address the quadratic memory/time requirement of Relation Network (RN) by sequentially attending (via multiple layers) on objects and gating the object vectors with the attention weights of each layer.[[INT-NEU], [null], [SMY], [GEN]] The proposed model obtains state of the art in bAbI story-based QA and bAbI dialog task[[MET-NEU], [null], [SMY], [GEN]].\n\nPros:\n- The model achieves the state of the art in bAbI QA and dialog. I think this is a significant achievement given the simplicity of the model.[[MET-POS], [EMP-POS], [APC], [MAJ]]\n- The paper is clearly written.[[OAL-POS], [CLA-POS], [APC], [MAJ]]\n\nCons:\n- I am not sure what is novel in the proposed model.[[PDI-NEG], [NOV-NEG], [CRT], [MAJ]] While the authors use notations used in Relation Network (e.g. 'g'), I don't see any relevance to Relation Network.[[MET-NEG], [PNF-NEG], [DFT], [MIN]] Rather, this exactly resembles End-to-end memory network (MemN2N) and GMemN2N.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] Please tell me if I am missing something, but I am not sure of the contribution of the paper.[[OAL-NEG], [IMP-NEG], [CRT], [MAJ]] Of course, I notice that there are small architectural differences, but if these are responsible for the improvements, I believe the authors should have conducted ablation study or qualitative analysis that show that the small tweaks are meaningful.[[EXP-NEU,MET-NEU,ANA-NEU], [EMP-NEU], [SUG], [MAJ]]\n \nQuestion:\n- What is the exact contribution of the paper with respect to MemN2N and GMemN2N?"[[PDI-NEU], [NOV-NEU], [QSN], [MIN]]