"Summary:\nThis paper proposes an approach to learn embeddings in new domains by leveraging the embeddings from other domains in an incremental fashion[[INT-NEU], [null], [SMY], [GEN]]. The proposed approach will be useful when the new domain does not have enough data available.[[DAT-NEU,MET-NEU], [SUB-NEU], [SMY], [GEN]] The baselines chosen are 1). no embeddings 2). generic embeddings from english wiki, common crawl and combining data from previous and new domains.[[RWK-NEU], [null], [SMY], [GEN]] Empirical performance is shown on 3 downstream tasks: Product-type classification, Sentiment Classification and Aspect Extraction.[[EXP-NEU], [null], [SMY], [GEN]] The proposed embeddings just barely beat the baseline on product classification and sentiment classification, but significantly beat them on aspect extraction task.[[EXP-NEU], [CMP-NEU], [SMY], [GEN]]\n\n\nComments:\n\nThe paper puts itself nicely in context of the previous work and the addressed problem of learning word embeddings for new domain in the absence of enough data is an important one that needs to be addressed.[[RWK-POS,MET-POS], [CMP-POS], [APC], [MAJ]] There is reasonable novelty in the proposed method compared to the existing literature.[[RWK-NEU,MET-NEU], [NOV-POS], [APC], [MAJ]] But, I was a little disappointed by the paper as several details of the model were unclear to me and the paper's writing could definitely be improved to make things clearer.[[MET-NEG], [CLA-NEU,EMP-NEG], [CRT], [MAJ]] \n\n1). In the \"Meta-learner\" section 4.1, the authors talk about word features (u{_w_{i,j,k}},u{_w_{i,j',k}}).[[MET-NEU], [null], [SMY], [GEN]] It is unclear what these word features are. Are they one-hot encodings or embeddings or something else?[[MET-NEU], [EMP-NEU], [QSN], [MAJ]] It would really help if the paper gave some expository examples.[[EXP-NEU], [EMP-NEU], [SUG], [MAJ]]\n\n2). In Algorithm 1, how do you deal with vocabulary items in the new domain that do not exist in the previous domains i.e. when the intersection of V_i and V_{n+1} is the null set.[[DAT-NEU,MET-NEU], [EMP-NEU], [QSN], [MAJ]] This is very important because the main appeal of this work is its applicability to new domains with scarce data which have far fewer words and hence the above scenario is more likely to happen.[[MET-NEU], [EMP-NEU], [DIS], [GEN]]\n\n3). The results in Table 3 are a little confusing.[[RES-NEG,TNF-NEU], [EMP-NEG], [CRT], [MAJ]] Why do the lifelong word embeddings relatively perform far worse on precision but significantly better on recall compared to the baselines?[[RWK-NEU,MET-NEU], [EMP-NEU], [QSN], [MAJ]] What is driving those difference in results?[[RES-NEU], [EMP-NEU], [QSN], [MAJ]]\n\n4). Typos: In Section 3, \"...is depicted in Figure 1 and Figure 3\". I think you mean \"Figure 1 and Figure 2\" as there is no Figure 3.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]] \n"