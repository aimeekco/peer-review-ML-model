"This paper proposes to train recursive neural network on subtask graphs in order to execute a series of tasks in the right order, as is described by the subtask graph's dependencies.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] Each subtask execution is represented by a (non-learned) option.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] Reward shaping allows the proposed model to outperform simpler baselines, and experiments show the model generalizes to unseen graphs.[[RWK-POS,EXP-NEU], [NOV-POS], [APC], [MAJ]]\n\nWhile this paper is as far as I can tell novel in how it does what it does,[[OAL-POS], [NOV-POS], [APC], [MAJ]] the authors have failed to convey to me why this direction of research is relevant.[[FWK-NEG], [IMP-NEG], [CRT], [MAJ]]\n- We know finding options is the hard part about options\[[EXT-NEU], [null], [DIS], [MIN]]n- We already have good algorithms that take subtask graphs and execute them in the right order from the planning litterature[[MET-NEU], [null], [DIS], [MIN]]\n\nAn interesting avenue would be if the subtask graphs were instead containing some level of uncertainty, or representing stochasticity, or anything that more traditional methods are unable to deal with efficiently, then I would see a justification for the use of neural networks.[[MET-NEU], [CMP-NEU], [DIS], [MIN]] Alternatively, if the subtask graphs were learned instead of given, that would open the door to scaling an general learning.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Yet, this is not discussed in the paper.[[MET-NEG], [SUB-NEG], [DFT], [MIN]]\n\nAnother interesting avenue would be to learn the options associated with each task, possibly using the information from the recursive neural networks to help learn these options.\n\n\nThe proposed algorithm relies on fairly involved reward shaping, in that it is a very strong signal of supervision on what the next action should be.[[EXP-NEU,MET-NEU], [EMP-NEU], [SUG], [MIN]] Additionaly, it's not clear why learning seems to completely \"fail\" without the pre-trained policy.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MIN]] The justification given is that it is \"to address the difficulty of training due to the complex nature of the problem\" but this is not really satisfying as the problems are not that hard.[[PDI-NEU], [null], [DIS], [MIN]] This also makes me question the generality of the approach since the pre-trained policy is rather simple while still providing an apparently strong score.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\n\nIn your experiments, you do not compare with any state-of-the-art RL or hierarchical RL algorithm on your domain, and use a new domain which has no previous point of reference.[[EXP-NEG,MET-NEG], [SUB-NEG,EMP-NEG], [DFT], [MIN]] It it thus hard to properly evaluate your method against other proposed methods.[[MET-NEG], [CMP-NEU], [CRT], [MAJ]]\n\nWhat the authors propose is a simple idea,[[PDI-NEU], [EMP-NEU], [DIS], [MIN]] everything is very clearly explained,[[PDI-POS], [CNT], [APC], [MAJ]] the experiments are somewhat lacking[[EXP-NEG], [SUB-NEG,EMP-NEG], [DFT,CRT], [MAJ]] but at least show an improvement over more a naive approach,[[EXP-POS], [EMP-POS], [APC], [MAJ]] however, due to its simplicity, I do not think that this paper is relevant for the ICLR conference.[[OAL-NEG], [REC-NEG], [FBK], [MAJ]] \n\nComments:\n- It is weird to use both a discount factor \\gamma *and* a per-step penalty.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] While not disallowed by theory, doing both is redundant because they enforce the same mechanism.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n- It seems weird that the smoothed logical AND/OR functions do not depend on the number of inputs; that is unless there are always 3 inputs (but it is not explained why; logical functions are usually formalised as functions of 2 inputs) as suggested by Fig 3.[[MET-NEG,TNF-NEG], [EMP-NEG], [CRT], [MAJ]]\n- It does not seem clear how the whole training is actually performed (beyond the pre-training policy).[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]] The part about the actor-critic learning seems to lack many elements (whole architecture training?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]] why is the policy a sum of \"p^{cost}\" and \"p^{reward}\"? [[MET-NEU], [EMP-NEU], [QSN], [MIN]]is there a replay memory? How are the samples gathered?). [[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]](On the positive side, the appendix provides some interesting details on the tasks generations to understand the experiments.)[[EXP-POS], [EMP-POS], [APC], [MAJ]]\n- The experiments cover different settings with different task difficulties.[[EXP-POS], [EMP-POS], [APC], [MAJ]] However, only one type of tasks is used.[[EXP-POS], [EMP-POS], [APC], [MAJ]] It would be good to motivate (in addition to the paragraph in the intro) the cases where using the algorithm described in the paper may be (or not?) the only viable option and/or compare it to other algorithms.[[MET-NEU], [CMP-NEU], [SUG], [MIN]] Even tough not mandatory, it would also be a clear good addition to also demonstrate more convincing experiments in a different setting.[[EXP-NEU], [null], [SUG], [MIN]]\n- \"The episode length (time budget) was randomly set for each episode in a range such that 60% \u2212 80% of subtasks are executed on average for both training and testing.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]]\" --> this does not seem very precise: under what policy is the 60-80% defined?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]] Is the time budget different for each new generated environment?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]]\n- why wait until exactly 120 epochs for NTS-RProp before fine-tuning with actor-critic? [[EXP-NEU], [EMP-NEU], [QSN], [MIN]]It seems that much less would be sufficient from figure 4?[[TNF-NEU], [EMP-NEU], [QSN], [MIN]]\n- In the table 1 caption, it is written \"same graph structure with training set\" --> do you mean \"same graph structure than the training set\"?"[[TNF-NEG], [CLA-NEG], [QSN], [MIN]]