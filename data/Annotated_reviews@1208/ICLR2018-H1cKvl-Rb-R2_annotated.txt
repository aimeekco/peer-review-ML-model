"The authors propose a new exploration algorithm for Deep RL[[INT-NEU,MET-POS], [null], [SMY], [GEN]]. They maintain an ensemble of Q-values (based on different initialisations) to model uncertainty over Q. The ensemble is then used to derive a confidence interval at each step, which is used to select actions UCB-style.[[MET-NEU], [null], [SMY], [GEN]]\n\nThere is some attempt at a Bayesian interpretation for the Bellman update. But to me it feels a bit like shoehorning the probabilistic interpretation into an already existing update - I\u2019m not sure this is justified and necessary here. Moreover, the UCB strategy is generally not considered a Bayesian strategy, so I wasn\u2019t convinced by the link to Bayesian RL in this paper.\[[RWK-NEG,MET-NEG], [CMP-NEG,EMP-NEG], [CRT], [MAJ]]n\nI liked the actual proposed method otherwise, and the experimental results on Atari seem good (but see also latest SOTA Atari results, for example the Rainbow paper). [[MET-POS,RES-POS], [null], [APC], [MAJ]]Some questions about the results:\n-How does it perform compared to epsilon-greedy added on top of Alg1, or is there evidence that this produces any meaningful exploration versus noise? [[RWK-NEU,MET-NEU], [CMP-NEU,EMP-NEU], [QSN], [MAJ]]\n-How does the distribution of Q values look like during different phases of learning?[[RES-NEU,ANA-NEU,TNF-NEU], [IMP-NEU], [QSN], [MAJ]]\n-Was epsilon-greedy used in addition to UCB exploration? Question for both Alg 1 and Alg 2.[[MET-NEU], [null], [QSN], [MAJ]]\n-What\u2019s different between Alg 1 and bootstrapped DQN (other than the action selection)?[[MET-NEU], [null], [QSN], [MAJ]]\n\nMinor things:\n-Missing propto in Eq 7[[MET-NEU], [PNF-NEG], [DFT], [MIN]]?\n-Maybe mention that the leftarrows are not hard updates. Maybe you already do somewhere\u2026\n-it looks more a Bellman residual update as written in (11).\n"[[OAL-NEU], [PNF-NEG], [DFT], [MIN]]