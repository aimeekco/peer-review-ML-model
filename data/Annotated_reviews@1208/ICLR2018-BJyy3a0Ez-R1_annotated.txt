"Pros:\n* asynchronous model-parallel training of deep learning models would potentially help in further scaling of deep learning models[[EXP-POS], [EMP-POS], [APC], [MAJ]]\n* the paper is clearly written and easy to understand[[OAL-POS], [CLA-POS], [APC], [MAJ]]\n\nCons:\n* weak experiments: performance of algorithms are not analyzed in terms of wall-clock, and important baselines are not compared against, making it difficult to judge the practical usefulness of the proposed algorithm[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n* weak theory: although the algorithm is claimed to be motivated by continuous-time formulation of gradient descent, neither convergence proof nor algorithm design really use the continuous-time formulation and discrete-time formulation seems to suffice; the proof is straightforward corollary of Lin et al.[[RWK-NEG,MET-NEG], [CMP-NEG,EMP-NEG], [CRT], [MAJ]]\n\nSummary: This paper proposes to update parameters of each layer in deep neural network asynchronously, instead of updating all layers simultaneously and synchronously.[[PDI-NEU], [null], [SMY], [GEN]] Authors derive this algorithm by first formulating gradient descent in continuous-time form and then modifying time dependencies between layers.[[MET-NEU], [null], [SMY], [GEN]] While asynchronous updates of parameters in stochastic gradient descent has been explored (dating back to [1] in 1986, and authors should also be referring to [2]), to my knowledge application of these ideas to layer-by-layer model parallelism for deep neural networks has not been studied.[[RWK-NEG,MET-NEG], [null], [SMY], [GEN]] Since model-parallel training across machines has not been very successful, and model-parallelism has been only exploited within machines, asynchronous model-parallel optimization is an important topic of research which has the promise of scaling deep learning models beyond the memory capacity of a single machine.[[MET-POS,FWK-POS], [EMP-POS], [APC], [MAJ]]\n\nUnfortunately, the practical usefulness of the algorithm has not been demonstrated.[[MET-NEG], [SUB-NEG], [DFT], [MIN]] It remains unanswered whether this algorithm can be implemented efficiently in modern hardware architectures, or in which situations this algorithm will be more useful than existing algorithms.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Experiments are all reported in terms of the number of updates (epochs), but this is not useful in judging the practical advantage of the proposed algorithm.[[EXP-NEG], [EMP-NEG], [CRT], [MIN]] What matters in practice is how fast the algorithm is in improving the performance as a function of _wall-clock time_, and I would expect that synchronous algorithms would be much faster than the proposed algorithm in terms of wall-clock time, as they can better exploit optimized tensor arithmetic on CPUs and GPUs.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Also, authors should compare against mini-batch gradient descent, because this is the most popular way of training deep neural networks; authors has the burden of proof that the proposed algorithm is practically more useful than the existing standard method.[[EXP-NEG,MET-NEG], [CMP-NEG], [DFT], [MIN]]\n\nAuthors argue their algorithm is motivated by continuous-time formulation of stochastic gradient descent, but it is unclear to me whether the continuous-time formulation was really necessary to derive the proposed algorithm.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] The algorithm operates in discrete time horizon, and continuous time is not used anywhere.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Authors rely mostly on Lin et al for the convergence proof, which is also based on discrete time horizon.[[RWK-NEU], [CMP-NEU], [DIS], [GEN]]\n\nAuthors argue in page 1 that Continuous Propagation is statistically superior to mini-batch gradient descent, but I cannot find statistical superiority of the method.[[MET-NEG], [CMP-NEG], [CRT], [MIN]] Also, the upper bound of the time-delay T slows down the convergence rate (Proposition in the appendix), so it is unclear whether asynchronous update is theoretically faster than synchronous mini-batch gradient descent.[[EXP-NEG,MET-NEG], [CMP-NEG], [CRT], [MIN]] I think which algorithm is faster depends on values of L, T and M.[[MET-NEU], [EMP-NEU], [DIS], [GEN]]\n\nAuthors do not provide enough citations.[[BIB-NEG], [SUB-NEG], [CRT], [MIN]] Continuous-time characterization of gradient descent has a long history, and authors should provide citation of it, for example when (5) is introduced.[[BIB-NEG], [SUB-NEG], [CRT], [MIN]] Authors should provide more discussion of the history of model-parallel asynchronous SGD (such as [1] and [2]), and when mentioning alternatives like Czarnecki et al (2017), authors should discuss what advantages and disadvantages the proposed algorithm has against these alternatives.[[RWK-NEG,MET-NEG], [SUB-NEG], [DFT], [MIN]]\n\n\n[1] Distributed Asynchronous Deterministic and Stochastic Gradient Optimization Algorithms (Tsitsiklis, Bertsekas and Athans, 1986)\n[2] Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent (Niu et al, 2011)\n"[[RWK-NEU,BIB-NEU], [null], [DIS], [MIN]]