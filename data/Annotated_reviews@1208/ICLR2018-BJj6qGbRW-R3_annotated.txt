"This paper studies the problem of one-shot and few-shot learning using the Graph Neural Network (GNN) architecture that has been proposed and simplified by several authors.[[INT-NEU], [null], [SMY], [GEN]] The data points form the nodes of the graph with the edge weights being learned, using ideas similar to message passing algorithms similar to Kearnes et al and Gilmer et al.[[RWK-NEU,DAT-NEU], [CMP-NEU], [QSN], [MIN]] This method generalizes several existing approaches for few-shot learning including Siamese networks, Prototypical networks and Matching networks.[[MET-NEU], [CMP-NEU], [DIS], [MIN]] The authors also conduct experiments on the Omniglot and mini-Imagenet data sets, improving on the state of the art.[[RWK-POS,DAT-POS,EXP-POS], [CMP-POS,EMP-POS], [APC], [MAJ]]\n\nThere are a few typos and the presentation of the paper could be improved and polished more.[[OAL-NEG], [PNF-NEG], [CRT], [MIN]] I would also encourage the authors to compare their work to other unrelated approaches such as Attentive Recurrent Comparators of Shyam et al, and the Learning to Remember Rare Events approach of Kaiser et al, both of which achieve comparable performance on Omniglot.[[RWK-NEU,DAT-NEU,MET-NEU], [SUB-NEU,CMP-NEU], [SUG], [MIN]] I would also be interested in seeing whether the approach of the authors can be used to improve real world translation tasks such as GNMT. "[[MET-NEU], [EMP-NEU], [SUG], [MIN]]