"## Review Summary\n\nOverall, the paper's paper core claim, that increasing batch sizes at a linear\nrate during training is as effective as decaying learning rates, is\ninteresting but doesn't seem to be too surprising given other recent work in\nthis space.[[PDI-NEU,RWK-NEG], [CMP-NEG], [SMY,CRT], [MAJ]] The most useful part of the paper is the empirical evidence to\nbackup this claim, which I can't easily find in previous literature.[[RWK-POS,MET-POS], [CMP-POS], [APC], [MAJ]] I wish\nthe paper had explored a wider variety of dataset tasks and models to better\nshow how well this claim generalizes, better situated the practical benefits\nof the approach (how much wallclock time is actually saved?[[DAT-NEG,MET-NEU], [SUB-NEG,EMP-NEU], [QSN], [MIN]] how well can it be\nintegrated into a distributed workflow?), and included some comparisons with\nother recent recommended ways to increase batch size over time.[[RWK-NEU,MET-NEU], [CMP-NEU], [SUG], [MIN]]\n\n\n## Pros / Strengths\n\n+ effort to assess momentum / Adam / other modern methods[[MET-POS], [EMP-POS], [APC], [MAJ]]\n\n+ effort to compare to previous experimental setups[[EXP-POS], [CMP-POS], [APC], [MAJ]]\n\n\n## Cons / Limitations\n\n- lack of wallclock measurements in experiments[[EXP-NEG], [SUB-NEG], [DFT], [MAJ]]\n\n- only ~2 models / datasets examined, so difficult to assess generalization[[DAT-NEG], [SUB-NEG], [DFT], [MAJ]]\n\n- lack of discussion about distributed/asynchronous SGD[[ANA-NEG], [SUB-NEG], [DFT], [MAJ]]\n\n\n## Significance\n\nMany recent previous efforts have looked at the importance of batch sizes\nduring training, so topic is relevant to the community.[[FWK-POS], [IMP-NEG], [APC], [MAJ]] Smith and Le (2017)\npresent a differential equation model for the scale of gradients in SGD,\nfinding a linear scaling rule proportional to eps N/B, where eps = learning\nrate, N = training set size, and B = batch size.[[RWK-NEU], [null], [DIS], [GEN]] Goyal et al (2017) show how\nto train deep models on ImageNet effectively with large (but fixed) batch\nsizes by using a linear scaling rule.[[RWK-NEU], [null], [DIS], [GEN]]\n\nA few recent works have directly tested increasing batch sizes during\ntraining.[[RWK-NEU], [null], [DIS], [GEN]] De et al (AISTATS 2017) have a method for gradually increasing batch\nsizes, as do Friedlander and Schmidt (2012).[[RWK-NEU], [null], [DIS], [GEN]] Thus, it is already reasonable to\npractitioners that the proposed linear scaling of batch sizes during training\nwould be effective.[[RWK-NEU,EXP-NEU], [null], [DIS], [GEN]]\n\nWhile increasing batch size at the proposed linear scale is simple and seems\nto be effective, a careful reader will be curious how much more could be\ngained from the backtracking line search method proposed in De et al.[[RWK-NEU], [null], [DIS], [GEN]]\n\n\n## Quality\n\nOverall, only single training runs from a random initialization are used.[[EXP-POS], [EMP-POS], [APC], [MAJ]] It\nwould be better to take the best of many runs or to somehow show error bars,\nto avoid the reader wondering whether gains are due to changes in algorithm or\nto poor exploration due to bad initialization.[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [GEN]] This happens a lot in Sec. 5.2.[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [GEN]] \n\nSome of the experimental setting seem a bit haphazard and not very systematic.[[EXP-NEG], [PNF-NEG], [CRT], [MIN]]\nIn Sec. 5.2, only two learning rate scales are tested (0.1 and 0.5).[[EXP-NEG], [SUB-NEG], [DFT], [MIN]] Why not\nexamine a more thorough range of values?[[EXP-NEG], [EMP-NEG], [QSN], [MIN]]\n\nWhy not report actual wallclock times?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]] Of course having reduced number of\nparameter updates is useful, but it's difficult to tell how big of a win this\ncould be.[[EXP-NEG], [EMP-NEG], [CRT], [MIN]]\n\nWhat about distributed SGD or asyncronous SGD (hogwild)?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]  Small batch sizes\nsometimes make it easier for many machines to be working simultaneously.[[DAT-NEU,MET-NEU], [null], [SUG], [MIN]]  If we\nscale up to batch sizes of ~ N/10, we can only get 10x speedups in\nparallelization (in terms of number of parameter updates).[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]]  I think there is\nsome subtle but important discussion needed on how this framework fits into\nmodern distributed systems for SGD.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \n\n\n## Clarity\n\nOverall the paper reads reasonably well.[[OAL-POS], [CLA-POS], [APC], [MAJ]] \n\nOffering a related work \"feature matrix\" that helps readers keep track of how\nprevious efforts scale learning rates or minibatch sizes for specific\nexperiments could be valueable.[[RWK-POS,EXP-NEU], [SUB-NEU], [SUG], [MIN]]  Right now, lots of this information is just\nprovided in text, so it's not easy to make head-to-head comparisons.[[RWK-NEG], [CMP-NEG], [CRT], [MIN]] \n\nSeveral figure captions should be updated to clarify which model and dataset\nare studied.[[TNF-NEU], [PNF-NEU], [DIS], [MIN]]  For example, when skimming Fig. 3's caption there is no such\ninformation.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]]\n\n## Paper Summary\n\nThe paper examines the influence of batch size on the behavior of stochastic\ngradient descent to minimize cost functions.[[PDI-NEU], [null], [DIS], [GEN]] The central thesis is that\ninstead of the \"conventional wisdom\" to fix the batch size during training and\ndecay the learning rate, it is equally effective (in terms of training/test\nerror reached) to gradually increase batch size during training while fixing\nthe learning rate.[[PDI-NEU], [null], [DIS], [GEN]] These two strategies are thus \"equivalent\".[[PDI-NEU], [null], [DIS], [GEN]] Furthermore,\nusing larger batches means fewer parameter updates per epoch, so training is\npotentially much faster.[[DAT-NEU,EXP-NEU], [null], [DIS], [GEN]]\n\nSection 2 motivates the suggested linear scaling using previous SGD analysis\nfrom Smith and Le (2017).[[RWK-NEU], [CMP-NEU], [DIS], [MIN]] Section 3 makes connections to previous work on\nfinding optimal batch sizes to close the generaization gap. [[RWK-NEU], [null], [DIS], [GEN]]Section 4 extends\nanalysis to include SGD methods with momentum.[[ANA-NEU], [null], [DIS], [GEN]]\n\nIn Section 5.1, experiments training a 16-4 ResNet on CIFAR-10 compare three\npossible SGD schedules: * increasing batch size * decaying learning rate *\nhybrid (increasing batch size and decaying learning rate) Fig. 2, 3 and 4 show\nthat across a range of SGD variants (+/- momentum, etc) these three schedules\nhave similar error vs. epoch curves.[[DAT-NEU,EXP-NEU,TNF-NEU], [null], [DIS], [GEN]] This is the core claimed contribution:\nempirical evidence that these strategies are \"equivalent\".[[PDI-NEU,MET-NEU], [null], [DIS], [GEN]]\n\nIn Section 5.3, experiments look at Inception-ResNet-V2 on ImageNet, showing\nthe proposed approach can reach comparable accuracies to previous work at even\nfewer parameter updates (2500 here, vs. \u223c14000 for Goyal et al 2007)\n"[[RWK-NEU,MET-NEU], [null], [DIS], [GEN]]