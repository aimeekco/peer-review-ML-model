"Summary:\nThis paper proposes a framework for private deep learning model inference using FHE schemes that support fast bootstrapping.[[INT-NEU], [null], [SMY], [GEN]]\nThe main idea of this paper is that in the two-party computation setting, in which the client's input is encrypted while the server's deep learning model is plain.[[PDI-NEU,MET-NEU], [EMP-NEU], [SMY], [MAJ]]\nThis \"hybrid\" argument enables to reduce the number of necessary bootstrapping, and thus can reduce the computation time.[[MET-NEU], [EMP-NEU], [SMY], [MAJ]]\nThis paper gives an implementation of adder and multiplier circuits and uses them to implement private model inference.[[MET-NEU], [EMP-NEU], [SMY], [GEN]]\n\nComments:\n1. I recommend the authors to tone down their claims.[[MET-NEU], [CLA-NEU], [SUG], [MAJ]] For example, the authors mentioned that \"there has been no complete implementation of established deep learning approaches\" in the abstract, however, the authors did not define what is \"complete\".[[ABS-NEU], [EMP-NEG], [CRT], [MAJ]] Actually, the SecureML paper in S&P'17 should be able to privately evaluate any neural networks, although at the cost of multi-round information exchanges between the client and server.[[RWK-NEU,MET-NEU], [EMP-NEU], [CRT], [MAJ]]\n\nAlso, the claim that \"we show efficient designs\" is very thin to me since there are no experimental comparisons between the proposed method and existing works.[[RWK-NEU,EXP-NEU], [CMP-NEG], [CRT], [MAJ]] Actually, the level FHE can be very efficient with a proper use of message packing technique such as [A] and [C]. [[RWK-NEU,MET-NEU], [EMP-NEU], [DIS], [MAJ]]For a relatively shallow model (as this paper has used), level FHE might be faster than the binary FHE.[[MET-NEU], [EMP-NEU], [SUG], [MAJ]]\n\n2. I recommend the author to compare existing adder and multiplier circuits with your circuits to see in what perspective your design is better.[[RWK-NEU,MET-NEU], [CMP-NEU], [SUG], [MAJ]] I think the hybrid argument (i.e., when one input wire is plain) is a very common trick that used in the circuit design field, such as garbled circuit [B], to reduce the depth of the circuit.[[MET-NEU], [EMP-NEU], [SUG], [MAJ]] \n\n3. I appreciate that optimizations such as low-precision and point-wise convolution are discussed in this paper.[[MET-NEU], [EMP-NEU], [APC], [MAJ]] Such optimizations are very common in deep learning field while less known in the field of security.[[EXT-NEU], [null], [DIS], [MAJ]]\n\n[A]: Dowlin et al. Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy.[[RWK-NEU,BIB-NEU], [null], [SUG], [MIN]]\n[B]: V. Kolesnikov et al. Improved garbled circuit: free xor gates and applications.[[RWK-NEU,BIB-NEU], [null], [SUG], [MIN]] \n[C]: Liu et al. Oblivious Neural Network Predictions via MiniONN transformations.[[RWK-NEU,BIB-NEU], [null], [SUG], [MIN]]"