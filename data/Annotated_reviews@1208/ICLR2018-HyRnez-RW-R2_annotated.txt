"This paper proposes a method that scales reading comprehension QA to large quantities of text with much less document truncation than competing approaches.[[INT-NEU,MET-NEU], [null], [SMY], [GEN]] The model also does not consider the first mention of the answer span as gold, instead formulating its loss function to incorporate multiple mentions of the answer within the evidence.[[MET-NEU], [null], [SMY], [GEN]] The reported results were state-of-the-art(*) on the TriviaQA dataset at the time of the submission deadline.[[DAT-NEU,RES-NEU], [EMP-NEU], [DIS], [MIN]] It's interesting that such a simple model, relying mainly on (weighted) word embedding averages, can outperform more complex architectures; however, these improvements are likely due to decreased truncation as opposed to bag-of-words architectures being superior to RNNs.[[MET-POS,RES-POS], [CMP-POS,EMP-POS], [APC], [MAJ]] \n\nOverall, I found the paper interesting to read, and scaling QA up to larger documents is definitely an important research direction.[[OAL-POS], [IMP-POS], [APC], [MAJ]] On the other hand, I'm not quite convinced by its experimental results (more below) and the paper is lacking an analysis of what the different sub-models are learning.[[EXP-NEG,RES-NEG,ANA-NEG], [EMP-NEG], [CRT], [MAJ]] As such, I am borderline on its acceptance.[[OAL-NEG], [REC-NEG], [FBK], [MAJ]]\n\n* The TriviaQA leaderboard shows a submission from 9/24/17 (by \"chrisc\") that has significantly higher EM/F1 scores than the proposed model.[[RWK-NEG,MET-NEG], [CMP-NEG], [CRT], [MAJ]] Why is this result not compared to in Table 1?[[RES-NEG,TNF-NEG], [CMP-NEG], [QSN], [MIN]] \n\nDetailed comments:\n- Did you consider pruning spans as in the end-to-end coreference paper of Lee et al., EMNLP 2017?[[RWK-NEU,BIB-NEU], [CMP-NEU], [QSN], [MIN]] This may allow you to avoid truncation altogether.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Perhaps this pruning could occur at level 1, making subsequent levels would be much more efficient.[[MET-NEU], [EMP-NEU], [SUG], [MIN]]\n- How long do you estimate training would take if instead of bag-of-words, level 1 used a biLSTM encoder for spans / questions?[[EXP-NEU,MET-NEU], [CMP-NEU], [QSN], [MIN]]\n- What is the average number of sentences per document?[[CNT], [CNT], [QSN], [MIN]] It's hard to get an idea of how reasonable the chosen truncation thresholds are without this.[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]]\n- In Figure 3, it looks like the exact match score is still increasing as the maximum tokens in document is increased.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]] Did the authors try truncating after more words (e.g., 10k)?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]]\n- I would have liked to see some examples of questions that are answered correctly by level 3 but not by level 2 or 1, for example, to give some intuition as to how each level works.[[MET-NEU,RES-NEU], [SUB-NEU,EMP-NEU], [SUG,DIS], [MIN]]\n- \"Krasner\" misspelled multiple times as \"Kramer\""[[CNT], [CLA-NEG], [CRT], [MIN]]