"This paper examines a distributed Deep RL system in which experiences, rather than gradients, are shared between the parallel workers and the centralized learner.[[INT-NEU], [null], [SMY], [GEN]] The experiences are accumulated into a central replay memory and prioritized replay is used to update the policy based on the diverse experience accumulated by all the of the workers.[[MET-NEU], [EMP-POS], [SMY], [GEN]] Using this system, the authors are able to harness much more compute to learn very high quality policies in little time.[[MET-POS], [null], [APC], [MAJ]] The results very convincingly show that Ape-X far outperforms competing algorithms such as recently published Rainbow.[[RES-POS], [CMP-POS], [APC], [MAJ]] \n\nIt\u2019s hard to take issue with a paper that has such overwhelmingly convincing experimental results.[[EXP-POS], [EMP-POS], [APC], [MAJ]] However, there are a couple additional experiments that would be quite nice:\n\u2022\tIn order to understand the best way for training a distributed RL agent, it would be nice to see a side-by-side comparison of systems for distributed gradient sharing (e.g. Gorila) versus experience sharing (e.g. Ape-X).[[EXP-NEU], [CMP-NEU,EMP-POS], [SUG], [MIN]] \n\u2022\tIt would be interesting to get a sense of how Ape-X performs as a function of the number of frames it has seen, rather than just wall-clock time.[[EXP-POS], [EMP-POS], [SUG], [MIN]] For example, in Table 1, is Ape-X at 200M frames doing better than Rainbow at 200M frames?[[EXP-NEU], [CMP-NEU], [QSN], [MIN]]\n\nPros:\n\u2022\tWell written and clear[[OAL-POS], [CLA-POS], [APC], [MAJ]].\n\u2022\tVery impressive results[[RES-POS], [null], [APC], [MAJ]].\n\u2022\tIt\u2019s remarkable that Ape-X preforms as well as it does given the simplicity of the algorithm.[[MET-POS], [EMP-POS], [APC], [MAJ]]\n\nCons:\n\u2022\tHard to replicate experiments without the deep computational pockets of DeepMind.\n"[[EXP-NEU], [SUB-NEG], [DFT], [MIN]]