"Summary:  The paper applies graph convolutions with deep neural networks to the problem of \"variable misuse\" (putting the wrong variable name in a program statement) in graphs created deterministically from source code.[[PDI-NEU], [null], [SMY], [GEN]]  Graph structure is determined by program abstract syntax tree (AST) and next-token edges, as well as variable/function name identity, assignment and other deterministic semantic relations.[[PDI-NEU], [null], [SMY], [GEN]]  Initial node embedding comes from both type and tokenized name information.[[PDI-NEU], [null], [SMY], [GEN]]  Gated Graph Neural Networks (GGNNs, trained by maximum likelihood objective) are then run for 8 iterations at test time.[[PDI-NEU], [null], [SMY], [GEN]]\n\nThe evaluation is extensive and mostly very good.[[MET-POS], [EMP-POS], [APC], [MAJ]]  Substantial data set of 29m lines of code.[[DAT-POS], [EMP-POS], [APC], [MAJ]]  Reasonable baselines.[[RWK-POS], [EMP-POS], [APC], [MAJ]]  Nice ablation studies.[[RES-POS], [EMP-POS], [APC], [MAJ]]  I would have liked to see separate precision and recall rather than accuracy.[[ANA-NEG], [EMP-NEG], [CRT], [MIN]]  The current 82.1% accuracy is nice to see,[[RES-POS], [EMP-POS], [APC], [MAJ]]  but if 18% of my program variables were erroneously flagged as errors, the tool would be useless.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]   I'd like to know if you can tune the threshold to get a precision/recall tradeoff that has very few false warnings, but still catches some errors.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\nNice work creating an implementation of fast GGNNs with large diverse graphs.[[TNF-POS], [EMP-POS], [APC], [MAJ]]  Glad to see that the code will be released.[[RES-POS], [EMP-POS], [APC], [MAJ]]  Great to see that the method is fast---it seems fast enough to use in practice in a real IDE.[[MET-POS], [EMP-POS], [APC], [MAJ]]\n\nThe model (GGNN) is not particularly novel, but I'm not much bothered by that.[[MET-NEG], [NOV-NEG], [CRT], [MIN]]   I'm very happy to see good application papers at ICLR.[[OAL-POS], [EMP-POS], [APC], [MAJ]]  I agree with your pair of sentences in the conclusion: \"Although source code is well understood and studied within other disciplines such as programming language research, it is a relatively new domain for deep learning.[[MET-POS], [NOV-POS], [APC], [MAJ]] It presents novel opportunities compared to textual or perceptual data, as its (local) semantics are well-defined and rich additional information can be extracted using well-known, efficient program analyses.[[DAT-POS,MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]]\"  I'd like to see work in this area encouraged.[[FWK-NEU], [IMP-NEU], [DIS], [MIN]]  So I recommend acceptance.[[OAL-POS], [REC-POS], [FBK], [MAJ]]  If it had better (e.g. ROC curve) evaluation and some modeling novelty, I would rate it higher still.[[MET-NEU,OAL-POS], [NOV-NEU,REC-POS], [FBK], [MAJ]]\n\nSmall notes:\nThe paper uses the term \"data flow structure\" without defining it.[[MET-NEG], [SUB-NEG], [CRT], [MIN]]\nYour data set consisted of C# code.[[DAT-NEG], [null], [DIS], [MIN]]  Perhaps future work will see if the results are much different in other languages.\n"[[FWK-NEU], [IMP-NEU], [DIS], [MIN]]