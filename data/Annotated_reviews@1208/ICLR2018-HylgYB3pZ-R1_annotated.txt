"This paper studies the impact of angle bias on learning deep neural networks, where angle bias is defined to be the expected value of the inner product of a random vectors (e.g., an activation vector) and a given vector (e.g., a weight vector).[[INT-NEU,PDI-NEU,MET-NEU], [null], [SMY], [GEN]]  The angle bias is non-zero as long as the random vector is non-zero in expectation and the given vector is non-zero.[[MET-NEU], [null], [SMY], [GEN]]  This suggests that the some of the units in a deep neural network have large values (either positive or negative) regardless of the input, which in turn suggests vanishing gradient.[[MET-NEU], [null], [SMY], [GEN]]  The proposed solution to angle bias is to place a linear constraint such that the sum of the weight becomes zero.[[MET-NEU,RES-NEU], [null], [SMY], [GEN]]  Although this does not rule out angle bias in general, it does so for the very special case where the expected value of the random vector is a vector consisting of a common value.[[RES-NEU], [null], [DIS], [GEN]]  Nevertheless, numerical experiments suggest that the proposed approach can effectively reduce angle bias and improves the accuracy for training data in the CIFAR-10 task.[[DAT-POS,EXP-POS,MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]]  Test accuracy is not improved, however.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nOverall, this paper introduces an interesting phenomenon that is worth studying to gain insights into how to train deep neural networks, but the results are rather preliminary both on theory and experiments.[[EXP-POS,MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]]\n\nOn the theoretical side, the linearly constrained weights are only shown to work for a very special case.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]]  There can be many other approaches to mitigate the impact of angle bias.[[MET-NEU], [EMP-NEU], [SUG,DIS], [MIN]]  For example, how about scaling each variable in a way that the mean becomes zero, instead of scaling it into [-1,+1] as is done in the experiments?[[MET-NEU,EXP-NEU], [EMP-NEU], [QSN], [MIN]]  When the mean of input is zero, there is no angle bias in the first layer.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]]  Also, what about if we include the bias term so that b + w a is the preactivation value?[[MET-NEU,EXP-NEU], [EMP-NEU], [QSN], [MIN]]\n\nOn the experimental side, it has been shown that linearly constrained weights can mitigate the impact of angle bias on vanishing gradient and can reduce the training error, but the test error is unfortunately increased for the particular task with the particular dataset in the experiments.[[DAT-NEU,EXP-NEU], [null], [DIS], [MIN]]  It would be desirable to identify specific tasks and datasets for which the proposed approach outperforms baselines.[[RWK-NEU,DAT-NEU,MET-NEU], [CMP-NEU], [SUG,DIS], [MIN]]  It is intuitively expected that the proposed approach has some merit in some domains, but it is unclear exactly when and where it is.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\nMinor comments:\n\nIn Section 2.2, is Layer 1 the input layer or the next?"[[CNT], [CNT], [QSN], [MIN]]