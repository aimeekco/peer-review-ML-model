"The paper takes a recent paper of Zhang et al 2016 as the starting point to investigate the generalization capabilities of models trained by stochastic gradient descent.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The main contribution are scaling rules that relate the batch size k used in SGD with the learning rate \\epsilon, most notably \\epsilon/k = const for optimal scaling.[[PDI-NEU], [null], [SMY], [GEN]]\n\nFirst of all, I have to say that the paper is very much focussed on the aforementioned paper, its experiments as well as its (partially speculative) claims.[[MET-NEU], [null], [DIS], [MIN]] This, in my opinion, is a biased and limited starting point, which ignores much of the literature in learning theory.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] \n\nChapter 2 provides a sort of a mini-tutorial to (Bayesian) model selection based on standard Bayes factors.[[MET-NEU], [null], [DIS], [GEN]] I find this of limited usefulness.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] First of all, I find the execution poor in the details: \n(i) Why is \\omega limited to a scalar?[[MET-NEG], [EMP-NEG], [QSN], [MIN]] Nothing major really depends on that.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Later the presentation switches to a more general case.[[MET-NEG], [PNF-NEG], [CRT], [MIN]] \n(ii) What is a one-hot label?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]] \"One-hot\" is the encoding of a categorical label.[[EXP-NEU], [null], [DIS], [MIN]] \n(iii) In which way is a Gaussian prior uncorrelated, if there is just a scalar random variable? [[EXP-NEU], [EMP-NEU], [QSN], [MIN]] \n(iv) How can one maximize a probability density function?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]] \n(v) Why is an incorrect \"pseudo\"-set notation used instead of the correct vectorial one?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] \n(vi) \"Exponentially large\", \"reasonably prior\" model etc. is very vague terminology[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n(vii) No real credit is given for the Laplace approximation presented up to Eq. 10.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] For instance, why not refer to the seminal paper by Kass & Raferty?[[RWK-NEG], [null], [QSN], [MIN]] Why spend so much time on a step-by-step derivation anyway, as this is all \"classic\" and has been carried out many times before (in a cleaner write-up)?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] \n(viii) \"P denotes the number of model parameters\" (I guess it should be a small p? hard to decipher)[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n(ix) Usually, one should think of the Laplace approximation and the resulting Bayes factors more in terms of a \"volume\" of parameters  close to the MAP estimate, which is what the matrix determinant expresses, more than any specific direction of \"curvature\".[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \n\nChapter 3 constructs a simple example with synthetic data to demonstrate the effect of Bayes factors.[[MET-NEU], [null], [DIS], [MIN]] I feel the discussion to be too much obsessed by the claims made in Zhang et al 2016 and in no way suprising.[[RWK-NEG], [CMP-NEG], [CRT], [MAJ]] In fact, the \"toy\" example is so much of a \"toy\" that I am not sure what to make of it.[[RWK-NEG], [CMP-NEG], [CRT], [MIN]] Statistics has for decades successfully used criteria for model selection, so what is this example supposed to proof (to whom?).[[CNT], [null], [QSN], [GEN]]\n\nChapter 4 takes the work of Mandt et al as a starting point to understand how SGD with constant step size effectively can be thought of as gradient descent with noise, the amplitude of which is controlled by the step size and the mini-batch size.[[MET-NEU], [null], [DIS], [GEN]] Here, the main goal is to use evidence-based arguments to distinguish good from poor local minima.[[ANA-NEU], [null], [DIS], [GEN]] There is some experimental evidence presented on how to resolve the tradeoff between too much noise (underfitting) and too little (overfitting).[[EXP-NEG], [PNF-NEG], [CRT], [MIN]]\n\nChapter 5 takes a stochastic differential equation as a starting point.[[MET-NEU], [null], [DIS], [GEN]] I see several issues:\n(i) It seems that you are not doing much with a SDE, as you diredctly jump to the discretized version (and ignore discussions of it's discretization).[[MET-NEG], [SUB-NEG,EMP-NEG], [DFT,CRT], [MIN]] So maybe one should not feature the term SDE so prominently.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n(ii) While it is commonly done, it would be nice to get some insights on why a Gaussian approx. is a good assumption.[[MET-NEU,ANA-NEU], [SUB-NEU], [DIS], [MIN]] Maybe you can verify this experimentally (as much of the paper consists of experimental findings)\n(iii) Eq. 13. Maybe you want this form to indicate a direction you want to move towards,  by I find adding and subtracting the gradient in itself not a very interesting manner of illustartion.[[EXP-NEG], [EMP-NEG], [CRT], [MIN]]\n(iv) I am not sure in whoch way g is \"measured\", but I guess you are determining it by comparing coefficients. [[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n(v) I am confused by the B_opt \\propto \\eps statement.[[MET-NEG], [CNT], [CRT], [MIN]] It seems you are scaling to mini-batrch gradient to be in expectation equal to the full gradient (not normalized by N), e.g. it scales ~N.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Now, if we think of a mini-batch as being a batched version of single pattern updates, then clearly the effective step length should scale with the batch size, which - because of the batch size normalization with N/B - means \\epsilon needs to scale with B.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Maybe there is something deeper going on here, but it is not obvious to me.[[EXT-NEU], [null], [DIS], [GEN]]\n(vi) The argument why B ~ N is not clear to me.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Is there one or are just making a conjecture?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nBottom line: The paper may contribute to the current discussion of the Zhang et al 2016 paper, but I feel  it does not make a significant contribution to the state of knowledge in machine learning.[[RWK-NEU,OAL-NEG], [CMP-NEU], [CRT], [MAJ]] On top of that, I feel the execution of the paper leaves much to be desired. \n"[[OAL-NEU], [null], [DIS], [GEN]]