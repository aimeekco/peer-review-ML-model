"The paper presents an interesting framework for bAbI QA.[[INT-POS], [null], [SMY], [GEN]]  Essentially, the argument is that when given a very long paragraph, the existing approaches for end-to-end learning becomes very inefficient (linear to the number of the sentences).[[PDI-NEU], [null], [SMY], [GEN]]  The proposed alternative is to encode the knowledge of each sentence symbolically as n-grams, which is thus easy to index.[[MET-NEU], [null], [SMY], [GEN]]  While the argument makes sense, it is not clear to me why one cannot simply index the original text.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] The additional encode/decode mechanism seems to introduce unnecessary noise.[[MET-NEG], [EMP-NEU], [CRT], [MAJ]]  The framework does include several components and techniques from latest recent work, which look pretty sophisticated.[[RWK-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] However, as the dataset is generated by simulation, with a very small set of vocabulary, the value of the proposed framework in practice remains largely unproven.[[DAT-NEU,MET-POS], [SUB-NEG,EMP-NEG], [DFT], [MAJ]]\n\nPros:\n  1. An interesting framework for bAbI QA by encoding sentence to n-grams[[MET-POS], [EMP-POS], [APC], [MAJ]]\n\nCons:\n  1. The overall justification is somewhat unclear[[OAL-NEG], [CLA-NEG], [CRT], [MAJ]]\n  2. The approach could be over-engineered for a special, lengthy version of bAbI and it lacks evaluation using real-world data\n"[[MET-NEU], [EMP-NEU], [SUG], [MAJ]]