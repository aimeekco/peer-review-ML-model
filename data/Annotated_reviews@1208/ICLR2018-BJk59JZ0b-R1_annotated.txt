"\n\nThe authors devise and explore use of the hessian of the\n(approximate/learned) value function (the critic) to update the policy\n(actor) in the actor-critic  approach to RL.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]]  They connect their\ntechnique, 'guide actor-critic' or GAC, to existing actor-critic\nmethods (authors claim only two published work use 1st order\ninformation on the critic).[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] They show that the 2nd order information\ncan be useful (in several of the 9 tasks, their GAC techniques were\nbest or competitive, and in only one, performed poorly compared to best).[[MET-NEU], [null], [SMY], [GEN]]\n\nThe paper has a technical focus.[[OAL-NEU], [null], [DIS], [MIN]]\n\npros:\n\n- Strict generalization of an existing (up to 1st order) actor-critic approaches[[MET-NEU], [null], [DIS], [GEN]]\n\n- Compared to many existing techniques, on 9 tasks[[MET-NEU], [CMP-NEU], [DIS], [MIN]]\n\ncons:\n\n- no mention of time costs, except that for more samples, S > 1, for\n taylor approximation, it can be very expensive.[[ANA-NEG], [SUB-NEG], [DFT], [MIN]]\n\n- one would expect more information to strictly improve performance,\n  but the results are a bit mixed (perhaps due to convergence to local optima and both actor and critic being learned at same time, \n  or the Gaussian assumptions, etc.).[[RES-NEG], [SUB-NEG,EMP-NEG], [DFT,CRT], [MIN]]\n\n- relevance: the work presents a new approach to actor-critique strategy for\n  reinforcement learning, remotely related to 'representation\n  learning' (unless value and policies are deemed a form of\n  representation).[[MET-POS], [NOV-POS,EMP-POS], [APC], [MAJ]]\n\n\nOther comments/questions:\n\n- Why does the performance start high on Ant (1000), then goes to 0\n(all approaches)?[[DAT-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\n- How were the tasks selected?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Are they all the continuous control tasks available in open ai?\n\n\n \n\n\n\n\n"[[MET-NEU], [EMP-NEU], [QSN], [MIN]]