"This paper focuses on the sub-problem of discovering previously unseen classes for open-world classification.[[MET-NEU], [EMP-NEU], [QSN], [MIN]] \nIt employs a previously proposed system, Open Classification Network, for classifying instances into known classes or rejecting as belonging to an unseen class, and applies hierarchical clustering to the rejected instances to identify unseen classes.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MIN]]\nThe key novel idea is to learn a pairwise similarity function using the examples from the known classes to apply to examples of unknown classes.[[MET-POS], [NOV-POS], [APC], [MAJ]] The argument is that we tend to use the same notion of similarity and dissimilarity to define classes (known or unknown) and one can thus expect the similarity function learned from known classes to carry over to the unknown classes.[[MET-NEU], [null], [DIS], [MIN]]  This concept is not new.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Similar idea has been explored in early 2000 by Finley and Joachims in their ICML paper titled \"Supervised Clustering with Support Vector Machines\".[[PDI-NEG], [NOV-NEG], [CRT], [MAJ]]  But to the best of my knowledge, this is the first paper that applies this concept to the open world classification task.[[OAL-POS], [EMP-POS], [APC], [MAJ]] \n\nOnce we learn the similarity function, the rest of the approach is straightforward, without any particular technical ingenuity.[[MET-NEU], [null], [DIS], [MIN]]  It simply applies hierarchical clustering on the learned similarities and use cross-validation to pick a stopping condition for deciding the number of clusters.[[MET-NEU], [null], [DIS], [MIN]]  \nI find the experiments to be limited, only on two hand-written digits/letters datasets.[[DAT-NEG,EXP-NEG], [SUB-NEG], [CRT], [MAJ]]  Such datasets are too simplistic.[[DAT-NEG], [CNT], [CRT], [MAJ]] For example, simply applying kmeans to PCA features of the images on the MNIST data can get you pretty good performance.[[DAT-NEU], [EMP-NEU], [DIS], [MIN]]  \nExperiments on more complex data is desired, for example on Imagenet classes.[[DAT-NEG], [EMP-NEG], [CRT], [MAJ]] \n\nAlso the results do not clearly demonstrate the advantage of the proposed method, in particular the benefit of using PCN.[[RES-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] The number of clusters found by the algorithm is not particularly accurate and the NMI values obtained by the proposed approach does not show any clear advantage over baseline methods that do not use PCN.[[RWK-NEG,MET-NEG], [CMP-NEG], [CRT], [MAJ]] \n\nSome minor comments:\nWhen applied to the rejected examples, wouldn't the ground truth # of clusters no longer be 4 or 10 because there are some known-class examples mixed in?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] \nFor the base line Encoder+HC, was the encoder trained independently?[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]] Or it's trained jointly with PCN and OCN? [[EXP-NEU], [EMP-NEU], [QSN], [MIN]] It is interesting to see the impact of incorporating PCN into the training of OCN and encoder.[[EXP-NEU], [EMP-NEU], [QSN], [MIN]] Does that have any impact on accuracy of OCN?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]] \nIt seems that one of the claimed benefit is that the proposed method is effective at identifying the k.[[MET-POS], [EMP-POS], [APC], [MAJ]] If so, it would be necessary to compared the proposed method to some classic methods for identifying k with kmeans, such as the elbow method, BIC, G-means etc, especially since kmeans seem to give much better NMI values.\n\n\n"[[MET-NEU], [CMP-NEU], [DIS], [MAJ]]