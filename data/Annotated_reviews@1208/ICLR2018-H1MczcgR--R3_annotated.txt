"This paper studies the issue of truncated backpropagation for meta-optimization.[[INT-NEU], [null], [SMY], [GEN]] Backpropagation through an optimization process requires unrolling the optimization, which due to computational and memory constraints, is typically restricted or truncated to a smaller number of unrolled steps than we would like.[[INT-NEU], [null], [SMY], [GEN]]\n\nThis paper highlights this problem as a fundamental issue limiting meta-optimization approaches.[[PDI-NEU], [null], [SMY], [GEN]] The authors perform a number of experiments on a toy problem (stochastic quadratics) which is amenable to some theoretical analysis as well as a small fully connected network trained on MNIST.[[EXP-NEU], [null], [SMY], [GEN]]  \n\n(side note: I was assigned this paper quite late in the review process, and have not carefully gone through the derivations--specifically Theorems 1 and 2)[[EXT-NEU], [null], [DIS], [GEN]].\n\nThe paper is generally clear and well written.[[OAL-POS], [CLA-POS], [APC], [MAJ]]\n\nMajor comments\n-------------------------\nI was a bit confused why 1000 SGD+mom steps pre-training steps were needed. As far as I can tell, pre-training is not typically done in the other meta-optimization literature? [[RWK-NEU,DAT-NEU,MET-NEG], [CMP-POS], [DIS], [MAJ]] The authors suggest this is needed because \"the dynamics of training are different at the very start compared to later stages\", which is a bit vague. Perhaps the authors can expand upon  this point?[[MET-NEG], [EMP-NEG], [DFT], [MAJ]]\n\nThe conclusion suggests that the difference in greedy vs. fully optimized schedule is due to the curvature (poor scaling) of the objective[[RES-NEU,ANA-NEU], [PNF-NEG], [DIS], [MAJ]]--but Fig 2. and earlier discussion talked about the noise in the objective as introducing the bias (e.g. from earlier in the paper, \"The noise in the problem adds uncertainty to the objective, resulting in failures of greedy schedule\").[[RES-NEG,ANA-NEU], [EMP-NEG], [DIS], [GEN]] Which is the real issue, noise or curvature?[[RES-NEU], [EMP-NEU], [QSN], [MAJ]] Would running the problem on quadratics with different condition numbers be insightful?[[MET-NEU], [EMP-NEU], [QSN], [MAJ]]\n\nMinor comments\n-------------------------\nThe stochastic gradient equation in Sec 2.2.2 is missing a subscript: \"h_i\" instead of \"h\"\[[MET-NEG], [CLA-NEG], [DFT], [MIN]]n\nIt would be nice to include the loss curve for a fixed learning rate and momentum for the noisy quadratic in Figure 2, just to get a sense of how that compares with the greedy and optimized curves.\[[MET-NEU,TNF-NEU], [PNF-NEU], [SUG], [MIN]]n\nIt looks like there was an upper bound constraint placed on the optimized learning rate in Figure 2--is that correct? [[MET-NEU,TNF-NEU], [EMP-NEU], [QSN], [MIN]]I couldn't find a mention of the constraint in the paper. (the optimized learning rate remains at 0.2 for the first ~60 steps)?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nFigure 2 (and elsewhere): I would change 'optimal' to 'optimized' to distinguish it from an optimal curve that might result from an analytic derivation. 'Optimized' makes it more clear that the curve was obtained using an optimization process.[[MET-NEU,RES-NEU,TNF-NEU], [CLA-POS], [SUG], [MIN]]\n\nFigure 2: can you change the line style or thickness so that we can see both the red and blue curves for the deterministic case? [[TNF-NEU], [PNF-NEU], [QSN], [MIN]]I assume the red curve is hiding beneath the blue one--but it would be good to see this explicitly.[[TNF-NEU], [PNF-NEU], [DIS], [MIN]]\n\nFigure 4 is fantastic--it succinctly and clearly demonstrates the problem of truncated unrolls[[TNF-POS], [PNF-POS], [APC], [MIN]]. I would add a note in the caption to make it clear that the SMD trajectories are the red curves, e.g.: \"SMD trajectories (red) during meta-optimization of initial effective ...\".[[TNF-NEG], [PNF-POS], [SUG], [MIN]] I would also change the caption to use \"meta-training losses\" instead of \"training losses\" (I believe those numbers are for the meta-loss, correct?)[[TNF-NEU], [CLA-POS], [SUG,QSN], [MIN]]. Finally, I would add a colorbar to indicate numerical values for the different grayscale values.[[TNF-NEU], [PNF-POS], [SUG], [MIN]]\n\nSome recent references that warrant a mention in the text:\n- both of these learn optimizers using longer numbers of unrolled steps:\nLearning gradient descent: better generalization and longer horizons, Lv et al, ICML 2017\nLearned optimizers that scale and generalize, Wichrowska et al, ICML 2017\n- another application of unrolled optimization:\nUnrolled generative adversarial networks, Metz et al, ICLR 2017[[RWK-NEU,BIB-NEU], [null], [SUG], [MIN]]\n\nIn the text discussing Figure 4 (middle of pg. 8) , \"which is obtained by using...\" should be \"which are obtained by using...[[TNF-NEG], [CLA-NEG], [SUG], [MIN]]\"\n\nIn the conclusion, \"optimal for deterministic objective\" should be \"deterministic objectives\""[[RES-NEU,ANA-NEU], [CLA-NEG], [SUG], [MIN]]