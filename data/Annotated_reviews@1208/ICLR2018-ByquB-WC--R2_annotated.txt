"This paper introduces Related Memory Network (RMN), an improvement over Relationship Networks (RN).[[INT-NEU], [null], [SMY], [GEN]] RMN avoids growing the relationship time complexity as suffered by RN (Santoro et. Al 2017).[[RWK-NEU,MET-POS,BIB-NEU], [null], [SMY], [GEN]] RMN reduces the complexity to linear time for the bAbi dataset.[[DAT-NEU,MET-POS], [null], [SMY], [GEN]] RN constructs pair-wise interactions between objects in RN to solve complex tasks such as transitive reasoning.[[RWK-NEU], [null], [SMY], [GEN]] RMN instead uses a multi-hop attention over objects followed by an MLP to learn relationships in linear time.[[MET-NEU], [EMP-NEU], [SMY], [GEN]]\n\nComments for the author:\n\nThe paper addresses an important problem since understanding object interactions are crucial for reasoning.[[PDI-POS], [EMP-POS], [APC], [MAJ]] However, how widespread is this problem across other models or are you simply addressing a point problem for RN?[[MET-NEU], [EMP-NEU], [QSN], [MAJ]] For example, Entnet is able to reason as the input is fed in and the decoding costs are low. [[RWK-NEU], [CMP-NEU], [SMY], [GEN]]Likewise, other graph-based networks (which although may require strong supervision) are able to decode quite cheaply.[[RWK-NEU], [CMP-NEU], [SMY], [GEN]] \n\nThe relationship network considers all pair-wise interactions that are replaced by a two-hop attention mechanism (and an MLP).[[MET-NEU], [EMP-NEU], [SMY], [GEN]] It would not be fair to claim superiority over RN since you only evaluate on bABi while RN also demonstrated results on other tasks.[[RWK-NEU,MET-NEU], [CMP-NEG,EMP-NEU], [CRT], [MAJ]] For more complex tasks (even over just text), it is necessary to show that you outperform RN w/o considering all objects in a pairwise fashion.[[RWK-NEU,MET-NEU], [CMP-NEU,EMP-NEU], [SUG], [MAJ]] More specifically, RN uses an MLP over pair-wise interactions, does that allow it to model more complex interactions than just selecting two hops to generate attention weights.[[MET-NEU], [EMP-NEU], [QSN], [MAJ]] Showing results with multiple hops (1,2,..) would be useful here.[[RES-NEU], [null], [SUG], [MAJ]]\n\nMore details are needed about Figure 3.[[TNF-NEU], [null], [DFT], [MIN]] Is this on bAbi as well?[[DAT-NEU,MET-NEU], [null], [QSN], [MIN]] How did you generate these stories with so many sentences?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Another clarification is the bAbi performance over Entnet which claims to solve all tasks.[[DAT-NEU,MET-NEU], [CMP-NEU,EMP-NEU], [SMY], [GEN]] Your results show 4 failed tasks, is this your reproduction of Entnet?[[MET-NEU,RES-NEU], [EMP-NEU], [QSN], [MIN]]\n\nFinally, what are the savings from reducing this time complexity?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Some wall clock time results or FLOPs of train/test time should be provided since you use multiple hops.[[MET-NEU], [EMP-NEU], [SUG], [MIN]]\n\nOverall, this paper feels like a small improvement over RN.[[MET-NEU], [EMP-NEU], [SMY], [MAJ]] Without experiments over other datasets and wall clock time results, it is hard to appreciate the significance of this improvement.[[DAT-NEG,EXP-NEG], [SUB-NEG,IMP-NEU], [DFT], [MAJ]] One direction to strengthen this paper is to examine if RMN can do better than pair-wise interactions (and other baselines) for more complex reasoning tasks.[[EXP-NEU,MET-NEU], [CMP-NEU,EMP-NEU], [SUG], [MAJ]]\n\n"