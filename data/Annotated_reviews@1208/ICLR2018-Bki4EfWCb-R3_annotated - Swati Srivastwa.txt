"This paper studies the amortization gap in VAEs.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] Inference networks, in general, have two sources of approximation errors.[[INT-NEU], [null], [SMY], [GEN]] One due to the function family of variational posterior distributions used in inference and the other due to choosing to amortize inference rather than doing per-data-point inference as in SVI.[[RWK-NEU], [null], [DIS], [MIN]]\n\nThey consider learning VAEs using two different choices of inference networks with (1) fully factorized Gaussian and (2) normalizing flows.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] The former is the de-facto choice of variational approximation used in VAEs and the latter is capable of expressing complex multi-modal distributions.[[MET-NEU], [null], [DIS], [MIN]]\n\nThe inference gap is log p(x) - L[q], the approximation gap is log p(x) - L[q^* ] and the amortization gap is L[q^* ] - L[q]. The amortization gap is easily evaluated.[[MET-NEU], [null], [DIS], [MIN]] To evaluate the first two, the authors use estimates (lower bounds of log p(x)) given from annealed importance sampling and the importance sampling based IWAE bound (the tighter of the two is used).[[MET-NEU], [null], [SMY], [GEN]]\n\nThere are several different observations made via experiments in this work but one of the more interesting ones is quantifying that a deep generative model, when trained with a fully factorized gaussian posterior, realizes a true posterior distribution that is (more) approximately Gaussian.[[EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]] While this might be (known) intuition that people rely on when learning deep generative models, it is important to be able to test it, as this paper does.[[EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]] The authors study several discrete questions about the aforementioned inference gaps and how they vary on MNIST and FashionMNIST.[[DAT-NEU,MET-NEU], [null], [DIS], [MAJ]] The concerns I have about this work revolve around their choice of two small datasets and how much their results are affected by variance in the estimators.[[DAT-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nQuestions:\n* How did you optimize the variational parameters for q^* and the flow parameters in terms of learning rate, stopping criteria etc.[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n* In Section 5.2, what is \"strong inference\"?[[CNT], [null], [QSN], [MIN]] This is not defined previously.[[CNT], [null], [DIS], [MIN]] \n* Have you evaluated on a larger dataset such as CIFAR?[[DAT-NEU], [SUB-NEU], [QSN], [MIN]]  FashionMNIST and MNIST are similar in many ways.[[DAT-NEU], [null], [DIS], [MIN]] \n* Which kind of error would using a convolution architecture for the encoder decrease?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]  Do you have insights on the role played by the architecture of the inference network and generative model?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nI have two specific concerns:\n* Did you perform any checks to verify whether the variance in the estimators use to bound log p(x) is controlled (for the specific # samples you use)?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] I'm concerned since the evaluation is only done on 100 points.[[DAT-NEG], [SUB-NEG], [CRT], [MIN]]\n* In Section 5.2.1, L_iw is used to characterize encoder overfitting where the argument is that L_ais is not a function of the encoder, but L_iw is, and so the difference between the two summarizes how much the inference network has overfit.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] How is L_iw affected by the number of samples used in the estimator?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]] Presumably this statement needs to be made while also keeping mind the number of importance samples.[[MET-NEU,ANA-NEU], [null], [DIS], [MIN]] For example, if I increase the number of importance samples, even if I'm overfitting in Fig 3(b), wouldn't the green line move towards the red simply because my estimator depends less on a poor q?[[MET-NEU,TNF-NEU], [EMP-NEU], [QSN], [MIN]]\n\nOverall, I think this paper is interesting and presents a quantitative analysis of where the errors accrue due to learning with inference networks.[[ANA-POS,OAL-POS], [EMP-POS], [APC], [MAJ]] The work can be made stronger by addressing some of the questions above such as what role is played by the neural architecture and whether the results hold up under evaluation on a larger dataset."[[DAT-NEU,MET-NEU,RES-NEU], [EMP-NEU], [QSN], [MIN]]