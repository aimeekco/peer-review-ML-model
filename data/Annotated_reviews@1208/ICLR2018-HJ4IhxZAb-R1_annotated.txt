"The approach solves an important problem as getting labelled data is hard.[[RWK-POS,DAT-NEU], [null], [APC], [MAJ]] The focus is on the key aspect, which is generalisation across heteregeneous data.[[RWK-NEU,PDI-NEU,DAT-NEU], [null], [SMY], [GEN]] The novel idea is the dataset embedding so that their RL policy can be trained to work across diverse datasets.\n\nPros: \n1.[[PDI-POS,DAT-POS], [NOV-POS], [APC], [MAJ]] The approach performs well against all the baselines, and also achieves good cross-task generalisation in the tasks they evaluated on.[[PDI-POS,DAT-POS,EXP-POS,ANA-POS], [SUB-POS,IMP-POS], [APC], [MAJ]] \n2. In particular, they alsoevaluated on test datasets with fairly different statistics from the training datasets,[[RWK-NEU,DAT-NEU], [null], [DIS], [GEN]] which isnt very common in most meta-learning papers today, so it\u2019s encouraging that the method works in that regime.\n[[RWK-POS,PDI-POS,MET-POS], [NOV-POS,SUB-POS,EMP-POS], [APC], [MAJ]]\nCons: \n1. The embedding strategy, especially the representative and discriminative histograms, is complicated.[[RWK-NEG,MET-NEG], [IMP-NEG,EMP-NEG], [DFT], [MIN]] It is unclear if the strategy is general enough to work on harder problems / larger datasets, or with higher dimensional data like images[[RWK-NEG,DAT-NEU], [null], [DFT], [MIN]]. More evidence in the paper for why it would work on harder problems would be great.[[INT-POS,PDI-POS,DAT-POS], [EMP-POS], [APC], [MAJ]] \n2. The policy network would have to output a probability for each datapoint in the dataset U,[[DAT-NEU,MET-NEG], [EMP-NEG], [DFT], [MIN]] which could be fairly large, thus the method is computationally much more expensive than random sampling.[[DAT-NEU,MET-NEG], [EMP-NEG], [DFT], [MIN]] A section devoted to showing what practical problems could be potentially solved by this method would be useful.[[RWK-NEU,MET-POS], [EMP-POS], [APC], [MAJ]]\n3. It is unclear to me if the results in table 3 and 4 are achieved by retraining from scratch with an RBF SVM, [[RWK-NEG,MET-NEG,RES-NEG,TNF-NEG], [CLA-NEG,EMP-NEG], [DFT], [MIN]]or by freezing the policy network trained on a linear SVM and directly evaluating it with a RBF SVM base learner.[[RWK-NEG,MET-NEU,ANA-NEU], [EMP-NEU], [SMY,DFT], [MIN]]\n\nSignificance/Conclusion: The idea of meta-learning or learning to learn is fairly common now[[RWK-NEU,PDI-NEU,EXP-NEU], [null], [DIS], [GEN]]. While they do show good performance,[[RWK-POS,RES-POS], [SUB-POS], [APC], [MAJ]] it\u2019s unclear if the specific embedding strategy suggested in this paper will generalise to harder tasks[[RWK-NEG,MET-NEG], [CLA-NEG,EMP-NEG], [DFT], [MIN]]. \n\nComments: There\u2019s lots of typos, please proof read to improve the paper.[[RWK-NEG], [PNF-NEG], [DFT], [MIN]]\n\nRevision: I thank the authors for the updates and addressing some of my concerns.[[RWK-POS], [APR-POS,REC-POS], [APC,FBK], [MAJ]] I agree the computational budget makes sense for cross data transfer,[[RWK-POS,DAT-POS,EXP-POS], [EMP-POS], [APC], [MAJ]] however the embedding strategy and lack of larger experiments makes it unclear if it'll generalise to harder tasks[[RWK-NEG,EXP-NEG,MET-NEG], [EMP-NEG], [DFT], [MIN]]. I update my review to 6[[RWK-NEG,EXT-NEG], [REC-NEG], [DFT,FBK], [MIN]]. "