"This paper suggests a simple yet effective approach for learning with weak supervision.[[MET-POS], [EMP-POS], [SMY,APC], [GEN]] This learning scenario involves two datasets, one with clean data (i.e., labeled by the true function) and one with noisy data, collected using a weak source of supervision.[[DAT-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]]   The suggested approach assumes a teacher and student networks, and builds the final representation incrementally, by taking into account the \"fidelity\" of the weak label when training the student at the final step.[[MET-NEU], [null], [SMY], [GEN]]  The fidelity score is given by the teacher, after being trained over the clean data, and it's used to build a cost-sensitive loss function for the students.[[DAT-NEU,EXP-NEU], [null], [SMY], [MIN]]  The suggested method seems to work well on several document classification tasks.[[MET-POS], [EMP-POS], [APC], [MAJ]]  \n\nOverall, I liked the paper.[[OAL-POS], [EMP-POS], [APC], [MAJ]]  I would like the authors to consider the following questions - \n\n- Over the last 10 years or so, many different frameworks for learning with weak supervision were suggested (e.g., indirect supervision, distant supervision, response-based, constraint-based, to name a few).[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [GEN]]  First, I'd suggest acknowledging these works and discussing the differences to your work.[[RWK-NEU], [CMP-POS], [APC], [MIN]] Second - Is your approach applicable to these frameworks?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]  It would be an interesting to compare to one of those methods  (e.g., distant supervision for relation extraction using a knowledge base), and see if by incorporating fidelity score, results improve.[[RWK-NEU,MET-NEU,RES-NEU], [CMP-NEU], [SUG], [MIN]] \n\n- Can this approach be applied to semi-supervised learning?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Is there a reason to assume the fidelity scores computed by the teacher would not improve the student in a self-training framework?[[MET-NEU,RES-NEU], [EMP-NEU], [QSN], [MIN]]\n\n- The paper emphasizes that the teacher uses the student's initial representation, when trained over the clean data. [[DAT-NEU,MET-NEU], [EMP-NEU], [DIS], [MAJ]] Is it clear that this step in needed? [[MET-NEU], [SUB-NEU], [QSN], [MIN]]Can you add an additional variant of your framework when the fidelity score are  computed by the teacher when trained from scratch?using different architecture than the student?[[MET-NEU,ANA-NEU], [EMP-NEU], [QSN], [MIN]] \n \n - I went over the authors comments and I appreciate their efforts to help clarify the issues raised."[[EXT-POS], [CNT], [APC], [GEN]] 