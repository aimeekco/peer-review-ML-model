"The paper presents an extensive framework for complex-valued neural networks.[[INT-NEU], [null], [SMY], [GEN]] Related literature suggests a variety of motivations for complex valued neural networks: biological evidence, richer representation capacity, easier optimization, faster learning, noise-robust memory retrieval mechanisms and more.[[RWK-POS], [null], [SMY], [GEN]] \n\nThe contribution of the current work does not lie in presenting significantly superior results, compared to the traditional real-valued neural networks, but rather in developing an extensive framework for applying and conducting research with complex-valued neural networks.[[RWK-NEU,MET-POS], [CMP-NEU,EMP-POS], [APC], [MAJ]] Indeed, the most standard work nowadays with real-valued neural networks depends on a variety of already well-established techniques for weight initialization, regularization, activation function, convolutions, etc.[[RWK-POS,MET-POS], [null], [SMY], [GEN]] In this work, the complex equivalent of many of these basics tools are developed, such as a number of complex activation functions, complex batch normalization, complex convolution, discussion of complex differentiability, strategies for complex weight initialization, complex equivalent of a residual neural network.[[RWK-NEU,MET-POS], [NULL], [SMY], [MAJ]] \n\nEmpirical results show that the new complex-flavored neural networks achieve generally comparable performance to their real-valued counterparts, on a variety of different tasks.[[MET-POS], [CMP-POS,EMP-POS], [DIS], [MAJ]] Then again, the major contribution of this work is not advancing the state-of-the-art on many benchmark tasks,[[MET-NEU], [EMP-NEU], [DFT], [MAJ]] but constructing a solid framework that will enable stable and solid application and research of these well-motivated models.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] \n"