"Summary: \nThe paper considers the problem of a single hidden layer neural network, with 2 RELU units (this is what I got from the paper - as I describe below, it was not clear at all the setting of the problem[[INT-NEU], [null], [SMY], [GEN]] - if I'm mistaken, I will also wait for the rest of the reviews to have a more complete picture of the problem).[[EXT-NEU], [null], [DIS], [GEN]]\nGiven this architecture, the authors focus on characterizing the objective landscape of such a problem.[[PDI-NEU], [null], [SMY], [GEN]]\nThe techniques used depend on previous work.[[RWK-NEU,PDI-NEU], [null], [SMY], [GEN]] According to the authors, this paper extends(?) previous results on a NN with a single layer with a single unit.[[RWK-NEU,RES-NEU], [null], [SMY], [GEN]]\n\nOriginality: \nThe paper heavily depends on the approach followed by Brutzkus and Globerson, 2017.[[RWK-NEU,MET-NEU], [NOV-NEU], [DIS], [MAJ]] To this end, slighly novel.[[MET-NEU], [NOV-NEU], [APC], [MIN]]\n\nImportance: \nUnderstanding the landscape (local vs global minima vs saddle points) is an important direction in order to further understand when and why deep neural networks work.[[RES-POS], [EMP-POS], [APC], [MAJ]] I would say that the topic is an important one.[[PDI-POS], [EMP-POS], [CRT], [MAJ]]\n\nPresentation/Clarity: \nTo the best of my understanding, the paper has some misconceptions.[[OAL-NEG], [CLA-NEU], [CRT], [MIN]] The title is not clear whether the paper considers a two layer RELU network or a single layer with with two RELU units.[[INT-NEG], [CLA-NEG], [CRT], [MIN]] In the abstract the authors state that it has to do with a two-layer RELU network with two hidden units (per layer? in total?).[[ABS-NEU], [CLA-NEU], [DIS], [MIN]] Later on, in Section 3, the expression at the bottom of page 2 seems to consider a single-layer RELU network, with two units.[[MET-NEU], [null], [DIS], [GEN]] \nThese are crucial for understanding the contribution of the paper; while reading the paper, I assumed that the authors consider the case of a single hidden unit with K = 2 RELU activations (however, that complicated my understanding on how it compares with state of the art).[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MIN]]\n\nAnother issue is the fact that, on my humble opinion, the main text looks like a long proof.[[OAL-NEG], [SUB-NEG], [CRT], [MIN]] It would be great to have more intuitions.[[OAL-NEU], [SUB-NEU], [CRT], [MIN]]\n\nComments:\n1. The paper mainly focuses on a specific problem instance, where the weight vectors are unit-normed and orthogonal to each other.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] While the authors already identify that this might be a restriction, it still does not lessen the fact that the configuration considered is a really specific one.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n\n2. The paper reads like a collection of lemmas, with no verbose connection.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] It was hard to read and understand their value, just because mostly the text was structured as one lemma after the other.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MIN]]\n\n3. It is not clear from the text whether the setting is already considered in Brutzkus and Globerson, 2017.[[RWK-NEG], [CMP-NEG], [CRT], [MAJ]] Please clarify how your work is different/new from previous works.\n"[[RWK-NEG], [CMP-NEG], [CRT], [MAJ]]