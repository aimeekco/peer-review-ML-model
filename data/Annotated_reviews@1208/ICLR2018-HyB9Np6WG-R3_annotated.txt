"This paper proposes to learn vector representations of prepositions by learning them as tensor decompositions of a triple of a left word (maybe head), the preposition, and right word (maybe dependent). [[INT-NEU,PDI-NEU], [null], [SMY], [GEN]]This is an interesting idea with linguistic validity, and practically possible because of the commonness and promiscuity of prepositions, reflecting their primary grammatical and relational roles (as function words not content words).[[PDI-POS], [EMP-POS], [APC], [GEN]] The resulting representations are show to be useful \u2013 they produce SOTA results on preposition selection by a decent margin (on a practically useful and recently studied tasks that is arguably the task that best reflects on the quality of the learned representations) and good (but not quite SOTA) results, without further linguistic features beyond POS tags, on the much more studied task of preposition attachment disambiguation.[[RWK-POS,RES-POS], [EMP-POS], [APC], [MAJ]] Overall, I really liked this paper.[[OAL-POS], [null], [APC], [MAJ]]\n\nDespite this enthusiasm, I am doubtful whether this is a good paper for ICLR.[[OAL-NEG], [APR-NEG], [CRT], [MAJ]] And I write this as a card-carrying computational linguist.[[OAL-NEU], [null], [DIS], [MIN]] This is partly because of the writing.[[OAL-NEU], [CLA-NEU], [DIS], [MIN]] It is very hard not to see the content of the introduction as addressing a linguistics/computational linguistics audience rather than the mainstream of the ICLR audience (you get this impression rather strongly from the start of each of the first 3 paragraphs of the introduction...).[[INT-NEG], [APR-NEG], [CRT], [MAJ]] More profoundly, this impression comes from the nature of the investigation and results.[[RES-NEG], [EMP-NEG], [CRT], [MIN]] While this paper makes a contribution to representation learning in suggesting a good way to learn a representation for prepositions, it does not make any contributions to methods of representation learning.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]Indeed, it is basically an application of the orthogonalized alternating least squares method of Sharan and Valiant (2017) and more generally of the tensor decomposition ideas of numerous papers of Anandkumar and colleagues.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] There aren't any new technical ideas here.[[MET-NEG], [SUB-NEG], [DFT,CRT], [MAJ]]\n\nWhile the learned representations are successful for the two main performance tasks discussed above,[[RES-POS], [EMP-POS], [APC], [MAJ]] the ancillary evidence provided from the paraphrasing of prepositional phrase seems highly uncompelling to me.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] That is, the task seems a completely valid one \u2013 one would like to be able to show that \"sparked off\" is a synonym of \"provoked\", but the actually results provided on this task seem quite uncompelling.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] Among other things, the example used in the text in section 3 seems bad to me.[[CNT], [EMP-NEG], [CRT], [MIN]] It isn't really the case that \"split off something\" means \"divided something\".  (\"Sally split off a sliver of wood\" does not mean \"Sally divided a sliver of wood\".[[MET-NEG], [EMP-NEG], [CRT], [MIN]]  \"separated\" would be much closer. [[MET-NEG], [EMP-NEG], [CRT], [MIN]]Indeed, of the examples in Table 2, the first 3 look bad, the fourth isn't generally  true but valid in certain contexts, the 5th is again wrong [[TNF-NEG], [EMP-NEG], [CRT], [MIN]]and only the 6th is really good.[[RES-POS], [EMP-POS], [APC], [MAJ]] Similar remarks for the many more examples in the supplementary materials.[[CNT], [null], [DIS], [MIN]]\n\nThe most intriguing question is the one raised in the first paragraph of the conclusion: While prepositions are natural for modeling via word triples and indeed their high frequency and small number of types makes this quite practical, the kind of concerns raised here are also applicable to a whole bunch of word types, and it would be natural to want to extend the method to them.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]  E.g., we would also like to learn synonymy with light verb like \"take note\" or \"pay attention\" means roughly \"notice\" or \"observe\"; or the widely studied SVO triples like <rock,sank,ship> would also seem to cry out for a tensor decomposition.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] It would be interesting to think about what further might be done here.[[FWK-NEU], [IMP-NEU], [DIS], [MIN]]\n\n\nMinor comments:\n - Abstract: saying that word2vec and GloVe treat prepositions as \"content words\" seems slightly wrong; really they treat them just as \"words\" since all words are treated the same \u2013 though one can argue that most words are content words and the method of modeling word meaning is generally much more appropriate for content words.[[ABS-NEG], [CLA-NEG], [CRT], [MIN]]\n - p.2: \"folklore within the NLP community\". I'm not sure whether this is true or not; while pairwise counts have been the method of choice in recent word vector learning methods, it wasn't true of older methods (Collobert and Weston or Bengio's NPLM) and n-gram counts for n > 2 are widespread in pre-neural NLP.[[RWK-NEG,MET-NEG], [CMP-NEG], [CRT], [MIN]] At any rate, it is rather unconvincing when the only evidence you cite is the paper by Sharan and Valiant, where AFAIK, neither author has ever published an NLP paper or attended  an NLP conference....\n - p.5 FEC should be FCE; Ng et al. (2014) should be (Ng et al., 2014)\n\n\n"[[RWK-NEG,RES-NEG,BIB-NEU], [EMP-NEG], [CRT], [MIN]]