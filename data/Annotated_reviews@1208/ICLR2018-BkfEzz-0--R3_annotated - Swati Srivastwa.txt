"The authors consider a Neural Network where the neurons are treated as rational agents.[[PDI-NEU], [null], [SMY], [GEN]] In this model, the neurons must pay to observe the activation of neurons upstream.[[PDI-NEU], [null], [SMY], [GEN]] Thus, each individual neuron seeks to maximize the sum of payments it receives from other neurons minus the cost for observing the activations of other neurons (plus an external reward for success at the task).[[PDI-NEU], [null], [SMY], [GEN]]  \n\nWhile this is an interesting idea on its surface,[[PDI-POS], [EMP-POS], [APC], [MAJ]] the paper suffers from many problems in clarity, motivation, and technical presentation.[[OAL-NEG], [CLA-NEG,PNF-NEG,EMP-NEG], [CRT], [MIN]] It would require very major editing to be fit for publication.[[OAL-NEU], [CLA-NEU,PNF-NEU], [SUG], [MIN]] \n\nThe major problem with this paper is its clarity.[[OAL-NEG], [CLA-NEG], [CRT], [MIN]] See detailed comments below for problems just in the introduction. [[INT-NEU], [null], [DIS], [GEN]]More generally, the paper is riddled with non sequiturs.[[OAL-NEU], [null], [DIS], [GEN]] The related work section mentions Generative Adversarial Nets.[[RWK-NEU], [null], [SMY], [GEN]] As far as I can tell, this paper has nothing to do with GANs.[[MET-NEU], [null], [DIS], [MIN]] The Background section introduces notation for POMDPs, never to be used again in the entirety of the paper, before launching into a paragraph about apoptosis in glial cells.[[PDI-NEG], [EMP-NEG], [CRT], [MIN]] \n\nThere is also a general lack of attention to detail.[[ANA-NEG], [SUB-NEG], [DFT], [MAJ]] For example, the entire network receives an external reward (R_t^{ex}), presumably for its performance on some task.[[ANA-NEU,RES-NEU], [EMP-NEU], [DIS], [MIN]] This reward is dispersed to the the individual agents who receive individual external rewards (R_{it}^{ex}).[[MET-NEU], [null], [DIS], [MIN]] It is never explained how this reward is allocated even in the authors\u2019 own experiments.[[EXP-NEG,ANA-NEG], [SUB-NEG], [CRT], [MAJ]] The authors state that all units playing NOOP is an equilibrium.[[MET-NEU], [null], [DIS], [GEN]] While this is certainly believable/expected, such a result would depend on the external rewards R_{it}^{ex}, the observation costs \\sigma_{jit}, and the network topology.[[RES-NEU], [EMP-NEU], [DIS], [MIN]] None of this is discussed.[[RES-NEG,ANA-NEG], [SUB-NEG], [DFT], [MAJ]] The authors discuss Pareto optimality without ever formally describing what multi-objective function defines this supposed Pareto boundary.[[MET-NEU], [null], [DIS], [GEN]] This is pervasive throughout the paper, and is detrimental to the reader\u2019s understanding.[[MET-NEU], [EMP-NEU], [SUG], [MIN]]  \n\nWhile this might be lost because of the clarity problems described above, the model itself is also never really motivated.[[MET-NEG], [CLA-NEG,EMP-NEG], [CRT], [MIN]] Why is this an interesting problem?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] There are many ways to create rational incentives for neurons in a neural net.[[MET-NEU], [null], [DIS], [GEN]] Why is paying to observe activations the one chosen here?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] The neuroscientific motivation is not very convincing to me, considering that ultimately these neurons have to hold an auction.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Is there an economic motivation?[[CNT], [CNT], [QSN], [MIN]] Is it just a different way to train a NN?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]] \n\nDetailed Comments:\n\u201cIn the of NaaA\u201d => remove \u201cof\u201d?[[CNT], [null], [QSN], [MIN]]\n\u201cpassing its activation to the unit as cost\u201d => Unclear.[[CNT], [null], [CRT], [MAJ]] What does this mean?[[CNT], [null], [QSN], [MIN]]\n\u201cperformance decreases if we naively consider units as agents\u201d => Performance on what?[[RES-NEU], [EMP-NEU], [QSN], [MIN]]\n\u201c.. we demonstrate that the agent obeys to maximize its counterfactual return as the Nash Equilibrium\u201c => Perhaps, this should be rewritten as \u201cAgents maximize their counterfactual return in equilibrium.[[MET-NEG], [CLA-NEG], [SUG], [MIN]] \n\u201cSubsequently, we present that learning counterfactual return leads the model to learning optimal topology\u201d => Do you mean \u2028\u201cmaximizing\u201d instead of learning.[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]] Optimal with respect to what task?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\u201cpure-randomly\u201d => \u201crandomly\u201d\n \u201cwith adaptive algorithm\u201d => \u201cwith an adaptive algorithm\u201d\n\u201cthe connection\u201d => \u201cconnections\u201d[[MET-NEG], [CLA-NEG], [CRT], [MIN]]\n\u201cIn game theory, the outcome maximizing overall reward is named Pareto optimality.[[MET-NEU], [null], [DIS], [GEN]]\u201d => This is simply incorrect. "[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]