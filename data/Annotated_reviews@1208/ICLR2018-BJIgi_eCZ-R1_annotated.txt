"The paper first analyzes recent works in machine reading comprehension (largely centered around SQuAD), and mentions their common trait that the attention is not \"fully-aware\" of all levels of abstraction, e.g. word-level, phrase-level, etc.[[RWK-NEU], [null], [SMY], [GEN]] In turn, the paper proposes a model that performs attention at all levels of abstraction, which achieves the state of the art in SQuAD.[[DAT-NEU,RWK-POS], [EMP-POS], [APC], [MAJ]] They also propose an attention mechanism that works better than others (Symmetric + ReLU).[[MET-NEU], [null], [SMY], [GEN]]\n\nStrengths:\n- The paper is well-written and clear.[[OAL-POS], [CLA-POS], [APC], [MAJ]]\n- I really liked Table 1 and Figure 2; it nicely summarizes recent work in the field.[[TNF-POS], [PNF-POS], [APC], [MAJ]]\n- The multi-level attention is novel and indeed seems to work, with convincing ablations.[[MET-POS,RES-POS], [NOV-POS,EMP-POS], [APC], [MAJ]]\n- Nice engineering achievement, reaching the top of the leaderboard (in early October).[[OAL-POS], [EMP-POS], [APC], [MAJ]]\n\n\nWeaknesses:\n- The paper is long (10 pages) but relatively lacks substances.[[OAL-NEG], [SUB-NEG], [DFT,CRT], [MAJ]] Ideally, I would want to see the visualization of the attention at each level (i.e. how they differ across the levels) and also possibly this model tested on another dataset (e.g. TriviaQA).[[DAT-NEG,MET-NEU], [SUB-NEG,EMP-NEU], [DIS,CRT], [MIN]]\n- The authors claim that the symmetric + ReLU is novel, but  I think this is basically equivalent to bilinear attention [1] after fully connected layer with activation, which seems quite standard.[[MET-NEG], [NOV-NEG], [CRT], [MAJ]] Still useful to know that this works better, so would recommend to tone down a bit regarding the paper's contribution.[[MET-NEU], [EMP-NEU], [SUG], [MIN]]\n\n\nMinor:\n- Probably figure 4 can be drawn better.[[TNF-NEG], [PNF-NEG], [SUG,CRT], [MIN]] Not easy to understand nor concrete.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]]\n- Section 3.2 GRU citation should be Cho et al. [2].[[RWK-NEG], [PNF-NEG], [CRT], [MIN]]\n\n\nQuestions:\n- Contextualized embedding seems to give a lot of improvement in other works too.[[RWK-NEU,MET-NEU], [EMP-NEU], [DIS], [GEN]] Could you perform ablation without contextualized embedding (CoVe)?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\n\nReference:\n[1] Luong et al. Effective Approaches to Attention-based Neural Machine Translation.[[BIB-NEU], [null], [DIS], [GEN]] EMNLP 2015.\n[2] Cho et al. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. EMNLP 2014."[[BIB-NEU], [null], [DIS], [GEN]]