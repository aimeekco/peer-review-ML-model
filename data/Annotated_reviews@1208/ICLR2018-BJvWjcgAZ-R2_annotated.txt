"The authors propose a simple modification to the DQN algorithm they call Episodic Backward Update.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The algorithm selects transitions in a backward order fashion from end of episode to be more effective in propagating learning of new rewards.[[PDI-NEU], [null], [SMY], [GEN]]  This issue of fast propagation of updates is a common theme in RL (cf eligibility traces, prioritised sweeping, and more recently DQN with prioritised replay etc.).[[MET-NEU], [null], [DIS], [MIN]]  Here the proposed update applies the max Bellman operator recursively on a trajectory (unsure whether this is novel), with some decay to prevent accumulating errors with the nested max.[[MET-POS], [EMP-POS], [APC], [MAJ]] \n\nThe paper is written in a clear way.[[OAL-POS], [CLA-POS], [APC], [MAJ]]   The proposed approach seems reasonable, but I would have guessed that prioritized replay would also naturally sample transitions in roughly that order - given that TD-errors would at first be higher towards the end of an episode and progress backwards from there.[[MET-POS], [EMP-POS], [APC], [MAJ]]  I think this should have been one of the baselines to compare to for that reason.[[RWK-NEU], [CMP-NEU], [DIS], [MIN]] \n\nThe experimental results seem promising in the illustrative MNIST domain.[[EXP-POS,RES-POS], [EMP-POS], [APC], [MAJ]]  Atari results seem decent, especially given that experiments are limited to 10M frames, though the advantage compared to the related approach of optimality tightening is not obvious. \n"[[DAT-POS,EXP-POS,RES-POS], [EMP-POS], [APC], [MAJ]] 