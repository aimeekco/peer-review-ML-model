"This paper studies learning to play two-player general-sum games with state (Markov games) with imperfect information.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The idea is to learn to cooperate (think prisoner's dilemma) but in more complex domains. Generally, in repeated prisoner's dilemma, one can punish one's opponent for noncooperation.[[PDI-NEU], [null], [SMY], [GEN]] In this paper, they design an apporach to learn to cooperate in a more complex game, like a hybrid pong meets prisoner's dilemma game.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] This is fun but I did not find it particularly surprising from a game-theoretic or from a deep learning point of view.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] \n\nFrom a game-theoretic point of view, the paper begins with a game-theoretic analysis of a cooperative strategy for these markov games with imperfect information.[[PDI-NEG,ANA-NEG], [EMP-NEG], [CRT], [MAJ]] It is basically a straightforward generalization of the idea of punishing, which is common in \"folk theorems\" from game theory, to give a particular equilibrium for cooperating in Markov games.[[PDI-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]] Many Markov games do not have a cooperative equilibrium, so this paper restricts attention to those that do.[[RWK-NEU], [CMP-NEU], [DIS], [MIN]] Even in games where there is a cooperative solution that maximizes the total welfare, it is not clear why players would choose to do so.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] When the game is symmetric, this might be \"the natural\" solution but in general it is far from clear why all players would want to maximize the total payoff.[[MET-NEU], [null], [SMY], [GEN]] \n\nThe paper follows with some fun experiments implementing these new game theory notions.[[EXP-NEU], [null], [SMY], [GEN]] Unfortunately, since the game theory was not particularly well-motivated, I did not find the overall story compelling.[[OAL-NEG], [EMP-NEG], [CRT], [MIN]] It is perhaps interesting that one can make deep learning learn to cooperate with imperfect information, but one could have illustrated the game theory equally well with other techniques.[[MET-POS], [EMP-POS], [APC], [MAJ]]\n\nIn contrast, the paper \"Coco-Q: Learning in Stochastic Games with Side Payments\" by Sodomka et. al. is an example where they took a well-motivated game theoretic cooperative solution concept and explored how to implement that with reinforcement learning.[[PDI-NEU,MET-NEU], [null], [DIS], [MIN]] I would think that generalizing such solution concepts to stochastic games and/or deep learning might be more interesting.[[MET-NEU], [EMP-NEU], [SUG], [MIN]]\n\nIt should also be noted that I was asked to review another ICLR submission entitled \"MAINTAINING COOPERATION IN COMPLEX SOCIAL DILEMMAS USING DEEP REINFORCEMENT LEARNING\" which amazingly introduced the same \"Pong Player\u2019s Dilemma\" game as in this paper.[[RWK-NEU], [null], [DIS], [GEN]] \n\nNotice the following suspiciously similar paragraphs from the two papers:\n\nFrom \"MAINTAINING COOPERATION IN COMPLEX SOCIAL DILEMMAS USING DEEP REINFORCEMENT LEARNING\":\nWe also look at an environment where strategies must be learned from raw pixels. [[RWK-NEU], [null], [DIS], [GEN]]We use the method\nof Tampuu et al. (2017) to alter the reward structure of Atari Pong so that whenever an agent scores a\npoint they receive a reward of 1 and the other player receives \u22122.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MIN]] We refer to this game as the Pong\nPlayer\u2019s Dilemma (PPD).[[RWK-NEU], [null], [DIS], [GEN]] In the PPD the only (jointly) winning move is not to play.[[RWK-NEU], [null], [DIS], [GEN]] However, a fully\ncooperative agent can be exploited by a defector.[[MEt-NEU], [EMP-NEU], [DIS], [MIN]]\n\nFrom \"CONSEQUENTIALIST CONDITIONAL COOPERATION IN SOCIAL DILEMMAS WITH IMPERFECT INFORMATION\":\nTo demonstrate this we follow the method of Tampuu et al. (2017) to construct a version of Atari Pong \nwhich makes the game into a social dilemma.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [GEN]] In what we call the Pong Player\u2019s Dilemma (PPD) when an agent \nscores they gain a reward of 1 but the partner receives a reward of \u22122.[[MET-NEU], [null], [DIS], [MIN]] Thus, in the PPD the only (jointly) winning\nmove is not to play, but selfish agents are again tempted to defect and try to score points even though\nthis decreases total social reward.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] We see that CCC is a successful, robust, and simple strategy in this\ngame."[[MET-POS], [EMP-POS], [APC], [MAJ]]