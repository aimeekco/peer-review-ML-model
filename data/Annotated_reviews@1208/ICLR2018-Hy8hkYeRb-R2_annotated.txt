"The paper \"A Deep Predictive Coding Network for Learning Latent Representations\" considers learning of a generative neural network.[[INT-NEU], [null], [SMY], [GEN]] The network learns unsupervised using a predictive coding setup.[[PDI-NEU], [null], [SMY], [GEN]] A subset of the CIFAR-10 image database (1000 images horses and ships) are used for training.[[DAT-NEU,EXP-NEU], [null], [SMY], [GEN]] Then images generated using the latent representations inferred on these images, on translated images, and on images of other objects are shown.[[RES-NEU], [null], [SMY], [GEN]] It is then claimed that the generated images show that the network has learned good latent representations.[[RES-NEU], [null], [SMY], [GEN]]\n\nI have some concerns about the paper, maybe most notably about the experimental result and the conclusions drawn from them.[[EXP-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] The numerical experiments are motivated as a way to \"understand the capacity of the network with regards to modeling the external environment\" (abstract).[[EXP-NEU], [EMP-NEU], [DIS], [GEN]] And it is concluded in the final three sentences of the paper that the presented network \"can infer effective latent representations for images of other objects\" (i.e., of objects that have not been used for training); and further, that \"in this regards, the network is better than most existing algorithms [...]\".[[RES-NEU], [null], [DIS], [GEN]]\n\nI expected the numerical experiments to show results instructive about what representations or what abstractions are learned in the different layers of the network using the learning algorithm and objectives suggested.[[EXP-NEG,MET-NEG,RES-NEG], [SUB-NEG,EMP-NEG], [DFT,CRT], [MIN]] Also some at least quantifiable (if not benchmarked) outcomes should have been presented given the rather strong claims/conclusions in abstract and discussion/conclusion sections.[[MET-NEG,RES-NEG], [SUB-NEG], [DFT], [MIN]] As a matter of fact, all images shown (including those in the appendix) are blurred versions of the original images, except of one single image: Fig. 4 last row, 2nd image (and that is not commented on).[[TNF-NEG], [PNF-NEG], [CRT], [MIN]] If these are the generated images, then some reconstruction is done by the network, fine, but also not unsurprising as the network was told to do so by the used objective function.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] What precisely do we learn here?[[RES-NEU], [EMP-NEU], [QSN], [GEN]] I would have expected the presentation of experimental results to facilitate the development of an understanding of the computations going on in the trained network.[[RES-NEU], [EMP-NEU], [DIS], [MIN]] How can the reader conclude any functioning from these images?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Using the right objective function, reconstructions can also be obtained using random (not learned) generative fields and relatively basic models.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] The fact that image reconstruction for shifted images or new images is evidence for a sophisticated latent representations is, to my mind, not at all shown here.[[MET-NEG], [SUB-NEG], [DFT], [MIN]] What would be a good measure for an \"effective latent representation\" that substantiates the claims made?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] The reconstruction of unseen images is claimed central but as far as I could see, Figures 2, 3, and 4 are not even referred to in the text, nor is there any objective measure discussed.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]] Studying the relation between predictive coding and deep learning makes sense, but I do not come to the same (strong) conclusions as the author(s) by considering the experimental results - and I do not see evidence for a sophisticated latent representation learned by the network.[[EXP-NEG], [SUB-NEG], [DFT], [MIN]] I am not saying that there is none, but I do not see how the presented experimental results show evidence for this.[[EXP-NEG,RES-NEG], [SUB-NEG], [DFT], [MIN]]\n\nFurthermore, the authors stress that a main distinguishing feature of their approach (top of page 3) is that in their network information flows from latent space to observed space (e.g. in contrast to CNNs).[[MET-NEU], [EMP-NEU], [DIS], [MIN]] That is a true statement but also one which is true for basically all generative models, e.g., of standard directed graphical models such as wake-sleep approaches (Hinton et al., 1995), deep SBNs and more recent generative models used in GANs (Goodfellow et al, 2014).[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MIN]] Any of these references would have made a lot of sense.[[BIB-NEG], [SUB-NEG], [DFT], [MIN]]\n\nWith my evaluation I do not want to be discouraging about the general approach.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] But I can not at all give a good evaluation given the current experimental results (unless substantial new evidence which make me evaluate these results differently is provided in a discussion).[[EXP-NEG,RES-NEG], [EMP-NEU], [DIS], [MIN]]\n\n\nMinor:\n\n- no legend for Fig. 1\n\n-notes -> noted\n\nhave focused\n\n\n\n\n"[[TNF-NEG], [PNF-NEG], [CRT], [MIN]]