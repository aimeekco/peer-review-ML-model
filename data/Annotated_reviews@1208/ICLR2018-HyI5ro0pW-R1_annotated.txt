"This is a mostly experimental paper which evaluates the capabilities of neural networks with weight matrices that are block diagonal.[[INT-NEU,EXP-NEU], [null], [DIS], [MIN]]  The authors describe two methods to obtain this structure: (1) enforced during training, (2) enforced through regularization and pruning.[[PDI-NEU], [null], [SMY], [GEN]]  As a second contribution, the authors show experimentally that the random matrix theory can provide a good model of the spectral behavior of the weight matrix when it is large.[[PDI-NEU], [null], [SMY], [GEN]] However, the authors only conjecture as to the potential of this method without describing clear ways of approaching this subject, which somewhat lessens the strength of their argument.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n\nQuality: this paper is of good quality[[OAL-POS], [EMP-POS], [APC], [MAJ]]\nClarity: this paper is clear,[[OAL-POS], [CLA-POS], [APC], [MAJ]] but would benefit from better figures and from tables to report the numerical results instead of inserting them into plain text.[[RES-NEU,TNF-NEU], [PNF-NEU], [SUG], [MIN]]\nOriginality: this paper introduces block diagonal matrices to structure the weights of a neural network.[[MET-NEU], [null], [DIS], [GEN]] The idea of structured matrices in this context is not new, but the diagonal block structure appears to be. [[MET-POS], [NOV-POS], [APC], [MAJ]] \nSignificance: This paper is somewhat significant.[[OAL-POS], [IMP-POS], [APC], [MAJ]]\n\nPROS \n- A new approach to analyzing the behavior of weight matrices during learning[[MET-POS], [EMP-POS], [APC], [MAJ]]\n- A new structure for weight matrices that provides good performance while reducing matrix storage requirements and speeding up forward and backward passes.[[RES-POS], [EMP-POS], [APC], [MAJ]]\n\nCONS\n- Some of the figures are hard to read (in particular Fig 1 & 2 left) and would benefit from a better layout.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]]\n- It would be valuable to see experiments on bigger datasets than only MNIST and CIFAR-10.[[DAT-NEG], [SUB-NEG], [CRT], [MIN]] \n- I understand that the main advantage of this method is the speedup; however, providing the final accuracy as a function of the nonzero entries for slower methods (e.g. the sparse pruning showed in Fig 1. a) would provide a more complete picture.[[MET-NEU], [EMP-NEU], [SUG], [MIN]]\n\nMain questions:\n- Could you briefly comment on the training time in section 4.1? [[EXP-NEG], [EMP-NEG], [QSN], [MAJ]]\n- Could you elaborate on the last sentence of section 4.1?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n- You state: \"singular values of an IP layer behave according to the MP distribution even after 1000s of training iterations.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]]\" Is this a known fact, or something that you observed empirically?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]] In practice, how large must the weight matrix be to observe this behavior?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]]\n\nNitpicks:\n- I believe the term \"fully connected\" is more standard than \"inner product\" and would add clarity to the paper, but I may be mistaken. "[[CNT], [CLA-NEU], [QSN], [MIN]]