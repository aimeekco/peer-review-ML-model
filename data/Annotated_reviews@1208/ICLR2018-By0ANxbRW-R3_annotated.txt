"1. This paper proposes a deep neural network compression method by maintaining the accuracy of deep models using a hyper-parameter.[[INT-NEU], [null], [SMY], [GEN]] However, all compression methods such as pruning and quantization also have this concern.[[RWK-NEU], [null], [SMY], [GEN]] For example, the basic assumption of pruning is to discard subtle parameters has little impact on feature maps thus the accuracy of the original network can be preserved.[[MET-NEU], [null], [SMY], [GEN]] Therefore, the novelty of the proposed method is somewhat weak.[[MET-NEU], [NOV-NEG], [CRT], [MAJ]]\n\n2. There are a lot of new algorithms on compressing deep neural networks such as [r1][r2][r3].[[RWK-NEU], [CMP-NEU], [DIS], [MAJ]] However, the paper only did a very simple investigation on related works.[[RWK-NEG], [SUB-NEG,CMP-NEU], [CRT], [MAJ]]\n[r1] CNNpack: packing convolutional neural networks in the frequency domain.[[RWK-NEU], [null], [SMY], [GEN]]\n[r2] LCNN: Lookup-based Convolutional Neural Network.[[RWK-NEU], [null], [SMY], [GEN]]\n[r3] Xnor-net: Imagenet classification using binary convolutional neural networks.[[RWK-NEU], [null], [SMY], [GEN]]\n\n3. Experiments in the paper were only conducted on several small datasets such as MNIST and CIFAR-10.[[DAT-NEU,EXP-NEU], [SUB-NEG], [DFT], [MAJ]] It is necessary to employ the proposed method on benchmark datasets to verify its effectiveness, e.g., ImageNet.[[DAT-NEU,EXP-NEU], [SUB-NEU,EMP-NEU], [SUG], [MAJ]]\n"