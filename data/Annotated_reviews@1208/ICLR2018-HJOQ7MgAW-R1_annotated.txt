"This paper proposes a simplified LSTM variants by removing the non-linearity of content item and output gate.[[PDI-NEU], [null], [DIS], [GEN]] It shows comparable results with standard LSTM.[[PDI-NEU,RES-NEU], [CMP-NEU], [DIS], [GEN]]\n\nI believe this is a updated version of https://arxiv.org/abs/1705.07393 (Recurrent Additive Networks) with stronger experimental results.[[RWK-POS,EXP-POS], [EMP-POS], [APC], [MAJ]] \n\nHowever, the formulation is very similar to \"[1] Semi-supervised Question Retrieval with Gated Convolutions\" 2016 by Lei, and \"Deriving Neural Architectures from Sequence and Graph Kernels\" which give theoretical view from string kernel about why this type of networks works.[[RWK-NEU], [null], [DIS], [GEN]] Both of the two paper don't have output gate and non-linearity of \"Wx_t\" and results on PTB also stronger than this paper.[[RWK-NEG,RES-NEG], [CMP-NEG], [CRT], [MAJ]] It also have some visualization about how the model decay the weights.[[RWK-NEU], [null], [DIS], [GEN]] Other AnonReviewer also point out some similar work.[[EXT-NEU], [null], [DIS], [GEN]] I won't repeat it here.[[EXT-NEU], [null], [DIS], [GEN]] In the paper, the author argued \"we propose and evaluate the minimal changes...\" but I think the these type of analysis also been covered by [1], Figure 5.[[ANA-NEU], [CMP-NEU], [DIS], [MAJ]] \n\nOn the experimental side, to draw the conclusion, \"weighted sum\" is enough for LSTM.[[EXP-NEU], [EMP-NEU], [DIS], [MAJ]] I think at least Machine Translation and other classification results should be added.[[RES-NEU], [SUB-NEU], [SUG], [MAJ]] I'm not very familiar with SQuAD dataset, but the results seems worse than \"Reading Wikipedia to answer open-domain questions\" Table 4 which seems use a vanilla LSTM setup.[[DAT-NEU,RES-NEG,TNF-NEU], [EMP-NEG], [CRT], [GEN]] \n\nUpdate: the revised version of the paper addresses all my concerns about experiments.[[EXP-NEU,OAL-NEU], [null], [DIS], [GEN]] So I increased my score. \n"[[OAL-POS], [REC-POS], [APC], [MAJ]]