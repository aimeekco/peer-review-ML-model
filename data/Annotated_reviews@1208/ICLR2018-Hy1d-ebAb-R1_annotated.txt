"The paper introduces a generative model for graphs.[[INT-NEU], [null], [SMY], [GEN]] The three main decision functions in the sequential process are computed with neural nets.[[PDI-NEU], [null], [SMY], [GEN]] The neural nets also compute node embeddings and graph embeddings and the embeddings of the current graph are used to compute the decisions at time step T.[[PDI-NEU], [null], [SMY], [GEN]] The paper is well written[[OAL-POS], [CLA-POS], [APC], [MAJ]] but, in my opinion, a description of the learning framework should be given in the paper.[[CNT], [SUB-NEG], [DIS], [GEN]] Also, a summary of the hyperparameters used in the proposed system should be given.[[MET-NEU], [EMP-NEU], [CRT], [MIN]] It is claimed that all possible types of graphs can be learned which seems rather optimistic.[[TNF-NEG], [CNT], [CRT], [MIN]] For instance, when learning trees, the system is tweaked for generating trees.[[TNF-NEG], [CNT], [CRT], [MIN]] Also, it is not clear whether models for large graphs can be learned.[[TNF-NEG], [CNT], [CRT], [MIN]] The paper contain many interesting contributions[[OAL-POS], [CNT], [APC], [MAJ]] but, in my opinion, the model is too general and the focus should be given on some retricted classes of graphs.[[TNF-NEU], [EMP-NEU], [APC], [MAJ]] Therefore, I am not convinced that the paper is ready for publication at ICLR'18.[[OAL-NEG], [REC-NEG], [FBK], [MAJ]]\n\n* Introduction. I am not convinced by the discussion on graph grammars in the second paragraph.[[INT-NEG,TNF-NEG], [CNT], [CRT], [MIN]] It is known that there does not exist a definition of regular grammars in graph (see Courcelle and Engelfriet, graph structure and monadic second-order logic ...).[[TNF-NEG], [SUB-NEG], [DFT], [MIN]] Moreover, many problems are known to be undecidable.[[OAL-NEG], [null], [CRT], [MIN]] For weighted automata, the reference Droste and Gastin considers weighted word automata and weighted logic for words.[[RWK-NEU,MET-NEU], [CNT], [DIS], [MIN]] Therefore I does not seem pertinent here.[[EXT-NEU], [null], [DIS], [MIN]] A more complete reference is \"handbook of weighted automata\" by Droste.[[RWK-NEU,BIB-NEU], [null], [DIS], [MIN]]  Also, many decision problems for wighted automata are known to be undecidable.[[CNT], [CNT], [DIS], [MIN]]  I am not sure that the paragraph is useful for the paper.[[CNT], [null], [CRT], [MIN]]  A discussion on learning as in footnote 1 shoud me more interesting.[[ANA-NEU], [SUB-NEU], [SUG], [MIN]] \n* Related work.[[RWK-NEU], [null], [DIS], [MIN]]  I am not expert in the field but I think that there are recent references which could be cited for probablistic models of graphs.[[RWK-NEU,BIB-NEU,TNF-NEU], [null], [DIS], [MIN]] \n* Section 3.1. Constraints can be introduced to impose structural properties of the generated graphs.[[TNF-NEU], [SUB-NEU], [SUG], [MIN]]  This leads to the question of cheating in the learning process.[[MET-NEU], [null], [DIS], [MIN]] \n* Section 3.2. The functions f_m and g_m for defining graph embedding are left undefined.[[MET-NEG,TNF-NEG], [EMP-NEG], [CRT], [MIN]]  As the graph embedding is used in the generating process and for learning, the functions must be defined and their choice explained and justified.[[MET-NEU,TNF-NEU], [null], [DIS], [MIN]] \n* Section 3. As said before, a general description of the learning framework should be given.[[MET-NEU], [EMP-NEU], [SUG], [MIN]]  Also, it is not clear to me how the node and graph embeddings are initialized and how they evolve along the learning process.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]  Therefore, it is not clear to me why the proposed updating framework for the embeddings allow to generate decision functions adapted to the graphs to be learned.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]   Consequently, it is difficult to see the influence of T.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]  Also, it should be said whether the node embeddings and graph embeddings for the output graph can be useful.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] \n* Section 3. A summary of all the hyperparameters should be given.[[CNT], [SUB-NEU], [DIS], [MIN]] \n* Section 4.1. The number of steps is not given.[[CNT], [SUB-NEU], [DIS], [MIN]] Do you present the same graph multiple times.[[RES-NEU], [EMP-NEU], [QSN], [MIN]] Why T=2 and not 1 or 10 ?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n* Section 4.2. From table 2, it seems that all permutations are used for training which is rather large for molecules of size 20.[[MET-NEU,TNF-NEU], [EMP-NEU], [DIS], [MIN]] Do you use tweaks in the generation process.[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n* Section 4.3. The generation process is adapted for generating trees which seems to be cheating.[[MET-NEG], [SUB-NEU], [DIS], [MIN]] Again the choice of T seems ad hoc and based on computational burden.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n* Section 5 should contain a discussion on complexity issues because it is not clear how the model can learn large graphs.[[MET-NEG,TNF-NEG], [EMP-NEU], [CRT], [MAJ]]\n* Section 5. The discussion on the difficulty of training shoud be emphasized and connected to the --missing-- description of the model architecture and its hyperparameters.[[EXP-NEG], [SUB-NEU,EMP-NEG], [DIS], [MIN]]\n* acronyms should be expansed at their first use"[[MET-NEU], [EMP-NEU], [SUG], [MIN]]