"The paper proposes a budgeted online kernel algorithm for multi-task learning."[[PDI-NEU], [null], [SMY], [GEN]] The main contribution of the paper is an online update of the output kernel, which measures similarity between pairs of tasks.[[PDI-NEU], [null], [SMY], [GEN]]  The paper also proposes a removal strategy that bounds the number of support vectors in the kernel machine.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]]  The proposed algorithm is tested on 3 data sets and compared with several baselines.[[RWK-NEU,DAT-NEU,MET-NEU], [null], [SMY], [GEN]] \n  Positives:\n- the output kernel update is well justified[[MET-POS], [EMP-POS], [APC], [MAJ]] \n- experimental results are encouraging[[EXP-POS,RES-POS], [EMP-POS], [APC], [MAJ]] \n  Negatives:\n- the methodological contribution of the paper is minimal[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] \n- the proposed approach to maintain the budget is simplistic[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n- no theoretical analysis of the proposed algorithm is provided[[MET-NEG,ANA-NEG], [SUB-NEG], [DFT], [MAJ]]\n- there are issues with the experiments: the choice of data sets is questionable (all data sets are very small so there is not need for online learning or budgeting; newsgroups is a multi-class problem, so we would want to see comparisons with some good multi-class algorithms; spam data set might be too small), it is not clear what were hyperparameters in different algorithms and how they were selected, the budgeted baselines used in the experiments  are not state of the art (forgetron and random removal are known to perform poorly in practice, projectron usually works much better), it is not clear how a practitioner would decide whether to use update (2) or(3)"[[RWK-NEU,DAT-NEG,EXP-NEG], [CMP-NEU,SUB-NEG,EMP-NEG], [CRT], [MAJ]]