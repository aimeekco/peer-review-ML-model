"The paper introduces a neural translation model that automatically discovers phrases.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]]  This idea is very interesting and tries to marry phrase-based statistical machine translation with neural methods in a principled way.[[PDI-POS], [EMP-POS], [APC], [MAJ]] However, the clarity of the paper could be improved.[[OAL-NEG], [CLA-NEG], [SUG], [MIN]]\n\nThe local reordering layer has the ability to swap inputs, however, how do you ensure that it actually does swap inputs rather than ignoring some inputs and duplicating others?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nAre all segments translated independently, or do you carry over the hidden state of the decoder RNN between segments?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]] In Figure 1 both a BRNN and SWAN layer are shown, is there another RNN in the SWAN layer, or does the BRNN emit the final outputs after the segments have been determined?"[[EXP-NEU,MET-NEU,TNF-NEU], [EMP-NEU], [QSN], [MIN]]