"The authors propose a decoupled backpropagation method, called continuous propagation, through the interpretation of backpropagation as a continuous differential system.[[INT-NEU,MET-NEU], [null], [SMY], [GEN]] Because the layer-wise decoupling, it can easily be applied for distributed training of the model.[[EXP-NEU], [null], [DIS], [MIN]] The authors provide a convergence proof on the proposed algorithm and also provides some empirical experiment results.[[EXP-NEU,MET-NEU,RES-NEU], [null], [SMY], [GEN]]\n\nAlthough I found the proposed method is interesting enough to investigate more thoroughly,[[MET-POS], [EMP-POS], [APC], [MAJ]] it is a shame to see the overall quality of the paper very weak.[[OAL-NEG], [CLA-NEG], [CRT], [MIN]] The writing requires a significant improvement: in addition to the overall unclarity of the exposition, it sometimes use unexplained abbreviation (e.g., CPGD, CP).[[OAL-NEG], [CLA-NEG], [CRT], [MIN]] The experiments are also very weak.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]] Important information on the experiment settings are missing, e.g., how the model is parallelized.[[EXP-NEG,MET-NEG], [SUB-NEG], [DFT], [MAJ]]\n\n- Mini-batch gradient descent (MBGD) is unfamiliar concept compared to SGD.[[MET-NEG], [CMP-NEG], [CRT], [MIN]] It needs to be better defined."[[MET-NEG], [EMP-NEG], [SUG], [MIN]]