"The paper addresses the task of dealing with named entities in goal oriented dialog systems.[[INT-NEU], [null], [SMY], [GEN]] Named entities, and rare words in general, are indeed troublesome since adding them to the dictionary is expensive, replacing them with coarse labels (ne_loc, unk) looses information, and so on.[[PDI-NEU], [null], [SMY], [GEN]] The proposed solution is to extend neural dialog models by introducing a named entity table, instantiated on the fly, where the keys are distributed representations of the dialog context and the values are the named entities themselves.[[MET-NEU], [null], [SMY], [GEN]] The approach is applied to settings involving interacting to a database and a mechanism for handling the interaction is proposed.[[DAT-NEU,MET-NEU], [null], [SMY], [GEN]] The resulting model is illustrated on a few goal-oriented dialog tasks.[[MET-NEU], [null], [SMY], [GEN]]\n\n\nI found the paper difficult to read.[[OAL-NEG], [CLA-NEG], [CRT], [MAJ]] The concrete mappings used to create the NE keys and attention keys are missing.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Providing more structure to the text would also be useful vs. long, wordy paragraphs.[[OAL-NEU], [PNF-NEU], [SUG], [MAJ]] Here are some specific questions:\n\n1. How are the keys generated? [[MET-NEU], [null], [QSN], [MIN]]That are the functions used?[[MET-NEU], [null], [QSN], [MIN]] Does the \"knowledge of the current user utterance\" include the word itself?[[MET-NEU], [null], [QSN], [MIN]] The authors should include the exact model specification, including for the HRED model.[[MET-NEU], [EMP-NEU], [SUG], [MAJ]]\n\n2. According to the description, referring to an existing named entity must be done by \"generating a key to match the keys in the NE table and then retrieve the corresponding value and use it\".[[MET-NEU], [EMP-NEU], [SMY], [MAJ]] Is there a guarantee that a same named entity, appearing later in the dialog, will be given the same key?[[MET-NEU], [EMP-NEU], [QSN], [MAJ]]  Or are the keys for already found entities retrieved directly, by value?[[MET-NEU], [EMP-NEU], [QSN], [MAJ]]\n\n3. In the decoding phase, how does the system decide whether to query the DB?[[MET-NEU], [EMP-NEU], [QSN], [MAJ]]\n\n4. How is the model trained?[[MET-NEU], [EMP-NEU], [QSN], [MAJ]]\n\nIn its current form, it's not clear how the proposed approach tackles the shortcomings mentioned in the introduction.[[MET-NEU], [EMP-NEG], [CRT], [MAJ]] Furthermore, while the highlighted contribution is the named entity table, it is always used in conjunction to the database approach.[[MET-NEU], [EMP-NEU], [SMY], [MAJ]] This raises the question whether the named entity table can only work in this context.[[MET-NEU], [EMP-NEU], [QSN], [MAJ]]\n\nFor the structured QA task, there are 400 training examples, and 100 named entities. This means that the number of training examples per named entity is very small.[[DAT-NEU,EXP-NEU], [SUB-NEG], [CRT], [MAJ]] Is that correct?[[DAT-NEU], [null], [QSN], [MIN]] If yes, then it's not very surprising that adding the named entities to the vocabulary leads to overfitting.[[DAT-NEU,EXP-NEG], [EMP-NEG], [CRT], [MAJ]] Have you compared with using random embeddings for the named entities?[[EXP-NEU], [CMP-NEU], [QSN], [MIN]]\n\nTypos: page 2, second-to-last paragraph: firs -> first, page 7, second to last paragraph: and and -> and.[[OAL-NEU], [CLA-NEG], [DFT], [MIN]]\n\n"