"Quality: The authors study non-saturating GANs and the effect of two penalized gradient approaches.[[INT-NEU], [NULL], [SMY], [GEN]] The authors consider a number of thought experiments to demonstrate their observations and validate these on real data experiments[[DAT-NEU,EXP-NEU], [NULL], [SMY], [GEN]]. \n\nClarity: The paper is well-written and clear[[OAL-POS], [CLA-POS], [APC], [MAJ]]. The authors could be more concise when reporting results[[RES-NEU], [CLA-NEU], [SUG], [MIN]]. I would suggest keeping the main results in the main body and move extended results to an appendix.[[RES-NEU], [NULL], [SUG], [MIN]]\n\nOriginality: The authors demonstrate experimentally that there is a benefit of using non-saturating GANs[[EXP-POS], [NULL], [APC], [MAJ]]. More specifically, the provide empirical evidence that they can fit problems where Jensen-Shannon divergence fails.[[RWK-NEG,MET-POS], [EMP-POS], [DIS], [GEN]] They also show experimentally that penalized gradients stabilize the learning process.[[EXP-NEU], [NULL], [SMY], [GEN]]\n\nSignificance: The problems the authors consider is worth exploring further[[PDI-POS], [NULL], [SMY], [MAJ]]. The authors describe their finding in the appropriate level of details and demonstrate their findings experimentally[[EXP-POS], [EMP-POS], [SMY], [GEN]]. However, publishing this  work is in my opinion premature for the following reasons:\n\n- The authors do not provide further evidence of why non-saturating GANs perform better or under which mathematical conditions (non-saturating) GANs will be able to handle cases where distribution manifolds do not overlap[[OAL-NEG], [REC-NEG], [FBK], [MAJ]];\n- The authors show empirically the positive effect of penalized gradients, but do not provide an explanation grounded in theory[[MET-POS], [EMP-NEG], [DFT], [MAJ]];\n- The authors do not provide practical recommendations how to set-up GANs and not that these findings did not lead to a bullet-proof recipe to train them.\n\n"[[MET-NEG], [EMP-NEG], [DFT], [MAJ]]