"This paper formulates a variant of convolutional neural networks which models both activations and filters as continuous functions composed from kernel bases.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] A closed-form representation for convolution of such functions is used to compute in a manner than maintains continuous representations, without making discrete approximations as in standard CNNs.[[PDI-NEU], [null], [SMY], [GEN]]\n\nThe proposed continuous convolutional neural networks (CCNNs) project input data into a RKHS with a Gaussian kernel function evaluated at a set of inducing points; the parameters defining the inducing points are optimized via backprop.[[MET-NEU], [null], [SMY], [GEN]] Filters in convolutional layers are represented in a similar manner, yielding a closed-form expression for convolution between input and filters.[[MET-NEU], [null], [SMY], [GEN]] Experiments train CCNNs on several standard small-scale image classification datasets: MNIST, CIFAR-10, STL-10, and SVHN.[[DAT-NEU,EXP-NEU], [null], [SMY], [GEN]]\n\nWhile the idea is interesting and might be a good alternative to standard CNNs,[[PDI-POS], [EMP-POS], [APC], [MAJ]] the paper falls short in terms of providing experimental validation that would demonstrate the latter point.[[MET-NEU], [null], [SMY], [GEN]] It unfortunately only experiments with CCNN architectures with a small number (eg 3) layers.[[EXP-NEG], [SUB-NEG], [CRT], [MIN]] They do well on MNIST, but MNIST performance is hardly informative as many supervised techniques achieve near perfect results.[[DAT-POS,EXP-POS], [EMP-POS], [APC], [MAJ]] The CIFAR-10, STL-10, and SVHN results are disappointing.[[DAT-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] CCNNs do not outperform the prior CNN results listed in Table 2,3,4.[[RES-POS], [CMP-POS], [APC], [MAJ]] Moreover, these tables do not even cite more recent higher-performing CNNs.[[TNF-NEG], [SUB-NEG], [CRT], [MIN]] See results table in (*) for CIFAR-10 and SVHN results on recent ResNet and DenseNet CNN designs which far outperform the methods listed in this paper.[[DAT-NEG,RES-NEG], [CMP-NEG], [CRT], [MIN]]\n\nThe problem appears to be that CCNNs are not tested in a regime competitive with the state-of-the-art CNNs on the datasets used.Why not?[[DAT-NEG,RES-NEG], [CMP-NEG,EMP-NEG], [QSN], [MIN]]  To be competitive, deeper CCNNs would likely need to be trained.[[EXP-NEU],[null], [DIS], [MIN]]  I would like to see results for CCNNs with many layers (eg 16+ layers) rather than just 3 layers.[[DAT-NEU,RES-NEU], [SUB-NEU], [DIS], [GEN]]  Do such CCNNs achieve performance compatible with ResNet/DenseNet on CIFAR or SVHN?[[DAT-NEU,RES-NEU], [EMP-NEU], [QSN], [MIN]]  Given that CIFAR and SVHN are relatively small datasets, training and testing larger networks on them should not be computationally prohibitive.[[DAT-NEG,MET-NEG], [SUB-NEU], [CRT], [MIN]] \n\nIn addition, for such experiments, a clear report of parameters and FLOPs for each network should be included in the results table.[[RES-NEU], [SUB-NEU], [DFT,SUG], [MIN]]  This would assist in understanding tradeoffs in the design space.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \n\nAdditional questions:\n\nWhat is the receptive field of the CCNNs vs those of the standard CNNs to which they are compared?[[MET-NEU], [CMP-NEU], [QSN], [MIN]]  If the CCNNs have effectively larger receptive field, does this create a cost in FLOPs compared to standard CNNs?[[MET-NEU], [CMP-NEU], [QSN], [MIN]] \n\nFor CCNNs, why does the CCAE initialization appear to be essential to achieving high performance on CIFAR-10 and SVHN? [[MET-NEU], [CMP-NEU], [QSN], [MIN]] Standard CNNs, trained on supervised image classification tasks do not appear to be dependent on initialization schemes that do unsupervised pre-training.[[EXP-NEG], [EMP-NEG], [CRT], [MIN]]  Such dependence for CCNNs appears to be a weakness in comparison."[[MET-NEG], [CMP-NEG], [CRT], [MIN]] 