"Summary: the paper proposes a tree2tree architecture for NLP tasks.[[INT-NEU], [null], [SMY], [GEN]] Both the encoder and decoder of this architecture make use of memory cells: the encoder looks like a tree-lstm to encode a tree bottom-up, the decoder generates a tree top-down by predicting the number of children first.[[MET-NEU], [null], [SMY], [GEN]] The objective function is a linear mixture of the cost of generating the tree structure and the target sentence.[[MET-NEU], [null], [SMY], [GEN]]  The proposed architecture outperforms recursive autoencoder on a self-to-self predicting trees, and outperforms an lstm seq2seq on En-Cn translation.[[MET-NEU, RES-POS], [EMP-POS], [APC], [MAJ]] \n\nComment:\n\n- The idea of tree2tree has been around recently but it is difficult to make it work.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] I thus appreciate the authors\u2019 effort.[[MET-POS], [EMP-POS], [APC], [MAJ]] However, I wish the authors would have done it more properly.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n- The computation of the encoder and decoder is not novel.[[MET-NEG], [NOV-NEG], [CRT], [MAJ]] I was wondering how the encoder differs from tree-lstm.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] The decoder predicts the number of children first, but the authors don\u2019t explain why they do that, nor compare this to existing tree generators.[[EXP-NEG, MET-NEG], [EMP-NEG], [CRT], [MIN]] \n- I don\u2019t understand the objective function (eq 4 and 5).[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Both Ls are not cross-entropy because label and childnum are not probabilities.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] I also don\u2019t see why using Adam is more convenient than using SGD.[[MET-NEG], [CMP-NEG], [CRT], [MIN]]\n- I think eq 9 is incorrect, because the decoder is not Markovian.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] To see this we can look at recurrent neural networks for language modeling: generating the current word is conditioning on the whole history (not only the previous word).[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n- I expect the authors would explain more about how difficult the tasks are (eg. some statistics about the datasets), how to choose values for lambda, what the contribution of the new objective is.[[DAT-NEU, MET-NEG, ANA-NEU], [SUB-NEG], [SUG], [MIN]]\n\nAbout writing:\n- the paper has so many problems with wording, e.g. articles, plurality.[[OAL-NEG], [CLA-NEG], [CRT], [MIN]]\n- many terms are incorrect, e.g. \u201cdependent parsing tree\u201d (should be \u201cdependency tree\u201d), \u201cconsistency parsing\u201d (should be \u201cconstituency parsing\u201d)[[OAL-NEG], [CLA-NEG], [CRT], [MIN]]\n- In 3.1, Socher et al. do not use lstm[[RWK-NEU,EXT-NEU], [CMP-NEU], [DIS], [GEN]]\n- I suggest the authors to do some more literature review on tree generation\n"[[RWK-NEG], [SUB-NEU], [SUG], [MIN]]