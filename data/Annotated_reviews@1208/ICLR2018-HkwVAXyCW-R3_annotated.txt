"This paper proposes an idea to do faster RNN inference via skip RNN state updates.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]]  \nI like the idea of the paper, in particular the design which enables calculating the number of steps to skip in advance.[[PDI-POS], [EMP-POS], [APC], [MAJ]]  But the experiments are not convincing enough.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]]  First the tasks it was tested on are very simple -- 2 synthetic tasks plus 1 small-scaled task.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]  I'd like to see the idea works on larger scale problems -- as that is where the computation/speed matters.[[PDI-NEU], [null], [DIS], [MIN]]  Also besides the number of updates reported in table, I think the wall-clock time for inference should also be reported, to demonstrate what the paper is trying to claim.[[MET-NEU], [null], [DIS], [MAJ]] \n\nMinor -- \nCite Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation by Yoshua Bengio, Nicholas Leonard and Aaron Courville for straight-through estimator."[[BIB-NEG], [null], [DIS], [MIN]] 