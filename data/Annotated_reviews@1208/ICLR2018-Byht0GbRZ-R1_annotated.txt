"Summary:\nThis paper introduces a structured attention mechanisms to compute alignment scores among all possible spans in two given sentences.[[INT-NEU], [null], [SMY], [GEN]] The span representations are weighted by the spans marginal scores given by the inside-outside algorithm.[[MET-NEU], [null], [SMY], [GEN]] Experiments on TREC-QA and SNLI show modest improvement over the word-based structured attention baseline (Parikh et al., 2016).[[RWK-POS,EXP-POS,BIB-NEU], [CMP-POS], [SMY], [MAJ]]\n\nStrengths:\nThe idea of using latent syntactic structure, and computing cross-sentence alignment over spans is very interesting.[[PDI-POS], [null], [APC], [MAJ]] \n\nWeaknesses:\nThe paper is 8.5 pages long[[OAL-NEG], [PNF-NEG], [DFT], [MAJ]].\n\nThe method did not out-perform other very related structured attention methods (86.8, Kim et al., 2017, 86.9, Liu and Lapata, 2017)[[RWK-NEU,MET-NEG], [CMP-NEU], [DFT], [MAJ]]\n\nAside from the time complexity from the inside-outside algorithm (as mentioned by the authors in conclusion), the comparison among all pairs of spans is O(n^4), which is more expensive.[[MET-NEU], [CMP-NEG], [DFT], [MAJ]] Am I missing something about the algorithm?[[MET-NEU], [null], [QSN], [MIN]]\n\nIt would be nice to show, quantitatively, the agreement between the latent trees and gold/supervised syntax.[[DAT-NEU,MET-NEU], [EMP-NEU], [SUG], [MIN]] The paper claimed \u201cthe model is able to recover tree structures that very closely mimic syntax\u201d, but it\u2019s hard to draw this conclusion from the two examples in Figure 2[[DAT-NEU,MET-NEU,TNF-NEG], [CMP-NEG,EMP-NEG], [DFT], [MAJ]].\n"