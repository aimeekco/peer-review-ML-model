"This paper proposes a principled methodology to induce distributional robustness in trained neural nets with the purpose of mitigating the impact of adversarial examples. [[INT-NEU,PDI-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]]The idea is to train the model to perform well not only with respect to the unknown population distribution, but to perform well on the worst-case distribution in some ball around the population distribution.[[PDI-NEU,EXP-NEU,ANA-NEU], [EMP-NEU], [SMY], [GEN]] In particular, the authors adopt the Wasserstein distance to define the ambiguity sets.[[RWK-NEU], [null], [SMY], [GEN]] This allows them to use strong duality results from the literature on distributionally robust optimization and express the empirical minimax problem as a regularized ERM with a different cost.[[RWK-NEU], [EMP-NEU], [DIS], [GEN]] The theoretical results in the paper are supported by experiments.[[RWK-NEU,EXP-NEU], [EMP-NEU], [SMY], [GEN]]\n\nOverall, this is a very well-written paper that creatively combines a number of interesting ideas to address an important problem."[[RWK-POS,PDI-POS,OAL-POS], [CLA-POS], [APC], [MAJ]]