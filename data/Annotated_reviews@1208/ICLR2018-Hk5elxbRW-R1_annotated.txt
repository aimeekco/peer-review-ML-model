"The paper is clear and well written.[[OAL-POS], [CLA-POS], [APC], [MAJ]] The proposed approach seems to be of interest and to produce interesting results.[[PDI-POS,RES-POS], [IMP-POS,EMP-POS], [APC], [MAJ]] As datasets in various domain get more and more precise, the problem of class confusing with very similar classes both present or absent of the training dataset is an important problem, and this paper is a promising contribution to handle those issues better.[[PDI-POS,DAT-NEG,OAL-POS], [IMP-POS,EMP-NEG], [APC,CRT], [MIN]]\n\nThe paper proposes to use a top-k loss such as what has been explored with SVMs in the past, but with deep models.[[PDI-NEU], [null], [SMY], [GEN]] As the loss is not smooth and has sparse gradients, the paper suggests to use a smoothed version where maximums are replaced by log-sum-exps.[[RWK-NEU], [null], [SUG], [GEN]]\n\nI have two main concerns with the presentation.[[OAL-NEU], [PNF-NEU], [SMY], [GEN]]\n\nA/ In addition to the main contribution, the paper devotes a significant amount of space to explaining how to compute the smoothed loss.[[RWK-NEG,OAL-NEG], [SUB-NEG], [DFT], [MIN]] This can be done by evaluating elementary symmetric polynomials at well-chosen values.[[RWK-NEU], [null], [SMY], [GEN]]\n\nThe paper argues that classical methods for such evaluations (e.g., using the usual recurrence relation or more advanced methods that compensate for numerical errors) are not enough when using single precision floating point arithmetic.[[PDI-NEG], [SUB-NEG,EMP-NEG], [DFT], [MIN]] The paper also advances that GPU parallelization must be used to be able to efficiently train the network.[[RWK-NEU], [EMP-NEU], [SMY], [GEN]]\n\nThose claims are not substantiated, however, and the method proposed by the paper seems to add substantial complexity without really proving that it is useful.[[RWK-NEG], [SUB-NEG,EMP-NEG], [DFT,QSN], [MIN]]\n\nThe paper proposes a divide-and-conquer approach, where a small amount of parallelization can be achieved within the computation of a single elementary symmetric polynomial value.[[PDI-NEU,DAT-NEU,EXP-NEU], [null], [SMY], [GEN]] I am not sure why this is of interest - can't the loss evaluation already be parallelized trivially over examples in a training/testing minibatch?[[EXT-NEU], [null], [SMY], [GEN]] I believe the paper could justify this approach better by providing a bit more insights as to why it is required. [[OAL-NEU], [CLA-NEU,SUB-NEU,IMP-NEU], [SUG,APC], [GEN]]For instance:\n\n- What accuracies and train/test times do you get using standard methods for the evaluation of elementary symmetric polynomials?[[RWK-NEU,EXP-NEU,MET-NEU,ANA-NEU], [null], [QSN], [GEN]]\n- How do those compare with CE and L_{5, 1} with the proposed method?[[PDI-NEU], [CMP-NEU], [QSN], [GEN]]\n- Are numerical instabilities making this completely unfeasible?[[PDI-NEU], [EMP-NEU], [QSN], [GEN]] This would be especially interesting to understand if this explodes in practice, or if evaluations are just a slightly inaccurate without much accuracy loss.[[RWK-NEU,EXP-NEU,ANA-NEG], [EMP-NEU], [SMY], [GEN]]\n\n\nB/ No mention is made of the object detection problem, although multiple of the motivating examples in Figure 1 consider cases that would fall naturally into the object detection framework.[[RWK-NEG], [IMP-NEG,EMP-NEG], [DFT], [GEN]] Although top-k classification considers in principle an easier problem (no localization), a discussion, as well as a comparison of top-k classification vs., e.g., discarding localization information out of object detection methods, could be interesting.[[RWK-NEU,OAL-NEU], [null], [SMY], [GEN]]\n\nAdditional comments:\n\n- Figure 2b: this visualization is confusing.[[TNF-NEG], [CLA-NEG], [DFT], [MIN]] This is presented in the same figure and paragraph as the CIFAR results,[[RWK-NEU,TNF-NEU], [null], [SMY], [GEN]] but instead uses a single synthetic data point in dimension 5, and k=1.[[RWK-NEU], [null], [SMY], [GEN]] This is not convincing.[[OAL-NEG], [CLA-NEG,IMP-NEG], [DFT], [MIN]] An actual experiment using full dataset or minibatch gradients on CIFAR and the same k value would be more interesting.[[RWK-NEU,DAT-NEG,EXP-NEG], [SUB-NEG,IMP-NEG,EMP-NEG], [SUG,DFT], [MIN]]\n\n"
