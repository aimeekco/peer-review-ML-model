"MAML (Finn+ 2017) is recast as a hierarchical Bayesian learning procedure.[[EXP-NEU], [null], [SMY], [GEN]] In particular the inner (task) training is initially cast as point-wise max likelihood estimation, and then (sec4) improved upon by making use of the Laplace approximation.[[INT-NEU,EXP-NEU], [null], [SMY], [GEN]] Experimental evidence of the relevance of the method is provided on a toy task involving a NIW prior of Gaussians, and the (benchmark) MiniImageNet task.[[EXP-NEU], [null], [SMY], [GEN]]\n\nCasting MAML as HB seems a good idea.[[MET-POS], [EMP-POS], [APC], [MAJ]] The paper does a good job of explaining the connection,[[RES-POS], [EMP-POS], [APC], [MAJ]] but I think the presentation could be clarified.[[OAL-NEG], [PNF-NEG], [SUG], [MIN]] The role of the task prior and how it emerges from early stopping (ie a finite number of gradient descent steps) (sec 3.2) is original and technically non-trivial, and is a contribution of this paper.[[EXP-POS,MET-POS], [NOV-POS], [APC], [MAJ]] \nThe synthetic data experiment sec5.1 and fig5 is clearly explained and serves to additionally clarify the proposed method.[[EXP-POS,MET-POS,TNF-POS], [EMP-POS], [APC], [MAJ]] \nRegarding the MiniImageNet experiments, I read the exchange on TCML and agree with the authors of the paper under review.[[DAT-NEU], [null], [DIS], [GEN]] However, I recommend including the references to Mukhdalai 2017 and Sung 2017 in the footnote on TCML to strengthen the point more generically, and show that not just TCML but other non-shallow architectures are not considered for comparison here.[[RWK-NEU,BIB-NEU], [null], [SUG], [MIN]] In addition, the point made by the TCML authors is fair (\"nothing prevented you from...\") and I would also recommend mentioning the reviewed paper's authors' decision (not to test deeper architectures) in the footnote.[[RWK-NEU,BIB-NEU], [null], [SUG], [MIN]] This decision is in order but needs to be stated in order for the reader to form a balanced view of methods at her disposal.[[MET-NEU], [EMP-NEU], [SUG], [MIN]]\nThe experimental performance reported Table 1 remains small and largely within one standard deviation of competitor methods.[[EXP-NEU], [null], [DIS], [MIN]]\n\nI am assessing this paper as \"7\" because despite the merit of the paper, the relevance of the reformulation of MAML, and the technical steps involved in the reformulation, the paper does not eg address other forms (than L-MAML) of the task-specific subroutine ML-..., and the benchmark improvements are quite small.[[OAL-NEG], [REC-NEG], [FBK], [MAJ]] I think the approach is good and fruitful.[[MET-POS], [EMP-POS], [APC], [MAJ]] \n\n\n# Suggestions on readability\n\n* I have the feeling the paper inverts $\\alpha, \\beta$ from their use in Finn 2017 (step size for meta- vs task-training).[[OAL-NEU], [CLA-NEU], [SUG], [MIN]] This is unfortunate and will certainly confuse readers; I advise carefully changing this throughout the entire paper (eg Algo 2,3,4, eq 1, last eq in sec3.1, eq in text below eq3, etc)\n\n*[[MET-NEG], [CLA-NEG], [CRT], [MIN]] I advise avoiding the use of the symbol f, which appears in only two places in Algo 2 and the end of sec 3.1.[[MET-NEG], [PNF-NEG], [SUG], [MIN]] This is in part because f is given another meaning in Finn 2017, but also out of general parsimony in symbol use.[[MET-NEG], [PNF-NEG], [SUG], [MIN]] (could leave the output of ML-... implicit by writing ML-...(\\theta, T)_j in the $sum_j$; if absolutely needed, use another symbol than f)\n\n*[[MET-NEG], [PNF-NEG], [SUG], [MIN]] Maybe sec3 can be clarified in its structure by re-ordering points on the quadratic error function and early stopping (eg avoiding to split them between end of 3.1 and 3.2).[[MET-NEG], [PNF-NEG], [SUG], [MIN]]\n\n* sec6 \"Machine learning and deep learning\": I would definitely avoid this formulation, seems to tail in with all the media nonsense on \"what's the difference between ML and DL ?\".[[MET-NEG], [EMP-NEG], [QSN], [MAJ]] In addition the formulation seems to contrast ML with hierarchical Bayesian modeling, which does not make sense/ is wrong and confusing.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\n# Typos\n\n* sec1 second parag: did you really mean \"in the architecture or loss function\"? unclear.[[CNT], [CLA-NEG], [QSN], [MAJ]]\n* sec2: over a family\n* \"common structure, so that\" (not such that)\n* orthgonal\n* sec2.1 suggestion: clarify that \\theta and \\phi are in the same space\n* sec2.2 suggestion: task-specific parameter $\\phi_j$ is distinct from ... parameters $\\phi_{j'}, j' \\neq j}[[MET-NEG], [CLA-NEG], [SUG], [MIN]]\n* \"unless an approximate ... is provided\" (the use of the subjunctive here is definitely dated :-) )[[MET-NEG], [PNF-NEG], [SUG], [MIN]]\n* sec3.1 task-specific parameters $\\phi_j$ (I would avoid writing just \\phi altogether to distinguish in usage from \\theta)[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n* Gaussian-noised\n* approximation of the it objective\n* before eq9: \"that solves\": well, it doesn't really \"solve\" the minimisation, in that it is not a minimum; reformulate this?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]]\n* sec4.1 innaccurate\n* well approximated\n* sec4.2 an curvature\n* (Amari 1989)\n* For the the Laplace\n* O(n^3) : what is n ?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]]\n* sec5.2 (Ravi and L 2017)\n* for the the \n"[[CNT], [CNT], [DIS], [GEN]]