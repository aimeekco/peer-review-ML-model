"Pros\n- The paper proposes a novel formulation of the problem of finding hidden units\n  that are crucial in making a neural network come up with a certain output.[[INT-POS,PDI-NEU,MET-NEU], [NOV-NEU,EMP-NEU], [SMY,APC], [GEN]]\n- The method seems to be work well in terms of isolating a few hidden units that\n  need to be kept while preserving classification accuracy.\[[RWK-POS,MET-NEU,RES-NEU], [NOV-NEU,SUB-NEU,EMP-NEU], [SMY,APC], [GEN]]n\nCons\n- Sections 3.1 and 3.2 are hard to understand.[[RWK-NEG], [null], [DFT], [MIN]] There seem to be inconsistencies\n  in the notation.[[RWK-NEG], [PNF-NEG], [DFT], [MIN]] For example,\n(1) It would help to clarify whether y^b_n is the prediction score or its\ntransformation into [0, 1].[[RWK-NEU,EXP-NEU,MET-NEU,ANA-NEU], [CLA-NEU,EMP-NEU], [SMY,DIS], [GEN]] The usage is inconsistent.[[RWK-NEG], [SUB-NEG], [DFT], [MIN]]\n(2) It is not clear how \"y^b_n can be expressed as \\sum_{k=1}^K z_{nk}f_k(x_n)\"\nin general.[[RWK-NEG], [CLA-NEG], [DFT], [MIN]] This is only true for the penultimate layer, and when y^b_n denotes\nthe input to the output non-linearity.[[RWK-NEU,EXP-NEU,ANA-NEU], [SUB-NEU,EMP-NEU], [SMY], [GEN]] However, this analysis seems to be\napplied for any hidden layer and y^b_n is the output of the non-linearity unit[[RWK-NEU,EXP-NEU,MET-NEU,ANA-NEU], [SUB-NEU,EMP-NEU], [SMY,DIS], [GEN]]\n(\"The new prediction scores are transformed into a scalar ranging from 0 to 1,\ndenoted as y^b_n.\[[RWK-NEU,EXP-NEU,MET-NEU,RES-NEU], [IMP-NEU,EMP-NEU], [SUG,DIS], [GEN]]")\n(3) Section 3.1 denotes the DNN classifier as F(.), but section 3.2 denotes the\nsame classifier as f(.).\n(4) Why is r_n called the \"center\" ?[[RWK-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SUG], [GEN]] I could not understand in what sense is\nthis the center, and of what ? It seems that the max value has been subtracted\nfrom all the logits into a softmax (which is a fairly standard operation).[[RWK-NEG,EXP-NEU,MET-NEU], [EMP-NEU], [SMY,DFT,QSN], [MIN]]\n\n- The analysis seems to be about finding neurons that contribute evidence for\n  a particular class.[[RWK-NEU,ANA-NEU], [IMP-NEU], [SMY,DIS], [GEN]] This does not address the issue of understanding why the\nnetwork makes a certain prediction for a particular input.[[RWK-NEG,MET-NEG], [EMP-NEG], [DFT], [MIN]] Therefore this\napproach will be of limited use.\[[RWK-NEG,PDI-NEG], [IMP-NEG], [DFT], [MIN]]n\n- The paper should include more analysis of how this method helps interpret the\n  actions of the neural net, once the core units have been identified.[[INT-NEG,RWK-NEG,MET-NEG,ANA-NEG], [SUB-NEG,IMP-NEG,EMP-NEG], [SMY,DFT], [MIN]]\nCurrently, the focus seems to be on demonstrating that the classifier\nperformance is maintained as a significant fraction of hidden units are masked.[[RWK-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [GEN]]\nHowever, there is not enough analysis on showing whether and how the identified\nhidden units help \"interpret\" the model.\[[RWK-NEG,ANA-NEG], [IMP-NEG], [DFT], [MIN]]n\nQuality\nThe idea explored in the paper is interesting and the experiments are described\nin enough detail. [[INT-POS,PDI-POS,EXP-NEG], [SUB-NEG,IMP-POS,EMP-NEG], [SMY,DFT,APC], [GEN]]However, the writing still needs to be polished.[[INT-NEU,RWK-NEU], [CLA-NEG,IMP-NEG], [DFT], [MIN]]\n\nClarity\nThe problem formulation and objective function (Section 3.1) was hard to follow.\[[INT-NEU,EXP-NEG,MET-NEG], [EMP-NEG], [DFT], [MIN]]n\nOriginality\nThis approach to finding important hidden units is novel[[RWK-POS,MET-POS], [NOV-POS], [APC], [MAJ]].\n\nSignificance\nThe paper addresses an important problem of trying to have more interpretable\nneural networks.[[INT-NEU,PDI-NEU,MET-NEU], [IMP-NEU,EMP-NEU], [SMY,DIS], [GEN]] However, it only identifies hidden units that are important for\na class, not what are important for any particular input.[[RWK-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SMY,DIS], [GEN]]  Moreover, the main\nthesis of the paper is to describe a method that helps interpret neural network\nclassifiers.[[RWK-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] However, the experiments only focus on identifying important hidden\nunits and fall short of actually providing an interpretation using these hidden\nunits."[[RWK-NEG,EXP-NEG,MET-NEG], [IMP-NEG,EMP-NEG], [DFT], [MIN]]