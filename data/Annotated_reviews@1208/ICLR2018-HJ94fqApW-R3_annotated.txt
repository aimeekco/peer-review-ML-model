"This paper proposes an interesting  approach to prune a deep model from a computational point of view. [[INT-POS,MET-POS], [IMP-POS,EMP-POS], [APC], [MAJ]]The idea is quite simple as pruning using the connection in the batch norm layer[[PDI-NEU,EXP-NEU], [EMP-NEU], [SMY], [GEN]]. It is interesting to add the memory cost per channel into the optimization process. [[RWK-NEU,EXP-NEG,ANA-NEU], [EMP-NEG], [SUG,DFT], [GEN]]\n\nThe paper suggests normal pruning does not necessarily preserve the network function.[[RWK-NEU,MET-NEU], [EMP-NEU], [SMY,DIS], [GEN]] I wonder if this is also applicable to the proposed method and how can this be evidenced.[[RWK-NEG,MET-NEG], [EMP-NEG], [DFT,QSN], [MIN]] \n\nAs strong points, the paper is easy to follow and does a good review of existing methods.[[INT-POS,RWK-POS,MET-POS], [SUB-POS,IMP-POS,EMP-POS], [APC], [MAJ]] Then, the proposal is simple and easy to reproduce and leads to interesting results[[PDI-POS,RES-POS], [IMP-POS], [APC], [MAJ]]. It is clearly written (there are some typos / grammar errors).[[INT-POS], [CLA-POS], [APC], [MAJ]] \n\nAs weak points:\n1) The paper claims the selection of \\alpha is critical but then, this is fixed empirically without proper sensitivity analysis.[[RWK-NEG,PDI-NEG,EXP-NEG], [EMP-NEG], [DFT], [MIN]] I would like to see proper discussion here. [[RWK-NEG], [IMP-NEG], [DIS], [GEN]]Why is \\alpha set to 1.0 in the first experiment while set to a different number elsewhere.[[RWK-NEU,EXP-NEU], [null], [QSN], [GEN]] \n\n2) how is the pruning (as post processing) performed for the base model (the so called model A).[[EXP-NEU,MET-NEU], [null], [QSN], [GEN]]\n\nIn section 4, in the algorithmic steps[[RWK-NEU,MET-NEU], [null], [DIS], [GEN]]. How does the 4th step compare to the statement in the initial part of the related work suggesting zeroed-out parameters can affect the functionality of the network?[[RWK-NEU,DAT-NEU,EXP-NEU], [IMP-NEU,EMP-NEU], [DIS], [GEN]]\n\n3) Results for CIFAR are nice although not really impressive as the main benefit comes from the fully connected layer as expected."[[EXP-POS,MET-POS,RES-POS], [IMP-POS,EMP-POS], [APC], [MAJ]]