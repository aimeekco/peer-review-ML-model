"The paper proposes training neural networks using a trust region method, in which at each iteration a (non-convex) quadratic approximation of the objective function is found, and the minimizer of this quadratic within a fixed radius is chosen as the next iterate, with the radius of the trust region growing or shrinking at each iteration based on how closely the gains of the quadratic approximation matched those observed on the objective function.[[PDI-NEU,EXP-NEU,MET-NEU], [null], [SMY], [GEN]] The authors claim that this approach is better at avoiding \"narrow\" local optima, and therefore will tend to generalize better than minibatched SGD.[[MET-NEU], [null], [SMY], [GEN]] The main novelty seems to be algorithm 2, which finds the minimizer of the quadratic approximation within the trust region by performing GD iterations until the boundary is hit (if it is--it might not, if the quadratic is convex), and then Riemannian GD along the boundary.[[MET-POS], [NOV-POS], [APC], [MAJ]]\n\nThe paper contains several grammatical mistakes, and in my opinion could explain things more clearly, particularly when arguing that the algorithm 2 will converge.[[MET-NEU,OAL-NEG], [CLA-NEG,EMP-NEU], [SUG,CRT], [MIN]] I had particular difficulty accepting that the phase 1 GD iterates would never hit the boundary if the quadratic was strongly convex, although I accept that it is true due to the careful choice of step size and initialization (assumptions 1 and 2).[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\nThe central claim of the paper, that a trust region method will be better at avoiding narrow basins, seems plausible, since if the trust region is sufficiently large then it will simply pass straight over them.[[PDI-NEU,MET-NEU], [EMP-NEU], [DIS], [MAJ]] But if this is the case, wouldn't that imply that the quadratic approximation to the objective function is poor, and therefore that line 5 of algorithm 1 should shrink the trust region radius?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Additionally, at some times the authors seem to indicate that the trust region method should be good at escaping from narrow basins (as opposed to avoiding them in the first place), see for example the left plot of figure 4.[[MET-NEU,TNF-NEU], [null], [DIS], [MIN]] I don't see why this is true--the quadratic approximation would be likely to capture the narrow basin only.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nThis skepticism aside, the experiments in figure 2 do clearly show that, while the proposed approach doesn't converge nearly as quickly as SGD in terms of training loss, it does ultimately find a solution that generalizes better, as long as both SGD and TR use the same batch size (but I don't see why they should be using the same batch size).[[EXP-NEG,MET-NEG,TNF-NEG], [EMP-NEG], [CRT], [MAJ]] How does SGD with a batch size of 1 compare to TR with the batch sizes of 512 (CIFAR10) or 1024 (STL10)?[[MET-NEU], [CMP-NEU], [QSN], [MIN]]\n\nSection 4.3 (Figure 3) contain a very nice experiment that I think directly explores this issue, and seems to show that SGD with a batch size of 64 generalizes better than TR at any of the considered batch sizes (but not as well as the proposed TR+SGD hybrid).[[EXP-POS,RES-POS], [EMP-POS], [APC], [MAJ]] Furthermore, 64 was the smallest batch size considered, but SGD was performing monotonically better as the batch size decreased, so one would expect it to be still better for 32, 16, etc.[[DAT-NEU,MET-NEU], [SUB-NEU,EMP-NEU], [SUG], [MAJ]]\n\nSmaller comments:\n\nYou say that you base the Hessian and gradient estimates on minibatched samples.[[DAT-NEU], [null], [DIS], [GEN]] I assume that the same is true for the evaluations of F on line 4 of Algorithm 1?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Do these all use the same minibatch, at each iteration?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nOn the top of page 3: \"M is the matrix size\".[[MET-NEU], [null], [DIS], [GEN]] Is this the number of elements, or the number of rows/columns?[[CNT], [null], [QSN], [MIN]]\n\nLemma 1: This looks correct to me, but are these the KKT conditions, which I understand to be first order optimality conditions (these are second order)?[[MET-POS], [EMP-NEU], [QSN], [MIN]] You cite Nocedal & Wright, but could you please provide a page number (or at least a chapter)?[[BIB-NEU], [null], [QSN], [MIN]]\n\nOn the top of page 5, \"Line 10 of Algorithm 1\": I think you mean Line 11 of Algorithm 2."[[MET-NEU], [CLA-NEU], [DIS], [MIN]]