"The authors describe a new defense mechanism against adversarial attacks on classifiers (e.g., FGSM).[[INT-NEU], [null], [SMY], [GEN]] They propose utilizing Generative Adversarial Networks (GAN), which are usually used for training generative models for an unknown distribution, but have a natural adversarial interpretation.[[MET-NEU], [null], [SMY], [GEN]] In particular, a GAN consists of a generator NN G which maps a random vector z to an example x, and a discriminator NN D which seeks to discriminate between an examples produced by G and examples drawn from the true distribution.[[MET-NEU], [null], [SMY], [GEN]] The GAN is trained to minimize the max min loss of D on this discrimination task, thereby producing a G (in the limit) whose outputs are indistinguishable from the true distribution by the best discriminator.[[MET-NEU], [null], [SMY], [GEN]] \n\nUtilizing a trained GAN, the authors propose the following defense at inference time.[[MET-NEU], [null], [SMY], [GEN]] Given a sample x (which has been adversarially perturbed), first project x onto the range of G by solving the minimization problem z* = argmin_z ||G(z) - x||_2. This is done by SGD.[[EXP-NEU,MET-NEU], [null], [SMY], [GEN]] Then apply any classifier trained on the true distribution on the resulting x* = G(z*).[[EXP-NEU,MET-NEU], [null], [SMY], [GEN]] \n\nIn the case of existing black-box attacks, the authors argue (convincingly) that the method is both flexible and empirically effective.[[MET-NEU], [EMP-NEU], [SMY], [MAJ]] In particular, the defense can be applied in conjunction with any classifier (including already hardened classifiers), and does not assume any specific attack model.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] Nevertheless, it appears to be effective against FGSM attacks, and competitive with adversarial training specifically to defend against FGSM.[[MET-POS], [EMP-POS], [APC], [MAJ]] \n\nThe authors provide less-convincing evidence that the defense is effective against white-box attacks.[[EXP-NEU], [EMP-NEU], [DFT], [MAJ]] In particular, the method is shown to be robust against FGSM, RAND+FGSM, and CW white-box attacks.[[MET-POS], [EMP-POS], [APC], [MAJ]] However, it is not clear to me that the method is invulnerable to novel white-box attacks.[[MET-NEU], [EMP-NEU], [DFT], [MAJ]] In particular, it seems that the attacker can design an x which projects onto some desired x* (using some other method entirely), which then fools the classifier downstream.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]]\n\nNevertheless, the method is shown to be an effective tool for hardening any classifier against existing black-box attacks \n(which is arguably of great practical value).[[MET-POS], [EMP-POS], [APC], [MAJ]] It is novel and should generate further research with respect to understanding its vulnerabilities more completely.[[FWK-POS], [NOV-POS], [APC], [MAJ]] \n\nMinor Comments:\nThe sentence starting \u201cUnless otherwise specified\u2026\u201d at the top of page 7 is confusing given the actual contents of Tables 1 and 2, which are clarified only by looking at Table 5 in the appendix. This should be fixed.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]]  \n"