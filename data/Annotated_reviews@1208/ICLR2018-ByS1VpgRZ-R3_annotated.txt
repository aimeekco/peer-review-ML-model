"This manuscript makes the case for a particular parameterization of conditional GANs, specifically how to add conditioning information into the network. [[INT-NEU], [null], [SMY], [GEN]] It motivates the method by examining the form of the log density ratio in the continuous and discrete cases.[[MET-NEU], [null], [SMY], [GEN]]\n\nThis paper's empirical work is quite strong, bringing to bare nearly all of the established tools we currently have for evaluating implicit image models (MS-SSIM, FID, Inception scores).[[MET-POS], [EMP-POS], [APC], [MAJ]] \n\nWhat bothers me is mostly that, while hyperparameters are stated (and thank you for that), they seem to be optimized for the candidate method rather than the baseline.[[RWK-NEU,MET-NEU], [EMP-NEU], [DFT], [MAJ]] In particular, Beta1 = 0 for the Adam momentum coefficient seems like a bold choice based on my experience.[[MET-POS], [EMP-POS], [APC], [MAJ]] It would be an easier sell if hyperparameter search details were included and a separate hyperparameter search were conducted for the candidate and control, allowing the baseline to put its best foot forward.[[RWK-NEU,MET-NEU], [EMP-NEU], [SUG], [MAJ]]\n\nThe sentence containing \"assume that the network model can be shared\" had me puzzled for a few minutes.[[MET-NEG], [CLA-NEG], [DFT], [MIN]] I think what is meant here is just that we can parameterize the log density ratio directly (including some terms that belong to the data distribution to which we do not have explicit access). This could be clearer."[[DAT-NEU,MET-NEU], [EMP-NEU], [SUG], [MAJ]]