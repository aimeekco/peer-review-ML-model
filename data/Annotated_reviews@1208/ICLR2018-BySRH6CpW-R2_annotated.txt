"Summary of the paper:\nThe paper suggests to use stochastic parameters in combination with the local reparametrisation trick (previously introduced by Kingma et al. (2015)) to train neural networks with binary or ternary wights. [[INT-NEU,RWK-NEU,BIB-NEU], [null], [SMY], [GEN]]Results on MNIST, CIFAR-10 and ImageNet are very competitive[[RWK-NEU,RES-NEU], [null], [SMY], [GEN]]. \n\nPros:\n- The proposed method leads to state of the art results .[[RWK-NEU,RES-POS], [EMP-POS], [SMY], [MAJ]]\n- The paper is easy to follow and clearly describes the implementation details needed to reach the results. [[MET-POS,RES-NEU], [CLA-POS,EMP-POS], [APC], [MAJ]]\n\nCons:\n- The local reprarametrisation trick it self is not new and applying it to a multinomial distribution (with one repetition) instead of a Gaussian is straight forward,;[[PDI-NEG,MET-NEU], [EMP-NEU], [CRT], [MAJ]] but its application for learning discrete networks is to my best knowledge novel and interesting.[[PDI-POS], [NOV-POS], [APC], [MAJ]] \n\nIt could be nice to include the results of Zuh et al (2017) in the results table and to indicate the variance for different samples of weights resulting from your methods in brackets.[[TNF-NEU,BIB-NEU], [PNF-NEU], [SUG], [MAJ]] \n\n\nMinor comments:\n- Some citations have a strange format: e.g. \u201cin Hubara et al. (2016); Restegari et al. (2016)\u201c would be better readable as   \u201cby Hubara et al. (2016) and Restegari et al. (2016)\u201c.[[BIB-NEG], [PNF-NEG], [SUG], [MIN]] \n-  To improve notation, it could be directly written that W is the set of all w^l_{i,j} and \\mathcal{W} is the joint distribution resulting from independently sampling from  \\mathcal{W}^l_{i,j}.[[MET-NEU], [PNF-NEG], [SUG], [MIN]] \n- page 6: \u201con the last full precision network\u201d: should probably be \u201con the last full precision layer\u201d\n[[MET-NEU], [CLA-NEG], [SUG], [MIN]]                    \u201c distributions has\u201d ->  \u201c distributions have\u201d.[[MET-NEU], [CLA-NEG], [SUG], [MIN]] \n"