"This paper proposes a model for solving the WikiSQL dataset that was released recently.[[INT-NEU], [null], [SMY], [GEN]]\n\nThe main issues with the paper is that its contributions are not new.[[PDI-NEG], [NOV-NEG], [CRT], [MAJ]]\n\n* The first claimed contribution is to use typing at decoding time (they don't say why but this helps search and learning). [[PDI-NEU], [EMP-NEG], [SMY], [MAJ]]Restricting the type of the decoded tokens based on the programming language has already been done by the Neural Symbolic Machines of Liang et al. 2017. [[RWK-NEU,MET-NEU], [NOV-NEG], [CRT], [MAJ]]Then Krishnamurthy et al. expanded that in EMNLP 2017 and used typing in a grammar at decoding time.[[RWK-NEU], [NOV-NEG], [CRT], [MAJ]] I don't really see why the authors say their approach is simpler, it is only simpler because the sub-language of sql used in wikisql makes doing this in an encoder-decoder framework very simple, but in general sql is not regular.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Of course even for CFG this is possible using post-fix notation or fixed-arity pre-fix notation of the language as has been done by Guu et al. 2017 for the SCONE dataset, and more recently for CNLVR by Goldman et al., 2017.[[RWK-NEU], [NOV-NEG], [CRT], [MAJ]]\n\nSo at least 4 papers have done that in the last year on 4 different datasets, and it is now close to being common practice so I don't really see this as a contribution.[[RWK-NEU,PDI-NEG,DAT-NEU], [NOV-NEG], [CRT], [MAJ]]\n\n* The authors explain that they use a novel loss function that is better than an RL based function used by Zhong et al., 2017.[[RWK-NEU,MET-NEU], [NOV-NEU,CMP-NEU], [SMY], [GEN]] If I understand correctly they did not implement Zhong et al. only compared to their numbers which is a problem because it is hard to judge the role of optimization in the results.[[RWK-NEG,MET-NEG,RES-NEU], [CMP-NEU,EMP-NEG], [CRT], [MAJ]]\n\nMoreover, it seems that the problem they are trying to address is standard - they would like to use cross-entropy loss when there are multiple tokens that could be gold.[[MET-NEU], [EMP-NEU], [CRT], [MAJ]] the standard solution to this is to just have uniform distribution over all gold tokens and minimize the cross-entropy between the predicted distribution and the gold distribution which is uniform over all tokens. [[MET-NEU], [EMP-NEU], [SUG], [MAJ]]The authors re-invent this and find it works better than randomly choosing a gold token or taking the max.[[MET-NEU], [EMP-POS], [SMY], [GEN]] But again, this is something that has been done already in the context of pointer networks and other work like See  et al. 2017 for summarization and Jia et al., 2016 for semantic parsing.[[RWK-NEU], [NOV-NEG], [CRT], [MAJ]]\n\n* As for the good results - the data is new, so it is probable that numbers are not very fine-tuned yet so it is hard to say what is important and what not for final performance.[[DAT-NEU,RES-POS,ANA-NEU], [EMP-NEU], [DIS], [MAJ]] In general I tend to agree that using RL for this task is probably unnecessary when you have the full program as supervision."[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]