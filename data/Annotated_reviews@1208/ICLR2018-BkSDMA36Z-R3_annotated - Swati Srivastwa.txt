"() Summary\nIn this paper, the authors introduced a new simple model for text classification, which obtains state of the art results on several benchmark. [[INT-NEU,MET-POS], [CMP-POS], [APC], [MAJ]] The main contribution of the paper is to propose a new technique to learn vector representation of fixed-size text regions of up to a few words.[[PDI-NEU,MET-NEU], [NOV-NEU], [SMY], [GEN]]  In addition to learning a vector for each word of the vocabulary, the authors propose to also learn a \"context unit\" of size d x K, where d is the embedding size and K the region size.[[MET-NEU], [null], [SMY], [GEN]]  Thus, the model also have a vector representation for pair of word and position in the region.[[MET-NEU], [null], [SMY], [GEN]] Then, given a region of K words, its vector representation is obtained by taking the elementwise product of the \"context unit\" of the middle word and the matrix obtained by concatenating the K vectors of words appearing in the region (the authors also propose a second model where the role of word vectors and \"context\" vectors are exchanged)[[MET-NEU], [null], [SMY], [GEN]]. The max-pooling operation is then used to obtain a vector representation of size d.[[MET-NEU], [null], [SMY], [GEN]] Then a linear classifier is applied on top of the sum of the region embeddings. [[MET-NEU], [null], [SMY], [GEN]] The authors then compare their approach to previous work on the 8 datasets introduced by Zhang et al. (2015).[[RWK-NEU,DAT-NEU,MET-NEU], [CMP-NEU], [SMY], [GEN]] They obtain state of the art results on most of the datasets.[[DAT-NEU,RES-NEU], [null], [DIS], [GEN]] They also perform some analysis of their models, such as the influence of the region size, embedding size, or replacing the \"context units\" vector by a scalar.[[ANA-NEU], [null], [SMY], [GEN]] The authors also provide some visualisation of the parameters of their model.[[MET-NEU], [null], [SMY], [GEN]]\n\n() Discussion\nOverall, I think that the proposed method is sound and well justified.[[MET-POS], [EMP-POS], [APC], [MAJ]] The empirical evaluations, analysis and comparisons to existing methods are well executed.[[MET-POS,ANA-POS], [CMP-POS], [APC], [MAJ]] I liked the fact that the proposed model is very simple, yet very competitive compared to the state-of-the-art.[[RWK-POS,MET-POS], [CMP-POS], [APC], [MAJ]] I suspect that the model is also computationally efficient: can the authors report training time for different datasets?[[DAT-NEU,EXP-NEU,MET-POS,ANA-POS], [EMP-POS], [APC,QSN], [MAJ]] I think that it would make the paper stronger.[[OAL-NEU], [null], [DIS], [GEN]] One of the main limitations of the model, as stated by the authors, is its number of parameters.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Could the authors also report these?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nWhile the paper is fairly easy to read (because the method is simple and Figure 1 helps understanding the model), I think that copy editing is needed.[[MET-POS,TNF-POS], [CLA-POS], [APC], [MAJ]] Indeed, the papers contains many typos (I have listed a few), as well as ungrammatical sentences.[[OAL-NEG], [CLA-NEG], [CRT], [MIN]] I also think that a discussion of the \"attention is all you need\" paper by Vaswani et al. is needed, as both articles seem strongly related.[[RWK-NEU], [CMP-NEU], [DIS], [MIN]]\n\nAs a minor comment, I advise the authors to use a different letter for \"word embeddings\" and the \"projected word embeddings\" (equation at the bottom of page 3).[[MET-NEU], [CLA-NEU], [SUG], [MIN]] It would also make the paper more clear.[[OAL-NEU], [CLA-NEU], [DIS], [MIN]] \n\n() Pros / Cons:\n+ simple yet powerful method for text classification[[MET-POS], [EMP-POS], [APC], [MAJ]] \n+ strong experimental results[[MET-POS], [EMP-POS], [APC], [MAJ]] \n+ ablation study / analysis of influence of parameters[[ANA-POS], [EMP-POS], [APC], [MAJ]] \n- writing of the paper[[OAL-NEU], [CLA-NEU], [CNT], [GEN]] \n- missing discussion to the \"attention is all you need paper\", which seems highly relevant[[CNT], [SUB-NEU], [DFT,SUG], [MIN]] \n\n() Typos:\nPage 1\n\"a support vectors machineS\" -> \"a support vector machine[[CNT], [CLA-NEG], [CRT], [MIN]] \"\n\"performs good\" -> \"performs well[[CNT], [CLA-NEG], [CRT], [MIN]] \"\n\"the n-grams was widely\" -> \"n-grams were widely[[CNT], [CLA-NEG], [CRT], [MIN]] \"\n\"to apply large region size\" -> \"to apply to large region size\"\n\"are trained separately\" -> \"do not share parameters\"[[RWK-NEU], [CLA-NEG], [CRT], [MIN]]\n\nPage 2\n\"convolutional neural networks(CNN)\" -> \"convolutional neural networks (CNN)[[CNT], [CLA-NEG], [CRT], [MIN]] \"\n\"related works\" -> \"related work \"[[RWK-NEU], [CLA-NEG], [CRT], [MIN]]\n\"effective in Wang and Manning\" -> \"effective by Wang and Manning[[CNT], [CLA-NEG], [CRT], [MIN]] \"\n\"applied on text classification\" -> \"applied to text classification[[CNT], [CLA-NEG], [CRT], [MIN]] \"\n\"shard(word independent)\" -> \"shard (word independent)\"[[CNT], [CLA-NEG], [CRT], [MIN]] \n\nPage 3\n\"can be treat\" -> \"can be treated\"\n\"fixed length continues subsequence\" -> \"fixed length contiguous subsequence\"[[CNT], [CLA-NEG], [CRT], [MIN]]\n\"w_i stands for the\" -> \"w_i standing for the\"\n\"which both the unit\" -> \"where both the unit\"\n\"in vocabulary\" -> \"in the vocabulary\"\n\netc..."[[CNT], [CLA-NEG], [CRT], [MIN]]