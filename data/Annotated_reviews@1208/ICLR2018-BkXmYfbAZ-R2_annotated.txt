"This paper proposes a new approach for multi-task learning.[[INT-NEU], [null], [SMY], [GEN]] While previous approaches assumes the order of shared layers are the same between tasks, this paper assume the order can vary across tasks, and the (soft) order is learned during training.[[MET-NEU], [null], [SMY], [GEN]]  They show improved performance on a number of multi-task learning problems.[[EXP-NEU,RES-POS], [null], [SMY], [GEN]] \n\nMy primary concern about this paper is the lack of interpretation on permuting the layers.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] For example, in standard vision systems, low level filters \"V1\" learn edge detectors (gabor filters) and higher level filters learn angle detectors [1].[[RWK-NEU], [CMP-NEU], [DIS], [MAJ]] It is confusing why permuting these filters make sense.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] They accept different inputs (raw pixels vs edges).[[MET-NEU], [EMP-NEU], [SMY], [MAJ]] Moreover, if the network contains pooling layers, different locations of the pooling layer result in different shapes of the feature map, and the soft ordering strategy Eq. (7) does not work.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] \n\nIt makes sense that the more flexible model proposed by this paper performs better than previous models.[[MET-POS], [EMP-POS], [APC], [MAJ]] The good aspect of this paper is that it has some performance improvements.[[RES-POS], [null], [APC], [MAJ]] But I still wonder the effect of permuting the layers.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] The paper also needs more clarifications in the writing.[[OAL-NEU], [CLA-NEU], [SUG], [MAJ]] For example, in Section 3.3, how each s_(i, j, k) is sampled from S?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] The \"parallel ordering\" terminology also seems to be arbitrary...[[MET-NEU], [CLA-NEU], [CRT], [MIN]]\n\n[1] Lee, Honglak, Chaitanya Ekanadham, and Andrew Y. Ng. \"Sparse deep belief net model for visual area V2.\" Advances in neural information processing systems. 2008.[[BIB-NEU], [null], [SUG], [MIN]]"