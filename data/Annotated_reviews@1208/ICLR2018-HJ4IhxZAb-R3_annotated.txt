"Overview\n\nThe authors propose a reinforcement learning approach to learn a general active query policy from multiple heterogeneous datasets.[[INT-NEU,PDI-NEU,DAT-NEU,MET-NEU], [IMP-NEU,EMP-NEU], [SMY,DIS], [GEN]] The reinforcement learning part is based on a policy network, which selects the data instance to be labeled next.[[RWK-NEU,DAT-NEU], [EMP-NEU], [SMY], [GEN]]They use meta-learning on feature histograms to embed heterogeneous datasets into a fixed dimensional representation.[[RWK-NEU,DAT-NEU,MET-NEU], [EMP-NEU], [APC], [GEN]] The authors argue that policy-based reinforcement learning allows learning the criteria of active learning non-myopically.[[RWK-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] The experiments show the proposed approach is effective on 14 UCI datasets[[PDI-POS,DAT-NEU,EXP-NEU], [IMP-POS,EMP-POS], [APC], [MAJ]].\n\nstrength\n\n* The paper is mostly clear and easy to follow[[INT-POS], [CLA-POS], [APC], [MAJ]].\n* The overall idea is interesting and has many potentials.[[OAL-POS], [SUB-POS,IMP-POS], [APC], [MAJ]]\n* The experimental results are promising on multiple datasets.[[RWK-POS,DAT-POS,EXP-POS], [IMP-POS,EMP-POS], [APC], [MAJ]]\n* There are thorough discussion with related works[[RWK-NEG], [null], [DIS], [MIN]].\n\nweakness\n\n* The graph in p.3 don't show the architecture of the network clearly.[[RWK-NEG,TNF-NEG], [PNF-NEG], [DFT], [MIN]]\n* The motivation of using feature histograms as embedding is not clear[[PDI-NEG,MET-NEG], [EMP-NEG], [DFT], [MIN]].\n* The description of the 2-D histogram on p.4 is not clear.[[PDI-NEG], [CLA-NEG], [DFT], [MIN]] The term \"posterior value\" sounds ambiguous[[DAT-NEG], [null], [DFT], [MIN]].\n* The experiment sets a fixed budget of only 20 instances, which seems to be rather few in some active learning scenarios, especially for non-linear learners.[[RWK-NEU,DAT-NEU,EXP-NEG], [EMP-NEG], [DFT], [MIN]] Also, the experiments takes a fixed 20K iterations for training, and the convergence status (e.g. whether the accumulated gradient has stabilized the policy) is not clear.[[RWK-NEG,DAT-NEG,EXP-NEG], [EMP-NEG], [DFT], [MIN]]\n* Are there particular reasons in using policy learning instead of other reinforcement learning approaches[[RWK-NEU,MET-NEU], [null], [QSN], [GEN]]?\n* The term A(Z) in the objective function can be more clearly described[[RWK-NEG,MET-NEG], [CLA-NEG], [DFT], [MIN]].\n* While many loosely-related works were surveyed, it is not clear why literally none of them were compared.[[RWK-NEG], [CLA-NEG,CMP-NEU], [DFT], [MIN]] There is thus no evidence on whether a myopic bandit learner (say, Chu and Lin's work) is really worse than the RL policy.[[RWK-NEU,MET-NEU], [CMP-NEU], [SMY], [GEN]] There is also no evidence on whether adaptive learning on the fly is needed or not.[[RWK-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]]\n* In Equation 2, should there be a balancing parameter for the reconstruction loss?[[RWK-NEU,DAT-NEG,EXP-NEG,ANA-NEG], [CMP-NEG,EMP-NEG], [DFT], [MIN]]\n* Some typos\n    - page 4: some duplicate words in discriminative embedding session\n    - page 4: auxliary -> auxiliary\n    - page 7: tescting -> testing\n\n[[RWK-NEG,MET-NEG], [PNF-NEG,EMP-NEG], [DFT,CRT], [MIN]]"
