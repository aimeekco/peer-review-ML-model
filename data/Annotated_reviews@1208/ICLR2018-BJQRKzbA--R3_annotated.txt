"The authors present a novel evolution scheme applied to neural network architecture search.[[INT-POS,MET-POS], [NOV-POS], [APC], [MAJ]] It relies on defining an expressive search space for conducting optimization, with a constrained search space that leads to a lighter and more efficient algorithm.[[MET-NEU], [null], [SMY], [GEN]] To balance these constraints, they grow sub-modules in a hierarchical way to form more and more complex cells.[[MET-NEU], [null], [SMY], [GEN]] Hence, each level is limited to a small search space while the system as a whole converges toward a complex structure.[[MET-NEU], [null], [SMY], [GEN]] To speed up the search, they focus on finding cells instead of an entire network.[[MET-NEU], [null], [DIS], [GEN]] In evaluation time, they insert these cells between layers of a network comparable in size to known networks.[[EXP-NEU,MET-NEU], [null], [DIS], [GEN]] They find complex cells that lead to state-of-the-art performance on benchmark dataset CIFAR-10 and ImageNet.[[RWK-POS,DAT-POS], [EMP-POS], [APC], [MAJ]] They also claim that their method is reaching a new milestone in evolutionary search strategies performance.[[MET-NEU], [null], [DIS], [GEN]]\n\nThe method proposed for an hierarchical representation for optimizing over neural network designs is well thought and sound.[[MET-POS], [EMP-POS], [APC], [MAJ]] It could lead to new insight on automating design of neural networks for given problems.[[FWK-POS], [IMP-POS], [APC], [MAJ]] In addition, the authors present results that appear to be on par with the state-of-the-art with architecture search on CIFAR-10 and ImageNet benchmark datasets.[[RWK-POS,DAT-POS,RES-POS], [EMP-POS], [APC], [MAJ]] The paper presents a good work and is well articulated. [[OAL-POS], [EMP-POS], [APC], [MAJ]]However, it could benefit from additional details and a deeper analysis of the results.[[RES-NEU,ANA-NEU], [SUB-NEU], [SUG], [MIN]]\n\nThe key idea is a smart evolution scheme.[[PDI-POS], [EMP-POS], [APC], [MAJ]] It circumvents the traditional tradeoff between search space size and complexity of the found models.[[MET-NEU], [null], [DIS], [GEN]] The method is also appealing for its use of some kind of emergence between two levels of hierarchy.[[MET-NEU], [null], [DIS], [GEN]] In fact, it could be argued that nature tends to exploit the same phenomenon when building more and more complex molecules.[[MET-NEU], [null], [DIS], [GEN]] Thought, the paper could benefit from a more detailed analysis of the architectures found by the algorithm.[[ANA-NEU], [SUB-NEU], [DFT], [MIN]] Do the modules always become more complex as they jump from a level to another or there is some kind of inter-level redundancy?[[MET-NEU], [null], [QSN], [MIN]] Are the cells found interpretable?[[MET-NEU], [null], [QSN], [MIN]] The authors should try to give their opinion about the design obtained.[[EXT-NEU], [null], [DIS], [GEN]]\n\nThe implementation seems technically sound. The experiments and results section shows that the authors are confident and the evaluation seems correct. However, paragraphs on the architectures could be a bit clearer for the reader. The diagram could be more complete and reflect better the description.[[MET-POS], [EMP-POS], [APC], [MAJ]] During evaluation, what is a step?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] A batch or an epoch or other?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nThe method seems relatively efficient as it took 36 hours to converge in a field traditionally considered as heavy in terms of computation, but at the requirement of using 200 GPU. [[MET-POS], [EMP-POS], [APC], [MAJ]]It raises questions on the usability of the method for small labs.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] At some point, we will have to use insights from this search to stop early, when no improvement is expected.[[FWK-NEG], [IMP-NEG], [QSN], [MIN]] Also, authors claim that their method consume less computation time than reinforcement learning.[[MET-NEU], [null], [DIS], [GEN]] This should be supported by some quantitative results.[[RES-NEU], [SUB-NEU], [SUG], [MIN]]\n\nThe paper would greatly benefit from a deeper comparison over other techniques.[[MET-NEG], [null], [SUG], [MIN]] For instance, it could describe more the advantages over reinforcement learning.[[MET-NEU], [null], [DIS], [MIN]] An important contribution is to show that a well-defined architecture representation could lead to efficient cells with a simple randomized search. [[MET-NEU], [null], [DIS], [MIN]]It could have taken more spaces in the paper.[[OAL-NEU], [null], [DIS], [MIN]]\n\nI am also concerned the computational efficiency of the results obtained with this method on current processors.[[MET-NEU,RES-NEU], [EMP-NEU], [DIS], [MIN]] Indeed, the randomness of the found cells could be less efficient in terms of computation that what we can get from a well-structured network designed by hand.[[RES-NEG], [EMP-NEG], [CRT], [MIN]] Exploiting the structure of the GPUs (cache size, sequential accesses, etc.) allows to get best possible performance from the hardware at hand.[[RES-NEG], [EMp-NEG], [CRT], [MIN]] Does the solution obtained with the optimization can be run as efficiently?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] A short analysis forward pass time of optimized cells vs. popular models could be an interesting addition to the paper.[[ANA-NEU], [SUB-NEU], [SUG], [MIN]] This is a general comment over this kind of approach, but I think it should be addressed. \n"[[MET-NEU], [null], [DIS], [MIN]]