"The authors offer a novel version of the random feature map approach to approximately solving large-scale kernel problems: each feature map evaluates the \"fourier feature\" corresponding to the kernel at a set of randomly sampled quadrature points.[[INT-POS], [null], [SMY], [GEN]] This gives an unbiased kernel estimator; they prove a bound its variance and provide experiment evidence that for Gaussian and arc-cos kernels, their suggested qaudrature rule outperforms previous random feature maps in terms of kernel approximation error and in terms of downstream classification and regression tasks. [[RWK-POS,EXP-POS,MET-POS], [CMP-POS,EMP-POS], [APC], [MAJ]]The idea is straightforward,[[PDI-POS], [null], [APC], [MAJ]] the analysis seems correct,[[ANA-POS], [null], [APC], [MAJ]] and the experiments suggest the method has superior accuracy compared to prior RFMs for shift-invariant kernels.[[EXP-POS,MET-POS], [CMP-POS], [APC], [MAJ]] The work is original, but I would say incremental, and the relevant literature is cited.[[MET-POS,BIB-POS], [NOV-POS], [APC], [MAJ]]\n\nThe method seems to give significantly lower kernel approximation errors,[[MET-POS], [EMP-POS], [APC], [MIN]] but the significance of the performance difference in downstream ML tasks is unclear[[MET-NEU], [EMP-NEU], [DFT], [MIN]] --- the confidence intervals of the different methods overlap sufficiently to make it questionable whether the relative complexity of this method is worth the effort. [[MET-NEU], [EMP-NEU], [QSN], [MIN]]Since good performance on downstream tasks is the crucial feature that we want RFMs to have, it is not clear that this method represents a true improvement over the state-of-the-art.[[MET-NEU], [EMP-NEU], [DFT], [MIN]] The exposition of the quadrature method is difficult to follow, and the connection between the quadrature rules and the random feature map is never explicitly stated: e.g. equation 6 says how the kernel function is approximated as an integral, but does not give the feature map that an ML practitioner should use to get that approximate integral.[[MET-NEG], [EMP-NEG], [DFT], [MIN]]\n\nIt would have been a good idea to include figures showing the time-accuracy tradeoff of the various methods, which is more important in large-scale ML applications than the kernel approximation error. [[TNF-NEU], [SUB-NEU], [SUG], [MIN]]It is not clear that the method is *not* more expensive in practice than previous methods (Table 1 gives superior asymptotic runtimes, but I would like to see actual run times, as evaluating the feature maps sound relatively complicated compared to other RFMs).[[EXP-NEU,MET-NEU], [CMP-NEU], [DFT], [MIN]] On a related note, I would also like to have seen this method applied to kernels where the probability density in the Bochner integral was not the Gaussian density (e.g., the Laplacian kernel): the authors suggested that their method works there as well when one uses a Gaussian approximation of the density (which is not clear to me)[[EXP-NEU,MET-NEU], [EMP-NEU], [DFT], [MIN]],  --- and it may be the case that sampling from their quadrature distribution is faster than sampling from the original non-Gaussian density."[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]]