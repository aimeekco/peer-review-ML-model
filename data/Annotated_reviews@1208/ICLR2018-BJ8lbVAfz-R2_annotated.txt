"This paper proposes a supervised variant of Kohonen's self-organizing map\n(SOM), i.e., trained by gradient descent with gradient obtained by\nbackprop, using a grid of RBF neurons which respond to the input only if\nthey are in the grid-neighborhood of the 'winner' neuron (closest to the\ninput).[[INT-NEU,PDI-NEU], [CLA-NEG], [CRT], [MIN]] This in itself is not even new, but the authors replace a linear\noutput layer with squared error (proposed in another, earlier paper) by a\nsoftmax layer with cross-entropy.[[MET-NEU], [NOV-NEU], [DIS], [MIN]]  Unsuprisingly, this leads to an improvement.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\nThe title is misleading.[[INT-NEG], [CLA-NEG], [CRT], [MAJ]] There is nothing deep in this architecture.[[ANA-NEG], [SUB-NEG], [CRT], [MIN]] It is\na shallow architecture with a single RBF-like hidden layer.[[ANA-NEG], [SUB-NEG], [CRT], [MIN]]  There is a\ntiny ounce of novelty in that the authors propose to improve a supervised\nversion of the SOM by using what should have been used in the first place\naccording to modern good practice.[[MET-NEG], [NOV-NEG], [CRT], [MIN]] And the whole paper seems as if written\ncirca 2010 or 2012.[[OAL-NEU], [null], [DIS], [GEN]]  Another misleading thing is the term self-organizing\nused throughout, which is roughly synonym to learning according to me, and\nnot something uniquely belonging to the SOM family of models, as used by\nthe authors.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]  As an example of time-travel to the past, the authors talk\nabout RBMs and stacks of auto-encoders as if that was the deep learning\nstate-of-the-art.[[RWK-NEU], [CMP-NEU], [DIS], [MIN]] The authors even call these methods 'recent'! Clearly not\nthe case. [[MET-NEG], [EMP-NEG], [CRT], [MIN]] Unfortunately, it's not just talk, they are also the point of\ncomparison in the experiments, i.e., there are no comparison with modern\ndeep learning methods.[[EXP-NEG], [CMP-NEG], [CRT], [MAJ]] Even the datasets are outdated (from the 90s?).[[DAT-NEG], [NOV-NEG], [CRT], [MAJ]]\nVocabulary is wrong in other places, for example the word semi-supervised\nis wrongly understood and used. [[MET-NEG], [CLA-NEG], [CRT], [MIN]]Semi-supervised means that one combines\nlabeled and unlabeled data.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] Where the label 'semi-supervised' is used (page\n4) is actually wrong: yes the labels are used, but of course it is the\n*gradients* which show up in the update, not the labels themselves\ndirectly.[[CNT], [EMP-NEU], [DIS], [MIN]] It's also not true that there is little research in understanding\nthe formation of internal representations.[[CNT], [EMP-NEU], [DIS], [MIN]] There is a whole subfields of\npapers trying to interpret the features learned by deep networks, and much\nwork designing learning frameworks and objectives to achieve better\nrepresentations, e.g, to better disentangle the underlying factors.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\nIt's also common practice to analyze the representations learned, in\nmany deep learning papers.[[ANA-NEU], [EMP-NEU], [DIS], [MIN]]\n\nThe paper uses much space to show how to compute gradients in the proposed\narchitecture: there is obviously no need for this in a day and age where\ngradients are automatically derived by software.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\nThe cherry on the sundae are the experimental results.[[EXP-POS,RES-POS], [EMP-POS], [APC], [MAJ]] How could the authors\nget 16% on MNIST with an MLP of any kind?[[DAT-NEU,RES-NEU], [EMP-NEU], [QSN], [MAJ]] It does not seem right at all.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]]\nEven a linear regression would get at least half of that.[[RES-NEG], [EMP-NEG], [CRT], [MIN]] As there are not\nenough experimental details to judge, it's hard to figure out the problem,\nbut this ppaper is clearly not publishable at any of the quality machine\nlearning venues, for weakness in originality, quality of the writing,\nand poor experiments.\n"[[EXP-NEG,EXP-NEG,OAL-NEG], [CLA-NEG,NOV-NEG,REC-NEG], [FBK], [MAJ]]..