"This paper answers recent critiques about ``standard GAN'' that were recently formulated to motivate variants based on other losses, in particular using ideas from optimal transport.[[PDI-NEU], [null], [SMY], [GEN]]  It makes main points\n1) ``standard GAN'' is an ill-defined term that may refer to two different learning criteria, with different properties[[RWK-NEG], [null], [DIS], [GEN]]\n2) though the non-saturating variant (see Eq. 3) of ``standard GAN'' may converge towards a minimum of the Jensen-Shannon divergence, it does not mean that the minimization process follows gradients of the Jensen-Shannon divergence (and conversely, following gradient paths of the Jensen-Shannon divergence may not converge towards a minimum, but this was rather the point of the previous critiques about ``standard GAN''). [[RWK-NEG], [CMP-POS], [SMY], [GEN]]\n3) the penalization strategies introduced for ``non-standard GAN'' with specific motivations, may also apply successfully to the ``standard GAN'', improving robustness, thereby helping to set hyperparameters.[[MET-POS], [EMP-POS], [SMY], [GEN]]\nNote that item 2) is relevant in many other setups in the deep learning framework and is often overlooked.[[RWK-NEU], [null], [SMY], [GEN]]\n\nOverall, I believe that the paper provides enough material to substantiate these claims, even if the message could be better delivered.[[OAL-POS], [SUB-POS], [SUG,APC], [MAJ]] In particular, the writing is sometimes ambiguous (e.g. in Section 2.3, the reader who did not follow the recent developments on the subject on arXiv will have difficulties to rebuild the cross-references between authors, acronyms and formulae).[[MET-NEU], [CLA-NEG], [DFT], [MAJ]] The answers to the critiques referenced in the \n paper are convincing, though I must admit that I don't know how crucial it is to answer these critics, since it is difficult to assess wether they reached or will reach a large audience.[[MET-POS], [EMP-POS], [DIS], [MIN]]\n\nDetails:\n- p. 4 please do not qualify KL as a distance metric [[MET-NEG], [EMP-NEG], [SUG], [MIN]]\n- Section 4.3: \"Every GAN variant was trained for 200000 iterations, and 5 discriminator updates were done for each generator update\" is ambiguous: what is exactly meant by \"iteration\" (and sometimes step elsewhere)?[[DAT-NEU,EXP-NEU], [CLA-NEG], [QSN], [MIN]] \n- Section 4.3: the performance measure is not relevant regarding distributions. The l2 distance is somewhat OK for means, but it makes little sense for covariance matrices. "[[MET-NEG], [EMP-NEG], [DFT], [MIN]]