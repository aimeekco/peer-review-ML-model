"This paper considers the problem of Reinforcement Learning in time-limited domains.[[INT-NEU], [null], [SMY], [GEN]] It begins by observing that in time-limited domains, an agent unaware of the remaining time can experience state-aliasing.[[PDI-NEU], [null], [SMY], [GEN]] To combat this problem, the authors suggest modifying the state representation of the policy to include an indicator of the amount of remaining time. [[PDI-NEU], [null], [SMY], [GEN]]The time-aware agent shows improved performance in a time-limited gridworld and several control domains.[[RES-NEU], [null], [SMY], [GEN]] Next, the authors consider the problem of learning a time-unlimited policy from time-limited episodes.[[MET-POS], [EMP-POS], [APC], [MAJ]] They show that by bootstrapping from the final state of the time-limited domain, they are able to learn better policies for the time-unlimited case.[[MET-POS], [EMP-POS], [APC], [MAJ]]\n\nPros:\nThe paper is well-written and clear, if a bit verbose.[[OAL-POS], [CLA-POS], [APC], [MAJ]] \nThe paper has extensive experiments in a variety of domains.[[EXP-POS], [SUB-POS,EMP-POS], [APC], [MAJ]]\n\nCons:\nIn my opinion, the substance of the contribution is not enough to warrant a full paper and the problem of time-limited learning is not well motivated:[[PDI-NEG,OAL-NEG], [REC-NEG], [FBK], [MAJ]] \n\n1) It's not clear how frequently RL agents will encounter time-limited domains of interest.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Currently most domains are terminated by failure/success conditions rather than time.[[MET-NEU], [null], [DIS], [GEN]] The author's choice of tasks seem somewhat artificial in that they impose time limits on otherwise unlimited domains in order to demonstrate experimental improvement.[[EXP-NEU], [null], [DIS], [GEN]] Is there good reason to think RL agents will need to contend with time-limited domains in the future?[[MET-NEU], [null], [DIS], [GEN]] \n\n2) The inclusion of remaining-time as a part of the agent's observations and resulting improvement in time-limited domains is somewhat obvious.[[RES-NEU], [null], [DIS], [GEN]] It's well accepted that in any partially observed domain, inclusion of the latent variable(s) as a part of the agent's observation will result in a fully observed domain, less state-aliasing, more accurate value estimates, and better performance.[[RES-NEU], [EMP-NEU], [DIS], [MIN]] The author's inclusion of the latent time variable as a part of the agent's observations reconfirms this well-known fact, but doesn't tell us anything new.[[RES-NEG], [null], [DIS], [GEN]]\n\n3) I have the same questions about Partial Episode bootstrapping: Is there a task in which we find our RL agents learning in time-limited settings and then evaluated in unlimited ones?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] The experiments in this direction again feel somewhat contrived by imposing time limits and then removing them.[[EXP-NEG], [EMP-NEG], [CRT], [MIN]] The proposed solution of bootstrapping from the value of the terminal state v(S_T) clearly works, and I suspect that any RL-practitioner faced with training time-limited policies that are evaluated in time-unlimited settings might come up with the same solution.[[RES-NEG], [EMP-NEG], [CRT], [MIN]] While the experiments are well done,[[EXP-POS], [EMP-POS], [APC], [MAJ]] I don't think the substance of the algorithmic improvement is enough.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nI think this paper would improve by demonstrating how time-aware policies can help in domains of interest (which are usually not time-limited).[[OAL-NEU], [EMP-NEU], [SUG], [MIN]] I could imagine a line of experiments that investigate the idea of selectively stopping episodes when the agent is no longer experiencing useful transitions, and then showing that the partial episode bootstrapping can save on overall sample complexity compared to an agent that must experience the entirety of every episode.  "[[EXP-NEU], [EMP-NEU], [DIS], [MIN]]