"This work re-evaluates complex-valued neural networks: complex weights, complex activation functions.[[RWK-NEU,MET-NEU], [null], [DIS], [GEN]]\n\nThe paper acknowledges that complex networks are not new, and that the findings of previous authors is that complex networks perform less well than real-valued alternatives.[[INT-NEU,RWK-NEG], [IMP-NEG], [DFT], [MIN]]\n\nThe paper reports a comparison of real-valued and complex-valued neural networks, controlling for storage capacity (with an interesting discussion of controlling for capacity in terms of computational inference).[[INT-NEU,MET-NEU], [CMP-NEU], [SMY], [GEN]]\n\nThe paper concludes with \"Overall the complex-valued neural networks do not perform as well as expected[[INT-NEU,MET-NEG,OAL-NEU], [IMP-NEG,EMP-NEG], [DFT], [MAJ,MIN]].\" I didn't understand this conclusion, because previous work found complex-valued neural networks to be inferior, which is consistent with the results reported here.[[RWK-NEG,RES-NEG], [IMP-NEG], [DFT], [MIN]] I did not see support in this paper for the claim in the abstract that special architectures make complex networks work better, or that they are well suited to particular data sets[[ABS-NEU,INT-NEG,DAT-NEG], [SUB-NEG], [DFT], [MIN]].\n\nThe empirical results are only presented in table-of-numbers format (graphical comparisons would be easier to understand), and tables 5-8 are all zero, which doesn't make sense for these classification tasks."[[RWK-NEU,RES-NEU,TNF-NEU], [CMP-NEU,EMP-NEU], [SMY], [GEN]]