"This paper present a method for detecting adversarial examples in a deep learning classification setting.[[INT-NEU], [null], [SMY], [GEN]]   The idea is to characterize the latent feature space (a function of inputs) as observed vs unobserved, and use a module to fit a 'cluster-aware' loss that aims to cluster similar classes tighter in the latent space.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] \n\nQuestions/Comments:\n\n- How is the checkpointing module represented?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]  Which parameters are fit using the fine-tuning loss described on page 3?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] \n\n- What is the rationale for setting the gamma (concentration?) parameters to .01?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]  Is that a general suggestion or a data-set specific recommendation?[[DAt-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\n- Are the checkpointing modules designed to only detect adversarial examples?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]  Or is it designed to still classify adversarial examples in a robust way?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nClarity: I had trouble understanding some of this paper.[[OAL-NEG], [CLA-NEG,PNF-NEG], [CRT], [MAJ]]  It would be nice to have a succinct summary of how all of the pieces presented fit together, e.g. the original victim network, fine-tuning loss, per-class dictionary learning w/ OMP.[[EXP-NEU,MET-NEU,RES-NEU], [EMP-NEU], [DIS], [MIN]]  \n\nTechnical: It is hard to tell how some of the components of this approach are technically justified.[[EXP-NEG,MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAj]] \n\nNovel: I am not familiar enough with adversarial deep learning to assess novelty or impact. "[[MET-NEG,OAL-NEG], [NOV-NEG], [CRT], [MAJ]]