"A parallel aproach to DQN training is proposed, based on the idea of having multiple actors collecting data in parallel, while a single learner trains the model from experiences sampled from a central replay memory.[[INT-NEU], [null], [SMY], [GEN]] Experiments on Atari game playing and two MuJoCo continuous control tasks show significant improvements in terms of training time and final performance compared to previous baselines.[[EXP-POS], [EMP-POS], [APC], [GEN]]\n\nThe core idea is pretty straightforward but the paper does a very good job at demonstrating that it works very well, when implemented efficiently over a large cluster (which is not trivial). [[PDI-POS,DAT-POS], [EMP-POS], [APC], [MAJ]]I also appreciate the various experiments to analyze the impact of several settings (instead of just reporting a new SOTA).[[EXP-POS], [EMP-POS], [APC], [MAJ]] Overall I believe this is definitely a solid contribution that will benefit both practitioners and researchers... as long as they got the computational resources to do so![[OAL-POS], [null], [APC], [MAJ]]\n\nThere are essentially two more things I would have really liked to see in this paper (maybe for future work?):\n- Using all Rainbow components\n- Using multiple learners (with actors cycling between them for instance)[[FWK-POS], [SUB-POS], [SUG], [MAJ]]\nSharing your custom Tensorflow implementation of prioritized experience replay would also be a great bonus![[MET-NEU], [EMP-NEU], [SUG], [MAJ]]\n\nMinor points:\n- Figure 1 does not seem to be referenced in the text [[TNF-NEG,BIB-NEG], [CLA-NEG], [DFT], [MIN]]\n- \u00ab In principle, Q-learning variants are off-policy methods \u00bb => not with multi-step unless you do some kind of correction! I think it is important to mention it even if it works well in practice (just saying \u00ab furthermore we are using a multi-step return \u00bb is too vague)[[MET-NEG], [EMP-NEG], [SUG], [MIN]]\n- When comparing the Gt targets for DQN vs DPG it strikes me that DPG uses the delayed weights phi- to select the action, while DQN uses current weights theta. I am curious to know if there is a good motivation for this and what impact this can have on the training dynamics.[[MET-NEU,ANA-NEU], [CMP-NEU], [DIS], [MIN]]\n- In caption of Fig. 5 25K should be 250K[[TNF-NEG], [CLA-NEG], [DFT], [MIN]]\n- In appendix A why duplicate memory data instead of just using a smaller memory size?[[DAT-NEG], [SUB-NEG], [QSN], [MIN]]\n- In appendix D it looks like experiences removed from memory are chosen by sampling instead of just removing the older ones as in DQN. Why use a different scheme?[[DAT-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]]\n- Why store rewards and gamma\u2019s at each time step in memory instead of just the total discounted reward?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n- It would have been better to re-use the same colors as in Fig. 2 for plots in the appendix[[TNF-NEU], [PNF-NEU], [SUG], [MIN]]\n- Would Fig. 10 be more interesting with the full plot and a log scale on the x axis?"[[TNF-NEU], [PNF-NEU], [QSN], [MIN]]