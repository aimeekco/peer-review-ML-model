"Summary: \n\nThe authors consider the use of attention for sensor, or channel, selection.[[INT-NEU], [null], [SMY], [GEN]] The idea is tested on several speech recognition datasets, including TIDIGITS and CHiME3, where the attention is over audio channels, and GRID, where the attention is over video channels.[[PDI-NEU,DAT-NEU], [null], [SMY], [GEN]] Results on TIDIGITS and GRID show a clear benefit of attention (called STAN here) over concatenation of features.[[DAT-POS,RES-POS], [EMP-POS], [APC], [MAJ]] The results on CHiME3 show gain over the CHiME3 baseline in channel-corrupted data.[[DAT-POS,RES-POS], [EMP-POS], [APC], [MAJ]]\n\nReview:\n\nThe paper reads well,[[OAL-POS], [CLA-POS], [APC], [MIN]] but as a standard application of attention lacks novelty.[[OAL-NEG], [NOV-NEG], [DFT,CRT], [MAJ]] The authors mention that related work is generalized but fail to differentiate their work relative to even the cited references (Kim & Lane, 2016; Hori et al., 2017).[[RWK-NEG], [CMP-NEG], [CRT], [MAJ]] Furthermore, while their approach is sold as a general sensor fusion technique,[[MET-POS], [EMP-POS], [APC], [MAJ]] most of their experimentation is on microphone arrays with attention directly over magnitude-based input features, which cannot utilize the most important feature for signal separation using microphone arrays---signal phase.[[EXP-NEG,ANA-NEG], [SUB-NEG,EMP-NEG], [DFT,CRT], [MIN]] Their results on CHiME3 are terrible: the baseline CHiME3 system is very weak, and their system is only slightly better![[DAT-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] The winning system has a WER of only 5.8%(vs. 33.4% for the baseline system), while more than half of the submissions to the challenge were able to cut the WER of the baseline system in half or better! [[RES-NEG], [EMP-NEG], [DFT,CRT], [MIN]]http://spandh.dcs.shef.ac.uk/chime_challenge/chime2015/results.html. Their results wrt channel corruption on CHiME3, on the other hand, are reasonable, because the model matches the problem being addressed\u2026\n\nOverall Assessment: \n\n[[DAT-POS,RES-POS], [EMP-POS], [APC], [MAJ]]In summary, the paper lacks novelty wrt technique, and as an \u201capplication-of-attention\u201d paper fails to be even close to competitive with the state-of-the-art approaches on the problems being addressed.[[RWK-NEG,MET-NEG], [NOV-NEG,EMP-NEG], [CRT], [MAJ]] As such, I recommend that the paper be rejected.[[OAL-NEG], [REC-NEG], [FBK], [MAJ]]\n\n\nAdditional comments: \n\n-\tThe experiments in general lack sufficient detail: Were the attention masks trained supervised or unsupervised?[[EXP-NEG], [SUB-NEG], [DFT,QSN], [MAJ]] Were the baselines with concatenated features optimized independently?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Why is there no multi-channel baseline for the GRID results?[[RES-NEU], [EMP-NEU], [QSN], [MIN]] \n-\tIssue with noise bursts plot (Input 1+2 attention does not sum to 1)[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MIN]]\n-\tA concatenation based model can handle a variable #inputs: it just needs to be trained/normalized properly during test (i.e. like dropout)\u2026\n"[[MET-NEU], [EMP-NEU], [SUG], [GEN]]