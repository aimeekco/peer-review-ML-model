"The authors introduce a sequential/recurrent model for generation of small graphs.[[INT-NEU], [null], [SMY], [GEN]] The recurrent model takes the form of a graph neural network.[[PDI-NEU], [null], [SMY], [GEN]] Similar to RNN language models, new symbols (nodes/edges) are sampled from Bernoulli or categorical distributions which are parameterized by small fully-connected neural networks conditioned on the last recurrent hidden state.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] \n\nThe paper is very well written, nicely structured, provides extensive experimental evaluation, and examines an important problem that has so far not received much attention in the field.[[EXP-POS,MET-POS,OAL-POS], [CLA-POS,EMP-POS], [APC], [MAJ]]\n\nThe proposed model has several interesting novelties (mainly in terms of new applications/experiments, and being fully auto-regressive), yet also shares many similarities with the generative component of the model introduced in [1] (not cited): Both models make use of (recurrent) graph neural networks to learn intermediate node representations, from which they predict whether new nodes/edges should be added or not.[[EXP-POS,MET-POS], [NOV-POS,EMP-POS], [APC], [MAJ]] [1] speeds this process up by predicting multiple nodes and edges at once, whereas in this paper, such a multi-step process is left for future work.[[RWK-NEG,FWK-NEG], [CMP-NEG,IMP-NEG], [CRT], [MAJ]] Training the generative model with fixed ground-truth ordering was similarly performed in [1] (\u201cstrong supervision\u201d) and is thus not particularly novel.[[EXP-NEG,MET-NEG], [NOV-NEG], [CRT], [MAJ]]\n\nEqs.1-3: Why use recurrent formulation in both the graph propagation model and in the auto-regressive main loop (h_v -> h_v\u2019)?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Have the authors experimented with other variants (dropping the weight sharing in either or both of these steps)?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] \n\nOrdering problem: A solution for the ordering problem was proposed in [2]: learning a matching function between the orderings of model output and ground truth.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]  A short discussion of this result would make the paper stronger.[[ANA-NEU], [SUB-NEU], [SUG], [MIN]] \n\nFor chemical molecule generation, a direct comparison to some more recent work (e.g. the generator of the grammar VAE [3]) would be insightful.[[RWK-NEU], [CMP-NEU], [SUG], [MIN]] \n\nOther minor points:\n- In the definition of f_nodes: What is p(y)?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]  It would be good to explicitly state that (boldface) s is a vector of scores s_u (or score vectors, in case of multiple edge types) for all u in V.[[MET-NEU], [EMP-NEU], [SUG], [MIN]]  \u2028\n- The following statement is unclear to me: \u201cbut building a varying set of objects is challenging in the first place, and the graph model provides a way to do it.[[MET-NEG], [CLA-NEG], [CRT], [MIN]] \u201d Maybe this can be substantiated by experimental results (e.g. a comparison against Pointer Networks [4])?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]] \n- Typos in this sentence: \u201cLastly, when compared using the genaric graph generation decision sequence, the Graph architecture outperforms LSTM in NLL as well.[[MET-NEG,TNF-NEG], [CLA-NEG], [CRT], [MIN]] \u201d\n\nOverall I feel that this paper can be accepted with some revisions (as discussed above), as, even though it shares many similarities with previous work on a very related problem, it is well-written, well-presented and addresses an important problem.[[RWK-POS,EXP-POS,OAL-POS], [CLA-POS,PNF-POS,REC-POS], [APC,FBK], [MAJ]] \n\n[1] D.D. Johnson, Learning Graphical State Transitions, ICLR 2017\n[2] R. Stewart, M. Andriluka, and A. Y. Ng, End-to-End People Detection in Crowded Scenes, CVPR 2016[[BIB-NEU], [null], [DIS], [MIN]] \n[3] M.J. Kusner, B. Paige, J.M. Hernandez-Lobato, Grammar Variational Autoencoder, ICML 2017\n[4] O. Vinyals, M. Fortunato, N. Jaitly, Pointer Networks, NIPS 2015"[[BIB-NEU], [null], [DIS], [MIN]]