"This paper investigates the impact of noisy input on Machine Translation, and tests simple ways to make NMT models more robust.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]]\n\nOverall the paper is a clearly written, well described report of several experiments.[[EXP-POS,OAL-POS], [CLA-POS,EMP-POS], [APC], [MAJ]] It shows convincingly that standard NMT models completely break down on both natural \"noise\" and various types of input perturbations.[[MET-POS], [EMP-POS], [APC], [MAJ]] It then tests how the addition of noise in the input helps robustify the charCNN model somewhat.[[MET-NEU], [null], [DIS], [GEN]] The extent of the experiments is quite impressive: three different NMT models are tried, and one is used in extensive experiments with various noise combinations.[[EXP-POS], [EMP-POS], [APC], [MAJ]]\n\nThis study clearly addresses an important issue in NMT and will be of interest to many in the NLP community.[[PDI-POS], [EMP-POS], [APC], [MAJ]] The outcome is not entirely surprising (noise hurts and training and the right kind of noise helps)[[RES-NEG], [EMP-NEG], [CRT], [MIN]] but the impact may be.[[FWK-POS], [IMP-POS], [APC], [MAJ]] I wonder if you could put this in the context of \"training with input noise\", which has been studied in Neural Network for a while (at least since the 1990s).[[RWK-NEU], [null], [DIS], [MIN]] I.e. it could be that each type of noise has a different regularizing effect, and clarifying what these regularizers are may help understand the impact of the various types of noise.[[MET-NEU], [null], [DIS], [MIN]] Also, the bit of analysis in Sections 6.1 and 7.1 is promising, if maybe not so conclusive yet.[[ANA-POS], [EMP-POS], [APC], [MAJ]]\n\nA few constructive criticisms:\n\nThe way noise is included in training (sec. 6.2) could be clarified (unless I missed it) e.g. are you generating a fixed \"noisy\" training set and adding that to clean data?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]] Or introducing noise \"on-line\" as part of the training? [[EXP-NEU], [EMP-NEU], [QSN], [MIN]]If fixed, what sizes were tried?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]] More information on the experimental design would help.[[EXP-NEU], [SUB-NEU], [SUG], [MIN]]\n\nTable 6 is highly suspect: Some numbers seem to have been copy-pasted in the wrong cells, eg. the \"Rand\" line for German, or the Swap/Mid/Rand lines for Czech.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]] It's highly unlikely that training on noisy Swap data would yield a boost of +18 BLEU points on Czech -- or you have clearly found a magical way to improve performance.[[DAT-POS,EXP-POS], [EMP-POS], [APC], [MAJ]]\n\nAlthough the amount of experiment is already important, it may be interesting to check whether all se2seq models react similarly to training with noise: it could be that some architecture are easier/harder to robustify in this basic way.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]]\n\n[Response read -- thanks]\nI agree with authors that this paper is suitable for ICLR, although it will clearly be of interest to ACL/MT-minded folks."[[OAL-POS], [APR-POS], [APC], [MAJ]]