"This paper presented a multi-modal extension of variational autoencoder (VAE) for the task \"visually grounded imagination.[[INT-NEU,RWK-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]]\"  In this task,  the model learns a joint embedding of the images and the attributes[[RWK-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]]. The proposed model is novel but incremental comparing to existing frameworks.[[RWK-POS,PDI-POS], [NOV-POS,CMP-POS], [APC], [MAJ]]  The author also introduced new evaluation metrics to evaluate the model performance concerning correctness, coverage, and compositionality.[[INT-POS,PDI-POS,MET-POS,RES-POS], [IMP-POS,EMP-POS], [APC], [MAJ]] \n\nPros:\n1. The paper is well-written, and the contribution (both the model and the evaluation metric) potentially can to be very useful in the community. [[OAL-POS], [CLA-POS], [APC], [MAJ]] \n2. The discussion comparing the related work/baseline methods is insightful.[[RWK-POS,MET-POS], [CMP-POS,EMP-POS], [APC], [MAJ]] \n3. The proposed model addresses many important problems, such as attribute learning, disentanged representation learning, learning with missing values, and proper evaluation methods.[[PDI-POS,DAT-POS,EXP-POS,MET-POS], [IMP-POS,EMP-POS], [APC], [MAJ]] \n\nCons/questions:\n1. The motivation of the model choice of q is not clear.[[RWK-NEG,PDI-NEG,MET-NEG], [CLA-NEG,EMP-NEG], [DFT], [MIN]] \nComparing to BiVCCA, apart from the differences that the author discussed, a big difference is the choice of q.[[RWK-NEU,DAT-NEG,EXP-NEG,MET-NEG], [EMP-NEG], [DFT], [MIN]]  BiVCCA uses two inference networks q(z|x) and q(z|y), while the proposed method uses three. q(z|x), q(z|y), and q(z|x,y).[[RWK-NEU,PDI-NEU,EXP-NEU,MET-NEU], [CMP-NEU,EMP-NEU], [SMY,DIS], [GEN]]  How does such model choice affect the final performance?[[PDI-NEU,MET-NEU], [null], [QSN], [GEN]] \n\n2. Baselines are not necessarily sufficient[[RWK-NEG,MET-NEG], [IMP-NEG,EMP-NEG], [DFT], [MIN]]. \nThe paper compared the vanilla version of BiVCCA but not the one with factorized representation version[[INT-NEU,MET-NEU], [CMP-NEG], [SMY,DFT], [MIN]]. In the original VAECCA paper, the extension of using factorized representation (private and shared) improved the performance][[RWK-POS,RES-POS], [null], [APC], [MAJ]]. The author should also compare this extension of VAECCA.[[RWK-POS,MET-POS], [CMP-POS], [APC], [MAJ]]\n\n3. Some details are not clear.[[RWK-NEG], [CLA-NEG], [DFT], [MIN]] \na) How to set/learn the scaling parameter \\lambda_y and \\beta_y[[DAT-NEU,EXP-NEU], [null], [QSN], [GEN]]? If it is set as hyper-parameter, how does the performance change concerning them? [[RWK-NEU,DAT-NEU,EXP-NEU,RES-NEU], [null], [QSN], [GEN]]\nb) Discussion of the experimental results is not sufficient[[RWK-NEU,EXP-NEG], [IMP-NEG], [QSN], [GEN]]. For example, why JMVAE performs much better than the proposed model when all attributes are given.[[RWK-POS,PDI-NEG,MET-NEG], [CMP-NEG,EMP-NEG], [SMY,DIS], [MIN]]  What is the conclusion from Figure 4(b)?[[RWK-NEU,TNF-NEU], [null], [QSN], [GEN]] The JMVAE seems to generate more diverse (better coverage) results which are not consistent with the claims in the related work.[[RWK-NEG,MET-NEG,RES-NEG], [EMP-NEG], [DFT], [MIN]]  The same applies to figure 5. [[RWK-NEG,TNF-NEG], [null], [DFT], [MIN]]\n"