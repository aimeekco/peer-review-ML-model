"A large margin , end to end language model that uses a discriminative objective function is proposed.[[INT-NEU], [null], [SMY], [GEN]] The proposed objective imposes a hinge loss on the margin to ensure that the ground truth is at least  some fixed amount larger than the imposter.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] A variant on this, which also incorporates the ranks of the imposters sorted by a metric such as edit distance or BLEU metric with respect to the ground truth is also introduced.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]]\n\nThe paper is missing some of the original references to a discriminative LM (DLM) as well as  references to the use of a NN LM directly in decoding (presented in ICASSP and Interspeech conferences over the last 5 years).[[RWK-NEG,BIB-NEG], [SUB-NEG], [DFT], [MAJ]] For example, H.-K. J. Kuo, E. Fosler-Lussier, H. Jiang, and C.-H. Lee, \u201cDiscriminative training of language models for speech recognition,\u201d in Proc. ICASSP,\nvol. 1, 2002, pp. 325\u2013328.[[RWK-NEG,BIB-NEG], [SUB-NEG], [DFT], [MAJ]] \n\nHave you considered the widely-used NCE method to handle the large vocabulary?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] \n\nThe dev perplexity quoted in Section 4 for a 5 gram LM is very high.[[MET-NEU], [null], [CRT], [MIN]]   Also Table 4 and Table 5 on WSJ and  FIsher  show baseline experiments that are quite far away from the state-of-the-art in these tasks.[[RWK-NEG,TNF-NEG], [CMP-NEG], [CRT], [MIN]]  Even if you assume that you use the simplest possible acoustic model and/or an open source tool kit for the decoder,  these error rates are high (WSJ error rates are lower than 10%, not 16.7%).[[MET-NEG,EXP-NEG], [EMP-NEG], [CRT], [MIN]]  Even if the LM is trained on the common-crawl corpus, it has  a very low OOV rate, and fine tuning on the tasks only lowers it b t 1%.[[EXP-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]]   For reference,  please see papers from Saon et al., Seide et al, Povey et al, Yajie Miao et al in various ICASSP, Interspeech and arXiv papers. Comparisons with weak baselines can significantly color the conclusions.[[BIB-NEU], [null], [SUG], [MIN]]  On the Fisher test set, the interpolated LM offers very little over the baseline LM in Table 5.[[RWK-NEG,EXP-NEG], [EMP-NEG], [CRT], [MIN]]  This is contrary to what is observed in the literature.[[RWK-NEG], [CMP-NEG], [CRT], [MIN]]   There is not much difference between rankLM and LMLM as well to draw a clear conclusion between the two.[[MET-NEG], [CMP-NEG], [CRT], [MIN]]  Given that this is n-best rescoring, how are the N-best lists generated?[[MET-NEU,RES-NEG], [SUB-NEG], [DFT], [MAJ]]   You state that they are extracted from  64 beam candidates, are they unique N-best lists?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]   Can this method be applied to lattices?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] What is the perplexity of all the language models corresponding to  Tables 4 and 5?[[MET-NEU,TNF-NEU], [EMP-NEU], [QSN], [MIN]] This would have been useful to study in itself.[[MET-NEU], [EMP-NEU], [DIS], [GEN]]\n\nIn the SMT tasks, the baselines reported seem to be far away from results presented in the literature on the IWSLT task (see http://workshop2015.iwslt.org/downloads/IWSLT_2015_EP_3.pdf)[[RWK-NEG,RES-NEG], [CMP-NEG], [CRT], [MIN]]\n\nWhile the proposed objective is interesting and meaningful for several conversational applications, as well as sentence modeling, the presented experimental results are not convincing."[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]]