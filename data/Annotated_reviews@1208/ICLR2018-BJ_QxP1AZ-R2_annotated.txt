"The paper adds few operations after the pipeline for obtaining visual concepts from CNN as proposed by Wang et al. (2015).[[PDI-NEU,RWK-NEU], [null], [SMY], [GEN]] This latter paper showed how to extract from a CNN some clustered representations of the features of the internal layers of the network, working on a large training dataset.[[DAT-NEU,EXP-NEU], [null], [SMY], [GEN]] The clustered representations are the visual concepts.[[MET-NEU], [null], [SMY], [GEN]] This paper shows that these representations can be used as exemplars by test images, in the same vein as bag of words used word exemplars to create the bag of words of unseen images.[[PDI-NEU], [null], [SMY], [GEN]]\n\n A simple nearest neighborhood and a likelihood model is built to assign a picture to an object class[[MET-NEU], [null], [SMY], [GEN]].\n\nThe results a are convincing, even if they are not state of the art in all the trials.[[RES-POS], [EMP-POS], [APC], [MAJ]] \nThe paper is very easy to follows, and the results are explained in a very simple way.[[RES-POS,OAL-POS], [EMP-POS], [APC], [MAJ]]\n\n\nFew comments:\nThe authors in the abstract should revise their claims, too strong with respect to a literature field which has done many advancements on the cnn interpretation (see all the literature of Andrea Vedaldi) and the literature on zero shot learning, transfer learning, domain adaptation and fine tuning in general."[[ABS-NEG], [EMP-NEG], [CRT], [MIN]]