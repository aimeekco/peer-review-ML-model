"The authors propose a mechanism for learning task-specific region embeddings for use in text classification.[[INT-NEU,MET-NEU], [null], [SMY], [GEN]] Specifically, this comprises a standard word embedding an accompanying local context embedding.[[MET-NEU], [null], [SMY], [GEN]]  \n\nThe key idea here is the introduction of a (h x c x v) tensor K, where h is the embedding dim (same as the word embedding size), c is a fixed window size around a target word, and v is the vocabulary size.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]]  Each word in v is then associated with an (h x c) matrix that is meant to encode how it affects nearby words, in particular this may be viewed as parameterizing a projection to be applied to surrounding word embeddings.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]]  The authors propose two specific variants of this approach, which combine the K matrix and constituent word embeddings (in a given region) in different ways.[[MET-NEU], [null], [SMY], [GEN]]  Region embeddings are then composed (summed) and fed through a standard model.[[MET-NEU], [null], [SMY], [GEN]]  \n\nStrong points\n---\n+ The proposed approach is simple and largely intuitive: essentially the context matrix allows word-specific contextualization.[[MET-POS], [EMP-POS], [APC], [MAJ]]  Further, the work is clearly presented.[[OAL-POS], [CLA-POS], [APC], [MAJ]] \n\n+ At the very least the model does seem comparable in performance to various recent methods (as per Table 2), however as noted below the gains are marginal and I have some questions on the setup.[[MET-NEG], [CMP-NEG], [CRT], [MAJ]] \n\n+ The authors perform ablation experiments, which are always nice to see.[[EXP-POS], [EMP-POS], [APC], [MAJ]]  \n\nWeak points\n---\n- I have a critical question for clarification in the experiments. [[EXP-NEG], [EMP-NEG], [CRT], [MAJ]] The authors write 'Optimal hyperparameters are tuned with 10% of the training set on Yelp Review Full dataset, and identical hyperparameters are applied to all datasets' -- is this true for *all* models, or only the proposed approach?[[DAT-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]]  \n\n- The gains here appear to be consistent, but they seem marginal.[[RES-NEU], [EMP-NEU], [DIS], [MAJ]]  The biggest gain achieved over all datasets is apparently .7, and most of the time the model very narrowly performs better (.2-.4 range).[[DAT-POS,MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]]  Moreoever, it is not clear if these results are averaged over multiple runs of SGD or not (variation due to initialization and stochastic estimation can account for up to 1 point in variance -- see \"A sensitivity analysis of (and practitioners guide to) CNNs...\" Zhang and Wallace, 2015.)[[RWK-NEU,RES-NEG], [CMP-NEU,EMP-NEG], [CRT], [MAJ]] \n\n- The related work section seems light.[[RWK-NEG], [SUB-NEG], [DFT], [MAJ]]  For instance, there is no discussion at all of LSTMs and their application to text classificatio (e.g., Tang et al., EMNLP 2015) -- although it is noted that the authors do compare against D-LSTM,  or char-level CNNs for the same (see Zhang et al., NIPs 2015). Other relevant work not discussed includes Iyyer et al. (ACL 2015). In their respective ways, these papers address some of the same issues the authors consider here.[[RWK-NEG], [SUB-NEG,EMP-NEG], [DFT], [MAJ]] \n\n- The two approaches to inducing the final region embedding (word-context and then context-word in sections 3.2 and 3.3, respectively) feel a bit ad-hoc.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]  I would have appreciated more intuition behind these approaches.[[MET-NEU,ANA-NEU], [SUB-NEU], [DIS], [MIN]]  \n\nSmall comments\n---\nThere is a typo in Figure 4 -- \"Howerver\" should be \"However\"[[TNF-NEG], [CLA-NEG], [CRT], [MIN]] \n\n*** Update after author response ***\n\nThanks to the authors for their responses. My score is unchanged."[[OAL-NEU], [REC-NEU], [FBK], [MAJ]] 