"This paper presents a method for matrix factorization using DNNs.[[INT-NEU,MET-NEU], [null], [SMY], [GEN]] The suggestion is to make the factorization machine (eqn 1) deep, by grouping the features meaningfully (eqn 5), extracting nonlinear features from original inputs (deep-in, eqn 8), and adding additional nonlinearity after computing pairwise interactions (deep-out, eqn 7).[[PDI-NEU,EXP-NEU], [EMP-NEU], [SUG], [GEN]] From the methodology point of view, such extensions are relatively straightforward.[[PDI-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] As an example, from the experimental results, it seems the grouping of features is done mostly with domain knowledge (e.g., months of year) and not learned automatically[[RWK-NEU,EXP-NEU,RES-NEG], [SUB-NEG,EMP-NEG], [DFT], [MIN]]. The authors claim the proposed method can circumvent the cold-start problem, and presented some experimental results on recommendation systems with text features.[[RWK-NEU,PDI-NEU,EXP-NEU,RES-NEU], [IMP-NEU,EMP-NEU], [SMY], [GEN]]\n\nWhile the application problems look quite interesting, in my opinion, the paper needs to make the context and contribution clearer.[[RWK-NEG,PDI-POS], [SUB-NEG,EMP-NEG], [DFT,APC], [MIN]] In particular, there is a huge literature in collaborative filtering, and I believe there is by now sufficient work on collaborative filtering with input features (and possibly dealing with the cold-start problem). [[RWK-NEU,PDI-NEU], [EMP-NEU], [SMY], [GEN]]I think this paper does not connect very well with that literature.[[RWK-NEG,OAL-NEG], [SUB-NEG], [DFT], [MIN]] When reading it, at times I felt the main purpose of this paper is to solve the application problems presented in experimental results, instead of proposing a general framework.[[PDI-NEU,MET-NEU,RES-NEU], [EMP-NEU], [DIS], [GEN]] I suggest the authors to demonstrate their method on some well-known datasets (e.g., MovieLens, Netflix), to give the readers an idea if the proposed method is indeed advantageous over more classical methods,[[RWK-NEU,PDI-NEU,DAT-NEU,MET-NEU], [null], [SUG,CRT], [GEN]] or if the success of this paper is mostly due to clever processing of text features using DNNs.[[DAT-POS,EXP-POS,MET-POS,OAL-NEU], [EMP-POS], [SMY,APC], [MAJ]]\n\nSome detailed comments:\n1. eqn 4 does not indicate any rank-r factors. [[RWK-NEG], [null], [SMY], [MIN]]\n2. some statements do not seem straightforward/justified to me:  \n    -- the paper uses the word \"inference\" several times without definition[[RWK-NEG], [EMP-NEG], [CRT], [MIN]]\n    -- \"if we were interested in interpreting the parameters, we could constrain w to be non-negative ... \".[[RWK-NEU], [EMP-NEU], [SMY], [GEN]] Is this easy to do, and can the authors demonstrate this in their experiments and show interpretable examples?[[RWK-NEU,EXP-NEU,ANA-NEU], [EMP-NEU], [SMY,DIS], [GEN]]\n    -- \"Note that if the dot product is replaced with a neural function, fast inference for cold-start ...\"[[PDI-NEU], [EMP-NEU], [SUG], [GEN]]. \n3. the experimental setup seems quite unusual to me: \"since we only observe positive labels, for such tasks in the test set we sample a labels according to the label frequency\". [[PDI-NEU,EXP-NEU], [EMP-NEU], [SMY,DIS], [GEN]]This seems very problematic if most of the entries are not observed.[[OAL-NEG], [SUB-NEG], [DFT], [MIN]] Why cannot you use the typical evaluation procedure for collaborative filtering,[[PDI-NEU,EXP-NEU], [null], [QSN], [GEN]] where you hide some known entries during model training, and evaluate on these entries during test? "[[PDI-NEU,EXP-NEU,ANA-NEU], [null], [QSN], [GEN]]