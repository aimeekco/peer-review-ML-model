"\nThe paper was fairly easy to follow,[[OAL-POS], [PNF-POS], [APC], [MAJ]] but I would not say it was well written. [[OAL-NEG], [CLA-NEG], [CRT], [MAJ]]These are minor annoyances; there were some typos and a strange citation format.[[OAL-NEG], [CLA-NEU,PNF-NEU], [CRT], [MIN]] There is nothing wrong with the fundamental idea itself,[[PDI-NEU], [NOV-POS], [APC], [MAJ]] but given the experimental results it just is not clear that it is working.\[[EXP-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]]n\nThe bot performance significantly better than the fully trained agent.[[RWK-NEU,RES-NEU], [CMP-NEU], [DIS], [GEN]] This leads to a few questions:\n\n1. What was the performance of the \"regression policy\", that was learned during the supervised pretraining phase?\[[MET-NEU], [EMP-NEU], [QSN], [MIN]]n2. Given enough time would the basic RL agent reach similar performance? (Guessing no...) Why not?\n3.[[MET-NEG], [EMP-NEG], [QSN], [MAJ]] Considering the results of Figure 3 (right) shouldn't the conclusion be that the RL portion is essentially contributing nothing?[[RES-NEU,TNF-NEU], [EMP-NEU], [QSN], [MIN]]\n\nPros:\nThe regularization of the Q-values w.r.t. the policy of another agent is interesting\n\n[[MET-POS,ANA-POS], [EMP-POS], [APC], [MAJ]]Cons:\nNot very well setup experiments\nPerformance is lower than you would expect just using supervised training\nNot clear what parts are working and what parts are not\n\n\n"[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]]