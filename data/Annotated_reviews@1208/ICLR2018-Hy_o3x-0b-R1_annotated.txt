"The paper combines several recent advances on generative modelling including a ladder variational posterior and a PixelCNN decoder together with the proposed convolutional stochastic layers to boost the NLL results of the current VAEs.[[INT-NEU,RWK-NEU,PDI-NEU], [CMP-NEU], [SMY], [GEN]] The numbers in the tables are good but I have several comments on the motivation, originality and experiments.[[EXP-NEU,TNF-POS], [NOV-NEU,EMP-NEU], [APC,DIS], [MIN]]\n\nMost parts of the paper provide a detailed review of the literature.[[OAL-NEU], [null], [DIS], [MIN]] However, the resulting model is quite like a combination of the existing advances and the main contribution of the paper, i.e. the convolution stochastic layer, is not well discussed.[[RWK-NEG,MET-NEG], [NOV-NEG,EMP-NEG], [CRT], [MAJ]] Why should we introduce the convolution stochastic layers?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Could the layers encode the spatial information better than a deterministic convolutional layer with the same architecture?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]]  What's the exact challenge of training VAEs addressed by the convolution stochastic layer?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]]  Please strengthen the motivation and originality of the paper.[[OAL-NEG], [NOV-NEG], [SUG,DIS], [MIN]]\n\nThough the results are good,[[RES-POS], [EMP-POS], [APC], [MAJ]] I still wonder what is the exact contribution of the convolutional stochastic layers to the NLL results?[[MET-NEU,RES-NEU], [EMP-NEU], [QSN], [MIN]]  Can the authors provide some results without the ladder variational posterior and the PixelCNN decoder on both the gray-scaled and the natural images?[[MET-NEU,EXP-NEU,RES-NEU], [SUB-NEU], [QSN], [MIN]]\n\nAccording to the experimental setting in the Section 3 (Page 5 Paragraph 2), \"In case of gray-scaled images the stochastic latent layers are dense with sizes 64, 32, 16, 8, 4 (equivalent to S\u00f8nderby et al. (2016)) and for the natural images they are spatial (cf. Table 1).[[RWK-NEU,RES-NEU,TNF-NEU], [CMP-NEU], [DIS], [MIN]] There was no significant difference when using feature maps (as compared to dense layers) for modelling gray-scaled images "there is no stochastic convolutional layer.[[MET-NEG], [CMP-NEG], [CRT], [MIN]]   Then is there anything new in FAME on the gray images?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]  Furthermore, how could FAME advance the previous state-of-the-art?[[RWK-NEU], [CMP-NEU], [QSN], [MIN]]  It seems because of other factors instead of the stochastic convolutional layer.[[MET-NEU], [CNT], [DIS], [GEN]]  \n\nThe results on the natural images are not complete.[[RES-NEG,TNF-NEG], [SUB-NEG], [DFT], [MAJ]]  Please present the generation results on the ImageNet dataset and the reconstruction results on both the CIFAR10 and ImageNet datasets.[[DAT-NEU,RES-NEG], [SUB-NEU], [DIS,DFT], [MIN]] The quality of the samples on the CIFAR10 dataset seems not competitive to the baseline papers listed in the table.[[RWK-NEG,DAT-NEG], [CMP-NEG], [CRT], [MAJ]] Though the visual quality does not necessarily agree with the NLL results but such large gap is still strange.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] Besides, why FAME can obtain both good NLL and generation results on the MNIST and OMNIGLOT datasets when there is no stochastic convolutional layer?[[DAT-NEU,MET-NEU,RES-NEU], [EMP-NEG], [QSN], [MIN]] Meanwhile, why FAME cannot obtain good generation results on the CIFAR10 dataset?[[DAT-NEG,MET-NEG,RES-NEG], [EMP-NEG], [QSN], [MAJ]] Is it because there is a lot randomness in the stochastic convolutional layer?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] It is better to provide further analysis and it is not safe to say that the stochastic convolutional layer helps learn better latent representations based on only the NNL results.[[MET-NEU,RES-NEU,ANA-NEU], [SUB-NEU], [SUG], [MIN]]\n\nMinor things:\n\nPlease rewrite the sentence \"When performing reconstructions during training ... while also using the stochastic latent variables z = z 1 , ..., z L.[[EXP-NEU], [CLA-NEU,PNF-NEU], [SUG], [MIN]]\" in the caption of Figure 1."