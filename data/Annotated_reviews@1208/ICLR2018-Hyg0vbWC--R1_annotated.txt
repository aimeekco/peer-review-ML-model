"This paper considers the task of generating Wikipedia articles as a combination of extractive and abstractive multi-document summarization task where input is the content of reference articles listed in a Wikipedia page along with the content collected from Web search and output is the generated content for a target Wikipedia page.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The authors at first reduce the input size by using various extractive strategies and then use the selected content as input to the abstractive stage where they leverage the Transformer architecture with interesting modifications like dropping the encoder and proposing alternate self-attention mechanisms like local and memory compressed attention. [[PDI-NEU,MET-NEU], [null], [SMY], [GEN]]  \n\nIn general, the paper is well-written and the main ideas are clear.[[OAL-POS], [CLA-POS], [APC], [MAJ]] However, my main concern is the evaluation.[[MET-NEG], [CNT], [CRT], [GEN]] It would have been nice to see how the proposed methods perform with respect to the existing neural abstractive summarization approaches.[[RWK-NEU,MET-NEU], [SUB-NEU,CMP-NEU], [SUG,DIS], [MIN]] Although authors argue in Section 2.1 that existing neural approaches are applied to other kinds of datasets where the input/output size ratios are smaller,  experiments could have been performed to show their impact.[[RWK-NEU,DAT-NEU,EXP-NEU,MET-NEU], [CMP-NEU], [SUG,DIS], [MIN]] Furthermore, I really expected to see a comparison with Sauper & Barzilay (2009)'s non-neural extractive approach of Wikipedia article generation, which could certainly strengthen the technical merit of the paper.[[RWK-NEU,MET-NEU], [CMP-NEU], [SUG,DIS], [MIN]]\n\nMore importantly, it was not clear from the paper if there was a constraint on the output length when each model generated the Wikipedia content.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] For example, Figure 5-7 show variable sizes of the generated outputs.[[RES-NEU,TNF-NEU], [null], [DIS], [MIN]] With a fixed reference/target Wikipedia article, if different models generate variable sizes of output, ROUGE evaluation could easily pose a bias on a longer output as it essentially counts overlaps between the system output and the reference.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \n\nIt would have been nice to know if the proposed attention mechanisms account for significantly better results than the T-ED and T-D architectures.[[MET-NEU,RES-NEG], [CMP-NEU,EMP-NEU], [SUG], [MIN]] Did you run any statistical significance test on the evaluation results?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]] \n\nAuthors claim that the proposed model can generate \"fluent, coherent\" output, however, no evaluation has been conducted to justify this claim.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] The human evaluation only compares two alternative models for preference, which is not enough to support this claim.[[MET-NEG], [SUB-NEG], [CRT], [MIN]]  I would suggest to carry out a DUC-style user evaluation (http://www-nlpir.nist.gov/projects/duc/duc2007/quality-questions.txt) methodology to really show that the proposed method works well for abstractive summarization.[[MET-NEU], [SUB-NEU,EMP-NEU], [SUG], [MIN]]\n\nDoes Figure 8 show an example input after the extractive stage or before?[[TNF-NEU], [null], [QSN], [MIN]] Please clarify.\n\n---------------\nI have updated my scores as authors clarified most of my concerns."[[OAL-POS], [REC-POS], [FBK], [MAJ]]