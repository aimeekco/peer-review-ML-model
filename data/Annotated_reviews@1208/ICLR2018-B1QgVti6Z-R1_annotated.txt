"This paper studies empirical risk in deep neural networks.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] Results are provided in Section 4 for linear networks and in Section 5 for nonlinear networks.[[RES-NEU], [null], [SMY], [GEN]]\nResults for deep linear neural networks are puzzling.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] Whatever the number of layers, a deep linear NN is simply a matrix multiplication and minimizing the MSE is simply a linear regression.[[MET-NEU], [null], [DIS], [GEN]] So results in Section 4 are just results for linear regression and I do not understand why the number of layers come into play?[[MET-NEU,RES-NEU], [EMP-NEU], [QSN], [MIN]] \nAlso this is never explicitly mentioned in the paper, I guess the authors make an assumption that the samples (x_i,y_i) are drawn i.i.d. from a given distribution D.[[MET-NEG], [PNF-NEG,EMP-NEG], [CRT], [MIN]] In such a case, I am sure results on the population risk minimization can be found for linear regression and should be compare to results in Section 4.\n\n"[[RES-NEU], [CMP-NEU], [SUG], [MIN]]