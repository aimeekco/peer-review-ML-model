"This paper investigates numerically and theoretically the reasons behind the empirical success of binarized neural networks.[[PDI-NEU], [null], [SMY], [GEN]] Specifically, they observe that:\n\n(1) The angle between continuous vectors sampled from a spherical symmetric distribution and their binarized version is relatively small in high dimensions (proven to be about 37 degrees when the dimension goes to infinity), and this demonstrated empirically to be true for the binarized weight matrices of a convenet.[[MET-NEU,RES-NEU], [null], [SMY], [GEN]] \n\n(2) Except the first layer, the dot product of weights*activations in each layer is highly correlated with the dot product of (binarized weights)*activations in each layer.[[MET-NEU], [null], [SMY], [GEN]] There is also a strong correlation between (binarized weights)*activations and (binarized weights)*(binarized activations).[[MET-NEU], [null], [SMY], [GEN]] This is claimed to entail that the continuous weights of the binarized neural net approximate the continuous weights of a non-binarized neural net trained in the same manner.[[MET-NEU,RES-NEU], [null], [SMY], [GEN]]\n\n(3) To correct the issue with the first layer in (2) it is suggested to use a random rotation, or simply use continues weights in that layer.[[MET-NEU], [EMP-NEU], [SUG], [MAJ]]\n\nThe first observation is interesting, is explained clearly and convincingly, and is novel to the best of my knowledge.[[MET-POS,RES-POS], [NOV-POS,EMP-POS], [APC], [MAJ]]\n\nThe second observation is much less clear to me.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Specifically,\na.\tThe author claim that \u201cA sufficient condition for \\delta u to be the same in both cases is L\u2019(x = f(u)) ~ L\u2019(x = g(u))\u201d.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] However, I\u2019m not sure if I see why this is true: in a binarized neural net, u also changes, since the previous layers are also binarized.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] \nb.\tRelated to the previous issue, it is not clear to me if in figure 3 and 5, did the authors binarize the activations of that specific layer or all the layers?[[MET-NEG,TNF-NEG], [EMP-NEG], [QSN], [MIN]] If it is the first case, I would be interested to know the latter: It is possible that if all layers are binarized, then the differences between the binarized and non-binarized version become more amplified.[[MET-NEU,RES-NEU], [SUB-NEU,EMP-NEU], [DIS], [MIN]]\nc.\tFor BNNs, where both the weights and activations are binarized, shouldn\u2019t we compare weights*activations to (binarized weights)*(binarized activations)?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\nd.\tTo make sure, in figure 4, the permutation of the activations was randomized (independently) for each data sample?[[MET-NEU,TNF-NEU], [EMP-NEU], [QSN], [MIN]] If not, then C is not proportional the identity matrix, as claimed in section 5.3.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\ne.\tIt is not completely clear to me that batch-normalization takes care of the scale constant (if so, then why did XNOR-NET needed an additional scale constant?),perhaps this should be further clarified.[[MET-NEG], [SUB-NEG,EMP-NEG], [SUG,CRT], [MIN]]  \n\nThe third observation seems less useful to me.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]]  Though a random rotation may improve angle preservation in certain cases (as demonstrated in Figure 4), it may hurt classification performance (e.g., distinguishing between 6 and 9 in MNIST).[[DAT-NEG,TNF-NEG], [EMP-NEG], [CRT], [MIN]]  Furthermore, since it uses non-binary operations, it is not clear if this rotation may have some benefits (in terms of resource efficiency) over simply keeping the input layer non-binarized.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MIN]] \n\nTo summarize, the first part is interesting and nice,[[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]]  the second part was not clear to me, and the last part does not seem very useful.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]]  \n\n%%% After Author's response %%%\na. My mistake. Perhaps it should be clarified in the text that u are the weights.[[EXT-NEG], [null], [DIS], [MIN]] I thought that g(u) is a forward propagation function, and therefore u is the neural input (i.e., pre-activation).[[MET-NEU], [null], [DIS], [GEN]]\n\nFollowing the author's response and revisions, I have raised my grade.\n"[[OAL-POS], [REC-POS], [FBK], [MAJ]]