"This work proposes to replace the gradient step for updating the network parameters to a proximal step (implicit gradient) so that a large stepsize can be taken.[[INT-NEU], [null], [SMY], [GEN]]  Then to make it fast, the implicit step is approximated using conjugate gradient method because the step is solving a quadratic problem.[[PDI-NEU], [null], [SMY], [GEN]] \n\nThe theoretical result of the ProxProp considers the full batch, and it can not be easily extended to the stochastic variant (mini-batch).[[RES-NEU], [EMP-NEU], [DIS], [GEN]] The reason is that the gradient of proximal is evaluated at the future point, and different functions will have different future points.[[RES-NEU], [EMP-NEU], [DIS], [GEN]]  While for the explicit gradient, it is assessed at the current point, and it is an unbiased one.[[RES-NEU], [EMP-NEU], [DIS], [GEN]]   \n\nIn the numerical experiment, the parameter \\tau_\\theta is sensitive to the final solution.[[EXP-NEU,RES-NEU], [EMP-NEU], [DIS], [MIN]]  Therefore, how to choose this parameter is essential.[[MET-NEU,RES-NEU], [EMP-NEU], [DIS], [MIN]]  Given a new dataset, how to determine it for a good performance.[[DAT-NEU], [EMP-NEU], [QSN], [MIN]]  \n\nIn Fig 3. The full batch loss of Adam+ProxProp is higher than Adam+BackProp regarding time, which is different from Fig. 2.[[TNF-NEU], [null], [DIS], [GEN]]  Also, the figure shows that the performance of Adam+BackProp is worst than Adam+ProxProp even though the training loss of Adam+BackProp is smaller that of Adam+ProxProp.[[MET-NEU,RES-NEU,TNF-NEU], [CMP-NEU], [DIS], [MIN]] Does it happen on this dataset only or it is the case for many datasets? "[[DAT-NEU], [EMP-NEU], [QSN], [MIN]]
