"Summary:\n\nUsing a penalty formulation of backpropagation introduced in a paper of Carreira-Perpinan and Wang (2014), the current submission proposes to minimize this formulation using explicit step for the update of the variables corresponding to the backward pass, but implicit steps for the update of the parameters of the network.[[RWK-NEU,PDI-NEU], [null], [SMY], [GEN]]  The implicit steps have the advantage that the choice of step-size is replaced by a choice of a proximity coefficient, which the advantage that while too large step-size can increase the objective, any value of the proximity coefficient yields a proximal mapping guaranteed to decrease the objective.[[PDI-NEU], [null], [SMY], [GEN]] \nThe implicit are potentially one order of magnitude more costly than an explicit step since they require\nto solve a linear system, but can be solved (exactly or partially) using conjugate gradient steps.[[PDI-NEU], [null], [SMY], [GEN]] The experiments demonstrate that the proposed algorithm are competitive with standard backpropagation and potentially faster if code is optimized further.[[EXP-NEU], [null], [SMY], [GEN]] The experiments show also that in on of the considered case the generalization accuracy is better for the proposed method.[[EXP-NEU,MET-NEU], [null], [SMY], [GEN]] \n\nSummary of the review: \n\nThe paper is well written, clear, tackles an interesting problem.[[PDI-POS,OAL-POS], [EMP-POS], [APC], [MAJ]]  \nBut, given that the method is solving a formulation that leverages second order information, it would seem reasonable to compare with existing techniques that leverage second order information to learn neural networks, namely BFGS, which has been studied for deep learning (see the references to Li and Fukushima (2001) and Ngiam et al (2011) below).[[RWK-NEU,MET-NEU], [CMP-NEU,EMP-NEU], [SUG], [MIN]] \n\nReview:\n\nUsing an implicit step leads to a descent step in a direction which is different than the gradient step.[[MET-NEU], [null], [DIS], [GEN]] \nBased on the experiment, the step in the implicit direction seems to decrease faster the objective, but the paper does not make an attempt to explain why. [[EXP-NEG,ANA-NEG], [EMP-NEG], [DFT], [MIN]] The authors must nonetheless have some intuition about this.[[ANA-NEU], [null], [DIS], [GEN]]  Is it because the method can be understood as some form of block-coordinate Newton with momentum?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]  It would be nice to have an even informal explanation.[[ANA-NEU], [CNT], [SUG], [MIN]] \n\nSince a sequence of similar linear systems have to be solved could a preconditioner be gradually be solved and updated from previous iterations, using for example a BFGS approximation of the Hessian or other similar technique.[[MET-NEU], [EMP-NEU], [QSN], [MIN]]  This could be a way to decrease the number of CG iterations that must done at each step.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Or can this replaced by a single BFGS style step?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nThe proposed scheme is applicable to the batch setting when most deep network are learned using stochastic gradient type methods.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] What is the relevance/applicability of the method given this context?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n \nIn fact given that the proposed scheme applies in the batch case, it seems that other contenders that are very natural are applicable, including BFGS variants for the non-convex case (\n\nsee e.g. Li, D. H., & Fukushima, M. (2001).[[MET-NEU], [CMP-NEU], [DIS], [MIN]] On the global convergence of the BFGS method for nonconvex unconstrained optimization problems. SIAM Journal on Optimization, 11(4), 1054-1064.[[BIB-NEU], [null], [DIS], [GEN]]\n\nand\n\nJ. Ngiam, A. Coates, A. Lahiri, B. Prochnow, Q. V. Le, and A. Y. Ng,[[BIB-NEU], [null], [DIS], [GEN]]\n\u201cOn optimization methods for deep learning,\u201d in Proceedings of the 28th\nInternational Conference on Machine Learning, 2011, pp. 265\u2013272.\n\n) \n\nor even a variant of BFGS which makes a block-diagonal approximation to the Hessian with one block per layer.[[BIB-NEU], [null], [DIS], [GEN]] To apply BFGS, one might have to replace the RELU function by a smooth counterpart..[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n \nHow should one choose tau_theta?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nIn the experiments the authors compare with classical backpropagation, but they do not compare with \nthe explicit step of Carreira-Perpinan and Wang?[[EXP-NEU], [CMP-NEU], [QSN], [MIN]] This might be a relevant comparison to add to establish more clearly that it is the implicit step that yields the improvement.[[EXP-NEU], [CMP-NEU], [SUG], [MIN]]\n\n\n\n\n\nTypos or question related to notations, details etc:[[OAL-NEG], [CLA-NEG], [CRT], [MIN]]\n\nIn the description of algorithm 2: the pseudo-code does not specify that the implicit step is done with regularization coefficient tau_theta[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n\nIn equation (10) is z_l=z_l^k or z_l^(k+1/2) (I assume the former).[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\n6th line of 5.1 theta_l is initialised uniformly in an interval -> could you explain why and/or provide a reference motivating this ?[[MET-NEU,BIB-NEU], [EMP-NEU], [QSN], [MIN]]\n\n8th line of 5.1 you mention Nesterov momentum method -> a precise reference and precise equation to lift ambiguities might be helpful.[[MET-NEU,BIB-NEU], [null], [SUG], [MIN]]\n\nIn section 5.2 the reference to Table 5.2 should be Table 1.\n"[[TNF-NEU], [PNF-NEU], [SUG], [MIN]]