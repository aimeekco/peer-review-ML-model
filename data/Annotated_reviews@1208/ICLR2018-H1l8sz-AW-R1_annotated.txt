"\nGENERAL IMPRESSION:\n\nOverall, the revised version of the paper is greatly improved.[[OAL-POS], [null], [APC], [MAJ]] The new derivation of the method yields a much simpler interpretation, although the relation to the natural gradient remains weak (see below).[[MET-NEU], [EMP-NEU], [SMY], [MAJ]] The experimental evaluation is now far more solid.[[EXP-POS], [null], [APC], [MAJ]] Multiple data sets and network architectures are tested, and equally important, the effect of parameter settings is investigated.[[DAT-NEU,EXP-NEU,MET-NEU], [null], [SMY], [MAJ]] I enjoyed the investigation of the effect of L_2 regularization on qualitative optimization behavior.[[MET-POS], [EMP-POS], [APC], [MAJ]]\n\n\nCRITICISM:\n\nMy central criticism is that the introduction of the L_2 norm as a replacement of KL divergence is completely ad-hoc; how it is related to KL divergence remains unclear.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] It seems that other choices are equally well justified, including the L_2 norm in parameter space, which then defeats the central argument of the paper.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] I do believe that L_2 distance is more natural in function space than in parameter space, but I am missing a strict argument for this in the paper.[[MET-NEU], [EMP-NEU], [CRT], [MAJ]]\n\nAlthough related work is discussed in detail in section 1, it remains unclear how exactly the proposed algorithm overlaps with existing approaches.[[RWK-POS,MET-NEU], [CMP-NEG], [CRT], [MAJ]] I am confident that it is easy to identify many precursors in the optimization literature, but I am not an expert on this.[[EXT-NEU], [null], [DIS], [MAJ]] It would be of particular interest to highlight connections to algorithm regularly applied to neural network training.[[MET-NEU], [EMP-NEU], [SMY], [MAJ]] Adadelta, RMSprop, and ADAM are mentioned explicitly, but what exactly are differences and similarities?[[RWK-NEU], [CMP-NEU], [QSN], [MAJ]]\n\nThe interpretation of figure 2 is off.[[TNF-NEU], [null], [QSN], [MIN]] It is deduced that HCGD generalizes better, however, this is the case only at the very end of training, while SGD with momentum and ADAM work far better initially.[[RWK-NEU,EXP-NEU], [CMP-NEU], [SMY], [MAJ]] With the same plot one could sell SGD as the superior algorithm.[[RWK-NEU,MET-NEU], [EMP-NEU], [DIS], [MAJ]] Overall, also in the light of figure 4, the interpretation that the new algorithm results in better generalization seems to stand on shaky ground, since differences are small.[[RWK-NEU,MET-NEU,TNF-NEU], [CMP-NEU], [CRT], [MAJ]]\n\nI like the experiment presented in figure 5 in particular.[[EXP-POS,TNF-NEU], [EMP-POS], [APC], [MAJ]] It adds insights that are of value even if the method should turn out to have significant overlap with existing work (see above), and perform \"only\" on par with these:;[[RWK-NEU,MET-POS], [CMP-POS], [APC], [MAJ]] it adds an interesting perspective to the discussion of how network optimization \"works\", how it handles local optima and which role they play, and how the objective function landscape is \"perceived\" by different optimizers.[[MET-POS], [EMP-POS], [DIS], [MAJ]] This is where I learned something new.[[MET-POS], [EMP-POS], [APC], [MAJ]]\n\n\nMINOR POINTS:\n\npage 5: \"the any\" (typo)\n\npage 5: \"ture\" -> \"true\" (typo).[[CNT], [PNF-NEG], [DFT], [MIN]]\n"