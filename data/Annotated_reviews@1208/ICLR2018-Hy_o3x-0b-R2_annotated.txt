"The description of the proposed method is very unclear.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] From the paper it is very difficult to make out exactly what architecture is proposed.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] I understand that the prior on the z_i in each layer is a pixel-cnn, but what is the posterior?[[MET-NEG], [EMP-NEG], [DFT], [MIN]] Equations 8 and 9 would suggest it is of the same form (pixel-cnn) but this would be much too slow to sample during training.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] I'm guessing it is just a factorized Gaussian, with a separate factorized Gaussian pseudo-prior?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [GEN]] That is, in figure 1 all solid lines are factorized Gaussians and all dashed lines are pixel-cnns?[[MET-NEG,TNF-NEU], [PNF-NEU], [QSN], [MIN]]\n\n* The word \"layers\" is sometimes used to refer to latent variables z, and sometimes to parameterized neural network layers in the encoder and decoder.[[EXP-NEU], [PNF-NEU], [DIS], [MIN]] E.g. \"The top stochastic layer z_L in FAME is a fully-connected dense layer\".[[MET-NEU], [null], [DIS], [GEN]] No, z_L is a vector of latent variables.[[MET-NEU], [null], [DIS], [GEN]] Are you saying the encoder produces it using a fully-connected layer?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]]\n* Section 2.2 starts talking about \"deterministic layers h\".[[MET-NEU], [null], [DIS], [GEN]] Are these part of the encoder or decoder?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]] What is meant by \"number of layers connecting the stochastic latent variables\"?[[MET-NEU], [CNT], [QSN], [MIN]]\n* Section 2.3: What is meant by \"reconstruction data\"?[[DAT-NEU], [null], [QSN], [MIN]]\n\nIf my understanding of the method is correct, the novelty is limited.[[MET-NEG], [NOV-NEG], [CRT], [MAJ]] Autoregressive priors were used previously in e.g. the Lossy VAE by Chen et al. and IAF-VAE by Kingma et al.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MIN]] The reported likelihood results are very impressive though, and would be reason for acceptance if correct.[[RES-POS,OAL-POS], [REC-POS,EMP-POS], [FBK], [MAJ]] However, the quality of the sampled images shown for CIFAR-10 doesn't match the reported likelihood.[[DAT-NEG], [EMP-NEG], [CRT], [MAJ]] There are multiple possible reasons for this, but after skimming the code I believe it might be due to a faulty implementation of the variational lower bound.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] Instead of calculating all quantities in the log domain, the code takes explicit logs and exponents and stabilizes them by adding small quantities \"eps\": this is not guaranteed to give the right result.[[EXP-NEG,MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] Please fix this and re-run your experiments. (I.e. in _loss.py don't use x/(exp(y)+eps) but instead use x*exp(-y).[[EXP-NEG], [EMP-NEG], [SUG], [MAJ]] Don't use log(var+eps) with var=softplus(x), but instead use var=softplus(x)+eps or parameterize the variance directly in the log domain)."[[EXP-NEG], [EMP-NEG], [SUG], [MAJ]]