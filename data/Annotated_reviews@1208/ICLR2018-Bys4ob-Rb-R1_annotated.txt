"This paper develops a new differentiable upper bound on the performance of classifier when the adversarial input in l_infinity is assumed to be applied.[[INT-NEU], [null], [SMY], [GEN]]\nWhile the attack model is quite general, the current bound is only valid for linear and NN with one hidden layer model, so the result is quite restrictive.[[MET-NEU], [null], [SMY], [GEN]]\n\nHowever the new bound is an \"upper\" bound of the worst-case performance which is very different from the conventional sampling based \"lower\" bounds. [[MET-NEU], [null], [SMY], [GEN]]Therefore minimizing this upper bound together with a classification loss makes perfect sense and provides a theoretically sound approach to train a robust classifier.[[MET-POS], [EMP-POS], [APC], [GEN]]\nThis paper provides a gradient of this new upper bound with respect to model parameters so we can apply the usual first order optimization scheme to this joint optimization (loss + upper bound).[[MET-NEU], [EMP-NEU], [DIS], [GEN]]\nIn conclusion, I recommend this paper to be accepted, since it presents a new and feasible direction of a principled approach to train a robust classifier,[[MET-POS,OAL-POS], [REC-POS], [APC,FBK], [MAJ]] and the paper is clearly written and easy to follow.[[OAL-POS], [CLA-POS], [APC], [MAJ]]\n \nThere are possible future directions to be developed.[[FWK-NEU], [null], [DIS], [GEN]]\n\n1. Apply the sum-of-squares (SOS) method.\nThe paper's SDP relaxation is the straightforward relaxation of Quadratic Program (QP), and in terms of SOS relaxation hierarchy, it is the first hierarchy.[[MET-NEU], [EMP-NEU], [SUG], [MIN]] One can increase the complexity going beyond the first hierarchy, and this should provides a computationally more challenging but tighter upper bound.[[MET-NEU], [EMP-NEU], [DIS], [GEN]]\nThe paper already mentions about this direction and it would be interesting to see the experimental results.[[EXP-NEU], [null], [DIS], [MIN]]\n\n2. Develop a similar relaxation for deep neural networks.\nThe author already mentioned that they are pursuing this direction.[[MET-NEU], [EMP-NEU], [SUG,DIS], [MIN]] While developing the result to the general deep neural networks might be hard, residual networks maybe fine thanks to its structure."[[MET-NEU,RES-NEU], [EMP-NEU], [DIS], [GEN]]