"Summary:\n\nThis paper:\n- provides a compehensive review of existing techniques for verifying properties of neural networks.[[INT-NEU], [null], [SMY], [GEN]]\n- introduces a simple branch-and-bound approach.[[MET-NEU], [null], [SMY], [GEN]]\n- provides fairly extensive experimental comparison of their method and 3 others (Reluplex, Planet, MIP) on 2 existing benchmarks and a new synthetic one.[[RWK-NEU,EXP-NEU], [CMP-NEU], [SMY], [GEN]]\n\nRelevance: Although there isn't any learning going on, the paper is relevant to the conference.[[OAL-POS], [APR-POS], [FBK], [MAJ]]\n\nClarity: Writing is excellent, the content is well presented and the paper is enjoyable read.[[OAL-POS], [CLA-POS], [APC], [MAJ]]\n\nSoundness: As far as I can tell, the work is sound.[[OAL-POS], [EMP-POS], [APC], [MAJ]]\n\nNovelty: This is in my opinion the weakest point of the paper.[[OAL-NEU], [NOV-NEU], [CRT], [MAJ]] There isn't really much novelty in the work.[[OAL-NEG], [NOV-NEG], [CRT], [MAJ]] The branch&bound method is fairly standard, two benchmarks were already existing and the third one is synthetic with weights that are not even trained (so not clear how relevant it is).[[MET-NEU], [EMP-NEG], [CRT], [MAJ]] The main novel result is the experimental comparison, which does indeed show some surprising results (like the fact that BaB works so well).[[EXP-POS,RES-NEU], [NOV-POS,CMP-POS], [APC], [MAJ]]\n\nSignificance: There is some value in the experimental results, and it's great to see you were able to find bugs in existing methods. [[EXP-POS,RES-POS], [EMP-POS], [APC], [MAJ]]Unfortunately, there isn't much insight to be gained from them.[[EXP-NEU,RES-NEU], [IMP-NEG], [CRT], [MAJ]] I couldn't see any emerging trend/useful recommendations (like \"if your problem looks like X, then use algorithm B\").[[MET-NEU], [EMP-NEG], [CRT], [MAJ]] This is unfortunately often the case when dealing with combinatorial search/optimization.[[EXT-NEU], [null], [DIS], [MAJ]] "