"This paper extends the loss-aware weight binarization scheme to ternarization and arbitrary m-bit quantization and demonstrate its promising performance in the experiments.[[INT-NEU,EXP-POS], [null], [SMY], [GEN]]\n\nReview:\n\nPros\nThis paper formulates the weight quantization of deep networks as an optimization problem in the perspective of loss and solves the problem with a proximal newton algorithm. [[PDI-POS,MET-POS], [EMP-POS], [SMY], [MAJ]] They extend the scheme to allow the use of different scaling parameters and to m-bit quantization.[[MET-POS], [EMP-POS], [SMY], [GEN]] Experiments demonstrate the proposed scheme outperforms the state-of-the-art methods.[[EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]] \n\nThe experiments are complete and the writing is good.[[EXP-POS], [CLA-POS], [APC], [MAJ]]\n\nCons\nAlthough the work seems convincing, it is a little bit straight-forward derived from the original binarization scheme (Hou et al., 2017) to tenarization or m-bit since there are some analogous extension ideas (Lin et al., 2016b, Li & Liu, 2016b)[[RWK-NEU,PDI-NEU,BIB-NEU], [NOV-NEU,CMP-NEG], [DFT], [MAJ]]. Algorithm 2 and section 3.2 and 3.3 can be seen as additive complementary. \n"[[MET-NEU], [null], [SUG], [MIN]]