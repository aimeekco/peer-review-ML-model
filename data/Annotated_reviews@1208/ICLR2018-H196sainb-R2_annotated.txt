"The paper proposes a method to learn bilingual dictionaries without parallel data using an adversarial technique.[[PDI-NEU,DAT-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] The task is interesting and relevant, especially for in low-resource language pair settings.\[[RWK-POS,PDI-POS], [IMP-POS], [APC], [MAJ]]n\nThe paper, however, misses comparison against important work from the literature that is very relevant to their task \u2014 decipherment (Ravi, 2013; Nuhn et al., 2012; Ravi & Knight, 2011) and other approaches like CCA.[[RWK-NEG,BIB-NEU], [CMP-NEG], [DFT], [MIN]]\n\nThe former set of works, while focused on machine translation also learns a translation table in the process[[RWK-NEU,MET-NEU,TNF-NEU], [EMP-NEU], [SMY], [GEN]]. Besides, the authors also claim that their approach is particularly suited for low-resource MT and list this as one of their contributions[[INT-NEU,RWK-POS,PDI-NEU,MET-NEU], [IMP-POS,EMP-NEU], [SMY], [GEN]]. Previous works have used non-parallel and comparable corpora to learn MT models and for bilingual lexicon induction. [[RWK-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]]The authors seem aware of corpora used in previous works (Tiedemann, 2012) yet provide no comparison against any of these methods.[[INT-NEU,RWK-NEU,MET-NEU], [CMP-NEG], [SMY,DFT], [GEN]] While some of the bilingual lexicon extraction works are cited (Haghighi et al., 2008; Artetxe et al., 2017), they do not demonstrate how their approach performs against these baseline methods.[[RWK-NEG,PDI-NEG,BIB-NEG], [IMP-NEG], [DFT], [MIN]] Such a comparison, even on language pairs which share some similarities (e.g., orthography), is warranted to determine the effectiveness of the proposed approach[[RWK-NEU,PDI-POS], [IMP-POS,CMP-NEU], [APC], [MAJ]].\n\nThe proposed methodology is not novel, it rehashes existing adversarial techniques instead of other probabilistic models used in earlier works.[[RWK-NEU,PDI-NEG,MET-NEG], [NOV-NEG,EMP-NEG], [DFT], [MIN]] \n\nFor the translation task, it would be useful to see performance of a supervised MT baseline (many tools available in open-source) that was trained on similar amount of parallel training data (60k pairs) and see the gap in performance with the proposed approach.[[PDI-NEU,DAT-NEU,MET-NEU,RES-NEU], [IMP-NEU,EMP-NEU], [SMY], [GEN]]\n\nThe paper mentions that the approach is \u201cunsupervised\u201d. [[INT-NEU,PDI-NEU], [null], [SMY,DIS], [GEN]]However, it relies on bootstrapping from word embeddings learned on Wikipedia corpus, which is a comparable corpus even though individual sentences are not aligned across languages[[RWK-NEU,PDI-NEU], [CMP-NEU], [SMY], [GEN]]. How does the quality degrade if word embeddings had to be learned from scratch or initialized from a different source?"[[RWK-NEU,PDI-NEU,MET-NEU], [null], [QSN], [GEN]]