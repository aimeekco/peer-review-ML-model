"Paper Summary:\n\nThis paper looks at providing better bounds for the number of linear regions in the function represented by a deep neural network.[[INT-NEU], [null], [SMY], [GEN]] It first recaps some of the setting: if a neural network has a piecewise linear activation function (e.g. relu, maxout), the final function computed by the network (before softmax) is also piecewise linear and divides up the input into polyhedral regions which are all different linear functions.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] These regions also have a correspondence with Activation Patterns, the active/inactive pattern of neurons over the entire network.[[MET-NEU], [null], [SMY], [GEN]] Previous work [1], [2], has derived lower and upper bounds for the number of linear regions that a particular neural network architecture can have.[[RWK-NEU], [CMP-NEU], [SMY], [GEN]] This paper improves on the upper bound given by [2] and the lower bound given by [1].[[RES-POS], [EMP-POS], [APC], [MAJ]] They also provide a tight bound for the one dimensional input case.[[RES-POS], [EMP-POS], [APC], [MAJ]] Finally, for small networks, they formulate finding linear regions as solving a linear program, and use this method to compute the number of linear regions on small networks during training on MNIST[[DAT-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]]\n\nMain Comments:\nThe paper is very well written and clearly states and explains the contributions.[[OAL-POS], [CLA-POS], [APC], [MAJ]] However, the new bounds proposed (Theorem 1, Theorem 6), seem like small improvements over the previously proposed bounds,[[RWK-POS, MET-NEG], [CMP-POS], [APC], [MAJ]] with no other novel interpretations or insights into deep architectures.[[MET-NEG], [NOV-NEG], [CRT], [MAJ]] (The improvement on Zaslavsky's theorem is interesting.)[[MET-POS], [EMP-POS], [APC], [MAJ]] The idea of counting the number of regions exactly by solving a linear program is interesting,[[MET-POS], [EMP-POS], [APC], [MAJ]] but is not going to scale well,[[FWK-NEG], [SUB-NEG], [CRT], [MAJ]] and as a result the experiments are on extremely small networks (width 8), which only achieve 90% accuracy on MNIST.[[DAT-NEG,EXP-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] It is therefore hard to be entirely convinced by the empirical conclusions that more linear regions is better.[[RES-NEG], [EMP-NEG], [CRT], [MIN]] I would like to see the technique of counting linear regions used even approximately for larger networks, where even though the results are an approximation, the takeaways might be more insightful.[[EXP-NEU,RES-NEG], [SUB-NEU], [CRT], [MIN]]\n\nOverall, while the paper is well written and makes some interesting points,[[OAL-POS], [CLA-POS], [APC], [MAJ]] it presently isn't a significant enough contribution to warrant acceptance.[[OAL-NEG], [REC-NEG], [CRT], [MAJ]]\n\n[1] On the number of linear regions of Deep Neural Networks, 2014, Montufar, Pascanu, Cho, Bengio[[RWK-NEU], [CNT], [DIS], [MIN]]\n[2] On the expressive power of deep neural networks, 2017, Raghu, Poole, Kleinberg, Ganguli, Sohl-Dickstein"[[RWK-NEU], [CNT], [DIS], [MIN]]