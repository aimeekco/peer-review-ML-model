"The authors show that two types of singularities impede learning in deep neural networks: elimination singularities (where a unit is effectively shut off by a loss of input or output weights, or by an overly-strong negative bias), and overlap singularities, where two or more units have very similar input or output weights.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] They then demonstrate that skip connections can reduce the prevalence of these singularities, and thus speed up learning.[[MET-NEU], [null], [SMY], [GEN]]\n\nThe analysis is thorough: the authors explore alternative methods of reducing the singularities, and explore the skip connection properties that more strongly reduce the singularities, and make observations consistent with their overarching claims.[[MET-POS,ANA-POS], [EMP-POS], [APC], [MAJ]]\n\nI have no major criticisms.[[OAL-NEU], [CNT], [DIS], [MIN]]\n\nOne suggestion for future work would be to provide a procedure for users to tailor their skip connection matrices to maximize learning speed and efficacy.[[FWK-NEU], [SUB-NEU,EMP-NEU], [SUG], [MIN]] The authors could then use this procedure to make highly trainable networks, and show that on test (not training) data, the resultant network leads to high performance."[[DAT-NEU,MET-NEU], [SUB-NEU,EMP-NEU], [SUG,DIS], [GEN]]