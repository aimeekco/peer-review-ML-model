"The authors propose a model for sequence classification and sequential decision making. [[INT-NEU], [null], [SMY], [GEN]]The model interweaves attention layers, akin to those used by Vaswani et al, with temporal convolution.[[RWK-NEU], [CMP-NEU], [DIS], [GEN]] The authors demonstrate superior performance on a variety of benchmark problems, including those for supervised classification and for sequential decision making.[[RES-POS], [EMP-POS], [APC], [MAJ]]\n\nUnfortunately, I am not an expert in meta-learning, so I cannot comment on the difficulty of the tasks (e.g. Omniglot) used to evaluate the model or the appropriateness of the baselines the authors compare against (e.g. continuous control).[[EXT-NEU], [null], [DIS], [GEN]]\n\nThe experiment section definitely demonstrate the effort put into this work.[[EXP-POS], [EMP-POS], [APC], [MAJ]] However, my primary concern is that the model seems somewhat lacking in novelty.[[MET-NEG], [NOV-NEG], [CRT], [MAJ]] Namely, it interweaves the Vaswani style attention with with temporal convolutions (along with TRPO.[[MET-NEG], [NOV-NEG], [CRT], [MAJ]] The authors claim that Vaswani model does not incoporate positional information, but from my understanding, it actually does so using positional encoding.[[MET-NEG], [NOV-NEG], [CRT], [MAJ]] I also do not see why the Vaswani model cannot be lightly adapted for sequential decision making.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] I think comparison to such a similar model would strengthen the novelty of this paper (e.g. convolution is a superior method of incorporating positional information).[[MET-NEU], [NOV-NEU,CMP-NEU], [SUG], [MIN]]\n\nMy second concern is that the authors do not provide analysis and/or intuitions on why the proposed models outperform prior art in few-shot learning.[[ANA-NEG], [SUB-NEG], [DFT], [MIN]] I think this information would be very useful to the community in terms of what to take away from this paper.[[FWK-NEU], [IMP-NEU], [DIS], [MIN]] In retrospect, I wish the authors would have spent more time doing ablation studies than tackling more task domains.[[ANA-NEG], [SUB-NEG], [CRT], [MIN]]\n\nOverall, I am inclined to accept this paper on the basis of its experimental results.[[EXP-POS,RES-POS,OAL-POS], [REC-POS], [FBK], [MAJ]] However I am willing to adjust my review according to author response and the evaluation of the experiment section by other reviewers (who are hopefully more experienced in this domain).[[EXT-NEU], [null], [DIS], [GEN]]\n\nSome minor feedback/questions for the authors:\n- I would prefer mathematical equations as opposed to pseudocode formulation[[MET-NEU], [SUB-NEU], [SUG], [MIN]]\n- In the experiment section for Omniglot, when the authors say \"1200 classes for training and 432 for testing\", it sounds like the authors are performing zero-shot learning.[[DAT-NEG,MET-NEG], [EMP-NEG], [CRT], [MIN]] How does this particular model generalize to classes not seen during training?"[[EXP-NEG,MET-NEG], [EMP-NEG], [QSN], [MIN]]