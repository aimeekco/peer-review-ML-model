"This work shows that a simple non-parametric approach of storing state embeddings with the associated Monte Carlo returns is sufficient to solve several benchmark continuous control problems with sparse rewards (reacher, half-cheetah, double pendulum, cart pole) (due to the need to threshold a return the algorithms work less well with dense rewards, but with the introduction of a hyper-parameter is capable of solving several tasks there).[[INT-NEU], [null], [SMY], [GEN]] The authors argue that the success of these simple approaches on these tasks suggest that more changing problems need to be used to assess new RL algorithms.[[MET-NEU], [EMP-NEU], [SMY], [GEN]]\n\nThis paper is clearly written[[OAL-POS], [CLA-POS], [APC], [MAJ]] and it is important to compare simple approaches on benchmark problems[[RWK-NEG], [CMP-NEG], [SUG], [MAJ]]. There are a number of interesting and intriguing side-notes and pieces of future work mentioned.[[RWK-POS], [null], [SMY], [GEN]]\n\nHowever, the originality and significance of this work is a significant drawback.[[MET-NEU], [NOV-NEG,IMP-NEG], [DFT], [MAJ]] The use non-parametric approaches to the action-value function go back to at least [1] (and probably much further). So the algorithms themselves are not particularly novel, and are limited to nearly-deterministic domains with either single sparse rewards (success or failure rewards) or introducing extra hyper-parameters per task.[[MET-NEG], [EMP-NEG], [DFT], [MAJ]]\n\nThe significance of this work would still be quite strong if, as the author's suggest, these benchmarks were being widely used to assess more sophisticated algorithms and yet these tasks were mastered by such simple algorithms with no learnable parameters. [[MET-NEU], [IMP-NEU,EMP-NEU], [DFT], [MAJ]]Yet, the results do not support the claim.[[RES-NEG], [null], [DFT], [MAJ]] Even if we ignore that for most tasks only the sparse reward (which favors this algorithm) version was examined, these author's only demonstrate success on 4, relatively simple tasks.[[MET-NEU], [EMP-NEU], [DFT], [MAJ]]\n\nWhile these simple tasks are useful for diagnostics, it is well-known that these tasks are simple and, as the author's suggest \"more challenging tasks  .... are necessary to properly assess advances made by sophisticated, optimization-based policy algorithms.\" [[MET-NEU], [EMP-NEU], [DFT], [MAJ]]Lillicrap et al. (2015) benchmarked against 27 tasks, Houtfout et al. (2016) compared in the paper also used Walker2D and Swimmer (not used in this paper) as did [2], OpenAI Gym contains many more control environments than the 4 solved here and significant research is pursing complex manipulation and grasping tasks (e.g. [3]). This suggests the author's claim has already been widely heeded and this work will be of limited interest.[[RWK-NEU,MET-NEG], [CMP-NEG], [CRT], [MAJ]]\n\n[1] Juan, C., Sutton, R. S., & Ram, A. Experiments with Reinforcement Learning in Problems with Continuous State and Action Spaces.\n\n[2] Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., & Meger, D. (2017). Deep reinforcement learning that matters. arXiv preprint arXiv:1709.06560.\n\n[3] Nair, A., McGrew, B., Andrychowicz, M., Zaremba, W., & Abbeel, P. (2017). Overcoming exploration in reinforcement learning with demonstrations. arXiv preprint arXiv:1709.10089."[[BIB-NEU], [null], [SUG], [GEN]]