"This paper focuses on imitation learning with intentions sampled \nfrom a multi-modal distribution.[[INT-NEU,EXP-NEU,MET-NEU], [null], [SMY], [GEN]] The papers encode the mode as a hidden \nvariable in a stochastic neural network and suggest stepping around posterior \ninference over this hidden variable (which is generally required to \ndo efficient maximum likelihood) with a biased importance \nsampling estimator.[[RWK-NEU], [EMP-NEU], [SMY], [GEN]] Lastly, they incorporate attention for large visual inputs.[[RWK-NEU,MET-NEU], [null], [SMY], [GEN]] \n\nThe unimodal claim for distribution without randomness is weak.[[RWK-NEU,MET-NEG], [IMP-NEG,EMP-NEG], [DFT], [MIN]] The distribution \ncould be replaced with a normalizing flow. [[RWK-NEU,EXP-NEG], [EMP-NEG], [SUG,DFT], [MIN]]The use of a latent variable \nin this setting makes intuitive sense, but I don't think multimodality motivates it.[[RWK-NEU,PDI-NEU,MET-NEG], [EMP-NEG], [CRT], [MIN]]\n\nMoreover, it really felt like the biased importance sampling approach should be \ncompared to a formal inference scheme.[[EXP-NEG,MET-NEG,OAL-NEG], [IMP-NEG,EMP-NEG], [DFT], [MIN]] I can see how it adds value over sampling \nfrom the prior, but it's unclear if it has value over a modern approximate inference \nscheme like a black box variational inference algorithm or stochastic gradient MCMC.[[RWK-NEU,PDI-NEG,EXP-NEU,MET-NEU], [EMP-NEG], [DFT,CRT], [MIN]]\n\nHow important is using the pretrained weights from the deterministic RNN?[[PDI-NEU,MET-NEU], [null], [QSN], [GEN]]\n\nFinally, I'd also be curious about how much added value you get from having \naccess to extra rollouts.[[PDI-NEU], [null], [QSN], [GEN]]\n"