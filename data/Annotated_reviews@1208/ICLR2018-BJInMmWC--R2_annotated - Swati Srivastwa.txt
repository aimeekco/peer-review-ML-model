"**Summary**\nThe paper proposes an extension of the attend, infer, repeat generative model of Eslami, 2016 and extends it to handle ``visual attribute descriptions.[[INT-NEU,PDI-NEU], [null], [DIS], [GEN]] This straightforward extension is claimed to improve image quality and shown to improve performance on a previously introduced image caption ranking task.[[PDI-NEU], [null], [DIS], [GEN]] In general, the paper shows improvements on an image caption agreement task introduced in Kuhnle and Copestake, 2017.[[RWK-POS], [CNT], [APC], [MAJ]]  The paper seems to have weaknesses pertaining to the approach taken, clarity of presentation and comparison to baselines which mean that the paper does not seem to meet the acceptance threshold for ICLR.[[OAL-NEG], [APR-NEG,PNF-NEG], [CRT], [MAJ]] See more detailed points below in Weaknesses.[[CNT], [null], [DIS], [GEN]]\n\n**Strengths**\nI like the high-level motivation of the work, that one needs to understand and establish that language or semantics can help learn better representations for images. [[PDI-POS], [EMP-POS], [APC], [MAJ]]I buy the premise and think the work addresses an important issue.[[PDI-POS], [EMP-POS], [APC], [MAJ]] \n\n**Weakness**\n\nApproach:\n* A major limitation of the model seems to be that one needs access to both images and attribute vectors at inference time to compute representations which is a highly restrictive assumption (since inference networks are discriminative).[[PDI-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] The paper should explain how/if one can compute representations given just the image, for instance, say by not using amortized inference. [[MET-NEU,ANA-NEU], [null], [SUG], [MIN]]The paper does propose to use an image-only encoder but that is intended in general as a modeling choice to explain statistics which are not captured by the attributes (in this case location and orientation as explained in the Introduction of the paper).[[MET-NEU], [null], [DIS], [GEN]]\n\nClarity:\n* Eqn. 5, LHS can be written more clearly as \\hat{a}_k.[[MET-NEG], [CLA-NEG], [SUG,CRT], [MIN]] \n\n* It would also be good to cite the following related work, which closely ties into the model of Eslami 2016, and is prior work: \n\nEfficient inference in occlusion-aware generative models of images,\nJonathan Huang, Kevin Murphy.[[RWK-NEU], [SUB-NEU], [SUG], [MIN]]\nICLR Workshops, 2016\n\n* It would be good to clarify that the paper is focusing on the image caption agreement task from Kuhnle and Copestake, as opposed to generic visual question answering.[[RWK-NEU], [CMP-NEU], [SUG], [MIN]]\n\n* The claim that the paper works with natural language should be toned down and clarified.[[OAL-NEU], [null], [SUG], [MIN]] This is not natural language, firstly because the language in the dataset is synthetically generated and not \u201cnatural\u201d.[[DAT-NEG], [EMP-NEG], [CRT], [MAJ]] Secondly, the approach parses this \u201csynthetic\u201d language into structured tuples which makes it even less natural.[[DAT-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] Also, Page. 3. What does \u201cpartial descriptions\u201d mean?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\n* Section 3: It would be good to explicitly draw out the graphical model for the proposed approach and clarify how it differs from prior work (Eslami, 2016).[[MET-NEU], [CMP-NEU], [SUG], [MIN]]\n\n* Sec. 3. 4 mentions that the \u201conly image\u201d encoder is used to obtain the representation for the image, but the \u201conly image\u201d encoder is expected to capture the \u201cindescribable component\u201d from the image, then how is the attribute information from the image captured in this framework?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]] One cannot hope to do image caption association prediction without capturing the image attributes...\n\n*,[[RES-NEU], [null], [DIS], [GEN]] In general, the writing and presentation of the model seem highly fragmented, and it is not clear what the specifics of the overall model are.[[MET-NEG], [CLA-NEG,EMP-NEG], [CRT], [MIN]] For instance, in the decoder, the paper mentions for the first time that there are variables \u201cz\u201d, but does not mention in the encoder how the variables \u201cz\u201d were obtained in the first place (Sec. 3.1).[[EXP-NEG], [EMP-NEG], [DFT], [MAJ]] For instance, it is also not clear if the paper is modeling variable length sequences in a similar manner to Eslami, 2016 or not, and if this work also has a latent variable [z, z_pres] at every timestep which is used in a similar manner to Eqn. 2 in Eslami, 2016.[[MET-NEG], [CMP-NEG], [CRT], [MIN]] Sec. 3.4 \u201cGEN Image Encoder\u201d has some typo, it is not clear what the conditioning is within q(z) term.[[EXP-NEG,MET-NEG], [CLA-NEG], [CRT], [MIN]]\n\n* Comparison to baselines: \n  1. How well does this model do against a baseline discriminative image caption ranking approach, similar to [D]?[[RWK-NEU,MET-NEU], [CMP-NEU], [QSN], [MIN]] This seems like an important baseline to report for the image caption ranking task.[[RWK-NEU], [CMP-NEU], [DIS], [MIN]]\n  2. Another crucial baseline is to train the Attend, Infer, Repeat model on the ShapeWorld images, and then take the latent state inferred at every step by that model, and use those features instead of the features described in Sec. 3.4[[RWK-NEU,EXP-NEU], [CMP-NEU], [DIS], [MIN]] \u201cGen Image Encoder\u201d and repeat the rest of the proposed pipeline.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]] Does the proposed approach still show gains over Attend Infer Repeat?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n  3. The results shown in Fig. 7 are surprising -- in general, it does not seem like a regular VAE would do so poorly.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]] Are the number of parameters in the proposed approach and the baseline VAE similar? [[MET-NEU], [EMP-NEU], [QSN], [MIN]]Are the choices of decoder etc. similar?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]] Did the model used for drawing Fig. 7 converge?[[MET-NEU,TNF-NEU], [EMP-NEU], [QSN], [MIN]] Would be good to provide its training curve.[[EXP-NEU,TNF-NEU], [EMP-NEU], [SUG], [MIN]] Also, it would be good to evaluate the AIR model from Eslami, 2016 on the same simple shapes dataset and show unconditional samples.[[RWK-NEU,DAT-NEU], [CMP-NEU], [SUG], [MIN]] If the claim from the work is true, that model should be just as bad as a regular VAE and would clearly establish that using language is helping get better image samples.[[RWK-NEU,DAT-NEU], [CMP-NEU], [SUG], [MIN]]\n\n* Page 2: In general the notion of separating the latent space into content and style, where we have labels for the \u201ccontent\u201d is an old idea that has appeared in the literature and should be cited accordingly.[[RWK-NEU,MET-NEU], [CMP-NEU], [DFT], [MIN]] See [B] for an earlier treatment, and an extension by [A]. See also the Bivcca-private model of [C] which has \u201cprivate\u201d latent variables for vision similar to this work (this is relevant to Sec. 3.2.)[[BIB-NEU], [null], [DIS], [GEN]]\n\nReferences:\n[A]: Upchurch, Paul, Noah Snavely, and Kavita Bala. 2016. \u201cFrom A to Z: Supervised Transfer of Style and Content Using Deep Neural Network Generators.\u201d arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1603.02003.\n\n[B]: Kingma, Diederik P., Danilo J. Rezende, Shakir Mohamed, and Max Welling. 2014.[[BIB-NEU], [null], [DIS], [GEN]] \u201cSemi-Supervised Learning with Deep Generative Models.\u201d arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1406.5298.\n\n[C]: Wang, Weiran, Xinchen Yan, Honglak Lee, and Karen Livescu. 2016. \u201cDeep Variational Canonical Correlation Analysis.\u201d arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1610.03454.[[BIB-NEU], [null], [DIS], [GEN]]\n\n[D]: Kiros, Ryan, Ruslan Salakhutdinov, and Richard S. Zemel. 2014. \u201cUnifying Visual-Semantic Embeddings with Multimodal Neural Language Models.\u201d arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1411.2539.\n"[[BIB-NEU], [null], [DIS], [GEN]]