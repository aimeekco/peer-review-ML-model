"The paper presents interesting algorithms for minimizing softmax with many classes.[[INT-POS], [null], [SMY], [GEN]] The objective function is a multi-class classification problem (using softmax loss) and with linear model.[[MET-NEU], [null], [SMY], [GEN]] The main idea is to rewrite the obj as double-sum using the dual formulation and then apply SGD to solve it.[[PDI-NEU], [null], [SMY], [GEN]] At each iteration, SGD samples a subset of training samples and labels.[[MET-NEU], [null], [SMY], [GEN]] The main contribution of this paper is: 1) proposing a U-max trick to improve the numerical stability and 2) proposing an implicit SGD approach.[[MET-NEU], [null], [SMY], [GEN]] It seems the implicit SGD approach is better in the experimental comparisons. [[EXP-POS], [EMP-POS], [APC], [MAJ]]\n\nI found the paper quite interesting, but meanwhile I have the following comments and questions:[[OAL-POS], [null], [APC,QSN], [GEN]] \n\n- As pointed out by the authors, the idea of this formulation and doubly SGD is not new. [[PDI-NEG], [NOV-NEG], [CRT], [MAJ]](Raman et al, 2016) has used a similar trick to derive the double-sum formulation and solved it by doubly SGD.[[RWK-NEU], [CMP-NEU], [DIS], [GEN]] The authors claim that  the algorithm in (Raman et al) has an O(NKD) cost for updating u at the end of each epoch.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [GEN]] However, since each epoch requires at least O(NKD) time anyway (sometimes larger, as in Proposition 2), is another O(NKD) a significant bottleneck?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Also, since the formulation is similar to (Raman et al., 2016), a comparison is needed.[[RWK-NEU,MET-NEU], [CMP-NEU], [SUG], [MIN]] \n\n- I'm confused by Proposition 1 and 2. In appendix E.1, the formulation of the update is derived, but why we need Newton to get log(1/epsilon) time complexity? [[MET-NEU], [null], [QSN], [MIN]]I think most first order methods instead of Newton will have linear converge (log(1/epsilon) time)?[[MET-NEU], [null], [QSN], [MIN]] Also, I guess we are assuming the obj is strongly convex?[[MET-NEU], [null], [QSN], [MIN]]\n\n- The step size is selected in one dataset and used for all others. This might lead to divergence of other algorithms, since usually step size depends on data.[[DAT-NEU,MET-NEU], [EMP-NEU], [DFT], [MIN]] As we can see, OVE, NCE and IS diverges on Wiki-small, which may be fixed if the step size is chosen for each data (in practice we can choose using subsamples for each data).[[DAT-NEU,MET-NEU], [EMP-NEU], [SUG], [MIN]] \n\n- All the comparisons are based on \"epochs\", but the competing algorithms are quite different and can have very different running time for each epoch.[[MET-NEU], [CMP-NEG], [DFT], [GEN]] For example, implicit SGD has another iterative solver for each update. Therefore, the timing comparison is needed in this paper to justify that implicit SGD is faster. [[MET-NEU], [CMP-NEU], [SUG], [MIN]]\n\n- The claim that \"implicit SGD never overshoots the optimum\" needs more supports. Is it proved in some previous papers? [[MET-NEU], [null], [QSN], [MIN]]\n\n- The presentation can be improved.[[OAL-NEU], [PNF-NEG], [SUG,DFT], [MIN]] I think it will be helpful to state the algorithms explicitly in the main paper."[[MET-NEU], [null], [SUG], [MIN]]