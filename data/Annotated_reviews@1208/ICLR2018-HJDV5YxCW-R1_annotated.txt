"This paper suggests a method for varying the degree of quantization in a neural network during the forward propagation phase.[[PDI-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]]\n\nThough this is an important direction to investigate, there are several issues:\n\n1. Comparison with previous results is misleading:\na.\t1-bit weights and floating point activations: Rastegari et al. got 56.8% accuracy on Alexnet, which is better than this paper 1.4bit result of 55.2%.\nb.\tHubara et al. got 51% results on 1-bit weights and 2-bit activations included also quantization first and last layer, in contrast to this paper. [[RWK-NEG,RES-NEG,OAL-NEG], [IMP-NEG,EMP-NEG], [DFT,CRT], [MIN]]Therefore, it is not clear if there is a significant benefit in the proposed method which achieves 51.5% when decreasing the activation precision to 1.4bit. [[RWK-NEG], [CLA-NEG,IMP-NEG,EMP-NEG], [DFT], [MIN]]\n\nTherefore, it is not clear that the proposed methods improve over previous approaches.[[PDI-NEG], [CLA-NEG], [DFT], [MIN]]\n\n2. It is not clear to me: in which dimension of the tensors are we saving the scale factor?[[RWK-NEG], [CLA-NEG], [QSN,CRT], [MIN]] If it is per feature map, or neuron, this eliminates the main benefits of quantization: doing efficient binarized operations when doing Weight*activation during the forward pass?\n\n3.[[RWK-NEU,PDI-NEU,EXP-NEU], [EMP-NEU], [SMY], [GEN]] The review of the literature is inaccurate.[[RWK-NEG], [CLA-NEG,SUB-NEG], [DFT,CRT], [MIN]] For example, it is not true that Courbariaux et al. (2016) \u201cfurther improved accuracy on small datasets\u201d: the main novelty there was binarizing the activations (which typically decreased the accuracy).[[RWK-NEG,BIB-NEU,OAL-NEG], [NOV-NEU,EMP-NEG], [SMY,CRT], [MIN]] Also, it is not clear if the scale factors introduced by XNOR-Net indeed allowed \"a significant improvement over previous work\" in ImageNet (e.g., see DoReFA and Hubara et al. who got similar results using binarized weigths and activations on ImageNet without scale factors).[[RWK-NEG,BIB-NEU], [EMP-NEG], [DIS,CRT], [MIN]]  Lastly, the statement \u201cTypical approaches include linearly placing the quantization points\u201d is inaccurate: it was observed that logarithmic quantization works better in various cases.[[RWK-NEU], [EMP-NEU], [SMY], [GEN]] For example, see Miyashita, Lee and Murmann 2016, and Hubara et al.[[RWK-NEU,BIB-NEU], [null], [SMY], [GEN]]\n\n%%% After Author's Clarification %%%\nThis paper results seem more positive now, and I have therefore have increased my score, assuming the authors will revise the paper accordingly.[[OAL-POS], [IMP-POS], [APC], [MAJ]]\n\n"