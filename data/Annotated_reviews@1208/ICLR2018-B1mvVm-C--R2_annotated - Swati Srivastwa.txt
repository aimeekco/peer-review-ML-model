"In this paper a modular architecture is proposed with the aim of separating environment specific (dynamics) knowledge and task-specific knowledge into different modules.[[INT-NEU,PDI-NEU], [null], [SMY], [MIN]] Several complex but discrete control tasks, with relatively small action spaces, are cast as continuous control problems, and the task specific module is trained to produce non-linear representations of goals in the domain of transformed high-dimensional inputs.[[PDI-NEU], [EMP-NEU], [DIS], [MIN]]\n\nPros\n- \u201cMonolithic\u201d policy representations can make it difficult to reuse or jointly represent policies for related tasks in the same environment; a modular architecture is hence desirable.[[MET-POS], [EMP-POS], [APC], [MAJ]]\n- An extensive study of methods for dimensionality reduction is performed for a task with sparse rewards.[[MET-NEU], [null], [SMY], [GEN]]\n- Despite all the suggestions and questions below, the method is clearly on par with standard A3C across a wide range of tasks, which makes it an attractive architecture to explore further.[[MET-POS,FWK-POS], [IMP-POS,EMP-POS], [APC], [MAJ]]\n\nCons\n- In general, learning a Path function could very well turn out to be no simpler than learning a good policy for the task at hand.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] I have 2 main concerns:\nThe data required for learning a good Path function may include similar states to those visited by some optimal policy.[[DAT-NEG], [EMP-NEG], [CRT], [MAJ]] However, there is no such guarantee for random walks; indeed, for most Atari games which have several levels, random policies don\u2019t reach beyond the first level, so I don\u2019t see how a Path function would be informative beyond the \u2018portions\u2019 of the state space which were visited by policies used to collect data.[[DAT-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]]\nHence, several policies which are better than random are likely to be required for sampling this data, in general. [[DAT-NEU,MET-NEU], [null], [DIS], [MIN]]In my mind this creates a chicken-and-egg issue: how to get the data, to learn the right Path function which does not make it impossible to still reach optimal performance on the task at hand? [[MET-NEU], [EMP-NEU], [QSN], [MIN]]How can we ensure that some optimal policy can still be represented using appropriate Goal function outputs? [[DAT-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]]I don\u2019t see this as a given in the current formulation.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n- Although the method is atypical compared to standard HRL approaches, the same pitfalls may apply, especially that of \u2018option collapse\u2019: given a fixed Path function, the Goal function need only figure out which goal state outputs almost always lead to the same output action in the original action space, irrespective of the current state input phi(s), and hence bypass the Path function altogether; then, the role of phi(s) could be taken by tau(s), and we would end up with the original RL problem but in an arguably noisier (and continuous) action space. [[MET-NEG,RES-NEG], [CMP-NEG], [CRT], [MIN]]I recommend comparing the Jacobian w.r.t the phi(s) and tau(s) inputs to the Path function using saliency maps [1, 2]; alternatively, evaluating final policies with out of date input states s to phi, and the correct tau(s) inputs to Path function should degrade performance severely if it playing the role assumed.[[RWK-NEU,DAT-NEU,MET-NEU], [CMP-NEU], [SUG], [MIN]] Same goes for using a running average of phi(s) and the correct tau(s) in final policies.[[MET-NEU], [null], [DIS], [MIN]]\n- The ability to use state restoration for Path function learning is actually introducing a strong extra assumption compared to standard A3C, which does not technically require it.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] For cheap emulators and fully deterministic games (Atari) this assumption holds, but in general restoring expensive, stochastic environments to some state is hard (e.g. robot arms playing ping-pong, ball at given x, y, z above the table, with given velocity vector).[[MET-NEG], [EMP-NEG], [CRT], [MIN]] \n- If reported results are single runs, please replace with averages over several runs, e.g. a few random seeds.[[RES-NEU], [EMP-NEU], [SUG], [MIN]]  Given the variance in deep RL training curves, it is hard to make definitive claims from single runs.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]  If curves are already averages over several experiment repeats, some form of error bars or variance plot would also be informative.[[EXP-NEU,RES-NEU], [SUB-NEU], [SUG], [MIN]] \n- How much data was actually used to learn the Path function in each case?[[DAT-NEU], [SUB-NEU], [QSN], [MIN]]  If the amount is significant compared to task-specific training, then UA/A3C-L curves should start later than standard A3C curves, by that amount of data.[[DAT-NEU,MET-NEG], [EMP-NEU], [DIS], [MIN]] \n\n\nReferences\n[1] Simonyan, K., Vedaldi, A., and Zisserman, A. Deep inside\nconvolutional networks: Visualising image classification\nmodels and saliency maps. arXiv preprint arXiv:1312.6034, 2013.\n[2] Z Wang, T Schaul, M Hessel, H Van Hasselt, M Lanctot, N De Freitas, Dueling network architectures for deep reinforcement learning arXiv preprint arXiv:1511.06581\n"[[BIB-NEU], [null], [DIS], [GEN]] 