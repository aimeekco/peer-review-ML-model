"The authors define a deep learning model composed of four components:  a student model, a teacher model, a loss function, and a data set.[[DAT-NEU,MET-NEU], [null], [SMY], [GEN]] The student model is a deep learning model (MLP, CNN, and RNN were used in the paper).[[MET-NEU], [null], [DIS], [GEN]] The teacher model learns via reinforcement learning which items to include in each minibatch of the data set.[[DAT-NEU,MET-NEU], [null], [SMY], [GEN]] The student model learns according to a standard stochastic gradient descent technique (Adam for MLP and CNN, Momentum-SGD for RNN), appropriate to the data set (and loss function), but only uses the data items of the minibatch chosen by teacher model.[[MET-NEU], [null], [SMY], [GEN]] They evaluate that their method can learn to provide learning items in an efficient manner in two situations: (1) the same student model-type on a different part of the same data set, and (2) adapt the teaching model to teach a new model-type for a different data set.[[MET-NEU], [null], [SMY], [GEN]] In both circumstances, they demonstrate the efficacy of their technique and that it performs better than other reasonable baseline techniques: self-paced learning, no teaching, and a filter created by randomly reordering the data items filtered out from a teaching model.[[RWK-POS,MET-POS], [EMP-POS], [APC], [MAJ]]\n\nThis is an extremely impressive manuscript and likely to be of great interest to many researchers in the ICLR community.[[OAL-POS], [APR-POS], [APC], [MAJ]] The research itself seems fine,[[OAL-POS], [CNT], [APC], [MAJ]] but there are some issues with the discussion of previous work.[[RWK-NEG], [CNT], [CRT], [MAJ]] Most of my comments focuses on this.[[RWK-NEU], [null], [DIS], [GEN]]\n\nThe authors write that artificial intelligence has mostly overlooked the role of teaching, but this claim is incorrect.[[CNT], [CNT], [CRT], [MAJ]] There is a long history of research on teaching in artificial intelligence.[[EXT-NEU], [null], [DIS], [GEN]] Two literatures of note are intelligent tutoring and machine teaching in the computational learnability literature.[[RWK-NEU], [null], [DIS], [GEN]] A good historical hook to intelligent tutoring is Anderson, J. R., Boyle, C. F., & Reiser, B. J. (1985).[[RWK-NEU], [null], [DIS], [GEN]] Intelligent tutoring systems. Science, 228. 456-462.[[RWK-NEU], [null], [DIS], [GEN]] The literature is still healthy today.[[EXT-NEU], [null], [DIS], [GEN]] One offshoot of it has its own society with conferences and a journal devoted to it (The International Artificial intelligence in Education Society: http://iaied.org/about/).[[EXT-NEU], [null], [DIS], [GEN]] \n\nFor the computational learnability literature, complexity analysis for teaching has a subliterature devoted to it (analogous to the learning literature).[[EXT-NEU], [null], [DIS], [GEN]] Here is a hook into that literature: Goldman, S., & Kerns. M. (1995).[[RWK-NEU], [null], [DIS], [GEN]] On the complexity of teaching. Journal of Computer and Systems Sciences, 50(1), 20-31.[[RWK-NEU], [null], [DIS], [GEN]]\n\nOne last related literature is pedagogical teaching from computational cognitive science.[[RWK-NEU], [null], [DIS], [GEN]] This one is a more recent development.[[RWK-NEU], [null], [DIS], [GEN]] Here are two articles, one that provides a long and thorough discussion that is a definitive start to the literature, and another that is most relevant to the current paper, on applying pedagogical teaching to inverse reinforcement learning (a talk at NIPS 2016).[[RWK-NEU], [null], [DIS], [GEN]]\n\nShafto, P., Goodman, N. D., & Griffiths, T. L. (2014). A rational account of pedagogical reasoning: Teaching by, and learning from, examples.[[RWK-NEU], [null], [DIS], [GEN]] Cognitive Psychology, 71, 55-89.\n\nHo, M. K., Littman, M., MacGlashan, J., Cushman, F., & Austerweil, J. L. (NIPS 2016). [[RWK-NEU], [null], [DIS], [GEN]]\n\nI hope all of this makes it clear to the authors that it is inappropriate to claim that artificial intelligence has \u201clargely overlooked\u201d or \u201clargely neglected\u201d.[[RWK-NEG], [CMP-NEG], [CRT], [MAJ]] \n\nOne other paper of note given that the authors train a MLP is an optimal teaching analysis of a perceptron: (Zhang, Ohannessian, Sen, Alfeld, & Zhu, 2016; NIPS).\n\n\n"[[RWK-NEU], [null], [DIS], [GEN]]