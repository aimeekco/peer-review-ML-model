"This paper borrows the idea from dilated CNN and proposes a dilated convolution based module for fast reading comprehension, in order to deal with the processing of very long documents in many reading comprehension tasks. [[PDI-NEU], [null], [SMY], [GEN]]The method part is clear and well-written.[[MET-POS], [EMP-POS], [APC], [MAJ]] The results are fine when the idea is applied to the BiDAF model,[[RES-POS,MET-NEU], [EMP-POS], [APC], [MAJ]] but are not very well on the DrQA model.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\n(1) My biggest concern is about the motivation of the paper: \n\nFirstly, another popular approach to speed up reading comprehension models is hierarchical (coarse-to-fine) processing of passages, where the first step processes sentences independently (which could be parallelized), then the second step makes predictions over the whole passage by taking the sentence processing results.[[PDI-NEG], [EMP-NEG], [CRT], [MAJ]] Examples include , \"Attention-Based Convolutional Neural Network for Machine Comprehension\", \"A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data\", and \"Coarse-to-fine question answering for long documents\"[[PDI-NEU], [null], [DIS], [GEN]] \n\nThis paper does not compare to the above style of approach empirically,[[MET-NEG], [CMP-NEG], [CRT], [MAJ]] but the hierarchical approach seems to have more advantages and seems a more straightforward solution.[[MET-POS], [EMP-POS], [APC], [MAJ]] \n\nSecondly, many existing works on multiple passage reading comprehension (or open-domain QA as often named in the papers) found that dealing with sentence-level passages could result in better (or on par) results compared with working on the whole documents.[[PDI-NEU], [EMP-NEU], [SUG], [MIN]] Examples include \"QUASAR: Datasets for question answering by search and reading\", \"SearchQA: A new q&a dataset augmented with context from a search engine\", and \"Reinforced Ranker-Reader for Open-Domain Question Answering\".[[PDI-NEU], [EMP-NEU], [SUG], [MIN]] If in many applications the sentence-level processing is already good enough, the motivation of doing speedup over LSTMs seems even waker.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n\nEven on the SQuAD data, the sentence-level processing seems sufficient: as discussed in this paper about Table 5, the author mentioned (at the end of Page 7) that \"the Conv DrQA model only encode every 33 tokens in the passage, which shows that such a small context is ENOUGH for most of the questions\".[[MET-NEU,TNF-NEU], [EMP-NEU], [DIS], [MIN]]\n\nMoreover, the proposed method failed to give any performance boost, but resulted in a big performance drop on the better-performed DrQA system.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Together with the above concerns, it makes me doubt the motivation of this work on reading comprehension.[[PDI-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nI would agree that the idea of using dilated CNN (w/ residual connections) instead of BiLSTM could be a good solution to many online NLP services like document-level classification tasks.[[MET-NEU], [CMP-NEU], [DIS], [MIN]] Therefore, the motivation of the paper may make more sense if the proposed method is applied to a different NLP task.[[PDI-NEU,MET-NEU], [EMP-NEU], [SUG], [MAJ]]\n\n(2) A similar concern about the baselines: the paper did not compare with ANY previous work on speeding up RNNs, e.g. \"Training RNNs as Fast as CNNs\".[[RWK-NEG], [EMP-NEG], [CRT], [MAJ]] The example work and its previous work also accelerated LSTM by several times without significant performance drop on some RC models (including DrQA).[[RWK-NEG,MET-NEG], [CMP-NEG], [CRT], [MAJ]]\n\n(3) About the speedup: it could be imaged that the speedup from the usage of dilated CNN largely depends on the model architecture.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Considering that the DrQA is a better system on both SQuAD and TriviaQA, the speedup on DrQA is thus more important.[[DAT-NEU,MET-NEU], [EMP-NEU], [SUG], [MAJ]] However, the DrQA has less usage of LSTMs, and in order to cover a large reception field, the dilated CNN version of DrQA has a 2-4 times speedup, but still works much worse.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] This makes the speedup less impressive.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\n(4) It seems that this paper was finished in a rush.[[OAL-NEG], [PNF-NEG], [CRT], [MIN]] The experimental results are not well explained and there is not enough analysis of the results.[[EXP-NEG,RES-NEG,ANA-NEG], [SUB-NEG,PNF-NEG], [DFT,CRT], [MAJ]]\n\n(5) I do not quite understand the reason for the big performance drop on DrQA.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] Could you please provide more explanations and intuitions?"[[ANA-NEU], [SUB-NEU], [QSN], [MIN]]