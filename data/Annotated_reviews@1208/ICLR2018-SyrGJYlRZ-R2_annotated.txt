"The paper explores momentum SGD and an adaptive version of momentum SGD which the authors name YF (Yellow Fin).[[INT-NEU,MET-NEU], [null], [SMY], [GEN]] They compare YF to hand tuned momentumSGD and to Adam in several deep learning applications.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]]\n\n\nI found the first part which discusses the theoretical motivation behind YF to be very confusing and misleading:\nBased on the analysis of 1-dimensional problems, the authors design a framework and an algorithm that  supposedly ensures accelerated convergence.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] There are two major problems with this approach:[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\n-First: Exploring 1-dim functions is indeed a nice way to get some intuition.[[MET-POS], [EMP-POS], [APC], [MIN]] Yet,  algorithms that work in the 1-dim case do not trivially generalize to high dimensions, and such reasoning might lead to very bad solutions.[[MET-NEG, RES-NEG], [EMP-NEG], [CRT], [MAJ]]\n\n-Second: Accelerated GD does not benefit over GD in the 1-dim case. And therefore, this is not an appropriate setting to explore acceleration.\nConcretely, the definition of the generalized condition number $\\nu$, and relating it to the standard definition of the condition number $\\kappa$, is very misleading.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] This is since $\\kappa =1$ for 1-dim problems, and therefore accelerated GD does not have any benefits over non accelerated GD in this case.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\nHowever, $\\nu$ might be much larger than 1 even in the 1-dim case.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\n\nRegarding the algorithm itself: there are too many hyper-parameters (which depend on each other) that are tuned (per-dimension).[[MET-NEU], [EMP-NEU], [DIS], [GEN]]\nAnd as I have mentioned, the design of the algorithm is inspired by the analysis of 1-dim quadratic functions.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\nThus, it is very hard for me to believe that this algorithm works in practice unless very careful fine tuning is employed.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\nThe authors mention that their experiments were done without tuning or with very little tuning, which is very mysterious for me.[[EXP-NEG], [SUB-NEG, EMP-NEG], [CRT], [MAJ]]\n\nIn contrast to the theoretical part, the experiments seems very encouraging.[[EXP-POS], [EMP-POS], [APC], [MAJ]] Showing YF to perform very well on several deep learning tasks without (or with very little) tuning.[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]] Again, this seems a bit magical or even too good to be truth.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] I suggest the authors to perform a experiment with say a qaudratic high dimensional function, which is not aligned with the axes in order to illustrate how their method behaves and try to give intuition.\n"[[EXP-NEU,MET-NEU], [EMP-NEU], [SUG], [MIN]]