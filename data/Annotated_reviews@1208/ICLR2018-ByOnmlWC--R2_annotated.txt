"The authors present an algorithm for training ensembles of policy networks that regularly mixes different policies in the ensemble together by distilling a mixture of two policies into a single policy network, adding it to the ensemble and selecting the strongest networks to remain (under certain definitions of a \"strong\" network).[[INT-NEU,MET-NEU], [null], [SMY], [GEN]] The experiments compare favorably against PPO and A2C baselines on a variety of MuJoCo tasks, although I would appreciate a wall-time comparison as well, as training the \"crossover\" network is presumably time-consuming.[[RWK-NEU,EXP-POS], [CMP-POS], [APC], [MAJ]]\n\nIt seems that for much of the paper, the authors could dispense with the genetic terminology altogether - and I mean that as a compliment.[[MET-NEU], [CLA-NEU], [SUG], [MAJ]] There are few if any valuable ideas in the field of evolutionary computing and I am glad to see the authors use sensible gradient-based learning for GPO, even if it makes it depart from what many in the field would consider \"evolutionary\" computing.[[PDI-NEU,MET-NEU], [EMP-NEU], [SUG], [MAJ]] Another point on terminology that is important to emphasize - the method for training the crossover network by direct supervised learning from expert trajectories is technically not imitation learning but behavioral cloning.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] I would perhaps even call this a distillation network rather than a crossover network.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] In many robotics tasks behavioral cloning is known for overfitting to expert trajectories, but that may not be a problem in this setting as \"expert\" trajectories can be generated in unlimited quantities.[[RWK-NEU,MET-NEU], [EMP-NEU], [DIS], [MAJ]]"