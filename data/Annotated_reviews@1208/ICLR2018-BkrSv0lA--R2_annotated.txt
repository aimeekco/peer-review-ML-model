"This paper proposes a new method to train DNNs with quantized weights, by including the quantization as a constraint in a proximal quasi-Newton algorithm, which simultaneously learns a scaling for the quantized values (possibly different for positive and negative weights).[[INT-NEU,MET-NEU], [null], [SMY], [GEN]] \n\nThe paper is very clearly written, and the proposal is very well placed in the context of previous methods for the same purpose.[[RWK-NEU], [CLA-POS], [APC], [MAJ]] The experiments are very clearly presented and solidly designed.[[EXP-POS], [EMP-POS], [APC], [MAJ]]\n\nIn fact, the paper is a somewhat simple extension of the method proposed by Hou, Yao, and Kwok (2017), which is where the novelty resides.[[RWK-NEU,PDI-NEU], [NOV-NEU,CMP-NEU], [SMY], [MAJ]] Consequently, there is not a great degree of novelty in terms of the proposed method, and the results are only slightly better than those of previous methods.\[[MET-NEU,RES-POS], [NOV-NEG], [CRT], [MAJ]]n\nFinally, in terms of analysis of the algorithm, the authors simply invoke a theorem from Hou, Yao, and Kwok (2017), which claims convergence of the proposed algorithm.[[RWK-NEU,ANA-NEU], [NOV-NEG], [SMY,DIS], [MAJ]] However, what is shown in that paper is that the sequence of loss function values converges, which does not imply that the sequence of weight estimates also converges, because of the presence of a non-convex constraint ($b_j^t \\in Q^{n_l}$).[[EXP-NEU,ANA-NEG], [IMP-NEG], [CRT], [MAJ]] This may not be relevant for the practical results, but to be accurate, it can't be simply stated that the algorithm converges, without a more careful analysis."[[EXP-NEU,ANA-NEG], [IMP-NEG], [CRT], [MAJ]]