"The authors propose a new algorithm for exploration in Deep RL.[[MET-NEU], [null], [SMY], [GEN]] They apply Bayesian linear regression, given the last layer of a DQN network as features, to estimate the Q function for each action.[[MET-NEU], [null], [SMY], [GEN]] Posterior weights are sampled to select actions during execution (Thompson Sampling style).[[MET-NEU], [null], [SMY], [GEN]] I generally liked the paper and the approach, here are some more detailed comments.[[MET-POS,OAL-POS], [EMP-POS], [APC], [MAJ]]\n\nUnlike traditional regression, here we are not observing noisy realisations of the true target, since the algorithm is bootstrapping on non-stationary targets.[[MET-NEU], [CMP-NEU], [DIS], [MIN]] It\u2019s not immediately clear what the semantics of this posterior are then.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Take for example the case where a particular transition (s,a,r,s\u2019) gets replayed multiple times in a row, the posterior about Q(s,a) might then become overly confident even though no new observation was introduced.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \n\nPrevious applications of TS to MDPs (Strens, (A Bayesian framework for RL) 2000; Osband 2013) commit to a posterior sample for an episode.[[RWK-NEU], [CMP-NEU], [DIS], [MIN]] But the proposed algorithm samples every T_sample steps, did you find this beneficial to wait longer before resampling?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] It would be useful to comment on that aspect.[[MET-NEU], [EMP-NEU], [SUG], [MIN]]\n\nThe method is evaluated on 6 Atari games (How were the games selected? Do they have exploration challenges?) against a single baseline (DDQN).[[RWK-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]] DDQN wasn\u2019t proposed as an exploration method so it would be good to justify why this is an appropriate baseline (versus other exploration methods).[[RWK-NEU,MET-NEU], [null], [DIS], [MIN]] The authors argue they could not reproduce Osband\u2019s bootstrapped DQN, which is also TS-based, but you could at least have reported their scores.[[MET-NEU,RES-NEG], [SUB-NEG,EMP-NEU], [CRT], [MIN]]  \n\nOn these games versus (their implementation of) DDQN, the results seem encouraging.[[RES-POS], [EMP-POS], [APC], [MAJ]] But it would be good to know whether the approach works well across games and is competitive against other stronger baselines.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MIN]] Alternatively, some evidence that interesting exploratory behavior is obtained (in Atari or even smaller domain) would help convince the reader that the approach does what it claims in practice.[[MET-NEU,RES-NEU], [EMP-NEU], [DIS], [MIN]]\n\nIn addition, your reported score on Atlantis of ~2M seems too big.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] Did you cap the max episode time to 30mins?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] As is done in the baselines usually.[[RWK-NEU], [null], [DIS], [GEN]]\n\n\nMinor things:\n-\u201cTS finds the true Q-function very fast\u201d But that contradicts the previous statements, I think you mean something different.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] If TS does not select certain actions, the Q-function would not be updated for these actions.[[MET-NEU], [null], [DIS], [MIN]] It might find the optimal policy quickly though, even though it doesn\u2019t resolve the entire value function completely.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n-Which epsilon did you use for evaluation of DDQN in the experiments?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]] It\u2019s a bit suspicious that it doesn\u2019t achieve 20+ in Pong.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]]\n-The history of how to go from a Bellman equation to a sample-based update seems a bit distorted.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Sample-based RL did not originate in 2008.[[MET-NEU], [null], [DIS], [GEN]] Also, DQN does not optimize the Bellman residual, it\u2019s a TD update. \n"[[RWK-NEU,MET-NEU], [CMP-NEG], [CRT], [MIN]]