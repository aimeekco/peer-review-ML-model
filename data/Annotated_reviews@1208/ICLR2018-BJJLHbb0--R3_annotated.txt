"Summary\n\nThis applications paper proposes using a deep neural architecture to do unsupervised anomaly detection by learning the parameters of a GMM end-to-end with reconstruction in a low-dimensional latent space.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The algorithm employs a tailored loss function that involves reconstruction error on the latent space, penalties on degenerate parameters of the GMM, and an energy term to model the probability of observing the input samples.[[MET-NEU], [null], [SMY], [GEN]]\n\nThe algorithm replaces the membership probabilities found in the E-step of EM for a GMM with the outputs of a subnetwork in the end-to-end architecture.[[MET-NEU], [null], [SMY], [GEN]] The GMM parameters are updated with these estimated responsibilities as usual in the M-step during training.[[EXP-NEU,MET-NEU], [null], [SMY], [GEN]]\n\nThe paper demonstrates improvements in a number of public datasets.[[DAT-POS,OAL-POS], [EMP-POS], [APC], [MAJ]] Careful reporting of the tuning and hyperparameter choices renders these experiments repeatable, and hence a suitable improvement in the field.[[EXP-POS], [EMP-POS], [APC], [MAJ]] Well-designed ablation studies demonstrate the importance of the architectural choices made, which are generally well-motivated in intuitions about the nature of anomaly detection.[[MET-POS], [EMP-POS], [APC], [MAJ]]\n\nCriticisms\n\nBased on the performance of GMM-EN, the reconstruction error features are crucial to the success of this method.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Little to no detail about these features is included.[[MET-NEG], [SUB-NEG], [DFT], [MIN]] Intuitively, the estimation network is given the latent code conditioned and some (probably highly redundant) information about the residual structure remaining to be modeled.[[MET-NEU], [EMP-NEU], [CRT], [MIN]]\n\nSince this is so important to the results, more analysis would be helpful.[[RES-NEU,ANA-NEG], [SUB-NEU], [SUG], [MIN]] Why did the choices that were made in the paper yield this success?[[RES-NEU], [EMP-NEU], [QSN], [MIN]] How do you recommend other researchers or practitioners selected from the large possible space of reconstruction features to get the best results?[[RES-NEU], [EMP-NEU], [QSN], [MIN]]\n\nQuality\n\nThis paper does not set out to produce a novel network architecture.[[OAL-POS], [NOV-POS], [APC], [MAJ]] Perhaps the biggest innovation is the use of reconstruction error features as input to a subnetwork that predicts the E-step output in EM for a GMM.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] This is interesting and novel enough in my opinion to warrant publication at ICLR, along with the strong performance and careful reporting of experimental design.\n\n"[[EXP-POS,OAL-POS], [NOV-POS,EMP-POS], [APC], [MAJ]]