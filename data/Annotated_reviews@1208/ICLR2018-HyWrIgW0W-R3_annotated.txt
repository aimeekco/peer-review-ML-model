"This paper develop theory to study the impact of stochastic gradient noise for SGD, especially for deep neural network models.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] It is shown that when the gradient noise is isotropic normal, SGD converges to a distribution tilted by the original objective function.[[MET-NEU], [null], [SMY], [GEN]] However, when the gradient noise is non isotropic normal, which is shown common in many models especially in deep neural network models, the behavior of SGD is intriguing, which will not converge to the tilted distribution by the original objective function, sometimes more interestingly, will converge to limit cycles around some critical points of the original objective function.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] The paper also provides some hints on why using SGD can get good generalization ability than gradient descend.\n\nI think the finding of this paper is interesting, and the technical details are correct. I still have the following comments.\n\nFirst, Assumption 4 seems a bit too abstract.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] It is not easy to see what the assumption means.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] It would be better if an example is given, which is verified to satisfy the assumption.[[MET-NEU], [SUB-NEU], [SUG], [MIN]]\n\nAnother comment is related to the overall content of this paper.[[OAL-NEU], [CNT], [DIS], [MIN]] Thought the paper point out that SGD will have the out-of-equilibrium behavior when the gradient noise is non isotropic normal, it remains to show how far away this stationary distribution is from the original distribution defined by the objective function."[[MET-NEU,OAL-NEU], [EMP-NEU], [DIS], [MIN]]