"This paper is about low-precision training for ConvNets.[[INT-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] It proposed a \"dynamic fixed point\" scheme that shares the exponent part for a tensor, and developed procedures to do NN computing with this format.[[PDI-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] The proposed method is shown to achieve matching performance against their FP32 counter-parts with the same number of training iterations on several state-of-the-art ConvNets architectures on Imagenet-1K.[[PDI-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] According to the paper, this is the first time such kind of performance are demonstrated for limited precision training.[[INT-NEU,PDI-POS,EXP-POS,RES-POS,ANA-POS], [IMP-POS,EMP-POS], [APC], [MAJ]]\n\nPotential improvements:\n\t\n  - Please define the terms like FPROP and WTGRAD at the first occurance.[[RWK-NEG,PDI-NEG], [SUB-NEG,EMP-NEG], [DFT], [MIN]]\n  - For reference, please include wallclock time and actual overall memory consumption comparisons of the proposed methods and other methods as well as the baseline (default FP32 training)."[[PDI-NEU,EXP-NEU,MET-NEU,ANA-NEU,BIB-NEG], [SUB-NEG,EMP-NEU], [DFT], [MIN]]