"Overall:\nI had a really hard time reading this paper because I found the writing to be quite confusing.[[OAL-NEG], [CLA-NEG], [CRT], [MAJ]] For this reason I cannot recommend publication as I am not sure how to evaluate the paper\u2019s contribution.[[OAL-NEG], [REC-NEG], [FBK], [MAJ]] \n\nSummary\nThe authors study state space models in the unsupervised learning case.[[PDI-NEU], [null], [SMY], [GEN]] We have a set of observed variables Y, we posit a latent set of variables X, the mapping from the latent to the observed variables has a parametric form and we have a prior over the parameters.[[MET-NEU], [null], [SMY], [GEN]]  We want to infer a posterior density given some data.[[MET-NEU], [null], [SMY], [GEN]] \n\nThe authors propose an algorithm which uses sequential Monte Carlo + autoencoders.[[MET-NEU], [null], [SMY], [GEN]]  They use a REINFORCE-like algorithm to differentiate through the Monte Carlo.[[MET-NEU], [null], [SMY], [GEN]]  The contribution of this paper is to add to this a method which uses 2 different ELBOs for updating different sets of parameters.[[MET-NEU], [null], [DIS], [GEN]] \n\nThe authors show the AESMC works better than importance weighted autoencoders and the double ELBO method works even better in some experiments.[[EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]]  \n\nThe proposed algorithm seems novel,[[MET-POS], [NOV-POS], [APC], [MAJ]] but I do not understand a few points which make it hard to judge the contribution.[[OAL-NEG], [null], [DIS], [GEN]] Note that here I am assuming full technical correctness of the paper (and still cannot recommend acceptance).[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nIs the proposed contribution of this paper just to add the double ELBO or does it also include the AESMC (that is, should this paper subsume the anonymized pre-print mentioned in the intro)? [[INT-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]]This was very unclear to me.[[INT-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nThe introduction/experiments section of the paper is not well motivated.[[INT-NEG,EXP-NEG], [EMP-NEG], [CRT], [MAJ]] What is the problem the authors are trying to solve with AESMC (over existing methods)? [[MET-NEU], [EMP-NEU], [QSN], [MIN]]Is it scalability?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Is it purely to improve likelihood of the fitted model (see my questions on the experiments in the next section)?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] \n\nThe experiments feel lacking.[[EXP-NEG], [SUB-NEG], [DFT], [MIN]] There is only one experiment comparing the gains from AESMC, ALT to a simpler (?) method of IWAE.[[EXP-NEG], [EMP-NEG], [DFT], [MIN]] We see that they do better but the magnitude of the improvement is not obvious (should I be looking at the ELBO scores as the sole judge?[[RES-NEU], [EMP-NEU], [QSN], [MIN]] Does AESMC give a better generative model?).[[MET-NEU], [EMP-NEU], [QSN], [MIN]] The authors discuss the advantages of SMC and say that is scales better than other methods,[[MET-NEU], [CMP-NEU], [DIS], [MIN]] it would be good to show this as an experimental result if indeed the quality of the learned representations is comparable."[[RES-NEU], [EMP-NEU], [DIS], [MIN]]