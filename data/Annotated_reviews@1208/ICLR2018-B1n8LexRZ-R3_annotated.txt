"The paper introduces a non-volume-preserving generalization of HMC whose transitions are determined by a set of neural network functions.[[INT-NEU,PDI-NEU,MET-NEU], [null], [SMY], [GEN]] These functions are trained to maximize expected squared jump distance.[[MET-NEU], [null], [SMY], [GEN]]\nThis works because each variable (of the state space) is modified in turn, so that the resulting update is invertible, with a tractable transformation inspired by Dinh et al 2016.[[RWK-NEU,MET-NEU], [CMP-NEU], [SMY], [GEN]]\n\nOverall, I believe this paper is of good quality, clearly and carefully written, and potentially accelerates mixing in a state-of-the-art MCMC method, HMC, in many practical cases.[[MET-POS,OAL-POS], [CLA-POS,EMP-POS], [APC], [MAJ]] A few downsides are commented on below.\n\n[[OAL-NEG], [CNT], [DIS], [MIN]]The experimental section proves the usefulness of the method on a range of relevant test cases; in addition, an application to a latent variable model is provided sec5.2. [[EXP-POS,MET-NEU], [EMP-POS], [APC], [MAJ]]\nFig 1a presents results in terms of numbers of gradient evaluations, but I couldn't find much in the way of computational cost of L2HMC in the paper. [[MET-NEG,RES-NEU,ANA-NEG,TNF-NEU], [SUB-NEG,EMP-NEG], [DFT], [MIN]]I can't see where the number \"124x\" in sec 5.1 stems from.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] As a user, I would be interested in the typical computational cost of both \"MCMC sampler training\" and MCMC sampler usage (inference?), compared to competing methods.[[MET-NEU,ANA-NEU], [SUB-NEU], [DIS], [MIN]] This is admittedly hard to quantify objectively, but just an order of magnitude would be helpful for orientation.[[ANA-NEU], [EMP-NEU], [SUG], [MIN]] \nWould it be relevant, in sec5.1, to compare to other methods than just HMC, eg LAHMC?[[MET-NEU], [CMP-NEU], [QSN], [MIN]]\n\nI am missing an intuition for several things: eq7, the time encoding defined in Appendix C\n\nAppendix Fig5, I cannot quite see how the caption claim is supported by the figure (just hardly for VAE, but not for HMC).[[MET-NEG,TNF-NEG], [PNF-NEG], [CRT], [MIN]]\n\nThe number \"124x ESS\" in sec5.1 seems at odds with the number in the abstract, \"50x\".[[MET-NEG], [PNF-NEG], [CRT], [MIN]]\n\n# Minor errors\n- sec1: \"The sampler is trained to minimize a variation\": should be maximize\n\"as well as on a the real-world\"[[MET-NEG], [EMP-NEG], [SUG], [MIN]]\n- sec3.2 \"and 1/2 v^T v the kinetic\": \"energy\" missing\n[[MET-NEG], [SUB-NEG], [DFT], [MIN]]- sec4: the acronym L2HMC is not expanded anywhere in the paper\n[[MET-NEU], [CLA-NEG,PNF-NEG], [CRT], [MIN]]The sentence \"We will denote the complete augmented...p(d)\" might be moved to after \"from a uniform distribution\" in the same paragraph.[[CNT], [PNF-NEU], [SUG], [MIN]] \nIn paragraph starting \"We now update x\":\n    - specify for clarity: \"the first update, which yields x' \"/ \"the second update, which yields x''  \"\n [[MET-NEG], [CLA-NEG,PNF-NEG], [CRT], [MIN]]   - \"only affects $x_{\\bar{m}^t}$\": should be $x'_{\\bar{m}^t}$  (prime missing)\n [[MET-NEG], [PNF-NEG], [DFT], [MIN]]   - the syntax using subscript m^t is confusing to read; wouldn't it be clearer to write this as a function, eg \"mask(x',m^t)\"?\n [[MET-NEG], [PNF-NEG], [DFT,QSN], [MIN]]   - inside zeta_2 and zeta_3, do you not mean $m^t\" and $\\bar{m}^t$ ?\n[[MET-NEG], [PNF-NEG], [DFT,QSN], [MIN]]- sec5: add reference for first mention of \"A NICE MC\"\n[[MET-NEG], [PNF-NEG], [DFT], [MIN]]- Appendix A: \n    - \"Let's\" -> \"Let\"\n[[MET-NEG], [PNF-NEG], [SUG], [MIN]]   - eq12 should be x''=...\n-[[MET-NEG], [PNF-NEG], [SUG], [MIN]] Appendix C: space missing after \"Section 5.1\"[[MET-NEG], [PNF-NEG], [DFT], [MIN]]\n- Appendix D1: \"In this section is presented\" : sounds odd\[[MET-NEG], [PNF-NEG], [DFT], [MIN]]n- Appendix D3: presumably this should consist of the figure 5 ? Maybe specify."[[TNF-NEG], [PNF-NEG], [SUG], [MIN]]