"Experimental results have shown that deep networks (many hidden layers) can approximate more complicated functions with less neurons compared to shallow (single hidden layer) networks.[[EXT-NEU], [null], [DIS], [GEN]] \nThis paper gives an explicit proof when the function in question is a sparse polynomial, ie: a polynomial in n variables, which equals a sum J of monomials of degree at most c.[[MET-NEU], [null], [SMY], [GEN]] \nIn this setup, Theorem 4.3 says that a shallow network need at least ~ (1 + c/n)^n many neurons, while the optimal deep network (whose depth is optimized to approximate this particular input polynomial) needs at most  ~ J*n, that is, linear in the number of terms and the number of variables.[[MET-NEU], [null], [SMY], [GEN]] The paper also has bounds for neural networks of a specified depth k (Theorem 5.1), and the authors conjecture this bound to be tight (Conjecture 5.2).[[MET-NEU], [null], [SMY], [GEN]] \n\nThis is an interesting result, and is an improvement over Lin 2017 (where a similar bound is presented for monomial approximation).[[RWK-POS,RES-POS], [CMP-POS,EMP-POS], [APC], [MAJ]] \nOverall, I like the paper.[[OAL-POS], [null], [APC], [MAJ]]\n\nPros: new and interesting result, theoretically sound.[[EXP-POS,MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]] \nCons: nothing major.[[OAL-NEU], [EMP-NEU], [CRT], [MIN]]\nComments and clarifications:\n* What about the ability of a single neural network to approximate a class of functions (instead of a single p), where the topology is fixed but the network weights are allowed to vary?[[EXP-NEU, MET-NEU], [EMP-NEU], [QSN], [MIN]] Could you comment on this problem?[[EXP-NEU, MET-NEU], [EMP-NEU], [QSN], [MIN]]\n* Is the assumption that \\sigma has Taylor expansion to order d tight?[[EXP-NEU, MET-NEU], [EMP-NEU], [QSN], [MIN]] (That is, are there counter examples for relaxations of this assumption?)[[EXP-NEU, MET-NEU], [EMP-NEU], [QSN], [MIN]] \n* As noted, the assumptions of your theorems 4.1-4.3 do not apply to ReLUs,[[MET-NEG], [EMP-NEG], [CRT], [MIN]] but ReLUs network perform well in practice.[[RES-POS], [EMP-NEU], [DIS], [GEN]] Could you provide some further comments on this?\n\n"[[MET-NEU], [SUB-NEU], [QSN], [MIN]]