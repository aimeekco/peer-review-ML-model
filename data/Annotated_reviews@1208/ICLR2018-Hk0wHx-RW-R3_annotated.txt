"[====================================REVISION ======================================================]\nOk so the paper underwent major remodel, which significantly improved the clarity.[[EXT-NEU], [null], [CNT], [CNT]] I do agree now on Figure 5, which tips the scale for me to a weak accept. [[RWK-NEU,TNF-NEU], [null], [FBK], [GEN]]\n[====================================END OF REVISION ================================================]\n\nThis paper explores the problems of existing Deep variational bottle neck approaches for compact representation learning.[[RWK-NEU,PDI-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] Namely, the authors adjust deep variational bottle neck to conform to invariance properties (by making latent variable space to depend on copula only) - they name this model a  copula extension to dvib.[[RWK-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] They then go on to explore the sparsity of the latent space[[RWK-NEU], [null], [SMY], [GEN]]\n\nMy main issues with this paper are experiments: The proposed approach is tested only on 2 datasets (one synthetic, one real but tiny - 2K instances) and some of the plots (like Figure 5) are not convincing to me.[[PDI-NEG,DAT-NEG,EXP-NEG,OAL-NEG], [SUB-NEG,IMP-NEG,EMP-NEG], [DFT,CRT], [MIN]] On top of that, it is not clear how two methods compare computationally and how introduction of the copula  affects the convergence (if it does)[[EXP-NEG,MET-NEG,ANA-NEG,OAL-NEG], [CLA-NEG,CMP-NEG], [CRT], [MIN]]\n\nMinor comments\nPage 1: forcing an compact -> forcing a compact\n\u201cand and\u201d =>and\nSection 2: mention that I is mutual information, it is not obvious for everyone\n\nFigure 3: circles/triangles are too small, hard to see \nFigure 5: not really convincing.[[EXP-NEG,TNF-NEG,OAL-NEG], [IMP-NEG], [CRT], [MIN]] B does not appear much more structured than a, to me it looks like a simple transformation of a. [[RWK-NEG,EXP-NEG], [CMP-NEG], [CRT], [MIN]]\n"