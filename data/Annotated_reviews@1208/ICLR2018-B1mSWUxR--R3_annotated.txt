"The authors claim three contributions in this paper. (1) They introduce the framework of softmax Q-distribution estimation, through which they are able to interpret the role the payoff distribution plays in RAML.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] Specifically, the softmax Q-distribution serves as a smooth approximation to the Bayes decision boundary.[[MET-NEU], [null], [SMY], [GEN]] The RAML approximately estimates the softmax Q-distribution, and thus approximates the Bayes decision rule.[[MET-NEU], [null], [SMY], [GEN]] (2) Algorithmically, they further propose softmax Q-distribution maximum likelihood (SQDML) which improves RAML by achieving the exact Bayes decision boundary asymptotically.[[INT-NEU,EXP-NEU,MET-NEU], [null], [SMY], [GEN]] (3) Through one experiment using synthetic data on multi-class classi\ufb01cation and one using real data on image captioning, they show that SQDML is consistently as good or better than RAML on the task-speci\ufb01c metrics that is desired to optimize.[[DAT-NEU,EXP-NEU,MET-NEU], [null], [SMY], [GEN]] \n\nI found the first contribution is sound, and it reasonably explains why RAML achieves better performance when measured by a specific metric.[[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]] Given a reward function, one can define the Bayes decision rule.[[MET-NEU], [null], [DIS], [GEN]] The softmax Q-distribution (Eqn. 12) is defined to be the softmax approximation of the deterministic Bayes rule.[[MET-NEU], [null], [SMY], [GEN]] The authors show that the RAML can be explained by moving the expectation out of the nonlinear function and replacing it with empirical expectation (Eqn. 17).[[MET-NEU], [null], [SMY], [GEN]] Of course, the moving-out is biased but the replacing is unbiased.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] \n\nThe second contribution is partially valid,[[MET-NEU,RES-NEU], [EMP-NEU], [APC], [MAJ]] although I doubt how much improvement one can get from SQDML.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MIN]] The authors define the empirical Q-distribution by replacing the expectation in Eqn. 12 with empirical expectation (Eqn. 15).[[EXP-NEU,MET-NEU], [null], [SMY], [GEN]] In fact, this step can result in biased estimation because the replacement is inside the nonlinear function.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] When x is repeated sufficiently in the data, this bias is small and improvement can be observed, like in the synthetic data example.[[EXP-NEU,MET-NEU], [EMP-NEG], [SUG], [MIN]] However, when x is not repeated frequently, both RAML and SQDML are biased.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Experiment in section 4.1.2 do not validate significant improvement, either.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nThe numerical results are relatively weak.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] The synthetic experiment verifies the reward-maximizing property of RAML and SQDML.[[EXP-NEU], [null], [DIS], [MIN]] However, from Figure 2, we can see that the result is quite sensitive to the temperature \\tau.[[TNF-NEU,RES-NEU], [EMP-NEU], [DIS], [MIN]] Is there any guidelines to choose \\tau?[[MET-NEU,RES-NEU], [EMP-NEG], [QSN], [MAJ]] For experiments in Section 4.2, all of them are to show the effectiveness of RAML, which are not very relevant to this paper.[[EXP-NEG,OAL-NEG], [EMP-NEG], [CRT], [MAJ]] These experiment results show very small improvement compared to the ML baselines (see Table 2,3 and 5).[[EXP-NEG,TNF-NEG], [EMP-NEG], [CRT], [MAJ]]  These results are also lower than the state of the art performance.[[RWK-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] \n\nA few questions:\n(1). The author may want to check whether (8) can be called a Bayes decision rule.[[MET-NEU], [EMP-NEU], [QSN], [MIN]] This is a direct result from definition of conditional probability.[[MET-NEU], [EMP-NEU], [SMY], [MIN]] No Bayesian elements, like prior or likelihood appears here.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n(2). In the implementation of SQDML, one can sample from (15) without exactly computing the summation in the denominator.[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]] Compared with the n-gram replacement used in the paper, which one is better?[[MET-NEU], [CMP-NEU], [QSN], [MIN]]\n(3). The authors may want to write Eqn. 17 in the same conditional form of Eqn. 12 and Eqn. 14.[[MET-NEU], [CMP-NEU], [DIS], [MIN]] This will make the comparison much more clear.[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n(4). What is Theorem 2 trying to convey?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Although \\tau goes to 0, there is still a gap between Q and Q'.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] This seems to suggest that for small \\tau, Q' is not a good approximation of Q.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Are the assumptions in Theorem 2 reasonable?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] There are several typos in the proof of Theorem 2.[[MET-NEU], [CLA-NEU], [CRT], [MIN]]\n(5). In section 4.2.2, the authors write \"the rewards we directly optimized in training (token-level accuracy for NER and UAS for dependency parsing) are more stable w.r.t. \u03c4 than the evaluation metrics (F1 in NER), illustrating that in practice, choosing a training reward that correlates well with the evaluation metric is important\".[[EXP-NEU,MET-NEU], [null], [DIS], [MIN]] Could you explain it in more details?\n\n"[[EXP-NEU,MET-NEU], [PNF-NEU], [QSN], [MIN]]