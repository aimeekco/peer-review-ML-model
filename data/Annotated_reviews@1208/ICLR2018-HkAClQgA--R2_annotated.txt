"This is a very clearly written paper, and a pleasure to read.[[OAL-POS], [CLA-POS,IMP-POS], [APC], [MAJ]]\n\nIt combines some mechanisms known from previous work for summarization (intra-temporal attention; pointing mechanism with a switch) with novel architecture design components (intra-decoder attention), as well as a new training objective drawn from work from reinforcement learning, which directly optimizes ROUGE-L.[[RWK-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] The model is trained by a policy gradient algorithm.[[RWK-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] \n\nWhile the new mechanisms are simple variants of what is taken from existing work, the entire combination is well tested in the experiments.[[RWK-NEU,EXP-POS], [EMP-POS], [APC], [MAJ]] ROUGE results are reported for the full hybrid RL+ML model, as well as various versions that drop each of the new components (RL training; intra-attention).[[RWK-NEU,RES-NEU], [null], [SMY], [GEN]] The best method finally outperforms the lea-3d baseline for summarization.[[RWK-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] What makes this paper more compelling is that they compared against a recent extractive method (Durret et al., 2016), and the fact that they also performed human readability and relevance assessments to demonstrate that their ML+RL model doesn't merely over-optimize on ROUGE. [[RWK-NEU,EXP-NEU,MET-NEU,BIB-NEU], [null], [SMY], [GEN]]It was a nice result that only optimizing ROUGE directly leads to lower human evaluation scores, despite the fact that that model achieves the best ROUGE-1 and ROUGE-L performance on CNN/Daily Mail.[[EXP-POS,MET-POS,RES-POS,OAL-POS], [IMP-POS,EMP-POS], [APC], [MAJ]]\n\nSome minor points that I wonder about:\n - The heuristic against repeating trigrams seems quite crude. [[RWK-NEU,EXP-NEG], [IMP-NEG,EMP-NEG], [SMY,DFT,CRT], [MIN]]Is there a more sophisticated method that can avoid redundancy without this heuristic?[[RWK-NEU,MET-NEU], [null], [SMY,QSN], [GEN]]\n - What about a reward based on a general language model, rather than one that relies on L_{ml} in Equation (14)?[[RWK-NEU,MET-NEU], [null], [SMY,QSN], [GEN]] If the LM part really is to model grammaticality and coherence, a general LM might be suitable as well.[[EXP-POS,MET-POS,OAL-POS], [CLA-POS,IMP-POS,EMP-POS], [SMY,APC], [MAJ]]\n - Why does ROUGE-L seem to work better than ROUGE-1 or ROUGE-2 as the reward?[[RWK-NEU,MET-POS], [IMP-POS,EMP-POS], [APC], [MAJ]] Do you have any insights are speculations regarding this?[[RWK-NEU,MET-NEU], [EMP-NEU], [QSN], [GEN]]"