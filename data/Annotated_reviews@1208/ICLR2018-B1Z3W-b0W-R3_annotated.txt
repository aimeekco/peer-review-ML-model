"Instead of either optimization-based variational EM or an amortized inference scheme implemented via a neural network as in standard VAE models, this paper proposes a hybrid approach that essentially combines the two.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]]  In particular, the VAE inference step, i.e., estimation of q(z|x), is conducted via application of a recent learning-to-learn paradigm (Andrychowicz et al., 2016), whereby direct gradient ascent on the ELBO criteria with respect to moments of q(z|x) is replaced with a neural network that iteratively outputs new parameter estimates using these gradients.[[RWK-NEU,EXP-NEU], [null], [DIS], [MIN]]  The resulting iterative inference framework is applied to a couple of small datasets and shown to produce both faster convergence and a better likelihood estimate.[[EXP-POS,RES-POS], [EMP-POS], [APC], [MAJ]]\n\nAlthough probably difficult for someone to understand that is not already familiar with VAE models, I felt that this paper was nonetheless clear and well-presented, with a fair amount of useful background information and context.[[OAL-POS], [EMP-POS], [APC], [MAJ]]  From a novelty standpoint though, the paper is not especially strong given that it represents a fairly straightforward application of (Andrychowicz et al., 2016).[[RWK-NEG], [NOV-NEG,CMP-NEG], [CRT], [MAJ]]  Indeed the paper perhaps anticipates this perspective and preemptively offers that \"variational inference is a qualitatively different optimization problem\" than that considered in (Andrychowicz et al., 2016), and also that non-recurrent optimization models are being used for the inference task, unlike prior work.[[RWK-NEG], [NOV-NEG,CMP-NEG], [CRT], [MAJ]]  But to me, these are rather minor differentiating factors, since learning-to-learn is a quite general concept already, and the exact model structure is not the key novel ingredient.[[MET-NEG], [NOV-NEG,CMP-NEG], [CRT], [MAJ]]  That being said, the present use for variational inference nonetheless seems like a nice application, and the paper presents some useful insights such as Section 4.1 about approximating posterior gradients.[[MET-POS], [EMP-POS], [APC], [MAJ]]\n\nBeyond background and model development, the paper presents a few experiments comparing the proposed iterative inference scheme against both variational EM, and pure amortized inference as in the original, standard VAE. [[EXP-NEG,MET-NEU], [null], [DIS], [GEN]] While these results are enlightening,[[RES-POS], [EMP-POS], [CRT], [MAJ]] most of the conclusions are not entirely unexpected.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]]  For example, given that the model is directly trained with the iterative inference criteria in place, the reconstructions from Fig. 4 seem like exactly what we would anticipate, with the last iteration producing the best result.[[RES-POS], [EMP-POS], [CRT], [MAJ]]  It would certainly seem strange if this were not the case.[[RES-NEU], [null], [DIS], [MIN]]  And there is no demonstration of reconstruction quality relative to existing models, which could be helpful for evaluating relative performance.[[RWK-NEG], [SUB-NEG,CMP-NEG], [DFT,CRT], [MIN]]  Likewise for Fig. 6, where faster convergence over traditional first-order methods is demonstrated; but again, these results are entirely expected as this phenomena has already been well-documented in (Andrychowicz et al., 2016).[[RWK-NEU,RES-NEU], [null], [DIS], [MIN]]\n\nIn terms of Fig. 5(b) and Table 1, the proposed approach does produce significantly better values of the ELBO critera; however, is this really an apples-to-apples comparison? [[MET-POS], [EMP-POS], [APC,QSN], [MAJ]] For example, does the standard VAE have the same number of parameters/degrees-of-freedom as the iterative inference model, or might eq. (4) involve fewer parameters than eq. (5) since there are fewer inputs?  Overall, I wonder whether iterative inference is better than standard inference with eq. (4), or whether the recurrent structure from eq. (5) just happens to implicitly create a better neural network architecture for the few examples under consideration.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]  In other words, if one plays around with the standard inference architecture a bit, perhaps similar results could be obtained.[[MET-NEU,RES-NEU], [EMP-NEU], [DIS], [MIN]]\n\n\nOther minor comment:\n* In Fig. 5(a), it seems like the performance of the standard inference model is still improving[[TNF-POS,RES-POS], [EMP-POS], [APC], [MAJ]] but the iterative inference model has mostly saturated.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n* A downside of the iterative inference model not discussed in the paper is that it requires computing gradients of the objective even at test time, whereas the standard VAE model would not."[[MET-NEG], [SUB-NEG], [CRT], [MIN]]