"Quality\nThe theoretical results presented in the paper appear to be correct.[[RES-POS], [EMP-POS], [APC], [MAJ]] However, the experimental evaluation is globally limited,  hyperparameter tuning on test which is not fair.[[EXP-NEG], [SUB-NEG], [DFT,CRT], [MAJ]]\n\nClarity\nThe paper is mostly clear,[[OAL-POS], [CLA-POS], [APC], [MAJ]] even though some parts deserve more discussion/clarification (algorithm, experimental evaluation).[[EXP-NEG,MET-NEG,ANA-NEG], [SUB-NEG], [DFT], [MIN]]\n\nOriginality\nThe theoretical results are original, and the SGD approach is a priori original as well.[[MET-POS,RES-POS], [NOV-POS], [APC], [MAJ]]\n\nSignificance\nThe relaxed dual formulation and OT/Monge maps convergence results are interesting and can of of interest for researchers in the area,[[RES-POS,FWK-POS], [IMP-POS], [APC], [MAJ]] the other aspects of the paper are limited.[[OAL-NEG], [SUB-NEG], [DFT], [MAJ]]\n\nPros:\n-Theoretical results on the convergence of OT/Monge maps\n-Regularized formulation compatible with SGD[[RES-POS], [EMP-POS], [APC], [MAJ]]\nCons\n-Experimental evaluation limited[[EXP-NEG], [SUB-NEG], [DFT,CRT], [MAJ]]\n-The large scale aspect lacks of thorough analysis[[ANA-NEG], [SUB-NEG], [DFT,CRT], [MAJ]]\n-The paper presents 2 contributions but at then end of the day, the development of each of them appears limited[[MET-NEG], [SUB-NEG], [DFT,CRT], [MAJ]]\n\nComments:\n\n-The weak convergence results are interesting.[[RES-POS], [EMP-POS], [APC], [MAJ]] However, the fact that no convergence rate is given makes the result weak.[[MET-NEG,RES-NEG], [SUB-NEG], [DFT,CRT], [MAJ]] \nIn particular, it is possible that the number of examples needed for achieving a given approximation is at least exponential.[[RES-NEU], [SUB-NEU], [DIS], [MIN]]\nThis can be coherent with the problem of Domain Adaptation that can be NP-hard even under the co-variate shift assumption (Ben-David&Urner, ALT2012).[[RWK-NEU,MET-NEU], [SUB-NEG], [DFT,CRT], [MAJ]]\nThen, I think that the claim of page 6 saying that Domain Adaptation can be performed \"nearly optimally\" has then to be rephrased.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\nI think that results show that the approach is theoretically justified but optimality is not here yet.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nTheorem 1 is only valid for entropy-based regularizations, what is the difficulty for having a similar result with L2 regularization?[[MET-NEU,RES-NEU], [EMP-NEU], [QSN], [MIN]]\n\n-The experimental evaluation on the running time is limited to one particular problem.[[EXP-NEG], [SUB-NEG], [DFT], [MIN]] If this subject is important, it would have been interesting to compare the approaches on other large scale problems and possibly with other implementations.[[MET-NEU], [EMP-NEU], [DIS], [MIn]]\nIt is also surprising that the efficiency the L2-regularized version is not evaluated.[[MET-NEG], [EMP-NEG], [DFT], [MIN]]\nFor a paper interesting in large scale aspects, the experimental evaluation is rather weak.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]]\n \nThe 2 methods compared in Fig 2 reach the same objective values at convergence, but is there any particular difference in the solutions found?[[MET-NEU,RES-NEU], [EMP-NEU], [QSN], [MIN]]\n\n-Algorithm 1 is presented without any discussion about complexity, rate of convergence.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Could the authors discuss this aspect?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\nThe presentation of this algo is a bit short and could deserve more space (in the supplementary)[[MET-NEG], [SUB-NEG], [DFT], [MIN]]\n\n-For the DA application, the considered datasets are classic but not really \"large scale\", anyway this is a minor remark.[[DAT-NEG], [SUB-NEG], [CRT], [MIN]]\nThe setup is not completely clear,[[MET-NEG], [EMP-NEG], [CRT], [MIN]] since the approach is interesting for out of sample data,[[DAT-NEU,MET-POS], [EMP-POS], [APC], [MAJ]] so I would expect the map to be computed on a small sample of source data, and then all source instances to be projected on target with the learned map.[[DAT-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]] This point is not very clear and we do not know how many source instances are used to compute the mapping - the mapping is incomplete on this point[[MET-NEG], [SUB-NEG], [DFT,CRT], [MIN]] while this is an interesting aspect of the paper: this justifies even more the large scale aspect is the algo need less examples during learning to perform similar or even better classification.[[DAT-POS,MET-POS], [EMP-POS], [APC], [MAJ]]\nHyperparameter tuning is another aspect that is not sufficiently precise in the experimental setup: it seems that the parameters are tuned on test (for all methods), which is not fair since target label information will not be available from a practical standpoint.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nThe authors claim that they did not want to compete with state of the art DA, but the approach of Perrot et al., 2016 seems to a have a similar objective and could be used as a baseline.[[RWK-NEU,EXP-NEU,MET-NEU], [null], [DIS], [GEN]]\n\nExperiments on generative optimal transport are interesting and probably generate more discussion/perspectives[[EXP-POS], [EMP-POS], [APC], [MAJ]].\n\n--\nAfter rebuttal\n--\nAuthors have answered to many of my comments, I think this is an interesting paper, I increase my score.\n"[[OAL-POS], [EMP-POS], [APC], [MAJ]]