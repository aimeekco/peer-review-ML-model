"The paper presents the word embedding technique which consists of: (a) construction of a positive (i.e. with truncated negative values) pointwise mutual information order-3 tensor for triples of words in a sentence[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] and (b) symmetric tensor CP factorization of this tensor.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The authors propose the CP-S (stands for symmetric CP decomposition) approach which tackles such factorization in a \"batch\" manner by considering small random subsets of the original tensor.[[MET-NEU], [null], [SMY], [GEN]] They also consider the JCP-S approach, where the ALS (alternating least squares) objective is represented as the joint objective of the matrix and order-3 tensor ALS objectives.[[MET-NEU], [null], [SMY], [GEN]] The approach is evaluated experimentally on several tasks such as outlier detection, supervised analogy recovery, and sentiment analysis tasks.[[EXP-NEU,MET-NEU,ANA-NEU], [null], [SMY], [GEN]]\n\nCLARITY: The paper is very well written and is easy to follow.[[OAL-POS], [CLA-POS], [APC], [MAJ]] However, some implementation details are missing, which makes it difficult to assess the quality of the experimental results.[[EXP-NEU,RES-NEU,ANA-NEG], [SUB-NEG,EMP-NEG], [DFT,CRT], [MAJ]]\n\nQUALITY: I understand that the main emphasis of this work is on developing faster computational algorithms, which would handle large scale problems, for factorizing this tensor.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] However, I have several concerns about the algorithms proposed in this paper:[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n\n  - First of all, I do not see why using small random subsets of the original tensor would give a desirable factorization.[[DAT-NEG], [EMP-NEG], [CRT], [MIN]] Indeed, a CP decomposition of a tensor can not be reconstructed from CP decompositions of its subtensors.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Note that there is a difference between batch methods in stochastic optimization where batches are composed of a subset of observations (which then leads to an approximation of desirable quantities, e.g. the gradient, in expectation) and the current approach where subtensors are considered as batches.[[MET-NEU], [null], [DIS], [GEN]] I would expect some further elaboration of this question in the paper.[[MET-NEU,ANA-NEU], [SUB-NEU], [DIS], [MIN]] Although similar methods appeared in the tensor literature before, I don't see any theoretical ground for their correctness.\n\n  - Second, there is a significant difference between the symmetric CP tensor decomposition and the non-negative symmetric CP tensor decomposition.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] In particular, the latter problem is well posed and has good properties (see, e.g., Lim, Comon. Nonengative approximations of nonnegative tensors (2009)).[[RWK-POS,PDI-POS], [CMP-POS,EMP-POS], [APC], [MAJ]] However, this is not the case for the former (see, e.g., Comon et al., 2008 as cited in this paper).[[RWK-NEU], [CMP-NEU], [DIS], [GEN]] Therefore, (a) computing the symmetric and not non-negative symmetric decomposition does not give any good theoretical guarantees (while achieving such guarantees seems to be one of the motivations of this paper) [[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]]and (b) although the tensor is non-negative, its symmetric factorization is not guaranteed to be non-negative and further elaboration of this issue seem to be important to me.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\n  - Third, the authors claim that one of their goals is an experimental exploration of tensor factorization approaches with provable guarantees applied to the word embedding problem.[[MET-NEU], [null], [SMY], [GEN]] This is an important question that has not been addressed in the literature and is clearly a pro of the paper.[[ANA-NEG], [SUB-NEG], [DFT], [MAJ]] However, it seems to me that this goal is not fully implemented.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Indeed, (a) I mentioned in the previous paragraph the issues with the symmetric CP decomposition and (b) although the paper is motivated by the recent algorithm proposed by Sharan&Valiant (2017), the algorithms proposed in this paper are not based on this or other known algorithms with theoretical guarantees.[[RWK-NEG,MET-NEG], [CMP-NEG], [CRT], [MAJ]] This is therefore confusing and I would be interested in the author's point of view to this issue.[[ANA-NEG], [EMP-NEG], [CRT], [MIN]]\n\n  - Further, the proposed joint approach, where the second and third order information are combined requires further analysis.[[ANA-NEU], [SUB-NEU], [DFT], [MIN]] Indeed, in the current formulation the objective is completely dominated by the order-3 tensor factor, because it contributes O(d^3) terms to the objective vs O(d^2) terms contributed by the matrix part.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] It would be interesting to see further elaboration of the pros and cons of such problem formulation.[[PDI-NEU], [null], [SUG], [MIN]]\n\n  - Minor comment. In the shifted PMI section, the authors mention the parameter alpha and set specific values of this parameter based on experiments.[[MET-NEU], [null], [DIS], [GEN]] However, I don't think that enough information is provided, because, given the author's approach, the value of this parameter most probably depends on other parameters, such as the bach size.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MIN]]\n\n  - Finally, although the empirical evaluation is quite extensive and outperforms the state-of the art, I think it would be important to compare the proposed algorithm to other tensor factorization approaches mentioned above.[[RWK-POS,EXP-POS,MET-POS], [CMP-POS,EMP-POS], [APC], [MAJ]] \n\nORIGINALITY: The idea of using a pointwise mutual information tensor for word embeddings is not new, but the authors fairly cite all the relevant literature.[[PDI-NEG,MET-NEG], [NOV-NEG], [CRT], [MAJ]] My understanding is that the main novelty is the proposed tensor factorization algorithm and extensive experimental evaluation.[[MET-POS], [NOV-POS], [APC], [MAJ]] However, such batch approaches for tensor factorization are not new and I am quite skeptical about their correctness (see above).[[MET-NEU], [EMP-NEU], [DIS], [MIN]] The experimental evaluation presents indeed interesting results.[[EXP-POS,RES-POS], [EMP-POS], [APC], [MAJ]] However, I think it would also be important to compare to other tensor factorization approaches.[[RWK-NEU,MET-NEU], [CMP-NEU], [SUG], [MIN]] I would also be quite interested to see the performance of the proposed algorithm for different values of parameters (such as the butch size).[[DAT-NEU,MET-NEU], [EMP-NEU], [SUG], [MIN]]\n\nSIGNIFICANCE: I think the paper addresses very interesting problem and significant amount of work is done towards the evaluation, but there are some further important questions that should be answered before the paper can be published.[[PDI-POS,RES-POS,OAL-POS], [EMP-POS], [APC], [MAJ]]  To summarize, the following are the pros of the paper:\n\n  - clarity and good presentation;[[OAL-POS], [CLA-POS,PNF-POS], [APC], [MAJ]]\n  - good overview of the related literature;[[RWK-POS], [null], [APC], [MAJ]]\n  - extensive experimental comparison and good experimental results.[[EXP-POS,RES-POS], [EMP-POS], [APC], [MAJ]]\n\nWhile the following are the cons:\n\n  - the mentioned issues with the proposed algorithm, which in particular does not have any theoretical guarantees;[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n  - lack of details on how experimental results were obtained, in particular, lack of the details on the values of the free parameters in the proposed algorithm;[[MET-NEG,RES-NEG,ANA-NEG], [SUB-NEG], [DFT], [GEN]]\n  - lack of comparison to other tensor approaches to the word embedding problem (i.e. other algorithms for the tensor decomposition subproblem);[[MET-NEG], [SUB-NEG,CMP-NEG], [CRT], [MAJ]]\n  - the novelty of the approach is somewhat limited,[[MET-NEG], [NOV-NEG], [CRT], [MAJ]] although the idea of the extensive experimental comparison is good.\n\n\n"[[PDI-POS,EXP-POS], [EMP-POS], [APC], [MAJ]]