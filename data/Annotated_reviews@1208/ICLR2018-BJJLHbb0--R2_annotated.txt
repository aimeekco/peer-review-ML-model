"The paper presents a new technique for anomaly detection where the dimension reduction and the density estimation steps are jointly optimized.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The paper is rigorous and ideas are clearly stated.[[PDI-POS], [EMP-POS], [APC], [MAJ]] The idea to constraint the dimension reduction to fit a certain model, here a GMM, is relevant, and the paper provides a thorough comparison with recent state-of-the-art methods.[[RWK-POS,PDI-POS,MET-POS], [CMP-POS,EMP-POS], [APC], [MAJ]] My main concern is that the method is called unsupervised, but it uses the class information in the training, and also evaluation.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] I'm also not convinced of how well the Gaussian model fits the low-dimensional representation and how well can a neural network compute the GMM mixture memberships.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\n1. The framework uses the class information, i.e., \u201conly data samples from the normal class are used for training\u201d, but it is still considered unsupervised.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MIN]] Also, the anomaly detection in the evaluation step is based on a threshold which depends on the percentage of known anomalies, i.e., a priori information.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] I would like to see a plot of the sample energy as a function of the number of data points.[[DAT-NEU,RES-NEU], [EMP-NEU], [SUG], [MIN]] Is there an elbow that indicates the threshold cut?[[DAT-NEU], [EMP-NEU], [QSN], [MIN]] Better yet it would be to use methods like Local Outlier Factor (LOF) (Breunig et al., 2000 \u2013 LOF:Identifying Density-based local outliers) to detect the outliers (these methods also have parameters to tune, sure, but using the known percentage of anomalies to find the threshold is not relevant in a purely unsupervised context when we don't know how many anomalies are in the data).[[RWK-NEU,MET-NEU], [CMP-NEU,EMP-NEU], [SUG], [MIN]]\n2. Is there a theoretical justification for computing the mixture memberships for the GMM using a neural network?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] \n3. How do the regularization parameters \\lambda_1 and \\lambda_2 influence the results?[[RES-NEU], [EMP-NEU], [QSN], [MIN]]\n4. The idea to jointly optimize the dimension reduction and the clustering steps was used before neural nets (e.g., Yang et al., 2014 -  Unsupervised dimensionality reduction for Gaussian mixture model).[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Those approaches should at least be discussed in the related work, if not compared against.[[RWK-NEG,MET-NEG], [SUB-NEG,CMP-NEG], [SUG,DFT], [MIN]]\n5. The authors state that estimating the mixture memberships with a neural network for GMM in the estimation network instead of the standard EM algorithm works better.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Could you provide a comparison with EM?[[MET-NEU], [CMP-NEU], [QSN], [MIN]]\n6. In the newly constructed space that consists of both the extracted features and the representation error, is a Gaussian model truly relevant? [[MET-NEU], [EMP-NEU], [QSN], [MIN]]Does it well describe the new space?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Do you normalize the features (the output of the dimension reduction and the representation error are quite different)?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Fig. 3a doesn't seem to show that the output is a clear mixture of Gaussians.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]]\n7. The setup of the KDDCup seems a little bit weird, where the normal samples and anomalies are reversed (because of percentage), where the model is trained only on anomalies, and it detects normal samples as anomalies[[MET-NEG], [PNF-NEG], [CRT], [MIN]] ... I'm not convinced that it is the best example, especially that is it the one having significantly better results, i.e. scores ~ 0.9 vs. scores ~0.4/0.5 score for the other datasets.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]]\n8. The authors mention that \u201cwe can clearly see from Fig. 3a that DAGMM is able to well separate[[TNF-NEU], [null], [DIS], [MIN]] ...\u201d - it is not clear to me, it does look better than the other ones, but not clear.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]]  If there is a clear separation from a different view, show that one instead.[[TNF-NEU], [PNF-NEU], [DIS], [MIN]]  We don't need the same view for all methods.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]  \n9. In the experiments the reduced dimension used is equal to 1 for two of the experiments and 2 for one of them.  This seems very drastic![[EXP-NEU], [EMP-NEU], [DIS], [MIN]]\n\nMinor comments:\n\n1. Fig.1: what dimension reduction did you use? Add axis labels.\n2.[[TNF-NEU], [EMP-NEU], [QSN], [MIN]] \u201cDAGMM preserves the key information of an input sample\u201d - what does key information mean?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n3. In Fig. 3 when plotting the results for KDDCup, I would have liked to see results for the best 4 methods from Table 1, OC-SVM performs better than PAE.[[RES-NEU,TNF-NEU], [SUB-NEU], [DIS], [MIN]] Also DSEBM-e and DSEBM-r seems to perform very well when looking at the three measures combined.[[RES-POS], [EMP-POS], [APC], [MAJ]] They are the best in terms of precision.[[RES-POS], [EMP-POS], [APC], [MAJ]]\n4. Is the error in Table 2 averaged over multiple runs? If yes, how many?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nQuality \u2013 The paper is thoroughly written, and the ideas are clearly presented.[[PDI-POS,OAL-POS], [CLA-POS,PNF-POS], [APC], [MAJ]] It can be further improved as mentioned in the comments.[[OAL-NEU], [PNF-NEU], [DIS], [MIN]]\n\nClarity \u2013 The paper is very well written with clear statements, a pleasure to read.[[OAL-POS], [CLA-POS,PNF-POS], [APC], [MAJ]] \n\nOriginality \u2013 Fairly original, but it still needs some work to justify it better.[[OAL-NEG], [NOV-NEG], [SUG], [MIN]] \n\nSignificance \u2013 Constraining the dimension reduction to fit a certain model is a relevant topic, but I'm not convinced of how well the Gaussian model fits the low-dimensional representation and how well can a neural network compute the GMM mixture memberships. \n"[[MET-NEG], [EMP-NEG], [CRT], [MIN]]