"After reading the revision:\n\nThe authors addressed my detailed questions on experiments.[[EXP-NEU], [null], [DIS], [GEN]] It appears sometimes the entropy loss (which is not the main contribution of the paper) is essential to improve performance; this obscures the main contribution.[[MET-NEU], [SUB-NEU], [DFT], [GEN]]\n\nOn the other hand, the theoretical part of the paper is not really improved in my opinion,[[OAL-POS], [CNT], [APC], [MAJ]] I still can not see how previous work by Balestriero and Baraniuk 2017 motivates and backups the proposed method.[[RWK-NEG,MET-NEG], [SUB-NEG,CMP-NEG], [DFT,CRT], [MIN]]\n\nMy rating of this paper would remain the same.[[OAL-NEU], [REC-NEU], [FBK], [MAJ]] \n\n============================================================================================\n\nThis paper propose to use the reconstruction loss, defined in a somewhat unusual way, as a regularizar for semi-supervised learning.[[INT-NEU], [null], [SMY], [GEN]]\n\nPros:\n\nThe intuition is that the ReLU network output is locally linear for each input, and one can use the conjugate mapping (which is also linear) for reconstructing the inputs, as in PCA.[[MET-POS], [EMP-POS], [APC], [MAJ]] Realizing that the linear mapping is the derivative of network output w.r.t. the input (the Jacobian), the authors proposed to use the reconstruction loss defined in (8).[[MET-NEU], [null], [SMY], [GEN]] Different from typical auto-encoders, this work does not require another reconstruction network, but instead uses the \"derivative\".[[MET-NEU], [null], [DIS], [GEN]]  This observation is neat in my opinion, and does suggest a different use of the Jacobian in deep learning.[[MET-POS], [EMP-POS], [APC], [MAJ]]  The related work include auto-encoders where the weights of symmetric layers are tied.[[MET-NEU], [null], [DIS], [GEN]] \n\nCons:\n\nThe motivation (Section 2) needs to be improved.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] In particular, the introduction/review of the work of Balestriero and Baraniuk 2017 not very useful to the readers.[[RWK-NEG], [CMP-NEG], [CRT], [MIN]]  Notations in eqns (2) and (3) are not fully explained (e.g., boldface c).[[MET-NEG], [SUB-NEG], [DFT,CRT], [MIN]] Intuition and implications of Theorem 1 is not sufficiently discussed: what do you mean by optimal DNN, what is the criteria for optimality?[[MET-NEG], [EMP-NEG], [DFT,QSN], [MIN]] is there a generative assumption of the data underlying the theorem? and the assumption of all samples being norm 1 seems too strong and perhaps limits its application?[[MET-NEG], [EMP-NEG], [QSN], [MIN]] As far as I see, section 2 is somewhat detached from the rest of the paper.[[MET-NEG], [SUB-NEG], [DFT], [MIN]]\n\nThe main contribution of this paper is supposed to be the reconstruction mapping (6) and its effect in semi-supervised learning.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] The introduction of entropy regularization in sec 2.3 seems somewhat odd and obscures the contribution.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] It also bears the questions that how important is the entropy regularization vs. the reconstruction loss.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] In experiments, results with beta=1.0 need to be presented to assess the importance of network inversion and the reconstruction loss.[[MET-NEU,RES-NEU], [EMP-NEU], [CRT], [MIN]] Also, a comparison against typical auto-encoders (which uses another decoder networks, with weights possibly tied with the encoder networks) is missing.\n\n"[[EXP-NEG], [SUB-NEG], [DFT], [MIN]]