"This paper explores a new approach to optimal transport.[[INT-NEU], [null], [SMY], [GEN]] Contributions include a new dual-based algorithm for the fundamental task of computing an optimal transport coupling, the ability to deal with continuous distributions tractably by using a neural net to parameterize the functions which occur in the dual formulation, learning a Monge map parameterized by a neural net allowing extremely tractable mapping of samples from one distribution to another, and a plethora of supporting theoretical results.[[INT-NEU,MET-NEU,RES-NEU], [null], [SMY], [GEN]]  The paper presents significant, novel work in a straightforward, clear and engaging way.[[OAL-POS], [NOV-POS], [APC], [MAJ]]  It represents an elegant combination of ideas, and a well-rounded combination of theory and experiments.[[PDI-POS,EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]] \n\nI should mention that I'm not sufficiently familiar with the optimal transport literature to verify the detailed claims about where the proposed dual-based algorithm stands in relation to existing algorithms.[[EXT-NEU], [null], [DIS], [GEN]] \n\nMajor comments:\n\nNo major flaws.[[OAL-POS], [EMP-POS], [APC], [MAJ]]  The introduction is particular well written, as an extremely clear and succinct introduction to optimal transport.[[INT-POS], [CLA-POS,EMP-POS], [APC], [MAJ]] \n\nMinor comments:\n\nIn the introduction, for VAEs, it's not the case that f(X) matches the target distribution.[[INT-NEG,MET-NEG], [EMP-NEG], [CRT], [MIN]]  There are two levels of sampling: of the latent X and of the observed value given the latent.[[MET-NEU], [null], [DIS], [GEN]]  The second step of sampling is ignored in the description of VAEs in the first paragraph.[[INT-NEG,MET-NEG], [null], [CRT], [MIN]] \n\nIn the comparison to previous work, please explicitly mention the EMD algorithm, since it's used in the experiments.[[RWK-NEU,EXP-NEG,MET-NEG], [CMP-NEG], [DFT], [MIN]] \n\nIt would've been nice to see an experimental comparison to the algorithm proposed by Arjovsky et al. (2017), since this is mentioned favorably in the introduction.[[INT-NEU,MET-NEU], [SUB-NEU], [SUG], [MAJ]] \n\nIn (3), R is not defined.[[MET-NEG], [EMP-NEG], [DFT], [MIN]]  Suggest adding a forward reference to (5).[[RWK-NEU], [null], [SUG], [MIN]] \n\nIn section 3.1, it would be helpful to cite a reference to support the form of dual problem.[[MET-NEU], [SUB-NEU], [SUG], [MIN]] \n\nPerhaps the authors have just done a good job of laying the groundwork, but the dual-based approach proposed in section 3.1 seems quite natural.[[RWK-POS,RES-POS], [CMP-POS], [APC], [MAJ]]  Is there any reason this sort of approach wasn't used previously, even though this vein of thinking was being explored for example in the semi-dual algorithm?[[MET-NEU], [EMP-NEU], [QSN], [MAJ]]  If so, it would interesting to highlight the key obstacles that a naive dual-based approach would encounter and how these are overcome.\n\nIn algorithm 1, it is confusing to use u to mean both the parameters of the neural net and the function represented by the neural net.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] \n\nThere are many terms in R_e in (5) which appear to have no effect on optimization, such as a(x) and b(y) in the denominator and \"- 1\". It seems like R_e boils down to just the entropy.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n\nThe definition of F_\\epsilon is made unnecessarily confusing by the omission of x and y as arguments.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n\nIt would be great to mention very briefly any helpful intuition as to why F_\\epsilon and H_\\epsilon have the forms they do.[[MET-NEU], [EMP-NEU], [SUG], [MIN]]\n\nIn the discussion of Table 1, it would be helpful to spell out the differences between the different Bary proj algorithms, since I would've expected EMD, Sinkhorn and Alg. 1 with R_e to all perform similarly.[[TNF-NEU,MET-NEU], [EMP-NEU], [SUG], [MIN]]\n\nIn Figure 4 some of the samples are quite non-physical.[[TNF-NEG], [EMP-NEG], [CRT], [MIN]] Is their any helpful intuition about what goes wrong?[[MET-NEG], [EMP-NEG], [QSN], [MIN]]\n\nWhat cost is used for generative modeling on MNIST?[[DAT-NEU,ANA-NEU], [SUB-NEU], [QSN], [MIN]]\n\nFor generative modeling on MNIST, \"784d vector\" is less clear than \"784-dimensional vector\".[[MET-NEU], [EMP-NEU], [DIS], [MIN]] The fact that the variable d is equal to 768 is not explicitly stated.[[MET-NEU], [null], [DIS], [GEN]]\n\nIt seems a bit strange to say \"The property we gain compared to other generative models is that our generator is a nearly optimal map w.r.t. this cost\" as if this was an advantage of the proposed method, since arguably there isn't a really natural cost in the generative modeling case (unlike in the domain adaptation case); the latent variable seems kind of conceptually distinct from observation space.[[MET-POS,ANA-POS], [EMP-POS], [APC], [MAJ]]\n\nAppendix A isn't referred to from the main text as far as I could tell.[[CNT], [null], [DIS], [GEN]] Just merge it into the main text?\n\n\n\n"[[MET-NEU], [null], [QSN], [MIN]]