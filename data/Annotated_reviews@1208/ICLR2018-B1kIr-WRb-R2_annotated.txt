"The paper proposes to extend the usual PPMI matrix factorization (Levy and Goldberg, 2014) to a (3rd-order) PPMI tensor factorization. The paper chooses symmetric CP decomposition so that word representations are tied across all three views.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The MSE objective (optionally interpolated with a 2nd-order tensor) is optimized incrementally by SGD.[[PDI-NEU], [null], [SMY], [GEN]] \n\nThe paper's most clear contribution is the observation that the objective results in multiplicative compositionality of vectors, which indeed does not seem to hold in CBOW.[[RES-NEU], [null], [SMY], [GEN]] \n\nWhile the paper reports superior performance, the empirical claims are not well substantiated.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MIN]] It is *not* true that given CBOW, it's not important to compare with SGNS and GloVe.[[MET-NEG], [CMP-NEG], [CRT], [MIN]] In fact, in certain cases such as unsupervised word analogy, SGNS is clearly and vastly superior to other techniques (Stratos et al., 2015).[[RWK-NEG,MET-NEG], [CMP-NEG], [CRT], [MIN]] The word similarity scores are also generally low: it's easy to achieve >0.76 on MEN using the plain PPMI matrix factorization on Wikipedia.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] So it's hard to tell if it's real improvement.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] \n\nQuality: Borderline.[[OAL-NEU], [null], [DIS], [MIN]] The proposed approach is simple and has an appealing compositional feature,[[MET-POS], [EMP-POS], [APC], [MAJ]] but the work is not adequately validated and the novelty is somewhat limited.[[OAL-NEG], [NOV-NEG], [CRT], [MIN]] \n\nClarity: Clear.[[OAL-POS], [CLA-POS], [APC], [MAJ]]\n\nOriginality: Low-rank tensors have been used to derive features in many prior works in NLP (e.g., Lei et al., 2014).[[RWK-NEU,MET-NEU], [NOV-NEG], [CRT], [MAJ]] The paper's particular application to learning word embeddings (PPMI factorization), however, is new although perhaps not particularly original.[[MET-NEG], [NOV-NEG], [APC], [MAJ]] The observation on multiplicative compositionality is the main strength of the paper.[[RES-POS], [EMP-POS], [APC], [MAJ]]\n\nSignificance: Moderate.[[OAL-NEU], [IMP-NEU], [DIS], [MAJ]] For those interested in word embeddings, this work suggests an alternative training technique, but it has some issues (described above).  "[[MET-NEG], [EMP-NEG], [CRT], [MIN]]