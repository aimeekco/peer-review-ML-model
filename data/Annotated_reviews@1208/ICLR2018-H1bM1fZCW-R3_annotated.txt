"The paper addresses an important problem in multitask learning.[[INT-POS], [null], [SMY], [GEN]] But its current form has several serious issues.[[OAL-NEG], [null], [CRT], [MAJ]] \n\nAlthough I get the high-level goal of the paper, I find Sec. 3.1, which describes the technical approach, nearly incomprehensible.[[PDI-POS,MET-NEG], [EMP-NEG], [DFT], [MAJ]] There are many things unclear. For example:\n\n-  it starts with talking about multiple tasks, and then immediately talks about a \"filter F\", without defining what the kind of network is being addressed.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] \n\n- Also it is not clear what L_grad is. It looks like a loss, but Equation 2 seems to define it to be the difference between the gradient norm of a task and the average over all tasks.[[MET-NEG], [CLA-NEG,EMP-NEG], [CRT], [MAJ]] It is not clear how it is used. In particular, it is not clear how it is used to \"update the task weights\[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]"\n\n- Equation 2 seems sloppy. \u201cj\u201d appears as a free index on the right side, but it doesn\u2019t appear on the left side.[[MET-NEG], [CLA-NEG], [DFT], [MIN]] \n\nAs a result, I am unable to understand how the method works exactly, and unable to judge its quality and originality.[[MET-NEU], [NOV-NEU], [CRT], [MAJ]]\n\nThe toy experiment is not convincing.[[EXP-NEG], [null], [CRT], [MAJ]] \n\n- the evaluation metric is the sum of the relative losses, that is, the sum of the original losses weighted by the inverse of the initial loss of each task.[[DAT-NEU,MET-NEU], [null], [SMY], [GEN]] This is different from the sum of the original losses, which seems to be the one used to train the \u201cequal weight\u201d baseline.[[RWK-NEU], [CMP-NEG], [DIS], [MIN]] A more fair baseline is to directly use the evaluation metric as the training loss. \n- the curves seem to have not converged.[[RWK-NEU], [CMP-NEU], [SUG], [GEN]]\n\nThe experiments on NYUv2 involves non-standard settings, without a good justification.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]] So it is not clear if the proposed method can make a real difference on state of the art systems. [[MET-NEU], [EMP-NEU], [DFT], [MAJ]]\n\nAnd the reason that the proposed method outperforms the equal weight baseline seems to be that the method prevents overfitting on some tasks (e.g. depth)[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]. However, the method works by normalizing the norms of the gradients, which does not necessarily prevent overfitting \u2014 it can in fact magnify gradients of certain tasks and cause over-training and over-fitting.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] So the performance gain is likely dataset dependent, and what happens on NYU depth can be a fluke and does not necessarily generalize to other datasets. "[[DAT-NEU], [EMP-NEU], [QSN], [MAJ]]