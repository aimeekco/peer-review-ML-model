"The paper proposed a new activation function that tries to alleviate the use of  other form of normalization methods for RNNs.[[INT-NEU], [null], [SMY], [GEN]] The activation function keeps the activation roughly zero-centered.[[MET-NEU], [null], [SMY], [GEN]] \n\nIn general, this is an interesting direction to explore, the idea is interesting,;[[PDI-POS], [IMP-POS], [APC], [MAJ]] however, I would like to see more experiments.[[EXP-NEU], [SUB-NEU], [SUG], [MAJ]]\n\n1. The authors tested out this new activation function on RNNs.[[EXP-NEU], [null], [SMY], [GEN]] It would be interesting to see the results of the new activation function on LSTM.[[EXP-NEU,RES-NEU], [null], [SUG], [MAJ]]\n\n2. The experimental results are fairly weak compared to the other methods that also uses many layers.[[RWK-NEU,RES-NEG], [CMP-NEU], [CRT], [MAJ]] For PTB and Text8, the results are comparable to recurrent batchnorm with similar number of parameters, however the recurrent batchnorm model has only 1 layer, whereas the proposed architecture has 36 layers.[[MET-NEU,RES-NEU], [EMP-NEU], [DIS], [MAJ]]  \n\n3.  It would also be nice to show results on tasks that involve long term dependencies, such as speech modeling.[[MET-NEU], [EMP-NEU], [SUG], [MAJ]]\n\n4. If the authors could test out the new activation function on LSTMs, it would be interesting to perform a comparison between LSTM baseline, LSTM + new activation function, LSTM + recurrent batch norm.[[RWK-NEU,MET-NEU], [CMP-NEU], [SUG], [MAJ]]\n\n5. It would be nice to see the gradient flow with the new activation function compared to the ones without.[[MET-NEU], [CMP-NEU], [SUG], [MAJ]]\n\n6. The theorems and proofs are rather preliminary, they may not necessarily have to be presented as theorems."[[MET-NEU], [EMP-NEU], [SUG], [MAJ]]