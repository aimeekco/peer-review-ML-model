"[Main comments]\n\n* I would advice the authors to explain in more details in the intro\nwhat's new compared to Li & Malik (2016) and Andrychowicz et al.[[INT-NEU,RWK-NEU,BIB-NEU], [CMP-NEU], [SUG], [MAJ]] (2016).\nIt took me until section 3.5 to figure it out.[[OAL-NEU], [null], [DIS], [GEN]]\n\n* If I understand correctly, the only new part compared to Li & Malik (2016) is\nsection 3.5, where block-diagonal structure is imposed on the learned matrices.[[RWK-NEU,MET-NEU], [NOV-NEU,CMP-NEU], [DIS], [MAJ]]\nIs that correct?[[MET-NEU], [null], [QSN], [MIN]]\n\n* In the experiments, why not comparing with Li & Malik (2016)? (i.e., without\n  block-diagonal structure)[[RES-NEU], [CMP-NEU], [QSN], [MIN]]\n\n* Please clarify whether the objective value shown in the plots is wrt the training\n  set or the test set.[[MET-NEU], [EMP-NEU], [SUG], [MAJ]] Reporting the training objective value makes little\nsense to me, unless the time taken to train on MNIST is taken into account in\nthe comparison.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] \n\n* Please clarify what are the hyper-parameters of your meta-training algorithm\n  and how you chose them.[[MET-NEU], [EMP-NEU], [SUG], [MAJ]]\n\nI will adjust my score based on the answer to these questions.[[EXT-NEU], [null], [DIS], [MAJ]]\n\n[Other comments]\n\n* \"Given this state of affairs, perhaps it is time for us to start practicing\n  what we preach and learn how to learn\"\n\nThis is in my opinion too casual for a scientific publication...[[MET-NEU], [emp-NEG], [CRT], [MAJ]]\n\n* \"aim to learn what parameter values of the base-level learner are useful\n  across a family of related tasks\"\n\nIf this is essentially multi-task learning, why not calling it so?  \"Learning\nwhat to learn\" does not mean anything.[[MET-NEU], [CLA-NEG], [CRT], [MAJ]]  I understand that the authors wanted to\nhave \"what\", \"which\" and \"how\" sections but this is not clear at all.[[OAL-NEU], [CLA-NEG], [CRT], [MAJ]]\n\nWhat is a \"base-level learner\"? I think it would be useful to define it more\nprecisely early on.[[MET-NEU], [EMP-NEU], [SUG,QSN], [MAJ]]\n\n* I don't see the difference between what is described in Section 2.2\n  (\"learning which model to learn\") and usual machine learning (searching for\nthe best hypothesis in a hypothesis class).[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\n* Typo: p captures the how -> p captures how[[MET-NEG], [CLA-NEG], [CRT], [MIN]]\n\n* The L-BFGS results reported in all Figures looked suspicious to me.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]]  How do you\n  explain that it converges to a an objective value that is so much worse?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\nMoreover, the fact that there are huge oscillations makes me think that the\nauthors are measuring the function value during the line search rather than\nthat at the end of each iteration.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]]\n"