"The problem considered in the paper is of compressing large networks (GRUs) for faster inference at test time.[[PDI-NEU], [null], [SMY], [GEN]] \n\nThe proposed algorithm uses a two step approach: 1)  use trace norm regularization (expressed in variational form) on dense parameter matrices at training time without constraining the number of parameters,[[MET-NEU], [null], [SMY], [GEN]] b) initializing from the SVD of parameters trained in stage 1, learn a new network with reduced number of parameters.[[MET-NEU], [null], [SMY], [GEN]]\n\nThe experiments on WSJ dataset are promising towards achieving a trade-off between number of parameters and accuracy.[[DAT-POS,EXP-POS,RES-POS], [EMP-POS], [APC], [MAJ]] \n\nI have the following questions regarding the experiments:\n1. Could the authors confirm that the reported CERS are on validation/test dataset and not on train/dev data?[[DAT-NEU,EXP-NEU], [SUB-NEU,EMP-NEU], [QSN], [MIN]] It is not explicitly stated.[[EXP-NEG], [SUB-NEG], [DFT], [MIN]] I hope it is indeed the former, else I have a major concern with the efficacy of the algorithm as ultimately, we care about the test performance of the compressed models in comparison to uncompressed model.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] \n\n2. In B.1 the authors use an increasing number units in the hidden layers of the GRUs as opposed to a fixed size like in Deep Speech 2, an obvious baseline that is missing from the  experiments is the comparison with *exact* same GRU (with  768, 1024, 1280, 1536 hidden units) *without any compression*.[[RWK-NEG,EXP-NEG], [SUB-NEG,CMP-NEG], [DFT], [MAJ]] \n\n3.  What do different points in Fig 3 and 4 represent.[[TNF-NEU], [PNF-NEU], [QSN], [MIN]] What are the values of lamdas that were used to train (the l2 and trace norm regularization) the Stage 1 of models shown in Fig 4.[[EXP-NEU,MET-NEU,TNF-NEU], [EMP-NEU,PNF-NEU], [QSN], [MIN]] I want to understand what is the difference in the  two types of  behavior of orange points (some of them seem to have good compression while other do not - it the difference arising from initialization or different choice of lambdas in stage 1.[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]] \n\nIt is interesting that although L2 regularization does not lead to low \\nu parameters in Stage 1, the compression stage does have comparable performance to that of trace norm minimization.[[EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]] The authors point it out, but a further investigation might be interesting.[[ANA-NEU], [SUB-NEU], [DIS], [MIN]] \n\nWriting:\n1. The GRU model for which the algorithm is proposed is not introduced until the appendix.[[MET-NEG], [PNF-NEG], [DFT], [MIN]] While it is a standard network, I think the details should still be included in the main text to understand some of the notation referenced in the text like \u201c\\lambda_rec\u201d and \u201c\\lambda_norec\u201d"[[MET-NEU], [PNF-NEU], [DIS], [MIN]]