"The paper evaluates one proposed Deep RL-based model (Mirowski et al. 2016) on its ability to generally navigate.[[INT-NEU], [null], [SMY], [GEN]] This evaluation includes training the agent on a set of training mazes and testing it's performance on a set of held-out test mazes.[[PDI-NEU], [null], [DIS], [GEN]] Evaluation metrics include repeated latency to the goal and comparison to the shortest route.[[MET-NEU], [null], [SMY], [GEN]] Although there are some (minor) differences between the implementation with Mirowski et al. 2016,[[RWK-NEU], [CMP-NEU], [DIS], [MIN]] I believe the conclusions made by the authors are mostly valid.[[CNT], [null], [APC], [MAJ]] \n\nI would firstly like to point out that measuring generalization is not standard practice in RL.[[MET-NEU], [null], [DIS], [MIN]] Recent successes in Deep RL--including Atari and AlphaGo all train and test on exactly the same environment (except for random starts in Atari and no two games of Go being the same).[[EXP-NEU,MET-NEU], [null], [DIS], [MIN]] Arguably, the goal of RL algorithms is to learn to exploit their environment as quickly as possible in order to attain the highest reward.[[MET-NEU], [null], [DIS], [MIN]] However, when RL is applied to navigation problems it is tempting to evaluate the agent on unseen maps in order to assess weather the agent has learned a generic mapping & planning policy.[[MET-NEU], [null], [DIS], [MIN]] In the case of Mirowski et al. this means that the LSTM has somehow learned to do general SLAM in a meta-learning sense.[[RWK-NEU], [null], [DIS], [MIN]] To the best of my knowledge, Mirowski et al. never made such a bold claim (despite the title of their paper).[[RWK-NEU], [null], [DIS], [MIN]] \n\nSecondly, there seems to be a big disconnect between attaining a high score in navigation tasks and perfectly solving them by doing general SLAM & optimal path planning. [[RES-NEG], [EMP-NEG], [CRT], [MAJ]]Clearly if the agent receives the maximal possible reward for a well designed navigation task it must, by definition, be doing perfect SLAM & path planning.[[RES-NEU], [EMP-NEU], [DIS], [MIN]] However at less than optimal performance the reward fails to quality  the agent's ability to do SLAM.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] The relationship between reward and ability to do general SLAM is not clear.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Therefore it is my opinion that reinforcement learning approaches to SLAM lack a concrete goal in what they are trying to show.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] \n\nMinor points: Section 5.3 Square map: how much more reward will the agent gain by taking the optimal path?[[MET-NEU], [EMP-NEU], [QSN], [GEN]] Perhaps not that much?[[MET-NEU], [EMP-NEU], [QSN], [GEN]] Wrench map: the fact that the paths taken by the agent are not distributed evenly makes me suspicious.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Could the authors generate many wrench maps (same topology, random size, random wall textures) to make sure there is no bias? "[[MET-NEG], [EMP-NEG], [QSN], [MIN]]