"Paper Summary:\nThis is a very long paper (55 pages), and I did not read it in its entirety.[[OAL-NEG], [SUB-NEG,PNF-NEG], [CRT], [MIN]] The first part (up to page 11), focuses on better understanding the exploding gradients problem, and challenges the fact that current techniques to address gradient explosion work as claimed.[[INT-NEU,MET-NEU], [null], [SMY], [GEN]] To do so, they first motivate a new measure of gradient size, the Gradient Scale Coefficient which averages the singular values of the Jacobian and takes a ratio of different layers.[[PDI-NEU], [null], [SMY], [GEN]] The motivation for this measure is that it is invariant to simple rescaling of layers that preserves the function.[[PDI-NEU], [null], [SMY], [GEN]] (I would have liked to have seen what was meant by preserved the function here -- did you mean preserve the same class outputs e.g.?)[[PDI-NEU], [null], [QSN], [GEN]] \n\nThey focus on linear MLPs in the paper for computational simplicity.[[MET-NEU], [null], [SMY], [GEN]] With this setup, and assuming the Jacobian decomposes, they prove that the GSC increases exponentially (Proposition 5).[[MET-NEU], [null], [SMY], [GEN]] They empirically test this out for networks 50 layers deep and 100 layers wide, where they find that some architectures have exploding gradients after random initialization, and others do not, but those that do not have other drawbacks.[[EXP-NEU,MET-NEU], [null], [SMY], [GEN]] \n\nThey then overview the notion of effective depth for a residual network: a linear residual network can be written as a product of terms of the form (I + r_i).[[MET-NEU], [null], [DIS], [GEN]]  Expanding out, each term is a product of some of the r_i and some of the identities I.[[MET-NEU], [null], [DIS], [GEN]] If all r_i have a norm < 1, then the terms that dominate will be those that consist of fewer r_i, resulting in a lower effective depth.[[MET-NEU], [null], [DIS], [GEN]] This is described in Veit et al, 2016.[[RWK-NEU], [null], [DIS], [GEN]] While this analysis was originally used for residual networks, they relate this to any network by letting I turn into an arbitrary initial function. [[MET-NEU], [null], [DIS], [GEN]]Their main theoretical result from this is that deeper networks take exponentially longer to train (under certain conditions), which they test out with (linear?) networks of depth 50 and width 100.[[EXP-NEU,MET-NEU,RES-NEU], [null], [DIS], [GEN]]\n\nThey also propose that the reason gradients explode is because networks try to preserve their domain going forward, which requires Jacobians to have determinant 1 and leads to a higher Q-norm.[[MET-NEU,RES-NEU], [null], [DIS], [GEN]]\n\nMain Comments:\nThis could potentially be a very nice paper, but I feel the current presentation is not ready for acceptance.[[OAL-NEG], [REC-NEG], [FBK], [MAJ]] In particular, the paper would benefit greatly from being made much shorter, and having more of the important details or proof outlines for the various propositions in the main text.[[OAL-NEG], [CLA-NEG,PNF-NEG], [SUG,CRT], [MAJ]] Right now, it is quite confusing to follow, and I fail to see the motivation for some of the analysis.[[ANA-NEG], [EMP-NEG], [CRT], [MAJ]] For example, the Gradient Scale Coefficient appears to be motivated because (bottom page 3), with other norm measurements, we could take any architecture and rescale the parameters, and inversely scale the gradients to make it \"easy to train\".[[EXP-NEG,ANA-NEG], [EMP-NEG], [CRT], [MAJ]] But typically easy to train does not involve a specific preprocessing of gradients.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Other propositions e.g. Theorem 1, proposition 6, could do with clearer intuition leading to them.[[ANA-NEU], [EMP-NEU], [DIS], [MIN]] I think the assumptions made in the results should also be clearer.[[ANA-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] (It's fine to have results, but currently I can't tell under what conditions the results apply and under what conditions they don't. E.g. are there any extensions of this that apply to non-linear networks?)[[ANA-NEG,RES-NEG], [EMP-NEG], [QSN,CRT], [MAJ]]\n\nI also have issues with their experimental setup: why choose to experiment on networks of depth 50 and width 100?[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]] This doesn't really look anything like networks that are trained in practice.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]] Calling these \"popular architectures\" is misleading.[[EXP-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] \n\nIn summary, I think this paper needs more work on the presentation to make clear what they are proving and under what conditions, and with experiments that are closer to those used in practice to support their claims.\n"[[EXP-NEG,OAL-NEG], [PNF-NEG], [CRT], [MIN]]