"The principal problem that the paper addresses is how to integrate error-backpropagation learning in a network of spiking neurons that use a form of sigma-delta coding.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]]  The main observation is that static sigma-delta coding as proposed in OConnor and Welling (2016b), is not correct when the weights change during training, as past activations are taken into account with the old rather than the new weights.[[RWK-NEU,RES-NEU], [null], [SMY], [GEN]] \n\nThe solution proposed in this work is to have past activations decay exponentially, to reduce this problem.[[MET-NEU], [null], [SMY], [GEN]]  The coding scheme then mimics the proporitional-integral-derivative idea from control-theory.[[MET-NEU,RES-NEU], [null], [SMY], [GEN]]  The result, spikes having an exponentially decaying effect on the postsynaptic neuron, is similar to that observed in biological spiking neurons.[[RES-NEU], [null], [SMY], [GEN]]  \n\nThe authors show how spike-based learning can be implemented with spiking neurons using such coding, and demonstrate the results on an MLP with one hidden layer applied to the temporal MNIST dataset, and to the Youtube-BB dataset.[[DAT-NEU,MET-NEU], [null], [SMY], [GEN]]  \n\nThis approach is original and significant,[[MET-POS], [EMP-POS], [APC], [MAJ]]  though the presented results are a bit on the thin side.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]]  As presented, the spiking networks are not exactly \"deep\": I am puzzled by the statement that in the youtube-bb dataset only the top 3 layers are \"spiking\".[[MET-NEG], [null], [CRT], [MIN]]  The network for the MNIST dataset is similarly only 3 layers deep (input, hidden, output).[[DAT-NEU,MET-NEU], [null], [SMY], [GEN]]  Is there a particular reason for this?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]   The presentation right now suggests that the scheme does in practise not work for deep networks...[[RES-NEG], [EMP-NEG], [CRT], [MIN]] \n\nWith regard to the learning rule: while the rule is formulated in terms of spikes, it should be noted that for neuron with many inputs and outputs, this update will have to be computed very very often, even for networks with low average firing rates.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]  \n\nThe paper is clear in most points, with some parts that could use further elucidation.[[OAL-POS], [CLA-POS], [APC], [MAJ]]  In particular, in Sec 2.5 the feedback pass for weight updating is computed.[[MET-NEU], [null], [DIS], [MIN]]  It is unclear from the text that this is an ongoing process, in parallel to the feedforward pass.[[MET-NEG], [CLA-NEG], [CRT], [MIN]]  In Sec 2.6 e_t is termed the postsynaptic (pre-nonlinearity) activation, which is confusing as the computation is going the other way (post-to-pre).[[MET-NEG], [EMP-NEG], [CRT], [MIN]]  These two sections would benefit from a more careful layout of the process, what is going on in a forward pass, a backward pass, how does this interact.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]  \n\nSection 2.7 tries to relate the spike-based learning rule to the biologically observed STDP phenomenon.[[MET-NEU], [null], [DIS], [MIN]]  While the formulation in terms of pre-post spike-times is interesting, the result is clearly different from STDP, and ignores the fact that e_t refers to the backpropagating error (which presumably would be conveyed by a feedback network): applying the plotted pre-post spike-time rule in the same setting as where STDP is observed will not achieve error-backpropagation.[[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]]  \n\nThe shorthand notation in the paper is hard to follow in the first place btw, perhaps this could be elaborated/remedied in an appendix, there is also some rather colloquial writing in places:\"obscene wast of energy\" (abstract), \"There's\" \"aren't\" (2.6, p5).[[ABS-NEG,OAL-NEG], [PNF-NEG], [CRT], [MIN]]  \n\nThe correspondence of spiking neurons to sigma-delta modulation is incorrectly attributed to Zambrano and Bohte (2016), but is rather presented in Yoon (2017/2016, check original date of publication!). \n\n"[[RWK-NEG], [CMP-NEG], [CRT], [MIN]] 