"The main idea of this paper is to replace the feedforward summation\ny = f(W*x + b)\nwhere x,y,b are vectors, W is a matrix\nby an integral\n\\y = f(\\int W \\x + \\b)\nwhere \\x,\\y,\\b are functions, and W is a kernel.[[PDI-NEU], [null], [SMY], [GEN]] A deep neural network with this integral feedforward is called a deep function machine. [[INT-NEU], [null], [SMY], [GEN]]\n\nThe motivation is along the lines of functional PCA: if the vector x was obtained by discretization of some function \\x, then one encounters the curse of dimensionality as one obtains finer and finer discretization.[[MET-NEU], [null], [SMY], [GEN]] The idea of functional PCA is to view \\x as a function is some appropriate Hilbert space, and expands it in some appropriate basis.[[PDI-NEU], [null], [SMY], [GEN]] This way, finer discretization does not increase the dimension of \\x (nor its approximation), but rather improves the resolution.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] \n\nThis paper takes this idea and applies it to deep neural networks.[[PDI-NEU,MET-NEU], [null], [SMY], [MAJ]] Unfortunately, beyond rather obvious approximation results, the paper does not get major mileage out of this idea.[[MET-NEU], [IMP-NEG], [DFT], [MAJ]] This approach amounts to a change of basis - and therefore the resolution invariance is not surprising.[[MET-NEU], [EMP-NEG], [DIS], [MAJ]] In the experiments, results of this method should be compared not against NNs trained on the data directly, but against NNs trained on dimension reduced version of the data (eg: first fixed number of PCA components).[[EXP-NEU,RES-NEU], [CMP-NEU], [SUG], [MAJ]] Unfortunately, this was not done. I suspect that in this case, the results would be very similar. \n\n"[[RES-NEU], [CMP-NEG], [DFT], [MAJ]]