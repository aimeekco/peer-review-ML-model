"This paper proposed a training strategy for fully connected neural network, where per class, per weight learning rate is used.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] Results are reported on about 50 UCI datasets with different topologies.[[DAT-NEU,RES-NEU], [null], [SMY], [GEN]]\n\nPros:\n\n1. This paper proposes a simple and intuitive approach for training neural networks.[[MET-POS], [EMP-POS], [APC], [MAJ]] \n\n2. The idea in the approach seems novel (I have not found the similar training strategy) and they tested several UCI datasets with different fully connected network topologies and activation function settings.[[DAT-NEU,MET-POS], [EMP-POS], [APC], [MAJ]] They found it improved the accuracy.[[MET-NEU], [null], [DIS], [MIN]]\n\nCons:\n\n1. This paper proposed a rather ad hoc proposal for training neural networks.[[EXP-NEU], [null], [SMY], [GEN]] This is not supported by any strong theory or conceptual idea.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]] Grounding the proposal with some theoretical or conceptual arguments could have made the proposal sounder.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]] For now, it looks like more like a trick than anything else.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]]\n\n2. The analysis of all facets of the proposal is missing.[[ANA-NEG], [SUB-NEG], [DFT], [MAJ]] There is no indications on the extra computations required for handling this modification compared to standard training.[[EXP-NEG], [CMP-NEG], [CRT], [MAJ]] It is very probably more costful.[[ANA-NEU], [CNT], [DIS], [MIN]] Likewise analyzing results for varying number of classes (e.g. 100 classes with Cifar-100, 1000 classes with ImageNet), with imbalanced datasets is required.[[DAT-NEU,RES-NEU], [SUB-NEU], [DIS], [MIN]]\n\n3. More importantly, the paper tested only on small size datasets (UCI), where the inputs are relatively well-structured.[[DAT-NEU], [null], [DIS], [MIN]] There is not unstructured datasets (e.g., images, text documents, speech) tested, which is where deep learning is making sense.[[DAT-NEU], [CNT], [DIS], [MIN]] \n\n4. The Experiment section is not well structured, at least for me, I cannot understand it well.[[EXP-NEG], [PNF-NEG], [CRT], [MIN]] In particular, the strategy for choosing the hyperparameters (e.g., \\alpha, \\alpha_mu, local learning rate, \\alpha_mu) need to be developed .[[MET-NEU], [null], [SUG], [MIN]] Perhaps the authors can make a table listing the hyperparameters or a diagram describing the whole training procedures.[[EXP-NEU,TNF-NEU], [null], [SUG], [MIN]]\n\n5. For each weight w, we add K learning rates u_w^j.[[MET-NEU], [null], [DIS], [MIN]] It means that we multiplied by K the number of parameters in our model (K is the number of classes).[[MET-NEU], [null], [DIS], [MIN]] That\u2019s a lot. Given that we are thus optimizing with (K+1) times the number of weights of the network, I am not sure that comparing with the vanilla network trained in usual way is fair.[[MET-NEU], [CMP-NEU], [DIS], [MIN]]\n\nSpace was clearly not an issue with the paper, it still have available space to add further explanations[[OAL-NEU], [PNF-NEU], [DIS], [MIN]]\n\nThe paper is proposing a trick to train neural networks that is backed by strong arguments.[[EXP-NEU], [null], [DIS], [GEN]] The assessment of the method is incomplete and not convincing.[[MET-NEG], [SUB-NEG,EMP-NEG], [CRT], [MAJ]] I thus recommend a clear rejection.[[OAL-NEG], [REC-NEG], [FBK], [MAJ]]\n\nComments on the details of the paper:[[MET-NEU], [null], [DIS], [GEN]]\n\nThe paper does not write mathematically rigorous.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] For example, \n- Section Method, equation (4)[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \n\\partial L/ \\partial w (1 + \\alpha(\\mu_w)^T y (\\partial L / \\partial z \\partial w) = \\partial L/ \\partial w + O(\\alpha)  satisfied if and only if \\partial L / \\partial z \\partial w is bounded.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] i.e |\\partial L / \\partial z \\partial w|<= B.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]  However, the author did not suppose this condition.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Since L is NON Convex, it could not be automatically considered as bounded.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n - Section method, paragraph under equation (2) L(z(\\alpha),x,y)<=L(w,x,y) is NOT necessary.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] It should be supposed L is at least locally convex.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \n-   Equation (5)  should be - O(\\alpha^2)[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \n\nThe choosing of \\alpha_\\mu is generally large (10^4-10^5).[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Does it have some support?\n"[[MET-NEU], [null], [QSN], [MIN]]