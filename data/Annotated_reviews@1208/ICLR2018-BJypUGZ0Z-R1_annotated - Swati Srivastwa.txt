"\nThis paper explores the use of simple models for predicting the final\nvalidation performance of a neural network, from intermediate values\nduring training.[[EXP-NEU,MET-NEU], [null], [SMY], [GEN]]  It uses support vector regression to show that a\nrelatively small number of samples of hyperparameters, architectures,\nand validation time series can lead to reasonable predictions of\neventual performance.[[EXP-NEU,MET-NEU], [null], [SMY], [GEN]]  The paper performs a modest evaluation of such\nsimple models and shows surprisingly good r-squared values.[[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]]  The\nresulting simple prediction framework is then used for early stopping,\nin particular within the Hyperband hyperparameter search algorithm.[[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]]\n\nThere's a lot that I like about this paper, in particular the ablation\nstudy to examine which pieces matter, and the evaluation of a couple\nof simple models.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]  Ultimately, however, I felt like the paper was\nsomewhat unsatisfying as it left open a large number of obvious\nquestions and comparisons:[[MET-NEG], [CMP-NEG], [CRT], [MAJ]]\n\n- The use of the time series is the main novelty.[[MET-NEG], [NOV-NEG], [CRT], [MAJ]]  In the AP, HP and\n  AP+HP cases of Table 2, it is essentially the same predictive setup[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n  of SMAC, BO, and other approaches that are trying to model the map\n  from these choices to out-of-sample performance.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]  Doesn't the good\n  performance without TS on, e.g., ResNets in Table 2 imply that the\n  Deep ResNets subfigure in Figure 3 should start out at 80+?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\n- In light of the time series aspect being the main contribution, a\n  really obvious question is: what does it learn about the time\n  series?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]  The linear models do very well, which means it should be\n  possible to look at the magnitude of the weights.[[MET-POS], [EMP-POS], [APC], [MAJ]]  Are there any\n  surprising long-range dependencies? [[MET-NEU], [EMP-NEU], [QSN], [MIN]] The fact that LastSeenValue\n  doesn't do as well as a linear model on TS alone would seem to\n  indicate that there are higher order autoregressive coefficients.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n  That's surprising and the kind of thing that a scientific\n  investigation here should try to uncover; it's a shame to just put\n  up a table of numbers and not offer any analysis of why this works.[[MET-NEG,ANA-NEG], [EMP-NEG], [CRT], [MAJ]]\n\n- In Table 1 the linear SVM uniformly outperforms the RBF SVM, so why\n  use the RBF version?[[MET-NEU,TNF-NEU], [CMP-NEU], [QSN], [MIN]]\n\n- Given that the paper seeks to use uncertainty in estimates and the\n  entire regression setup could be trivially made Bayesian with no\n  significant computational cost over a kernelized SVM or OLS,\n  especially if you're doing LOOCV to estimate uncertainty in the\n  frequentist models.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]  Why not include Bayesian linear regression and\n  Gaussian process regression as baselines?[[RWK-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\n- Since the model gets information from the AP and HP before doing any\n  iterations, why not go on and use that to help select candidates?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\n- I don't understand how speedup is being computed in Figure 4.[[MET-NEU,TNF-NEU], [EMP-NEU], [DIS], [MIN]]\n\n- I'd like a more explicit accounting of whether 0.00006 seconds vs\n  0.024 seconds is something we should care about in this kind of\n  work, when the steps can take minutes or hours on a GPU.[[MET-NEU,RES-NEU], [EMP-NEU], [DIS], [MIN]]\n\n- How useful is r-squared as a measure of performance in this setting?[[MET-NEU,RES-NEU], [EMP-NEU], [DIS], [MIN]]\n  My experience has been that most of the search space has very poor\n  performance and the objective is to find the small regions that work\n  well.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nMinor things:\n\n- y' (prime) gets overloaded in Section 3.1 as a derivative and then\n  in Section 4 as a partial learning curve.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]]\n\n- \"... is more computationally and ...\"\n\n- \"... our results for performing final ...\"\n[[RES-NEU], [null], [DIS], [MIN]]"