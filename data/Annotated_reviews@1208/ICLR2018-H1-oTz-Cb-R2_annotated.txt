"Recent work on incorporating prior knowledge about invariances into neural networks suggests that the feature dimension in a stack of feature maps has some kind of group or manifold structure, similar to how the spatial axes form a plane.[[RWK-NEU], [null], [DIS], [GEN]] This paper proposes a method to uncover this structure from the filters of a trained ConvNet[[INT-NEU], [null], [SMY], [GEN]]. The method uses an InfoGAN to learn the distribution of filters. By varying the latent variables of the GAN, one can traverse the manifold of filters.[[MET-NEU], [EMP-NEU], [SMY], [GEN]] The effect of moving over the manifold can be visualized by optimizing an input image to produce the same activation profile when using a perturbed synthesized filter as when using an unperturbed synthesized filter.[[MET-NEU], [EMP-NEU], [SMY], [GEN]]\n\nThe idea of empirically studying the manifold / topological / group structure in the space of filters is interesting.[[PDI-POS], [null], [APC], [MAJ]] A priori, using a GAN to model a relatively small number of filters seems problematic due to overfitting, but the authors show that their InfoGAN approach seems to work well.[[DAT-NEG,MET-NEU], [EMP-POS], [APC], [MAJ]]\n\nMy main concerns are:\n\nControls\nTo generate the visualizations, two coordinates in the latent space are varied, and for each variation, a figure is produced.[[EXP-NEU,TNF-NEU], [EMP-NEU], [DIS], [GEN]] To figure out if the GAN is adding anything, it would be nice to see what would happen if you varied individual coordinates in the filter space (\"x-space\" of the GAN), or varied the magnitude of filters or filter planes.[[EXP-NEU,MET-NEU,RES-NEU], [EMP-NEU], [DIS], [MAJ]] Since the visualizations are as much a function of the previous layers as they are a function of the filters in layer l which are modelled by the GAN, I would expect to see similar plots for these baselines.[[RWK-NEU,EXP-NEU,RES-NEU], [EMP-NEU], [DIS], [MAJ]]\n\nLack of new Insights\nThe visualizations produced in this paper are interesting to look at, but it is not clear what they tell us, other than \"something non-trivial is going on in these networks\". In fact, it is not even clear that the transformations being visualized are indeed non-linear in pixel space (note that even a 2D diffeomorphism, which is a non-linear map on R^2, is a linear operator on the space of *functions* on R^2, i.e. on the space of images). [[MET-NEU,ANA-NEG], [PNF-NEG,EMP-NEU], [CRT], [MAJ]]In any case, no attempt is made to analyze the results, or provide new insights into the computations performed by a trained ConvNet.[[RES-NEG,ANA-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nInterpretation\nThis is a minor point, but I would not say (as the paper does) that the method captures the invariances learned by the model, but rather that it aims to show the variability captured by the model.[[EXP-NEG], [EMP-NEG], [DFT], [MIN]] A ReLU net is only invariant to changes that are mapped to zero by the ReLU, or that end up in the kernel of one of the linear layers.[[RWK-NEU], [null], [SMY], [GEN]] The presented method does not consider this and hence does not analyze invariances.[[EXP-NEG,MET-NEG], [EMP-NEG], [DFT], [MAJ]]\n\nMinor issues:\n- In the last equation on page 2, the right-hand side is missing a \"min max\"."[[EXP-NEG,MET-NEG], [CLA-NEG], [DFT], [MIN]]