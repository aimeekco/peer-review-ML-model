"[After author feedback]\nI think the approach is interesting and warrants publication.[[MET-POS], [REC-POS], [FBK], [MAJ]] However, I think some of the counter-intuitive claims on the proposal learning are overly strong, and not supported well enough by the experiments.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]] In the paper the authors also need to describe the differences between their work and the concurrent work of Maddison et al. and Naesseth et al.[[RWK-NEG], [CMP-NEG], [CRT], [MAJ]] \n\n[Original review]\nThe authors propose auto-encoding sequential Monte Carlo (SMC), extending the VAE framework to a new Monte Carlo objective based on SMC.[[MET-NEU], [null], [SMY], [GEN]] The authors show that this can be interpreted as standard variational inference on an extended space, and that the true posterior can only be obtained if we can target the true posterior marginals at each step of the SMC procedure.[[MET-NEU], [null], [SMY], [GEN]] The authors argue that using different number of particles for learning the proposal parameters versus the model parameters can be beneficial.[[MET-NEU], [null], [SMY], [GEN]]\n\nThe approach is interesting and the paper is well-written,[[MET-NEU,OAL-POS], [CLA-POS,EMP-POS], [APC], [MAJ]] however, I have some comments and questions:\n\n- It seems clear that the AESMC bound does not in general optimize for q(x|y) to be close to p(x|y), except in the IWAE special case.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] This seems to mean that we should not expect for q -> p when K increases?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n- Figure 1 seems inconclusive and it is a bit difficult to ascertain the claim that is made.[[MET-NEG,TNF-NEG], [EMP-NEG], [CRT], [MIN]] If I'm not mistaken K=1 is regular ELBO and not IWAE/AESMC?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Have you estimated the probability for positive vs. negative gradient values for  K=10?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] To me it looks like the probability of it being larger than zero is something like 2/3.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] K>10 is difficult to see from this plot alone.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n- Is there a typo in the bound given by eq. (17)?[[MET-NEU], [CLA-NEU], [QSN], [MIN]] Seems like there are two identical terms.[[MET-NEU], [CLA-NEU], [QSN], [MIN]] Also I'm not sure about the first equality in this equatiion, is I^2 = 0 or is there a typo?[[MET-NEG], [CLA-NEG], [QSN], [MIN]]\n- The discussion in section 4.1 and results in the experimental section 5.2 seem a bit counter-intuitive, especially learning the proposals for SMC using IS.[[MET-NEG,RES-NEG], [EMP-NEG], [QSN], [MIN]] Have you tried this for high-dimensional models as well?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Because IS suffers from collapse even in the time dimension I would expect the optimal proposal parameters learnt from a IWAE-type objective will collapse to something close to the the standard ELBO.[[EXP-NEU,MET-NEU], [null], [DIS], [MIN]] For example have you tried learning proposals for the LG-SSM in Section 5.1 using the IS objective as proposed in 4.1?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Might this be a typo in 4.1?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] You still propose to learn the proposal parameters using SMC but with lower number of particles?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] I suspect this lower number of particles might be model-dependent.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\nMinor comments:\n- Section 1, first paragraph, last sentence, \"that\" -> \"than\"?[[CNT], [CLA-NEG], [CRT], [MIN]]\n- Section 3.2, \"... using which...\" formulation in two places in the firsth and second paragraph was a bit confusing[[CNT], [CLA-NEG], [CRT], [MIN]]\n- Page 7, second line, just \"IS\"?[[CNT], [CLA-NEG], [QSN], [MIN]]\n- Perhaps you can clarify the last sentence in the second paragraph of Section 5.1 about computational graph not influencing gradient updates?[[CNT], [CLA-NEG], [QSN], [MIN]]\n- Section 5.2, stochastic variational inference Hoffman et al. (2013) uses natural gradients and exact variational solution for local latents so I don't think K=1 reduces to this?"[[RWK-NEU], [EMP-NEG], [QSN], [MIN]]