"The paper is about hyperparameter optimization, which is an important problem in deep learning due to the large number of hyperparameters in contemporary model architectures and optimization algorithms[[INT-NEU,PDI-NEU,EXP-NEU,MET-NEU,RES-NEU,ANA-NEU], [CMP-NEU,EMP-NEU], [SMY,DIS], [GEN]].\n\nAt a high-level, hyperparameter optimization (for the challenging case of discrete variables) can be seen as a black-box optimization problem where we have only access to a function evaluation oracle (but no gradients etc.).[[PDI-NEG,EXP-NEG,ANA-NEG], [CMP-NEG,EMP-NEG], [DFT], [MIN]] In the entirely unstructured case, there are strong lower bounds with an exponential dependence on the number of hyperparameters[[PDI-NEU,EXP-NEU,ANA-NEU], [EMP-NEU], [SMY], [GEN]]. In order to sidestep these impossibility results, the current paper assumes structure in the unknown function mapping hyperparameters to classification accuracy. [[PDI-POS,EXP-POS,RES-POS,ANA-POS], [SUB-POS,EMP-POS], [APC], [MAJ]]In particular, the authors assume that the function admits a representation as a sparse and low-degree polynomial[[RWK-NEG,EXP-NEG,RES-NEG,ANA-NEG], [IMP-NEG,EMP-NEG], [DFT], [MIN]]. While the authors do not empirically validate whether this is a good model of the unknown function, it appears to be a reasonable assumption (the authors *do* empirically validate their overall approach).[[RWK-NEG], [SUB-NEG], [DFT], [MIN]]\n\nBased on the sparse and low-degree assumption, the paper introduces a new algorithm (called Harmonica) for hyperparameter optimization.[[INT-POS,PDI-POS,MET-POS], [NOV-POS,SUB-POS,EMP-POS], [APC], [MAJ]] The main idea is to leverage results from compressed sensing in order to recover the sparse and low-degree function from a small number of measurements (i.e., function evaluations).[[PDI-POS,RES-POS,ANA-POS], [CMP-POS,EMP-POS], [APC], [MAJ]] The authors derive relevant sample complexity results for their approach.[[PDI-POS,RES-POS], [IMP-POS,CMP-POS], [APC], [MAJ]] Moreover, the method also yields new algorithms for learning decision trees.[[RWK-POS,MET-POS], [SUB-POS], [APC], [MAJ]]\n\nIn addition to the theoretical results , the authors conduct a detailed study of their algorithm on CIFAR10[[RWK-POS,RES-POS], [SUB-POS,IMP-POS], [APC], [MAJ]]. They compare to relevant recent work in hyperparameter optimization (Bayesian optimization, random search, bandit algorithms) and find that their method significantly improves over prior work. [[RWK-POS,MET-POS], [IMP-POS,CMP-POS,EMP-POS], [APC], [MAJ]]The best parameters found by Harmonica improve over the hand-tuned results for their \"base architecture\" (ResNets).\[[RWK-POS,RES-POS], [null], [APC], [MAJ]]n\nOverall, I find the main idea of the paper very interesting and well executed, both on the theoretical and empirical side. [[INT-POS,PDI-POS], [EMP-POS], [APC], [MAJ]]Hence I strongly recommend accepting this paper.[[INT-POS], [REC-POS], [APC,FBK], [MAJ]]\n\n\nSmall comments and questions[[RWK-NEG], [REC-NEG], [DFT], [MIN]]:\n\n1. It would be interesting to see how close the hyperparameter function is to a low-degree and sparse polynomial (e.g., MSE of the best fit)[[RWK-POS,EXP-POS,MET-POS,ANA-POS], [EMP-POS], [APC], [MAJ]].\n\n2. A comparison without dummy parameters would be interesting to investigate the performance differences between the algorithms in a lower-dimensional problem.[[PDI-POS,MET-POS], [EMP-POS], [APC], [MAJ]]\n\n3. The current paper does not mention the related work on hyperparameter optimization using reinforcement learning techniques (e.g., Zoph & Le, ICLR 2017).[[RWK-NEG,MET-NEG,BIB-NEG], [EMP-NEG], [SMY,CRT], [MIN]] While it might be hard to compare to this approach directly in experiments, it would still be good to mention this work and discuss how it relates to the current paper.[[INT-POS,RWK-POS,PDI-POS,EXP-POS], [NOV-POS,CMP-POS,EMP-POS], [APC], [MAJ]]\n\n4. Did the authors tune the hyperparameters directly using the CIFAR10 test accuracy?[[RWK-NEU,ANA-NEU], [CMP-NEU], [QSN], [GEN]] Would it make sense to use a slightly smaller training set and to hold out say 5k images for hyperparameter evaluation before making the final accuracy evaluation on the test set?[[EXP-NEU,MET-NEU,ANA-NEU], [EMP-NEU], [DIS,QSN], [GEN]] The current approach could be prone to overfitting[[PDI-NEG,EXP-NEG], [EMP-NEG], [DFT], [MIN]].\n\n5. While random search does not explicitly exploit any structure in the unknown function, it can still implicitly utilize smoothness or other benign properties of the hyperparameter space.[[RWK-NEU,EXP-NEU,ANA-NEU], [null], [SMY,DIS], [GEN]] It might be worth adding this in the discussion of the related work.\[[RWK-NEG], [null], [DFT], [MIN]]n\n6. Algorithm 1: Why is the argmin for g_i  (what does the index i refer to)?\n\n7[[MET-NEU], [null], [QSN], [GEN]]. Why does PSR truncate the indices in alpha?[[PDI-NEU,MET-NEU], [null], [QSN], [GEN]] At least in \"standard\" compressed sensing, the Lasso also has recovery guarantees without truncation (and empirically works sometimes better without).\[[RWK-NEU], [IMP-NEU], [SMY,DIS], [GEN]]n\n9. Definition 3: Should C be a class of functions mapping {-1, 1}^n to R?  (Note the superscript.)\n\n10.[[PDI-NEU,ANA-NEU], [CMP-NEU], [DIS], [GEN]] On Page 3 we assume that K = 1, but Theorem 6 still maintains a dependence on K[[PDI-NEU,ANA-NEU], [CMP-NEU], [DIS], [GEN]]. It might be cleaner to either treat the general K case throughout, or state the theorem for K = 1.\n\n11.[[PDI-NEU,MET-NEU,ANA-NEU], [CMP-NEU,EMP-NEU], [DIS], [GEN]] On CIFAR10, the best hyperparameters do not improve over the state of the art with other models (e.g., a wide ResNet)[[RWK-NEU,PDI-NEU], [NOV-NEU], [SMY,DIS], [GEN]]. It could be interesting to run Harmonica in the regime where it might improve over the best known models for CIFAR10.\n\n12[[RWK-POS,EXP-POS,ANA-POS], [EMP-POS], [APC], [MAJ]]. Similarly, it would be interesting to see whether the hyperparameters identified by Harmonica carry over to give better performance on ImageNet.[[RWK-POS,EXP-POS,ANA-POS], [IMP-POS], [APC], [MAJ]] The authors claim in C.3 that the hyperparameters identified by Harmonica generalize from small networks to large networks.[[PDI-NEU,EXP-NEG,ANA-NEG], [EMP-NEG], [DFT], [MIN]] Testing whether the hyperparameters also generalize from a smaller to a larger dataset would be relevant as well."[[DAT-NEU,EXP-NEU,ANA-NEU], [null], [SMY,DIS], [GEN]]