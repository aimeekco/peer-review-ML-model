"The paper presents a proof of the self normalization of NCE as a result of being a low-rank matrix approximation of low-rank approximation of the normalized conditional probabilities matrix.[[INT-NEU], [null], [SMY], [GEN]]  However, it seems that in equation 4, the authors assume that the noise distribution is a unigram model over words.[[MET-NEU], [EMP-NEU], [SMY], [GEN]] However, one is allowed to use any noise distribution in NCE, and convergence should be quicker with those distributions that are close to the true distribution.[[EXT-NEU], [null], [DIS], [GEN]] Does the argument hold for general noise distributions ?[[MET-NEU], [null], [QSN], [GEN]] With this assumption, they can borrow easily from Goldberg and Levy, 2014 for the proof.[[RWK-NEU], [null], [DIS], [GEN]] \nIn experiments, they find that while NCE does result in self-normalization, it is inversely correlated with perplexity which is a bit surprising.[[EXP-NEU], [EMP-NEU], [DIS], [GEN]] The paper is interesting but lacks strong empirical results.[[RES-NEG], [EMP-NEG], [FBK], [MIN]] It could be stronger if they could exploit some of their findings to improve language modeling over a strong baseline. "[[RWK-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]]