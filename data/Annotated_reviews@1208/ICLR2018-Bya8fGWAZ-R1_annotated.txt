"The original value-iteration network paper assumed that it was trained on near-expert trajectories and used that information to learn a convolutional transition model that could be used to solve new problem instances effectively without further training.[[RWK-NEU], [NULL], [SMY], [MAJ]]\n\nThis paper extends that work by\n- training from reinforcement signals only, rather than near-expert trajectories\n- making the transition model more state-depdendent\n- scaling to larger problem domains by propagating reward values for navigational goals in a special way[[MET-NEU], [null], [SMY], [GEN]]\n\nThe paper is fairly clear and these extensions are reasonable[[INT-POS], [CLA-POS], [SMY], [MAJ]].  However, I just don't think the focus on 2D grid-based navigation has sufficient interest and impact[[MET-NEG], [IMP-NEG], [DFT], [MAJ]].  It's true that the original VIN paper worked in a grid-navigation domain, but they also had a domain with a fairly different structure;  I believe they used the gridworld because it was a convenient initial test case, but not because of its inherent value.[[RWK-NEU], [CMP-POS], [DIS], [MAJ]] So, making improvements to help solve grid-worlds better is not so motivating[[MET-NEG], [IMP-NEG],[DFT], [MAJ]].  It may be possible to motivate and demonstrate the methods of this paper in other domains, however.[[MET-NEU], [EMP-NEU], [DIS], [GEN]]  The work on dynamic environments was an interesting step:  it would have been interesting to see how the \"models\" learned for the dynamic environments differed from those for static environments.[[MET-POS,ANA-POS], [CMP-POS], [APC], [MAJ]]