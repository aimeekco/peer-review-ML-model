"An approach to adjust inference speed, power consumption or latency by using incomplete dot products McDanel et al. (2017) is investigated.[[INT-NEU,RWK-NEU], [null], [SMY], [GEN]]\n\nThe approach is based on `profile coefficients\u2019 which are learned for every channel in a convolution layer, or for every column in the fully connected layer.[[MET-NEU], [null], [SMY], [GEN]] Based on the magnitude of this profile coefficient, which determines the importance of this `filter,\u2019 individual components in a neural net are switched on or off. McDanel et al. (2017) propose to train such an approach in a stage-by-stage manner.[[RWK-NEU,MET-NEU], [CMP-NEU], [SMY], [GEN]]\n\nDifferent from a recently proposed method by McDanel et al. (2017), the authors of this submission argue that the stage-by-stage training doesn\u2019t fully utilize the deep net performance.[[RWK-NEU,MET-NEU], [CMP-POS], [SMY], [GEN]] To address this issue a `loss aggregation\u2019 is proposed which jointly optimizes a deep net when multiple fractions of incomplete products are used.[[MET-NEU], [EMP-NEU], [SMY], [GEN]]\n\nThe method is evaluated on the MNIST and CIFAR-10 datasets and shown to outperform work on incomplete dot products by McDanel et al. (2017) by 32% in the low resource regime.[[RWK-NEU,DAT-NEU,EXP-POS,RES-POS], [CMP-POS], [SMY], [GEN]]\n\nSummary:\n\u2014\u2014\nIn summary, I think the paper proposes an interesting approach but more work is necessary to demonstrate the effectiveness of the discussed method.[[MET-POS], [SUB-NEG], [DFT], [MIN]] The results are preliminary and should be extended to CIFAR-100 and ImageNet to be convincing.[[DAT-NEU,RES-NEU], [SUB-NEG], [SUG], [MIN]] In addition, the writing should be improved as it is often ambiguous.[[OAL-NEG], [CLA-NEG], [SUG], [MIN]] See below for details.\n\nReview:\n\u2014\u2014\u2014\u2014\u2014\n1. Experiments are only provided on very small datasets. According to my opinion, this isn\u2019t sufficient to illustrate the effectiveness of the proposed approach.[[DAT-NEG,EXP-NEU], [SUB-NEG], [DFT], [MAJ]] As a reader I wouldn\u2019t want to see results on CIFAR-100 and ImageNet using multiple network architectures, e.g., AlexNet and VGG16.\n\n2. [[DAT-NEU,EXP-NEG,RES-NEU], [SUB-NEU], [CRT], [MAJ]]Usage of the incomplete dot product for the fully connected layer and the convolutional layer seems inconsistent.[[MET-NEG], [EMP-NEG], [DFT], [MIN]] More specifically, while the profile coefficient is applied for every input element in Eq. (1), it\u2019s applied based on output channels in Eq. (2). This seems inconsistent and a comment like `These two approaches, however, are equivalent with negligible difference induced by the first hidden layer\u2019 is more confusing than clarifying.\n\n3.[[MET-NEG], [EMP-NEG], [DFT], [MIN]] The writing should be improved significantly and statements should be made more precise, e.g., `From now on, x% DP, where \\leq x \\geq 100, means the x% of terms used in dot products\u2019. While sentences like those can be deciphered, they aren\u2019t that appealing.[[MET-NEG], [CLA-NEG], [SUG], [MIN]]\n\n4. The loss functions in Eq. (3) should be made more precise. It remains unclear whether the profile coefficients and the weights are trained jointly, separately, incrementally etc.[[MET-NEG], [CLA-NEG], [SUG,DFT], [MIN]]\n\n5. Algorithm 1 and Algorithm 2 call functions that aren\u2019t described/defined.[[MET-NEG], [CLA-NEG], [DFT], [MIN]]\n\n6. Baseline numbers for training on datasets without incomplete dot products should be provided.\n"[[RWK-NEG,DAT-NEU], [SUB-NEU], [DFT], [MIN]]