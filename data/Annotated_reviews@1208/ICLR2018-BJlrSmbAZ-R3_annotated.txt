"The authors show how the regularization procedure called batch normalization,\ncurrently being used by most deep learning systems, can be understood as\nperforming approximate Bayesian inference.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The authors compare this approach to\nMonte Carlo dropout (another regularization technique which can also be\nconsidered to perform approximate Bayesian inference).[[RWK-NEU,MET-NEU], [CMP-NEU], [APC], [MAJ]] The experiments\nperformed show that the Bayesian view of batch normalization performs similarly\nas MC dropout in terms of the estimates of uncertainty that it produces.[[EXP-NEG], [CMP-NEG], [CRT], [MIN]]\n\nQuality:\n\nI found the quality to be low in some aspects.[[OAL-NEG], [CLA-NEG], [CRT], [MIN]] First, the description of what\nis the prior used by batch normalization in section 3.3 is unsatisfactory.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] The\nauthors basically refer to Appendix 6.4 for the case in which the weight decay\npenalty is not zero.[[CNT], [null], [DIS], [MIN]] The details in that Appendix are almost none, they just\nsay \"it is thus possible to derive the prior...\".[[CNT], [SUB-NEG], [DIS], [MIN]] \n\nThe results in Table 2 are a bit confusing.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]]  The authors should highlight in\nbold face the results of the best performing method.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MIN]] \n\nThe authors indicate that they do not need to compare to variational methods\nbecause Gal and Ghahramani 2015 compare already to those methods.[[MET-NEG], [CMP-NEG], [CRT], [MIN]]  However, Gal\nand Ghahramani's code used Bayesian optimization methods to tune\nhyper-parameters and this code contains a bug that optimizes hyper-parameters\nby maximizing performance on the test data.[[MET-NEU], [null], [DIS], [MIN]]  In particular for hyperparameter\nselection, they average performance across (subsets of) 5 of the training sets\nfrom the 20x train/test split, and then using the tau which got the best\naverage performance for all of 20x train/test splits to evaluate performance:\n\nhttps://github.com/yaringal/DropoutUncertaintyExps/blob/master/bostonHousing/net/experiment_BO.py#L54[[MET-NEU], [null], [DIS], [MIN]]\n\nTherefore, the claim that \n\n\"Since we have established that MCBN performs on par with MCDO, by proxy we\nmight conclude that MCBN outperforms those VI methods as well.\"\n\nis not valid.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nAt the beginning of section 4.3 the authors indicate that they follow in their\nexperiments the setup of Gal and Ghahramani (2015).[[RWK-NEU,EXP-NEU], [null], [DIS], [MIN]] However, Gal and Ghahramani\n(2015) actually follow Hern\u00e1ndez-Lobato and Adams, 2015 so the correct\nreference should be the latter one.[[RWK-NEU,EXP-NEU], [null], [DIS], [MIN]]\n\nClarity:\n\nThe paper is clearly written and easy to follow and understand.[[OAL-POS], [EMP-POS], [APC], [MAJ]]\n\nI found confusing how to use the proposed method to obtain estimates of\nuncertainty for a particular test data point x_star.[[DAT-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] The paragraph just above\nsection 4 says that the authors sample a batch of training data for this, but\nassume that the test point x_star has to be included in this batch.[[DAT-NEU,EXP-NEU], [null], [DIS], [MIN]]\nHow is this actually done in practice?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]]\n\nOriginality:\n\nThe proposed contribution is original. [[OAL-POS], [NOV-POS], [APC], [MAJ]]This is the first time that a Bayesian\ninterpretation has been given to the batch normalization regularization\nproposal.[[MET-NEU], [null], [DIS], [MIN]]\n\nSignificance:\n\nThe paper's contributions are significant.[[OAL-POS], [IMP-POS], [APC], [MAJ]] Batch normalization is a very\npopular regularization technique and showing that it can be used to obtain\nestimates of uncertainty is relevant and significant.[[MET-POS], [IMP-POS], [APC], [MAJ]] Many existing deep\nlearning systems can use this to produce estimates of uncertainty in their\npredictions.\n"[[RWK-NEU], [CMP-NEU], [DIS], [MIN]]