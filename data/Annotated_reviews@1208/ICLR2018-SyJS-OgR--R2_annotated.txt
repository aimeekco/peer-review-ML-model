"I enjoyed reading the paper.[[OAL-POS], [null], [APC], [MAJ]] This is a very well written paper, the authors propose a method for speeding up the training time of Residual Networks based on the dynamical system view interpretation of ResNets.[[EXP-NEU,MET-NEU,OAL-POS], [CLA-POS], [APC], [MAJ]] In general I have a positive opinion about the paper, however, I\u2019d like to ask for some clarifications.[[OAL-POS], [null], [APC], [MAJ]]\n\nI\u2019m not fully convinced by the interpretation of Eq. 5: \u201c\u2026 d is inversely proportional to the norm of the residual modules G(Yj)\u201d.[[MET-POS], [EMP-POS], [APC], [MAJ]] Since F(Yj) is not a constant, I think that d is inversely proportional to ||G(Yj)||/||F(Yj)||, however, in the interpretation the dependence on ||F(Yj)|| is ignored.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] Could the authors comment on that?[[MET-NEG], [EMP-NEG], [QSN], [MIN]]\n\nSection 4. 1 \u201c Each cycle itself can be regarded as a training process, thus we need to reset the learning rate value at the beginning of each training cycle and anneal the learning rate during that cycle.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]]\u201d Is there any empirical evidence for this?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]] What would happen if the learning rate is not reset at the beginning of each cycle?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]] \n\nQuestions with respect to dynamical systems point of view: Eq. 4 assumes small value of h.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] However, for ResNet there is no guarantee that the h would be small (e. g. in Appendix C the values between 0.25 and 1 are used).[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Would the authors be willing to comment on the importance of the value of h?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] In figure 1, pooling (strided convolutions) are not depicted between network stages.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]] I have one question w.r.t. feature maps dimensionality changes inside a CNN: how does pooling (or strided convolution) fit into dynamical systems view?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nTable 3 and 4. I assume that the training time unit is a minute, I couldn\u2019t find this information in the paper.[[EXP-NEG], [SUB-NEG], [DFT], [MIN]] Is the batch size the same for all models (100 for CIFAR and 32 for STL-10)?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] I understand that the models with different #Blocks have different capacity, for clarity, would it be possible to add # of parameters to each model?[[MET-NEU], [SUB-NEU, EMP-NEU], [QSN], [MIN]] For multilevel method, would it be possible to show intermediate results in Table 3 and 4, e. g. at the end of cycle 1 and 2?[[MET-NEU, RES-NEU, TNF-NEU], [SUB-NEU, EMP-NEU], [QSN], [MIN]]  I see these results in Figure 6, however, the plots are condensed and it is difficult to see the exact number at the end of each cycle.[[RES-NEG], [EMP-NEG], [DIS,CRT], [MIN]] \n\nThe citation (E, 2017) seems to be wrong, could the authors check it?\n"[[BIB-NEG], [PNF-NEG], [QSN,CRT], [MIN]]