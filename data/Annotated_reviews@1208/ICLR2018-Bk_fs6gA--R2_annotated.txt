"# Summary\nThis paper proposes a neural network framework for solving binary linear programs (Binary LP).[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The idea is to present a sequence of input-output examples to the network and train the network to remember input-output examples to solve a new example (binary LP).[[PDI-NEU], [null], [SMY], [GEN]] In order to store such information, the paper proposes an external memory with non-differentiable reading/writing operations.[[MET-NEU], [null], [SMY], [GEN]] This network is trained through supervised learning for the output and reinforcement learning for discrete operations.[[MET-NEU], [null], [SMY], [GEN]] The results show that the proposed network outperforms the baseline (handcrafted) solver and the seq-to-seq network baseline.[[RWK-POS,EXP-POS,RES-POS], [CMP-POS,EMP-POS], [APC], [MAJ]]\n\n[Pros]\n- The idea of approximating a binary linear program solver using neural network is new.[[PDI-POS], [NOV-POS], [APC], [MAJ]]\n\n[Cons]\n- The paper is not clearly written (e.g., problem statement, notations, architecture description).[[OAL-NEG], [CLA-NEG], [CRT], [MAJ]] So, it is hard to understand the core idea of this paper.[[PDI-NEG], [EMP-NEG], [CRT], [MAJ]]\n- The proposed method and problem setting are not well-justified.[[PDI-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]] \n- The results are not very convincing.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]]\n\n# Novelty and Significance\n- The problem considered in this paper is new,[[PDI-POS], [NOV-POS], [APC], [MAJ]] but it is unclear why the problem should be formulated in such a way..[[PDI-NEG], [EMP-NEG], [CRT], [MAJ]] To my understanding, the network is given a set of input (problem) and output (solution) pairs and should predict the solution given a new problem.[[PDI-NEU], [EMP-NEU], [DIS], [MIN]] I do not see why this should be formulated as a \"sequential\" decision problem.[[PDI-NEU], [EMP-NEU], [DIS], [MIN]] Instead, we can just give access to all input/output examples (in a non-sequential way) and allow the network to predict the solution given the new input like Q&A tasks.[[PDI-NEU], [EMP-NEU], [SUG], [MIN]] This does not require any \"memory\" because all necessary information is available to the network.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n- The proposed method seems to require a set of input/output examples even during evaluation (if my understanding is correct), which has limited practical applications.[[MET-NEU,RES-NEG], [SUB-NEG], [DFT,CRT], [MIN]] \n\n# Quality\n- The proposed reward function for training the memory controller sounds a bit arbitrary.[[EXP-NEG], [EMP-NEG], [CRT], [MIN]] The entire problem is a supervised learning problem, and the memory controller is just a non-differentiable decision within the neural network.[[PDI-NEU], [EMP-NEU], [DIS], [MIN]] In this case, the reward function is usually defined as the sum of log-likelihood of the future predictions (see [Kelvin Xu et al.] for training hard-attention) because this matches the supervised learning objective.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MIN]] It would be good to justify (empirically) the proposed reward function.[[MET-NEU], [SUB-NEU,EMP-NEU], [SUG], [MIN]] \n- The results are not fully-convincing.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] If my understanding is correct, the LTMN is trained to predict the baseline solver's output.[[RWK-NEU,EXP-NEU], [CMP-NEU], [DIS], [MIN]] But, the LTMN significantly outperforms the baseline solver even in the training set.[[RWK-POS,EXP-POS], [CMP-POS,EMP-POS], [APC], [MAJ]] Can you explain why this is possible?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]]\n\n# Clarity\n- The problem statement and model description are not described well.[[PDI-NEG], [CLA-NEG,PNF-NEG], [CRT], [MAJ]] \n1) Is the network given a sequence of program/solution input?[[MET-NEU], [CNT], [QSN], [MIN]] If yes, is it given during evaluation as well?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] \n2) Many notations are not formally defined.[[OAL-NEG], [PNF-NEG], [CRT], [MIN]] What is the output (o_t) of the network?[[RES-NEU], [EMP-NEU], [QSN], [MIN]]  Is it the optimal solution (x_t)?[[RES-NEU], [EMP-NEU], [QSN], [MIN]]  \n3) There is no mathematical definition of memory addressing mechanism used in this paper.[[MET-NEG], [SUB-NEG], [DFT], [MAJ]] \n- The overall objective function is missing.[[MET-NEG], [SUB-NEG], [DFT], [MAJ]] \n\n[Reference]\n- Kelvin Xu et al., Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"[[BIB-NEU], [null], [DIS], [GEN]]