"The paper aims tackles the problem of generate vectorized sketch drawings by using a RNN-variational autoencoder.[[PDI-NEU], [null], [SMY], [GEN]] Each node is represented with (dx, dy) along with one-hot representation of three different drawing status.[[PDI-NEU], [null], [SMY], [GEN]] A bi-directional LSTM is used to encode latent space in the training stage.[[MET-NEU], [null], [SMY], [GEN]] Auto-regressive VAE is used for decoding.[[EXP-NEU,MET-NEU], [null], [SMY], [GEN]] \n\nSimilar to standard VAEs, log-likelihood has bee used as the data-term and the KL divergence between latent space and Gaussian prior is the regularisation term.[[DAT-NEU,MET-NEU], [null], [SMY], [GEN]] \n\nPros:\n- Good solution to an interesting problem.[[PDI-POS,RES-POS], [EMP-POS], [APC], [MAJ]] \n- Very interesting dataset to be released.[[DAT-POS], [EMP-POS], [APC], [MAJ]]\n- Intensive experiments to validate the performance.[[EXP-POS], [EMP-POS], [APC], [MAJ]] \n\nCons:\n- I am wondering whether the dataset contains biases regarding (dx, dy).[[DAT-NEU], [EMP-NEU], [DIS], [MIN]] In the data collection stage, how were the points lists generated from pen strokes? [[DAT-NEU], [EMP-NEU], [QSN], [MIN]] Did each points are sampled from same travelling distance or according to the same time interval? [[DAT-NEU], [EMP-NEU], [QSN], [MIN]] Are there any other potential biases brought because the data collection tools?[[DAT-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]]\n- Is log-likelihood a good loss here?[[RES-NEU], [EMP-NEU], [QSN], [MIN]] Think about the case where the sketch is exactly the same but just more points are densely sampled along the pen stroke.[[DAT-NEU], [EMP-NEU], [DIS], [MIN]] How do you deal with this case?[[DAT-NEU], [EMP-NEU], [QSN], [MIN]]\n- Does the dataset contain more meta-info that could be used for other tasks beyond generation, e.g. segmentation, classification, identification, etc.? "[[DAT-NEU,FWK-NEU], [SUB-NEU,IMP-NEU], [QSN], [MIN]]