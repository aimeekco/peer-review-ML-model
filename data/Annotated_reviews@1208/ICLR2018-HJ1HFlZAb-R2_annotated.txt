"The authors propose to evaluate how well generative models fit the training set by analysing their data augmentation capacity, namely the benefit brought by training classifiers on mixtures of real/generated data, compared to training on real data only.[[PDI-POS,DAT-POS,MET-POS,ANA-POS], [SUB-POS,CMP-POS,EMP-POS], [APC], [MAJ]] Despite the the idea of exploiting generative models to perform data augmentation is interesting, using it as an evaluation metric does not constitute an innovative enough contribution.[[RWK-POS,DAT-POS,MET-POS,ANA-NEG], [NOV-NEG,EMP-NEG], [DFT,FBK], [MIN]] \n\nIn addition, there is a fundamental matter which the paper does not address: when evaluating a generative model, one should always ask himself what purpose the data is generated for[[RWK-NEG,DAT-NEG,MET-NEG], [NOV-NEG,IMP-NEG,EMP-NEG], [SMY,QSN], [MIN]]. If the aim is to have realistic samples, a visual turing test is probably the best metric.[[PDI-NEU,EXP-POS], [SUB-POS,EMP-POS], [APC], [MAJ]] If instead the purpose is to exploit the generated data for classification, well, in this case an evaluation of the impact of artificial data over training is a good option.[[PDI-POS,DAT-POS,ANA-POS], [SUB-POS,IMP-POS,EMP-POS], [APC], [MAJ]]\n\nPROS:\nThe idea is interesting. [[PDI-POS], [NOV-POS], [APC], [MAJ]]\n\nCONS:\n1. The authors did not relate the proposed evaluation metric to other metrics cited (e.g., the inception score, or a visual turing test, as discussed in the introduction).[[RWK-NEG,PDI-NEG,EXP-NEG,ANA-NEG], [SUB-NEG,IMP-NEG,EMP-NEG], [DFT], [MIN]] It would be interesting to understand how the different metrics relate[[RWK-POS,EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]]. Moreover, the new metric is introduced with the following motivation \u201c[visual Turing test and Inception Score] do not indicate if the generator collapses to a particular mode of the data distribution\u201d. [[DAT-NEG,EXP-NEG,MET-NEG], [NOV-NEG,EMP-NEG], [DFT], [MIN]]The mode collapse issue is never discussed elsewhere in the paper. \[[RWK-NEG,PDI-NEG,EXP-NEG], [SUB-NEG,EMP-NEG], [DFT,CRT,FBK], [MIN]]n\n2. Only two datasets were considered, both extremely simple: generating MNIST digits is nearly a toy task nowadays.[[DAT-POS,MET-POS], [NOV-POS,EMP-POS], [APC], [MAJ]] Different works on GANs make use of CIFAR-10 and SVHN, since they entail more variability: those two could be a good start. [[RWK-POS,MET-POS], [SUB-POS,EMP-POS], [APC], [MAJ]]\n\n3. The authors should clarify if the method is specifically designed for GANs and VAEs[[RWK-NEG,MET-NEG], [CLA-NEG,CMP-NEG,EMP-NEG], [DFT,FBK], [MIN]]. If not, section 2.1 should contain several other works (as in Table 1). \n\n4[[RWK-NEG], [SUB-NEG], [DFT], [MIN]]. One of the main statements of the paper \u201cOur approach imposes a high entropy on P(Y) and gives unbiased indicator about entropy of both P(Y|X) and P(X|Y)\u201d is never proved, nor discussed.\[[RWK-NEG,PDI-NEG,EXP-NEG], [SUB-NEG,EMP-NEG], [DFT], [MIN]]n\n5. Equation 2 (the proposed metric) is not convincing: taking the maximum over tau implies training many models with different fractions of generated data, which is expensive. [[RWK-NEG,DAT-NEG,EXP-NEG,MET-NEG,ANA-NEG], [SUB-NEG,IMP-NEG,CMP-NEG,EMP-NEG], [DFT], [MIN]]Further, how many tau\u2019s one should evaluate?[[RWK-NEU,EXP-NEU], [EMP-NEU], [QSN], [GEN]] In order to evaluate a generative model one should test on the generated data only (tau=1) I believe.[[PDI-NEG,DAT-NEG,EXP-NEG,MET-NEG], [SUB-NEG,EMP-NEG], [SUG,DFT], [MIN]] In the worst case, the generator experiences mode collapse and performs badly.[[PDI-NEG,EXP-NEG,ANA-NEG], [IMP-NEG,EMP-NEG], [DFT], [MIN]] Differently, it can memorize the training data and performs as good as the baseline model. [[PDI-POS,DAT-POS,EXP-POS,MET-POS], [SUB-POS,EMP-POS], [APC], [MAJ]]If it does actual data augmentation, it should perform better[[PDI-POS,DAT-POS,EXP-POS,ANA-POS], [IMP-POS,EMP-POS], [APC], [MAJ]].\n\n6. The protocol of section 3 looks inconsistent with the aim of the work, which is to evaluate data augmentation capability of generative models.[[RWK-NEG,PDI-NEG,MET-NEU,ANA-NEU], [SUB-NEG,EMP-NEU], [DFT], [MIN]] In fact, the limit of training with a fixed dataset is that the model \u2018sees\u2019 the data multiple times across epochs with the risk of memorizing[[RWK-NEU,DAT-NEU,EXP-NEU,MET-NEU], [SUB-NEU,EMP-NEU], [SMY,DIS], [GEN]]. In the proposed protocol, the model \u2018sees\u2019 the generated data D_gen (which is fixed before training) multiple time across epochs. [[PDI-NEU,DAT-NEU,EXP-NEU,MET-NEU,ANA-NEU], [SUB-NEU,EMP-NEU], [SUG,DIS], [GEN]]This clearly does not allow to fully evaluate the capability of the generative model to generate newer and newer samples with significant variability.[[PDI-NEG,MET-NEU], [IMP-NEU], [SMY,DIS], [GEN]]\n\n\nMinor: \nSection 2.2 might be more readable it divided in two (exploitation and evaluation).[[RWK-POS,EXP-NEU,ANA-NEU], [IMP-POS,EMP-NEU], [APC], [MAJ]]   \n"