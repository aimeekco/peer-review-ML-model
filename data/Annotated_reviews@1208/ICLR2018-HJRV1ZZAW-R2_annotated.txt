"The paper proposes a simple dilated convolutional network as drop-in replacements for recurrent networks in reading comprehension tasks.[[PDI-NEU], [null], [SMY], [GEN]] The first advantage of the proposed model is short response time due to parallelism of non-sequential output generation, proved by experiments on the SQuAD dataset.[[DAT-POS,EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]] The second advantage is its potentially better representation, proved by better results compared to models using recurrent networks on the TriviaQA dataset.[[DAT-POS,MET-POS], [CMP-POS,EMP-POS], [APC], [MAJ]]\n\nThe idea of using dilated convolutional networks as drop-in replacements for recurrent networks should have more value than just reading comprehension tasks.[[MET-NEU], [EMP-NEU], [SUG], [MAJ]] The paper should stress on this a bit more.[[MET-NEU], [CNT], [SUG], [MAJ]] The paper also lacks discussion with other models that use dilated convolution in different ways, such as WaveNet[1].[[MET-NEG], [CMP-NEG], [CRT], [MAJ]]\n\nIn general, the proposed model has novelty.[[MET-POS], [NOV-POS], [APC], [MAJ]] The experimental results also sufficiently demonstrate the proposed advantages of the model.[[EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]] Therefore I recommend acceptance for it.[[OAL-POS], [REC-POS], [FBK], [MAJ]]\n\n[1] Oord, Aaron van den, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. \"Wavenet: A generative model for raw audio.\" arXiv preprint arXiv:1609.03499 (2016)."[[BIB-NEU], [null], [DIS], [GEN]]