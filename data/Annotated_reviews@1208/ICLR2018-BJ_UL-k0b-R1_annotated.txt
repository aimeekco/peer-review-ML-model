"Summary\nThe paper presents an interesting view on the recently proposed MAML formulation of meta-learning (Finn et al).[[INT-NEU,PDI-POS], [EMP-POS], [APC], [MAJ]] The main contribution is a) insight into the connection between the MAML procedure and MAP estimation in an equivalent linear hierarchical Bayes model with explicit priors,[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] b) insight into the connection between MAML and MAP estimation in non-linear HB models with implicit priors,[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] c) based on these insights, the paper proposes a variant of MALM using a Laplace approximation (with additional approximations for the covariance matrix.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] The paper finally provides an evaluation on the mini ImageNet problem without significantly improving on the MAML results on the same task.[[DAT-POS,MET-NEU,RES-POS], [EMP-POS], [APC], [MAJ]]\n\nPro:\n-            The topic is timely and of relevance to the ICLR community continuing a current trend in building meta-learning system for few-shot learning.[[OAL-POS], [APR-POS], [APC], [MAJ]]\n-            Provides valuable insight into the MAML objective and its relation to probabilistic models\n\n[[MET-POS], [EMP-POS], [APC], [MAJ]]Con:\n-            The paper is generally well-written [[OAL-POS], [CLA-POS], [APC], [MAJ]]but I find (as a non-meta-learner expert) that certain fundamental aspects could have been explained better or in more detail (see below for details).[[ANA-NEU], [SUB-NEU], [DIS], [MIN]]\n-            The toy example is quite difficult to interpret the first time around and does not provide any empirical insight into the converge of the proposed method (compared to e.g. MAML)[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n-            I do not think the empirical results provide enough evidence that it is a useful/robust method.[[MET-NEG,RES-NEG], [SUB-NEG], [DFT,CRT], [MAJ]] Especially it does not provide insight into which types of problems (small/large, linear/ non-linear) the method is applicable to.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] \n\n\nDetailed comments/questions:\n-            The use of Laplace approximation is (in the paper) motivated from a probabilistic/Bayes and uncertainty point-of-view.[[MET-NEU], [null], [DIS], [GEN]] It would, however, seem that the truncated iterations do not result in the approximation being very accurate during optimization as the truncation does not result in the approximation being created at a mode.[[RES-NEG], [EMP-NEG], [CRT], [MIN]] Could the authors perhaps comment on:\na) whether it is even meaningful to talk about the approximations as probabilistic distribution during the optimization (given the psd approximation to the Hessian), or does it only make sense after convergence? [[MET-NEU], [EMP-NEU], [QSN], [MIN]]\nb) the consequence of the approximation errors on the general convergence of the proposed method (consistency and rate)\n\n-[[MET-NEU], [null], [DIS], [MIN]]            Sec 4.1, p5: Last equation: Perhaps useful to explain the term $log(\\phi_j^* | \\theta)$ and why it is not in subroutine 4 .[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Should $\\phi^*$  be $\\hat \\phi$ ?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n-            Sec 4.2: \u201cA straightforward\u2026\u201d: I think it would improve readability to refer back to the to the previous equation (i.e. H) such that it is clear what is meant by \u201cstraightforward\u201d.\n-[[MET-NEG], [CLA-NEG], [SUG], [MAJ]]            Sec 4.2: Several ideas are being discussed in Sec 4.2 and it is not entirely clear to me what has actually been adopted here; perhaps consider formalizing the actual computations in Subroutine 4 \u2013 and provide a clearer argument (preferably proof) that this leads to consistent and robust estimator of \\theta.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n-            It is not clear from the text or experiment how the learning parameters are set.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]]\n-            Sec 5.1: It took some effort to understand exactly what was going on in the example and particular figure 5.1; e.g., in the model definition in the body text there is no mention of the NN mentioned/used in figure 5, the blue points are not defined in the caption, the terminology e.g.  \u201cpre-update density\u201d is new at this point.[[MET-NEG,ANA-NEG,TNF-NEG], [EMP-NEG], [CRT], [MIN]] I think it would benefit the readability to provide the reader with a bit more guidance.[[OAL-NEG], [CLA-NEG], [SUG], [MIN]]\n-            Sec 5.1: While the qualitative example is useful (with a bit more text), I believe it would have been more convincing with a quantitative example to demonstrate e.g. the convergence of the proposal compared to std MAML and possibly compare to a std Bayesian inference method from the HB formulation of the problem (in the linear case)[[MET-NEG], [CMP-NEG], [CRT], [MIN]]\n-            Sec 5.2: The abstract clams increased performance over MAML but the empirical results do not seem to be significantly better than MAML ?[[ABS-NEG,RES-NEG], [EMP-NEG], [QSN], [MAJ]] I find it quite difficult to support the specific claim in the abstract from the results without adding a comment about the significance.[[ABS-NEG,RES-NEG], [EMP-NEG], [QSN], [MAJ]]\n-            Sec 5.2: The authors have left out \u201cMishral et al\u201d from the comparison due to the model being significantly larger than others.[[RWK-NEG,MET-NEG], [CMP-NEG], [CRT], [MIN]] Could the authors provide insight into why they did not use the ResNet structure from the  tcml paper in their L-MLMA scheme ?[[MET-NEU], [SUB-NEU], [QSN], [MIN]]\n-            Sec 6+7: The paper clearly states that it is not the aim to (generally) formulate the MAML as a HB.[[ABS-NEG,RES-NEG], [EMP-NEG], [CRT], [MIN]] Given the advancement in gradient based inference for HB the last couple of years (e.g. variational, nested laplace , expectation propagation etc) for explicit models, could the authors perhaps indicate why they believe their approach of looking directly to the MAML objective is more scalable/useful than trying to formulate the same or similar objective in an explicit HB model and using established inference methods from that area ?[[RWK-NEU,MET-NEU], [CMP-NEU], [QSN], [MIN]]\n\nMinor:\n-            Sec 4.1 \u201c\u2026each integral in the sum in (2)\u2026\u201d eq 2 is a product\n"[[MET-NEG], [EMP-NEG], [CRT], [MIN]]