"Summary: This paper addresses the problem of training deep fully connected networks (FCNs) by introducing a class-dependent learning rate to each of the network weights.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The method works by introducing a parameter associated with each weight to scale its learning rate based on the class label.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] The introduced parameters are not involved in the forward pass and hence cannot be updated by Backprop.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] Therefore, the authors derived the update formulate based on the analytical continuation technique.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] The authors experimented on the UCI data sets with different activation functions to show the efficacy of their proposed method.[[DAT-NEU,EXP-NEU,MET-NEU], [null], [SMY], [GEN]]  The idea of using class label to adjust each weight\u2019s learning rate is interesting and somewhat novel,[[PDI-POS], [NOV-POS,EMP-POS], [APC], [MAj]] but unfortunately the efficacy is not well justified in theory.[[CNT], [CNT], [CRT], [MAJ]] The empirical study is not always convincing, and did not compare with many state-of-the-art baselines.[[RWK-NEU], [CMP-NEG], [CRT], [MAJ]] Overall,  the study is interesting and contains some new idea.[[PDI-POS], [NOV-POS,EMP-POS], [APC], [MAJ]] However, the work is not mature enough for publication.[[OAL-NEG], [REC-NEG], [FBK], [MAJ]] \n\nOriginality:  Training very deep networks have always been important in Deep Learning and the idea of using class label to adjust each weight\u2019s learning rate is somewhat novel.[[PDI-POS,EXP-POS], [NOV-POS], [APC], [MAJ]] However, the paper falls short in lacking of theoretical justification and convincing empirical results.[[RES-NEG,OAL-NEG], [SUB-NEG,EMP-NEG], [CRT], [MAJ]] \n\nClarity:  The paper is clearly presented and easy to follow.[[OAL-POS], [CLA-POS], [APC], [MAJ]] However, many related works are missing in the literature, for example, Highway Networks [1],  Deeply-Supervised Nets [2] and Deep Networks with Stochastic Depth [3], etc.[[RWK-NEG], [SUB-NEG], [DFT], [MAJ]]\n\nSignificance:  The paper lacks of theoretical justification as well as the experiments are not convincing.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]] Specifically, the paper lacks of justification on why adjusting the learning rate based on the class labels are crucial to improve training FCNs, more specifically, how does it help resolving the exploding and vanishing gradient problems?[[EXP-NEG,MET-NEG], [SUB-NEG], [DFT], [MAJ]] \n\nThe trick to reset $\\mu$ after half an epoch at the end of Section 3 is too heuristic.[[CNT], [null], [CRT], [MIN]] There lacks of explanation.[[CNT], [null], [CRT], [MIN]] \n\nThe experimental results are not convincing.[[EXP-NEG,RES-NEG], [EMP-NEG], [CRT], [MIN]] The proposed method doesn\u2019t seem always outperforming the baselines.[[RWK-NEG,MET-NEG], [CMP-NEG], [CRT], [MAJ]] There is no discussion on the failure cases.[[RWK-NEG,MET-NEG], [SUB-NEG], [DFT], [MAJ]]  In addition, the authors should compare with more baselines such as [1], [2], [3] and try with deeper networks as many networks used in the experiments are not very deep, with only 8 layers or less.[[RWK-NEG,EXP-NEG], [SUB-NEG,CMP-NEG], [CRT], [MAJ]] Besides, the idea of using adaptive learning rates are not completely new, and somewhat closely related to second order optimization methods.[[MET-NEG], [NOV-NEG], [CRT], [MAJ]] It will be interesting to compare with some existing second-order optimization algorithms for deep learning.[[MET-NEU], [CMP-NEU], [SUG], [GEN]] Last but not least, it would be more convincing to show the convergence speed of the proposed method.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] \n\nother question: In Eqn.4-5 , the terms $O(\\alpha)$ and $O(\\alpha^2)$ are omitted, however, since $\\mu_w^j$ are updated with its own learning rate, would it be better to increase the learning rate $\\alpha_{\\mu}$ and use the term $O(\\alpha^2)$ in the gradient update (6) as it would better approximate the gradient direction?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nReferences\n\n[1] Srivastava, Rupesh K., Klaus Greff, and J\u00fcrgen Schmidhuber.[[BIB-NEU], [null], [DIS], [GEN]] \"Training very deep networks.\" Advances in neural information processing systems.[[BIB-NEU], [null], [DIS], [GEN]] 2015.\n\n[2] Lee, Chen-Yu, et al.[[BIB-NEU], [null], [DIS], [GEN]] \"Deeply-supervised nets.\" Artificial Intelligence and Statistics. 2015.[[BIB-NEU], [null], [DIS], [GEN]]\n\n[3] Huang, Gao, et al. \"Deep networks with stochastic depth.\" European Conference on Computer Vision. Springer International Publishing, 2016.\n"[[BIB-NEU], [null], [DIS], [GEN]]