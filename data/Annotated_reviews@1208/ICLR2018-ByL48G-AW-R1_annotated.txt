"SUMMARY\nThe paper deal with the problem of RL.[[PDI-NEU], [null], [SMY], [GEN]]  It proposes a non-parametric approach that maps trajectories to the optimal policy.[[INT-NEU], [null], [SMY], [GEN]]  It avoids learning parameterized policies.[[MET-NEU], [null], [SMY], [GEN]]  The fundamental idea is to store passed trajectories.  When a policy is to be executed, it does nearest neighbor search to find then closest trajectory and executes it.[[PDI-NEU], [null], [SMY], [GEN]]\n\nCOMMENTS\n\nWhat happens if the agent finds it self  in a state that while is close to a state in the similar trajectory the action required to could be completely different.[[MET-NEU], [EMP-NEU], [QSN], [MAJ]]\n\nNot certain about the claim that standard RL policy learning algorithms make it difficult to assess the difficulty of a problem.[[RWK-NEU,MET-NEU], [EMP-NEU], [DFT], [MAJ]] \n\nHow do you execute a trajectory?[[MET-NEU], [null], [QSN], [MAJ]] Actions in RL are by definition stochastic, and this would make it unlikely that a same trajectory can be reproduced exactly.[[MET-NEU], [EMP-NEG], [DFT], [MAJ]]\n"