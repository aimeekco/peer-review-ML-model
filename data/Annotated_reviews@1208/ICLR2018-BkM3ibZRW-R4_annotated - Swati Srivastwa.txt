"The authors present a new variation of autoencoder, in which they jointly train (1) a discrete-space autoencoder to minimize reconstuction loss, and (2) a simpler continuous-space generator function to learn a distribution for the codes, and (3) a GAN formulation to constrain the distributions in the latent space to be similar.[[INT-NEU,MET-NEU], [null], [SMY], [GEN]] \n\nThe paper is very clearly written, very clearly presented, addresses an important issue, and the results are solid.[[PDI-POS,RES-POS], [CLA-POS,PNF-POS,EMP-POS], [APC], [MAJ]] \n\nMy primary suggestion is that I would like to know a lot more (even qualitatively, does not need to be extensively documented runs) about how sensitive the results were--- and in what ways were they sensitive--- to various hyperparameters.[[RES-NEU,ANA-NEU], [SUB-NEU], [DIS], [MIN]]Currently, the authors mention in the conclusion that, as is known to often be the case with GANS, that the results were indeed sensitive.[[RES-NEU], [null], [SMY], [GEN]] More info on this throughout the paper would be a valuable contribution. [[RES-NEU,ANA-NEU], [SUB-NEU], [SUG], [MIN]]Clearly the authors were able to make it work, with good results.[[RES-POS], [EMP-POS], [APC], [MAJ]] When does it not work?[[RES-NEU], [EMP-NEU], [QSN], [MIN]] Any observations about how it breaks down?[[RES-NEU], [EMP-NEU], [QSN], [MIN]]\n\nIt is interesting how strong the denoising effect is, as simply a byproduct of the adversarial regularization.[[RES-POS], [EMP-POS], [APC], [MIN]]\n\nSome of the results are quite entertaining indeed.[[RES-POS], [EMP-POS], [APC], [MAJ]] I found the yelp transfer results particularly impressive.[[RES-POS], [EMP-POS], [APC], [MAJ]]\n\n(The transfer from positive->negative on an ambiguous example was interesting: Original \"service is good but not quick\" -> \"service is good but not quick, but the service is horrible\", and \"service is good, and horrible, is the same and worst time ever\".[[RES-POS], [EMP-POS], [APC], [MIN]] I found it interesting to see what it does with the mixed signals of the word \"but\": on one hand, keeping it helps preserve the structure of the sentence, but on the other hand, keeping it makes it hard to flip the valence.[[RES-POS], [CLA-POS], [APC], [MAJ]] I guess the most accurate opposite would have been \"The service is quick but not good\"... )[[RES-NEU], [CLA-NEU], [DIS], [MIN]]\n\nI really like the reverse perplexity measure.[[MET-POS], [EMP-POS], [APC], [MAJ]] Also, it was interesting how that was found to be high on AAE due to mode-collapse.[[MET-POS,RES-POS], [EMP-POS], [APC], [MIN]]\n\nBeyond that, I only have a list of very insignificant typos:\n-p3, end of S3, \"this term correspond to minimizing\"\n-p3, S4, \"to approximate Wasserstein-1 term\" --> \"to approximate the Wasserstein-1 term\"[[CNT], [CLA-NEG], [CRT], [MIN]]\n-Figure 1, caption \"which is similarly decoded to $\\mathbf{\\~x}$\" .[[TNF-NEG], [CLA-NEG], [CRT], [MIN]] I would say that it is \"similarly decoded to $\\mathbf{c}$\", since it is \\mathbf{c} that gets decoded.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Unless the authors meant that it \"is similarly decoded to produce $\\mathbf{\\~x}$.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Alternately, I would just say something like \"to produce a code vector, which lies in the same space as \\mathbf{c}\", since the decoding of the generated code vector does not seem to be particularly relevant right here.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\n-p5, beginning of Section 6.1:  \"to regularize the model produce\" --> \"to regularize the model to produce\" ?[[CNT], [CLA-NEU], [QSN], [MIN]]\n-p6, end of first par. \"is quite high for the ARAE than in the case\" --> quite a bit higher than?[[CNT], [CLA-NEU], [QSN], [MIN]] etc...\n-p7, near the bottom \"shown in figure 6\". --> table, not figure...\n-p8  \"ability mimic\" -->\"ability to mimic[[TNF-NEG], [CLA-NEG], [CRT], [MIN]]\"\n-p9 Fig 3 -- the caption is mismatched with the figure.. top/bottom/left/right/etc.... Something is confusing there...[[TNF-NEG], [PNF-NEG], [CRT], [MIN]]\n-p9 near the bottom \"The model learns a improved\" --> \"The model learns an improved\"[[MET-NEG], [CLA-NEG], [CRT], [MIN]]\n-p14 left side, 4th cell up, \"Cross-AE\"-->\"ARAE\"[[CNT], [CLA-NEG], [CRT], [MIN]]\n\nThis is a very nice paper with a clear idea (regularize discrete autoencoder using a flexible rather than a fixed prior), that makes good sense and is very clearly presented.[[PDI-POS,OAL-POS], [CLA-POS,PNF-POS], [APC], [MAJ]] \n\nIn the words of one of the paper's own examples: \"It has a great atmosphere, with wonderful service.\" :)\nStill, I wouldn't mind knowing a little more about what happened in the kitchen...\n\n"[[ANA-NEU], [SUB-NEU], [DIS], [MIN]]