"Eigenoption Discovery Through the Deep Successor Representation[[INT-NEU], [null], [SMY], [GEN]]\n\nThe paper is a follow up on previous work by Machado et al. (2017) showing how proto-value functions (PVFs) can be used to define options called \u201ceigenoptions\u201d.[[RWK-NEU,PDI-NEU], [null], [SMY], [GEN]] In essence, Machado et al. (2017) showed that, in the tabular case, if you interpret the difference between PVFs as pseudo-rewards you end up with useful options.[[PDI-NEU], [null], [SMY], [GEN]] They also showed how to extend this idea to the linear case: one replaces the Laplacian normally used to build PVFs with a matrix formed by sampling differences phi(s') - phi(s), where phi are features.[[RWK-NEU,PDI-NEU], [null], [SMY], [GEN]] The authors of the current submission extend the approach above in two ways: they show how to deal with stochastic dynamics and how to replace a linear model with a nonlinear one.[[MET-NEU], [null], [SMY], [GEN]] Interestingly, the way they do so is through the successor representation (SR).[[PDI-NEU], [null], [SMY], [GEN]] Stachenfeld et al. (2014) have showed that PVFs can be obtained as a linear transformation of the eigenvectors of the matrix formed by stacking all SRs of an MDP.[[RWK-NEU], [null], [SMY], [GEN]] Thus, if we have the SR matrix we can replace the Laplacian mentioned above.[[RWK-NEU], [null], [DIS], [GEN]] This provides benefits already in the tabular case, since SRs naturally extend to domains with stochastic dynamics. [[MET-NEU], [null], [DIS], [GEN]]On top of that, one can apply a trick similar to the one used in the linear case --that is,  construct the matrix representing the diffusion model by simply stacking samples of the SRs.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Thus, if we can learn the SRs, we can extend the proposed approach to the nonlinear case[[MET-NEU], [null], [SMY], [GEN]]. The authors propose to do so by having a deep neural network similar to Kulkarni et al. (2016)'s Deep Successor Representation.[[RWK-NEU,MET-NEU], [null], [SMY], [GEN]] The main difference is that, instead of using an auto-encoder, they learn features phi(s) such that the next state s' can be recovered from it (they argue that this way psi(s) will retain information about aspects of the environment the agent has control over).[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\nThis is a well-written paper with interesting (and potentially useful) insights.[[OAL-POS], [EMP-POS], [APC], [MAJ]] I only have a few comments regarding some aspects of the paper that could perhaps be improved, such as the way eigenoptions are evaluated.[[MET-NEG,OAL-NEG], [EMP-NEG], [CRT], [MIN]]\n\nOne question left open by the paper is the strategy used to collect data in order to compute the diffusion model (and thus the options).[[DAT-NEU], [CNT], [QSN], [MIN]] In order to populate the matrix that will eventually give rise to the PVFs the agent must collect transitions.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] The way the authors propose to do it is to have the agent follow a random policy.[[MET-NEU], [EMP-NEU], [SMY], [GEN]] So, in order to have options that lead to more direct, \"purposeful\" behaviour, the agent must first wander around in a random, purposeless, way, and hope that this will lead to a reasonable exploration of the state space.[[MET-POS], [EMP-POS], [APC], [MAJ]] \n\nThis problem is not specific to the proposed approach, though: in fact, any method to build options will have to resolve the same issue.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] One related point that is perhaps more specific to this particular work is the strategy used to evaluate the options built: the diffusion time, or the expected number of steps between any two states of an MDP when following a random walk. [[MET-NEU], [EMP-NEU], [DIS], [MIN]]First, although this metric makes intuitive sense, it is unclear to me how much it reflects control performance, which is what we ultimately care about.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Perhaps more important, measuring performance using the same policy used to build the options (the random policy) seems somewhat unsatisfactory to me.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] To see why, suppose that the options were constructed based on data collected by a non-random policy that only visits a subspace of the state space.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] In this case it seems likely that the decrease in the diffusion time would not be as apparent as in the experiments of the paper.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Conversely, if the diffusion time were measured under another policy, it also seems likely that options built with a random policy would not perform so well (assuming that the state space is reasonably large to make an exhaustive exploration infeasible).[[MET-NEU], [EMP-NEU], [DIS], [MIN]] More generally, we want options built under a given policy to reduce the diffusion time of other policies (preferably ones that lead to good control performance).[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\nAnother point associated with the evaluation of the proposed approach is the method used to qualitatively assess options in the Atari experiments described in Section 4.2.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]] In the last paragraph of page 7 the authors mention that eigenoptions are more effective in reducing the diffusion time than \u201crandom options\u201d built based on randomly selected sub-goals. However, looking at Figure 4, the terminal states of the eigenoptions look a bit like randomly-selected  sub-goals.[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]] This is especially true when we note that only a subset of the options are shown: given enough random options, it should be possible to select a subset of them that are reasonably spread across the state space as well.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \n\nInterestingly, one aspect of the proposed approach that seems to indeed be an improvement over random options is made visible by a strategy used by the authors to circumvent computational constraints. As explained in the second paragraph of page 8, instead of learning policies to maximize the pseudo-rewards associated with eigenoptions the authors used a myopic policy that only looks one step ahead (which is the same as having a policy learned with a discount factor of zero). [[CNT], [EMP-NEU], [DIS], [MIN]]The fact that these myopic policies are able to navigate to specific locations and stay there suggests that the proposed approach gives rise to dense pseudo-rewards that are very informative.[[MET-POS], [EMP-POS], [APC], [MAJ]] As a comparison, when we define a random sub-goal the resulting reward is a very sparse signal that would almost certainly not give rise to useful myopic policies.[[RWK-POS,RES-POS], [CMP-POS], [APC], [MAJ]] Therefore, one could argue that the proposed approach not only generate useful options, it also gives rise to dense pseudo-rewards that make it easier to build the policies associated with them."[[MET-POS], [EMP-POS], [APC], [MAJ]]