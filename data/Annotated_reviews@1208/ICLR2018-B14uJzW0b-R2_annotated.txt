"In this paper the authors studied the theoretical properties of manifold descent approaches in a standard regression problem, whose regressor is a simple neural network.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] Leveraged by two recent results in global optimization, they showed that with a simple two-layer ReLU network with two hidden units, the problem with a standard MSE population loss function does not have spurious local minimum points.[[MET-NEU,RES-NEU], [null], [SMY], [GEN]] Based on the results by Lee et al, which shows that first order methods converge to local minimum solution (instead of saddle points), it can be concluded that the global minima of this problem can be found by any manifold descent techniques, including standard gradient descent methods.[[RWK-POS,MET-POS,RES-POS], [CMP-POS,EMP-POS], [APC], [MAJ]] In general I found this paper clearly written and technically sound.[[OAL-POS], [CLA-POS,EMP-POS], [APC], [MAJ]] I also appreciate the effort of developing theoretical results for deep learning, even though the current results are restrictive to very simple NN architectures.[[RES-POS], [EMP-POS], [APC], [MAJ]] \n\nContribution: \nAs discussed in the literature review section, apart from previous results that studied the theoretical convergence properties for problems that involves a single hidden unit NN, this paper extends the convergence results to problems that involves NN with two hidden units.[[RWK-NEU,RES-POS], [CMP-NEU], [DIS], [MIN]] The analysis becomes considerably more complicated,[[ANA-NEG], [EMP-NEG], [CRT], [MIN]] and the contribution seems to be novel and significant.[[OAL-POS], [NOV-POS], [APC], [MAJ]] I am not sure why did the authors mentioned the work on over-parameterization though.[[RES-NEG], [EMP-NEG], [CRT], [MIN]] It doesn't seem to be relevant to the results of this paper (because the NN architecture proposed in this paper is rather small).[[RES-NEG], [EMP-NEG], [CRT], [MIN]] \n\nComments on the Assumptions:\n- Please explain the motivation behind the standard Gaussian assumption of the input vector x.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \n- Please also provide more motivations regarding the assumption of the orthogonality of weights: w_1^\\top w_2=0 (or the acute angle assumption in Section 6).[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \nWithout extra justifications, it seems that the theoretical result only holds for an artificial problem setting.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] While the ReLU activation is very common in NN architecture, without more motivations I am not sure what are the impacts of these results.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] \n\nGeneral Comment:  \nThe technical section is quite lengthy, and unfortunately I am not available to go over every single detail of the proofs.[[EXP-NEG,MET-NEG], [SUB-NEG], [CRT], [MAJ]] From the analysis in the main paper, I believe the theoretical contribution is correct and sound.[[ANA-POS], [EMP-POS], [APC], [MAJ]] While I appreciate the technical contributions,[[ANA-POS], [EMP-POS], [APC], [MAJ]] in order to improve the readability of this paper, it would be great to see more motivations of the problem studied in this paper (even with simple examples).[[ANA-NEG], [SUB-NEG], [DFT], [MIN]] Furthermore, it is important to discuss the technical assumptions on the 1) standard Gaussianity of the input vector,[[ANA-NEU], [SUB-NEU,EMP-NEU], [DIS], [MIN]] and 2) the orthogonality of the weights (and the acute angle assumption in Section 6) on top of the discussions in Section 8.1, as they are critical to the derivations of the main theorems. "[[MET-NEU,ANA-NEU], [SUB-NEU,EMP-NEU], [DIS], [MIN]]