"The paper analyzed the composition abilities of Recurrent Neural Networks.[[PDI-NEU], [null], [SMY], [GEN]]  The authors analyzed the the generalization for the following scenarios\n\n- the generalization ability of RNNs on random subset of SCAN commands[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]]\n- the generalization ability of RNNs on longer SCAN commands[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]]\n- The generalization ability of composition over primitive commands.[[PDI-NEU], [null], [SMY], [GEN]]\n\nThe experiments supported the hypothesis that the RNNs are able to \n\n- generalize zero-shot to new commands.[[EXP-NEU], [EMP-NEU], [DIS], [MIN]] \n- difficulty generalizing to longer sequence (compared to training sequences) of commands.[[EXP-NEU], [null], [SMY], [GEN]]\n- the ability of the model generalizing to composition of primitive commands seem to depend heavily on the whether the action is seen during training.[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]] The model does not seem to generalize to completely new action and commands (like Jump),[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]  however, seems to generalize much better for Turn Left, since it has seen the action during training (even though not the particular commands)[[EXP-NEU,MET-POS], [EMP-POS], [APC], [MAJ]] \n\nOverall, the paper is well written and easy to follow.[[OAL-POS], [CLA-POS], [APC], [MAJ]]  The experiments are complete.[[EXP-POS], [EMP-POS], [APC], [MAJ]]  The results and analysis are informative.[[ANA-POS,RES-POS], [EMP-POS], [APC], [MAJ]] \n\nAs for future work, I think an interesting direction would also be to investigate the composition abilities for RNNs with latent (stochastic) variables.[[FWK-NEU], [IMP-NEU], [SUG], [MAJ]]  For example, analyzing whether the latent stochastic variables may shown to actually help with generalization of composition of primitive commands. \n\n "[[FWK-NEU], [IMP-NEU], [SUG], [MAJ]]