"The paper considers a problem of adversarial examples applied to the deep neural networks.[[PDI-NEU], [null], [SMY], [GEN]] The authors conjecture that the intrinsic dimensionality of the local neighbourhood of adversarial examples significantly differs from the one of normal (or noisy) examples.[[PDI-NEU], [null], [SMY], [GEN]] More precisely, the adversarial examples are expected to have intrinsic dimensionality much higher than the normal points (see Section 4).[[PDI-NEU], [null], [SMY], [GEN]]  Based on this observation they propose to use the intrinsic dimensionality as a way to separate adversarial examples from the normal (and noisy) ones during the test time.[[PDI-NEU], [null], [SMY], [GEN]] In other words, the paper proposes a particular approach for the adversarial defence.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]]\n\nIt turns out that there is a well-studied concept in the literature capturing the desired intrinsic dimensionality: it is called the local intrinsic dimensionality (LID, Definition 1) .[[MET-NEU], [null], [DIS], [GEN]] Moreover, there is a known empirical estimator of LID, based on the k-nearest neighbours.[[MET-NEU], [null], [DIS], [GEN]] The authors propose to use this estimator in computing the intrinsic dimensionalities for the test time examples.[[EXP-NEU,MET-NEU], [null], [DIS], [GEN]] For every test-time example X the resulting Algorithm 1 computes LID estimates of X activations computed for all intermediate layer of DNN.[[MET-NEU], [null], [DIS], [GEN]]These values are finally used as features in classifying adversarial examples from normal and noisy ones.[[RES-NEU], [null], [DIS], [GEN]] \n\nThe authors empirically evaluate the proposed technique across multiple state-of-the art adversarial attacks, 3 datasets (MNIST, CIFAR10, and SVHN) and compare their novel adversarial detection technique to 2 other ones recently reported in the literature.[[RWK-NEU,DAT-NEU,MET-NEU], [null], [DIS], [GEN]] The experiments support the conjecture mentioned above and show that the proposed technique *significantly* improves the detection accuracy compared to 2 other methods across all attacks and datasets (see Table 1).[[DAT-POS,EXP-POS,MET-POS,TNF-POS], [EMP-POS], [APC], [MAJ]]\n\nInterestingly, the authors also test whether adversarial attacks can bypass LID-based detection methods by incorporating LID in their design.[[MET-POS], [EMP-POS], [APC], [MAJ]] Preliminary results show that even in this case the proposed method manages to detect adversarial examples most of the time.[[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]]In other words, the proposed technique is rather stable and can not be easily exploited.[[MET-POS], [EMP-POS], [APC], [MAJ]]\n\nI really enjoyed reading this paper.[[OAL-POS], [CNT], [APC], [MAJ]] All the statements are very clear, the structure is transparent and easy to follow.[[OAL-POS], [CLA-POS,PNF-POS], [APC], [MAJ]] The writing is excellent.[[OAL-POS], [CLA-POS], [APC], [MAJ]] I found only one typo (page 8, \"We also NOTE that...\"),[[OAL-NEG], [CLA-NEG], [CRT], [MIN]] otherwise I don't actually have any comments on the text.[[OAL-NEU], [CLA-NEU], [DIS], [MIN]]\n\nUnfortunately, I am not an expert in the particular field of adversarial examples, and can not properly assess the conceptual novelty of the proposed method.[[EXT-NEU], [null], [DIS], [GEN]] However, it seems that it is indeed novel and given rather convincing empirical justifications, I would recommend to accept the paper. \n"[[OAL-POS], [NOV-POS,REC-POS], [FBK], [MAJ]]