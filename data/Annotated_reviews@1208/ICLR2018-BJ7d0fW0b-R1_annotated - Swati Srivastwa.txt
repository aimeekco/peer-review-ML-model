"SIGNIFICANCE AND ORIGINALITY:\n\nThe authors propose to accelerate the learning of complex tasks by exploiting traces of experts.[[PDI-NEU], [NOV-NEU], [DIS], [GEN]]\nUnlike the most common form of imitation learning or behavioral cloning, the authors \nformulate their solution in the case where the expert\u2019s state trajectory is observable, \nbut the expert\u2019s actions are not.[[PDI-NEU], [NOV-NEU], [DIS], [GEN]] This is an important and useful problem in robotics and other\napplications.[[PDI-POS], [EMP-POS], [APC], [MAJ]] Within this specific setting the authors differentiate their approach from others \nby developing a solution that does NOT estimate an explicit dynamics model ( e.g.,  P( S\u2019 | S, A ) ).[[MET-NEU], [CMP-NEU], [SMY], [GEN]]\nThe benefits of not estimating an explicit action model are not really demonstrated in a clear way.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MIN]]\n\nThe author\u2019s articulate a specific solution that provides heuristic guidance rewards that cause the \nlearner to favor actions that achieve subgoals calculated from expert behavior\nand refactors the representation of the Q function so that it \nhas a component that is a function of the subgoal extracted from the expert.[[MET-POS,RES-POS], [NOV-POS], [APC], [MAJ]]\nThese subgoals are linear functions of the expert\u2019s change in state (or change in state features).[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\nThe resultant policy is a function of the expert traces on which it depends.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n The authors show they can retrain a new policy that does not require the expert traces.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\nAs far as I am aware, this is a novel approach to the problem.[[MET-POS], [NOV-POS], [APC], [MAJ]] \nThe authors claim that this factorization is important and useful but the paper doesn\u2019t\nreally illustrate this well.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n\nThey demonstrate the usefulness of the algorithm against a DQN baseline on Doom game problems.[[MET-NEU], [null], [DIS], [GEN]]\nThe algorithm learns faster than unassisted DQN as shown by learning curve plots.[[MET-NEU], [null], [DIS], [GEN]] \nThey also evaluate the algorithms on the quality of the final policies for their approach, DQN, \nand  a supervised learning from demonstration approach ( LfD ) that requires expert actions.[[MET-NEU], [null], [DIS], [GEN]]\nThe proposed approach does as well or better than competing approaches.[[MET-POS], [CMP-POS], [APC], [MAJ]]\n\n\nQUALITY\n\nAblation studies show that the guidance rewards are important to achieving the improved performance of the proposed method which is important confirmation that the architecture is working in the intended way.[[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]] However, it would also be useful to do an ablation study of the \u201cfactorization\u201d of action values. [[ANA-NEU], [null], [SUG], [MIN]] Is this important to achieving better results as well or is the guidance reward enough? [[RES-NEU], [EMP-NEU], [QSN], [MIN]]This seems like a key claim to establish.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\n\nCLARITY\n\nThe details of the memory based kernel density estimation and neural gradient training seemed\ncomplicated by the way that the process was implemented.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] Is it possible to communicate\nthe intuitions behind what is going on?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n \nI was able to work out the intuitions behind the heuristic rewards, but I still don\u2019t clearly get \nwhat the Q-value factorization is providing:[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nTo keep my text readable, I assume we are working in feature space\ninstead of state space and use different letters for learner and expert:[[MET-NEU], [null], [DIS], [GEN]]\n\n   Learner: S = \\phi(s) \n   Expert\u2019s i^th state visit:  Ei = \\phi( \\hat{s}_i }  where Ei\u2019 is the successor state to Ei[[MET-NEU], [null], [DIS], [GEN]]\n\nThe paper builds upon approximate n-step discrete-action Q-learning \nwhere the Q value for an action is a linear function of the state features:\n\n    Qp(S,a) = Wa S + Ba\n\nwhere parameters p = ( Wa, Ba ).[[MET-NEU], [null], [DIS], [GEN]]\n\nAfter observing an experience ( S,A,R,S\u2019 ) we use Bellman Error as a loss function to optimize Qp for parameter p.[[MET-NEU], [null], [DIS], [GEN]]\nI ignore the complexities of n-step learning and discount factors for clarity.[[MET-NEU], [null], [DIS], [GEN]]\n\n    Loss = E[    R + MAXa\u2019 Qp(S\u2019,a\u2019)    -   Qp(S,a)   ]  \n\nThe authors suggest we can augment the environment reward R \nwith a heuristic reward Rh proportional to the similarity between \nthe learner \u201csubgoal\" and the expert \u201csubgoal\" in similar states.[[MET-NEU], [null], [DIS], [MIN]] \n\nThe authors propose to use cosine distance between representations \nof what they call the \u201csubgoals\u201d of learner and expert.[[MET-NEU], [null], [DIS], [MIN]] \nA subgoal is defined as a linear transformation of the distance traveled by an agent during a transition.[[MET-NEU], [null], [DIS], [GEN]]\nThe heuristic reward is proportional to the cosine distance between the learner and expert \u201csubgoals\"\n\n   Rh = B  <   Wv LearnerDirectionInStateS,  [[MET-NEU], [null], [DIS], [GEN]] \n                     Wv ExpectedExpertDirectionInStatesSimilarToS   >\n\nThe learner\u2019s direction in state S is just (S-S\u2019) in feature space.\n\nThe authors model the behavior of the expert as a kernel density type approximator\ngiving the expected direction of the expert starting from a states similar to the one the learner is in.[[MET-NEU], [null], [DIS], [GEN]] \nLet < Wk S, Wk Ej > be a weighted similarity between learner state features S and expert state features Ej\nand Ej\u2019 be the successor state features encountered by the expert.[[MET-NEU], [null], [DIS], [GEN]]\nThen the expected expert direction for learner state S is:\n\n     SUMj  < Wk S, Wk Ej > ( Ej - Ej\u2019 )[[MET-NEU], [null], [DIS], [GEN]] \n\nPresumably the linear Wk transform helps us pick out the important dimensions of similarity between S and Ej.\n\nMapping the learner and expert directions into subgoal space using Wv, the heuristic reward is\n\n   Rh = B <   Wv (S-S\u2019),  \n                    Wv SUMj  < Wk S, Wk Ej > ( Ej - Ej\u2019 ) >[[MET-NEU], [null], [DIS], [GEN]]\n\nI ignore the ReLU here, but I assume that is operates element-wise and just clips negative values?[[MET-NEU], [null], [QSN], [GEN]]\nThere is only one layer here so we don\u2019t have complex non-linear things going on?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nIn addition to introducing a heuristic reward term, the authors propose to alter the Q-function\nto be specific to the subgoal.[[MET-NEU], [null], [DIS], [GEN]]\n\n   Q( s,a,g ) = g(S) Wa S + Ba\n\nThe subgoal is the same as the first part, namely a linear transform of the expected expert direction in \nstates similar to state S.\n\n    g(S) =  Wv   SUMj  < Wk S, Wk Ej >  ( Ej - Ej\u2019 ) \n\nSo in some sense, the Q function is really just a function of S, as g is calculated from S.[[MET-NEU], [null], [DIS], [GEN]]\n\n    Q( S,a ) = g(S) Wa S + Ba \n\nSo this allows the Q-function more flexibility to capture each subgoal in a different linear space?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\nI don\u2019t really get the intuition behind this formulation.[[MET-NEU], [null], [DIS], [GEN]] It allows the subgoal to adjust the value \nof the underlying model?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]] Essentially the expert defines a new Q-value problem at every state \nfor the learner?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]]  In some sense are we are defining a model for the action taken by the expert?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]] \n\n\nADDITIONAL THOUGHTS\n\nWhile the authors compare to an unassisted baseline, they don\u2019t compare to methods that use an action model\nwhich is not a fatal flaw but would have been nice.[[RWK-NEG,MET-NEG], [CMP-NEU,EMP-NEG], [CRT], [MIN]]  \n\nOne can imagine there might be scenarios where the local guidance rewards of this \nform could be problematic, particularly in scenarios where the expert and learner are not identical\nand it is possible to return to previous states, such as the grid worlds the authors discuss:[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]] \nIf the expert\u2019s first few transitions were easily approximable,\nthe learner would get local rewards that cause it to mimic expert behavior.[[EXP-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]] \nHowever, if the next step in the expert\u2019s path was difficult to approximate, \nthen the reward for imitating the expert would be lower.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \nWould the learner then just prefer to go back towards those states that it can approximate and endlessly loop?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]] \nIn this case, perhaps expressing heuristic rewards as potentials as described in Ng\u2019s shaping paper might solve the problem.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] \n\n\nPROS AND CONS\n\nImportant problem generally.[[PDI-POS], [EMP-POS], [APC], [MAJ]]  Avoiding the estimation of a dynamics model was stated as a given, but perhaps more could be put into motivating this goal.[[MET-NEU], [EMP-NEU], [SUG], [MIN]]  Hopefully it is possible to streamline the methodology section to communicate the intuitions more easily.\n"[[MET-NEU], [PNF-NEU], [SUG], [MAJ]] 