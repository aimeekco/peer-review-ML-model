"The authors propose to improve the robustness of trained neural networks against adversarial examples by randomly zeroing out weights/activations.[[PDI-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] Empirically the authors demonstrate, on two different task domains, that one can trade off some accuracy for a little robustness -- qualitatively speaking.[[PDI-NEU,EXP-NEU,MET-NEU,ANA-NEU], [EMP-NEU], [SMY], [GEN]]\n\nOn one hand, the approach is simple to implement and has minimal impact computationally on pre-trained networks.[[PDI-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] On the other hand, I find it lacking in terms of theoretical support, other than the fact that the added stochasticity induces a certain amount of robustness.[[PDI-NEG], [SUB-NEG,EMP-NEG], [DFT,DIS], [MIN]] For example, how does this compare to random perturbation (say, zero-mean) of the weights?[[PDI-NEU], [CMP-NEU], [QSN], [GEN]] This adds stochasticity as well so why and why not this work? [[RWK-NEU], [EMP-NEU], [QSN], [GEN]]The authors do not give any insight in this regard.[[RWK-NEG,EXP-NEG,MET-NEG], [SUB-NEG,EMP-NEG], [DFT], [MIN]]\n\nOverall, I still recommend acceptance (weakly) since the empirical results may be valuable to a general practitioner.[[RES-POS,OAL-NEU], [REC-NEU], [APC], [MAJ]] The paper could be strengthened by addressing the issues above as well as including more empirical results (if nothing else).[[OAL-NEU], [null], [SUG], [GEN]]\n\n"