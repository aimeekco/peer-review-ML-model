"\nSummary:\n- The paper proposes a new activation function that looks similar to ELU but much cheaper by using the inverse square root function.[[RWK-NEU,PDI-NEU,MET-NEU], [CMP-NEU,EMP-NEU], [SMY], [GEN]]\n\nContributions:\n- The paper proposes a cheaper activation and validates it with an MNIST experiment.[[PDI-POS,EXP-POS], [EMP-POS], [APC], [MAJ]] The paper also shows major speedup compared to ELU and TANH (unit-wise speedup)[[RWK-NEU,MET-NEU], [CMP-NEU,EMP-NEU], [SMY], [GEN]].\n\nPros:\n- The proposed function has similar behavior as ELU but 4x cheaper.[[PDI-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]]\n- The authors also refer us to faster ways to compute square root functions numerically, which can be of general interests to the community for efficient network designs in the future.[[RWK-NEU,MET-NEU,FWK-NEU], [NOV-NEU,IMP-NEU,EMP-NEU], [SMY,FBK], [GEN]]\n- The paper is clearly written and key contributions are well present.[[INT-POS], [CLA-POS,NOV-POS,SUB-POS], [APC], [MAJ]]\n\nCons:\n- Clearly, the proposed function is not faster than ReLU.[[PDI-NEG,MET-NEG,RES-NEG], [IMP-NEG,EMP-NEG], [DFT,FBK], [MIN]] In the introduction, the authors explain the motivation that ReLU needs centered activation (such as BN).[[INT-NEU,PDI-NEU], [EMP-NEU], [SMY], [GEN]] But the authors also need to justify that ISRLU (or ELU) doesn\u2019t need BN[[RWK-NEG,MET-NEG], [CLA-NEG,EMP-NEG], [DIS], [GEN]]. In fact, in a recent study of ELU-ResNet (Shah et al., 2016) finds that ELU without BN leads to gradient explosion.[[RWK-NEG,MET-NEG,BIB-NEG], [IMP-NEG,EMP-NEG], [DFT], [MIN]] To my knowledge, BN (at least in training time) is much more expensive than the activation function itself, so the speedup get from ISRLU may be killed by using BN in deeper networks on larger benchmarks.[[EXT-NEU], [null], [FBK], [GEN]] At inference time, all of ReLU, ELU, and ISRLU can fuse BN weights into convolution weights, so again ISRLU will not be faster than ReLU.[[RWK-NEG,PDI-NEG,MET-NEG], [CMP-NEG,EMP-NEG], [DFT], [MIN]] The core question here is, whether the smoothness and centered zero property of ELU can buy us any win, compared to ReLU?[[RWK-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [GEN]] I couldn\u2019t find it based on the results presented here[[RWK-NEU,RES-NEU], [null], [SMY], [GEN]].\n- The authors need to validate on larger datasets (e.g. CIFAR, if not ImageNet) so that their proposed methods can be widely adopted.[[RWK-NEU,DAT-NEU,EXP-NEU,MET-NEU], [EMP-NEU], [SUG], [MIN]]\n- The speedup is only measured on CPU.[[RWK-NEU,MET-NEU,ANA-NEU], [EMP-NEU], [SMY], [GEN]] For practical usage, especially in computer vision, GPU speedup is needed to show an impact[[RWK-NEU,EXP-NEG], [IMP-NEG], [DFT], [MIN]].\n\nConclusion:\n- Based on the comments above, I recommend weak reject.[[RWK-NEG], [REC-NEG], [DFT,FBK], [MIN]]\n\nReferences:\n- Shah, A., Shinde, S., Kadam, E., Shah, H., Shingade, S.. Deep Residual Networks with Exponential Linear Unit.[[BIB-NEU], [null], [CNT], [CNT]] In Proceedings of the Third International Symposium on Computer Vision and the Internet (VisionNet'16)."[[RWK-NEU], [null], [SMY], [GEN]]