"This paper describes the use of latent context-free derivations, using\na CRF-style neural model, as a latent level of representation in neural\nattention models that consider pairs of sentences.[[INT-NEU], [null], [SMY], [GEN]] The model implicitly\nlearns a distribution over derivations, and uses marginals under this\ndistribution to bias attention distributions over spans in one sentence\ngiven a span in another sentence.[[MET-NEU], [null], [SMY], [GEN]]\n\nThis is an intriguing idea[[PDI-POS], [null], [APC], [MAJ]]. I had a couple of reservations however:\n\n* The empirical improvements from the method seem pretty marginal, to the\npoint that it's difficult to know what is really helping the model.[[MET-NEU,ANA-NEU], [CMP-NEU,EMP-NEU], [DFT], [MAJ]] I would\nliked to have seen more explanation of what the model has learned, and\nmore comparisons to other baselines that make use of attention over spans.[[RWK-NEU,MET-NEU], [SUB-NEG], [DFT], [MAJ]]\nFor example, what happens if every span is considered as an independent random\nvariable, with no use of a tree structure or the CKY chart?[[MET-NEU], [EMP-NEU], [QSN], [MAJ]]\n\n* The use of the \\alpha^0 vs. \\alpha^1 variables is not entirely clear.[[MET-NEU], [PNF-NEG], [DFT], [MIN]] Once they\nhave been calculated in Algorithm 1, how are they used?[[MET-NEU], [null], [QSN], [MIN]] Do the \\rho values\nsomewhere treat these two quantities differently?[[MET-NEU], [null], [QSN], [MIN]]\n\n* I'm skeptical of the type of qualitative analysis in section 4.3, unfortunately.[[MET-NEU], [EMP-NEG], [DFT], [MAJ]]\nI think something much more extensive would be interesting here. As one\nexample, the PP attachment example with \"at a large venue\" is highly suspect;\nthere's a 50/50 chance that any attachment like this will be correct, there's\nabsolutely no way of knowing if the model is doing something interesting/correct\nor performing at a chance level, given a single example. "[[EXP-NEU,MET-NEG], [EMP-NEG], [DFT], [MAJ]]