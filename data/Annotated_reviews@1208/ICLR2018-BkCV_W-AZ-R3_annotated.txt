"Quality and clarity:\n\nThe paper provides a game-theoretic inspired variant of policy-gradient algorithm based on the idea of counter-factual regret minimization.[[INT-NEU], [CLA-NEU], [SMY], [GEN]] The paper claims that the approach can deal with the partial observable domain better than the standard methods.[[MET-NEU], [null], [SMY], [GEN]] However the results only show that the algorithm converges, in some cases, faster than the previous work  reaching asymptotically to a same or worse performance.[[RWK-NEG,MET-NEG], [CMP-NEG,EMP-NEG], [CRT], [MAJ]] Whereas one would expect that the algorithm achieve a better asymptotic performance in compare to methods which are designed for fully observable domains and thus performs sub-optimally in the POMDPs.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MIN]] \n\nThe paper dives into the literature of counter-factual regret minimization without providing much intuition on why this type of ideas should provide improvement in the case of partial observable domain.[[MET-NEG], [SUB-NEG], [SUG,DFT], [MIN]] To me it is not clear at all why this idea should help in the partial observable domains beside the argument that this method is designed in the game-theoretic settings   which makes no Markov assumption .[[PDI-NEG,MET-NEG], [EMP-NEG], [CRT], [MIN]]  The way that I interpret this algorithm is that by adding A+ to the return the algorithm  introduces some bias for actions which are likely to be optimal so it is in some sense implements the optimism in the face of uncertainty principle.[[MET-NEU], [null], [DIS], [MIN]]  This may explains why this algorithm converges faster than the baseline as it produces better exploration strategy.[[RWK-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]]  To me it is not clear that the boost comes from the fact that the algorithm deals with partial observability more efficiently.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] \n\n\nOriginality and Significance:\n\nThe proposed algorithm seems original.[[MET-POS], [NOV-POS], [APC], [MAJ]]  However,  as it is acknowledged by the authors this type of optimistic policy gradient algorithms have been previously used in RL (though maybe not with the game theoretic justification).[[MET-NEU], [CMP-NEG], [DIS], [MIN]]  I believe the algorithm introduced  in this paper, if it is presented well, can be  an interesting addition to the literature of Deep RL, e.g.,  in terms of improving the rate of convergence.[[MET-POS,FWK-POS], [IMP-POS], [APC], [MAJ]]  However, the current version of paper  does not provide conclusive evidence for that as in most of the domains the algorithm only converge marginally faster than the standard ones.[[MET-NEG], [SUB-NEG,CMP-NEG], [DFT], [MIN]]  Given the fact that algorithms like dueling DQN and DDPG are   for the best asymptotic results and not  for the best convergence rate, this improvement  can be due to the choice of hyper parameter such as step size or epsilon decay scheduling.[[MET-NEU,RES-NEU], [EMP-NEU], [DIS], [MIN]]  More experiments over a range of hyper parameter is needed before one can conclude that this algorithm improves the rate of convergence.\n "[[MET-NEG], [SUB-NEG], [DFT], [MIN]] 