"I have read comments and rebuttal - i do not have the luxury of time to read in depth the revision.[[EXT-NEU], [null], [DIS], [MAJ]]\nIt seems that the authors have made an effort to accommodate reviewers' comments.[[EXT-POS], [null], [APC], [MAJ]] I upgraded the rating.[[OAL-POS], [REC-POS], [FBK], [MAJ]]\n\n-----------------------------------------------------------------------------------------------------------------------\n\nSummary: The paper considers the use of natural gradients for learning.[[INT-NEU], [null], [SMY], [GEN]] The added twist is the substitution of the KL divergence with the Wasserstein distance, as proposed in GAN training.[[RWK-NEU,MET-NEU], [null], [SMY], [GEN]] The authors suggest that Wasserstein regularization improves generalization over SGD with a little extra cost.[[MET-POS], [null], [SMY], [GEN]]\n\nThe paper is structured as follows:\n1. KL divergence is used as a similarity measure between two distributions.[[MET-NEU], [null], [SMY], [GEN]]\n2. Regularizing the objective with KL div. seems promising, but expensive.[[MET-POS], [null], [SMY], [MAJ]]\n3. We usually approximate the KL div. with its 2nd order approximation - this introduces the Hessian of the KL divergence, known as Fisher information matrix.[[MET-NEU], [null], [SMY], [MAJ]]\n4. However, computing and inverting the Fisher information matrix is computationally expensive.[[MET-NEU], [null], [CRT], [MAJ]]\n5. One solution is to approximate the solution F^{-1} J using gradient descent.[[MET-NEU], [null], [SMY], [MAJ]] However, still we need to calculate F.[[MET-NEU], [null], [DIS], [MAJ]] There are options where F could be formed as the outer product of a collection gradients of individual examples ('empirical Fisher').[[MET-NEU], [EMP-NEU], [DIS], [MAJ]]\n6. This paper does not move towards Fisher information, but towards Wasserstein distance: after a \"good\" initialization via SGD is obtained, the inner loop continues updating that point using the Wasserstein regularized objective.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] \n7. No large matrices need to be formed or inverted, however more passes needed per outer step.[[EXP-NEU,MET-NEU], [EMP-NEU], [SUG], [MAJ]]\n\nImportance:\nSomewhat lack of originality and poor experiments lead to low importance.[[EXP-NEG], [NOV-NEG], [CRT], [MAJ]]\n\nClarity:\nThe paper needs major revision w.r.t. presenting and highlighting the new main points.[[OAL-NEG], [CLA-NEG,PNF-NEG], [CRT], [MAJ]] E.g., one needs to get to page 5 to understand that the paper is just based on the WGAN ideas in Arjovsky et al., but with a different application (not GANS).[[RWK-NEU,PDI-NEU], [CLA-NEG,PNF-NEG], [CRT], [MAJ]]\n\nOriginality/Novelty:\nThe paper, based on WGAN motivation, proposes Wasserstein distance regularization over KL div. regularization for training of simple models, such as neural networks.[[RWK-NEU,MET-NEU], [EMP-NEU], [SMY], [MAJ]] Beyond this, the paper does not provide any futher original idea.[[MET-NEU], [NOV-NEU], [CRT], [MAJ]] So, slight to no novelty.[[MET-NEU], [NOV-NEG], [CRT], [MAJ]]\n\nMain comments:\n1. Would the approximation of C_0 by its second-order Taylor expansion (that also introduces a Hessian) help?[[MET-NEU], [EMP-NEU], [QSN], [MAJ]] This would require the combination of two Hessian matrices.[[MET-NEU], [EMP-NEU], [SUG], [MAJ]]\n\n2. Experiments are really demotivating: it is not clear whether using plain SGD or the proposed method leads to better results.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]] \n\nOverall:\nRejection.[[OAL-NEG], [REC-NEG], [FBK], [MAJ]]\n"