"The paper proposes to learn a custom translation or rotation invariant kernel in the Fourier representation to maximize the margin of SVM.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]]  Instead of using Monte Carlo approximation as in the traditional random features literature, the main point of the paper is to learn these Fourier features in a min-max sense.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]]  This perspective leads to some interesting theoretical results and some new interpretation.[[RES-POS], [EMP-POS], [APC], [MAJ]]  Synthetic and some simple real-world experiments demonstrate the effectiveness of the algorithm compared to random features given the fix number of bases.[[EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]] \n\nI like the idea of trying to formulate the feature learning problem as a two-player min-max game and its connection to boosting.[[PDI-POS], [EMP-POS], [APC], [MAJ]] As for the related work, it seems the authors have missed some very relevant pieces of work in learning these Fourier features through gradient descent [1, 2].[[RWK-NEG], [SUB-NEG], [DFT], [MIN]] It would be interesting to compare these algorithms as well.[[RWK-NEU,MET-NEU], [SUB-NEU], [SUG], [MIN]]\n\n[1] Zichao Yang, Marcin Moczulski, Misha Denil, Nando de Freitas, Alex Smola, Le Song, Ziyu Wang. Deep Fried Convnets. ICCV 2015.[[RWK-NEU,BIB-NEU], [null], [DIS], [MIN]]\n[2] Zichao Yang, Alexander J. Smola, Le Song, Andrew Gordon Wilson. A la Carte \u2014 Learning Fast Kernels. AISTATS 2015."[[RWK-NEU,BIB-NEU], [null], [DIS], [MIN]]