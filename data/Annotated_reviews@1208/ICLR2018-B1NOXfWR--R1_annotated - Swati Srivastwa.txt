"In the context of multitask reinforcement learning, this paper considers the problem of learning behaviours when given specifications of subtasks and the relationship between them, in the form of a task graph.[[PDI-NEU], [null], [SMY], [GEN]] The paper presents a neural task graph solver (NTS), which encodes this as a recursive-reverse-recursive neural network.[[MET-NEU], [null], [SMY], [GEN]] A method for learning this is presented, and fine tuned with an actor-critic method.[[MET-NEU], [null], [SMY], [GEN]] The approach is evaluated in a multitask grid world domain.[[MET-NEU], [null], [SMY], [GEN]]\n\nThis paper addresses an important issue in scaling up reinforcement learning to large domains with complex interdependencies in subtasks.[[PDI-NEU], [null], [SMY], [GEN]] The method is novel, and the paper is generally well written.[[OAL-POS], [CLA-POS,NOV-POS], [APC], [MAJ]] I unfortunately have several issues with the paper in its current form, most importantly around the experimental comparisons.[[EXP-NEG], [CMP-NEG], [CRT], [MIN]]\n\nThe paper is severely weakened by not comparing experimentally to other learning (hierarchical) schemes, such as options or HAMs.[[EXP-NEG], [CMP-NEG], [CRT], [MAJ]] None of the comparisons in the paper feature any learning.[[EXP-NEG], [CMP-NEG], [CRT], [MIN]] Ideally, one should see the effect of learning with options (and not primitive actions) to fairly compare against the proposed framework.[[MET-NEU], [CMP-NEU], [DIS], [MIN]] At some level, I question whether the proposed framework is doing any more than just value function propagation at a task level, and these experiments would help resolve this.[[EXP-NEU,EXP-NEU], [EMP-NEU], [DIS], [MIN]]\n\nAdditionally, the example domain makes no sense.[[EXP-NEG], [EMP-NEG], [CRT], [MIN]] Rather use something more standard, with well-known baselines, such as the taxi domain.[[RWK-NEU], [EMP-NEU], [DIS], [MIN]]\n\nI would have liked to see a discussion in the related work comparing the proposed approach to the long history of reasoning with subtasks from the classical planning literature, notably HTNs.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MIN]]\n\nI found the description of the training of the method to be rather superficial, and I don't think it could be replicated from the paper in its current level of detail.[[EXP-NEG], [EMP-NEG], [CRT], [MIN]]\n\nThe approach raises the natural questions of where the tasks and the task graphs come from.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Some acknowledgement and discussion of this would be useful.[[EXP-NEU], [SUB-NEU], [SUG], [MIN]]\n\nThe legend in the middle of Fig 4 obscures the plot (admittedly not substantially).[[TNF-NEG], [PNF-NEG], [CRT], [MIN]]\n\nThere are also a number of grammatical errors in the paper, including the following non-exhaustive list:\n2: as well as how to do -> as well as how to do it[[OAL-NEG], [CLA-NEG], [CRT], [MIN]]\nFig 2 caption: through bottom-up -> through a bottom-up\n3: Let S be a set of state -> Let S be a set of states\n3: form of task graph -> form of a task graph[[TNF-NEG], [CLA-NEG], [CRT], [MIN]]\n3: In addtion -> In addition\n4: which is propagates -> which propagates\n5: investigated following -> investigated the following"[[CNT], [CMP-NEG], [CRT], [MIN]]