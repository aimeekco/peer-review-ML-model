"This paper proposes a method for multitask and few-shot learning by completing a performance matrix (which measures how well the classifier for task i performs on task j).[[PDI-NEU,MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\nThe matrix completion approach is based on robust PCA.[[MET-NEU], [null], [DIS], [MIN]] When used for multitask learning (MTL) with N tasks, the method has to first train one classifier for each task (and so train a total of N classifiers), and then evaluate the performance of each classifier on each and every task (and so involves N^2 testing rounds).[[EXP-NEU,MET-NEU], [null], [SMY], [GEN]] This can be computationally demanding.[[EXP-NEU,MET-NEU], [null], [DIS], [GEN]]\n\nThe key assumption in the paper is that task classifier i that performs well on task j means tasks i and j belong to the same cluster, and if task classifier i does not perform well on task j, then tasks i and j belong to different cluster. The proposed algorithm then uses these performance values to perform task clustering.[[MET-NEU], [null], [DIS], [MIN]] However, in MTL, we usually assume that there are not enough samples to learn each task, and so this performance matrix may not be reliable.[[MET-NEG], [EMP-NEG], [CRT], [MIN]]\n\nThere have been a number of MTL methods based on task clustering.[[MET-NEU], [null], [DIS], [GEN]] For example,\n[1] A convex formulation for learning task relationships in multi-task learning (UAI)[[MET-NEU], [null], [DIS], [GEN]]\n[2] A dirty model for multi-task learning (NIPS)[[MET-NEU], [null], [DIS], [GEN]]\n[3] Clustered multi-task learning: A convex formulation (NIPS)[[MET-NEU], [null], [DIS], [GEN]]\n[4] Convex multitask learning with flexible task clusters (ICML)[[MET-NEU], [null], [DIS], [GEN]]\n[5] Integrating low-rank and group-sparse structures for robust multi-task learning (KDD)\n[6] Learning incoherent sparse and low-rank patterns from multiple tasks (KDD)\nIn particular, [5] assumes that the combined weight matrix (for all the tasks) follows the robust PCA model.[[MET-NEU], [null], [DIS], [GEN]] This is thus very similar to the proposed method (which assumes that the performance matrix follows the robust PCA model). [[MET-NEU], [null], [DIS], [GEN]]However, a disadvantage of the proposed method is that it is a two-step approach (first perform task clustering, then re-learn the cluster weights), while [5] is not.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nFor few-shot learning, the authors mentioned that the \\alpha's are adaptable parameters but did not mention how they are adapted.[[MET-NEG], [SUB-NEG], [CRT], [MAJ]]\n\nExperimental results are not convincing.[[EXP-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]]\n- Comparison with existing clustered MTL methods mentioned above are missing.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n- As mentioned above, the proposed method can be computationally expensive (when used for MTL), but no timing results are reported.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n- As the authors mentioned in section 4.2, most of the tasks have a significant amount of training data (and single-task baselines achieve good results), and so this is not a good benchmark dataset for MTL."[[DAT-NEG], [EMp-NEG], [CRT], [MAJ]]