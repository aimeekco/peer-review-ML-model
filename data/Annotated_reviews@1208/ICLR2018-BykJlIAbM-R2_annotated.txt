"This paper describes how to use a set of sentences in the source language mapped to another set of sentences in the target sentences instead of using single sentence to sentence samples.[[INT-NEU], [null], [SMY], [GEN]] The paper claims superior results using the described method.[[MET-POS,RES-POS], [null], [SMY], [GEN]]\n\nOverall, there are a few problems with the paper.[[OAL-NEG], [null], [DFT], [GEN]] 1) The arguments for using clusters instead of single sentences are questionable.  [[MET-NEU], [EMP-NEU], [DFT], [MAJ]]The paper claims several times that MLE training for NMT faces over-training (or data sparsity) -- while that can be true depending on the corpus and model used, there are well-known remedies for that, for example regularization via dropout (almost everybody uses that).[[DAT-NEU,MET-NEU], [EMP-NEU], [DIS], [MAJ]] It is not clear why that is not used or at least compared to the method presented.[[DAT-NEU,MET-NEU], [EMP-NEG], [DFT], [MAJ]] 2) The writing of the paper is often unclear (and sometimes grammatically wrong, typos etc. but that aside), there are some made up words/concepts (What is 'Golden Centroid Augmentation\" or \"Model Centroid Augmentation\"?[[OAL-NEG], [CLA-NEG], [CRT], [MIN]] The reason for attention is not to better memorize input information, it is to be able to attend to certain regions in the input.[[MET-NEU], [CLA-NEG], [DFT], [MIN]] The reason to use RL is to focus on optimizing directly for BLEU score or other metrics instead of likelihood but not for improving on the train/test loss discrepancy.[[MET-NEU], [CLA-NEG], [DFT], [MIN]] There are lots more examples of unclear statements in this paper -- it should be heavily improved.[[OAL-NEG], [CLA-NEG], [DFT], [MIN]] 3) Section 3 and 4 are very hard/impossible to understand, it is not clear how the formulas help the reader to better understand the concept in any way.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] 5) The results presented in this paper given the complexity of the method are just not great -- for example, WMT en-de is 21.3 BLEU reported by you while much older papers report for example 24.67 BLEU (Google's Neural Machine Translation System) -- why not first try to get to state-of-the-art with already published methods and then try to improve on top of that? . [[RWK-NEG,MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] 6) Finally, what is missing most is simply why a much simpler method (just generate some data using a trained system and use that as additional training data, with details on how much etc.) -- is not directly compared to this very complicated looking method.[[DAT-NEU,EXP-NEG,MET-NEG], [EMP-NEG], [DFT], [MAJ]]\n"