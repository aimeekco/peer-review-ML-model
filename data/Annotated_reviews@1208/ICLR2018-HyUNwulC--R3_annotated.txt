"This paper abstracts two recently-proposed RNN variants into a family of RNNs called the Linear Surrogate RNNs which satisfy  Blelloch's criteria for parallelizable sequential computation.[[ABS-NEU,PDI-NEU,MET-NEU], [null], [SMY], [GEN]] The authors then propose an efficient parallel algorithm for this class of RNNs, which produces speedups over the existing implements of Quasi-RNN, SRU, and LSTM.[[PDI-POS,EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]] Apart from efficiency results, the paper also contributes a comparison of model convergence on a long-term dependency task due to (Hochreiter and Schmidhuber, 1997).[[RWK-POS,BIB-NEU], [IMP-POS,CMP-POS,EMP-POS], [APC], [MAJ]] A novel linearized version of the LSTM outperforms traditional LSTM on this long-term dependency task, and raises questions about whether RNNs and LSTMs truly need the nonlinear structure.[[RWK-NEU], [NOV-NEU], [SMY], [GEN]]\n\nThe paper is written very well, with explanation (as opposed to obfuscation) as the goal.[[RWK-POS,OAL-POS], [CLA-POS,IMP-POS], [APC], [MAJ]] Linear Surrogate RNNs is an important concept that is useful to understand RNN variants today, and potentially other future novel architectures.[[RWK-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]]\n\nThe paper provides argument and experimental evidence against the rotation used typically in RNNs. [[RWK-NEU], [EMP-NEU], [SMY], [GEN]]While this is an interesting insight, and worthy of further discussion, such a claim needs backing up with more large-scale experiments on real datasets.[[DAT-POS,OAL-POS], [IMP-POS], [APC], [MAJ]]\n\nWhile the experiments on toy tasks is clearly useful, the paper could be significantly improved by adding experiments on real tasks such as language modelling.[[RWK-POS,EXP-POS,MET-POS], [CLA-POS,IMP-POS,EMP-POS], [APC], [MAJ]]"
