"This paper proposes a device placement algorithm to place operations of tensorflow on devices.[[INT-NEU,MET-NEU], [EMP-NEU], [SMY], [GEN]] \n\nPros:\n\n1. It is a novel approach which trains the placement end to end[[PDI-POS,EXP-POS], [NOV-POS,EMP-POS], [APC], [MAJ]].\n2. The experiments are solid to demonstrate this method works very well[[RWK-POS,MET-POS], [IMP-POS], [APC], [MAJ]].\n3. The writing is easy to follow.[[RWK-POS], [CLA-POS], [APC], [MAJ]]\n4. This would be a very useful tool for the community if open sourced.[[RWK-POS,MET-POS], [IMP-POS,EMP-POS], [APC], [MAJ]]\n\nCons:\n\n1. It is not very clear in the paper whether the training happens for each model yielding separate agents, or a shared agent is trained and used for all kinds of models[[INT-NEG,PDI-NEG,EXP-NEG,MET-NEG], [CLA-NEG,EMP-NEG], [DFT], [MIN]]. The latter would be more exciting.[[INT-POS], [CLA-POS], [APC], [MAJ]] The adjacency matrix varies size for different graphs, so I guess a separate agent is trained for each graph?[[RWK-NEG,EXP-NEG], [EMP-NEG], [DFT], [MIN]] However, if the agent is not shared, why not just use integer to represent each operation in the graph, since overfitting would be more desirable in this case.[[RWK-NEG,PDI-NEG,DAT-NEG,EXP-NEG], [IMP-NEG,CMP-NEG,EMP-NEG], [DFT], [MIN]]\n2. Averaging the embedding is hard to understand especially for the output sizes and number of outputs.\[[PDI-NEG,DAT-NEG,MET-NEG,RES-NEG], [IMP-NEG,EMP-NEG], [DFT], [MIN]]n3. It is not clear how the adjacency information is used.\n"[[PDI-NEG,ANA-NEG], [CLA-NEG,IMP-NEG,CMP-NEG], [DFT], [MIN]]