"This paper proposes the use of an ensemble of regression SVM models to predict the performance curve of deep neural networks.[[INT-NEU], [null], [SMY], [GEN]] This can be used to determine which model should be trained (further).[[MET-NEU], [null], [SMY], [GEN]] The authors compare their method, named Sequential Regression Models (SRM) in the paper, to previously proposed methods such as BNN, LCE and LastSeenValue and claim that their method has higher accuracy and less time complexity than the others.[[MET-NEU], [null], [SMY], [GEN]] They also use SRM in combination with a neural network meta-modeling method and a hyperparameter optimization one and show that it can decrease the running time in these approaches to find the optimized parameters.[[MET-NEU], [null], [SMY], [GEN]]\n\nPros: The paper is proposing a simple yet effective method to predict accuracy.[[MET-POS], [EMP-POS], [APC], [MAJ]] Using SVM for regression in order to do accuracy curve prediction was for me an obvious approach, I was surprised to see that no one has attempted this before.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Using features sur as time-series (TS), Architecture Parameters (AP) and Hyperparameters (HP) is appropriate, and the study of the effect of these features on the performance has some value.[[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]] Joining SRM with MetaQNN is interesting as the method is a computation hog that can benefit from such refinement.[[MET-POS], [EMP-POS], [APC], [MAJ]] The overall structure of the paper is appropriate.[[OAL-POS], [PNF-POS], [APC], [MAJ]] The literature review seems to cover and categorize well the field.[[RWK-POS], [EMP-POS], [APC], [MAJ]]\n\nCons: I found the paper difficult to read.[[OAL-NEG], [CLA-NEG], [CRT], [MIN]] In particular, the SRM method, which is the core of the paper, is not described properly, I am not able to make sense of the description provided in Sec. 3.1.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] The paper is not talking about the weaknesses of the method at all.[[OAL-NEU], [EMP-NEU], [DIS], [GEN]] The practicability of the method can be controversial, the number of attempts require to build the (meta-)training set of runs can be huge and lead to something that would be much more costful that letting the runs going on for more iterations.[[MET-NEU], [EMP-NEU], [CRT], [MAJ]] \n\nQuestions:\n1. The approach of sequential regression SVM is not explained properly.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Nothing was given about the combination weights of the method.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] How is the ensemble of (1-T) training models trained to predict the f(T)?[[EXP-NEU], [EMP-NEU], [QSN], [MIN]]\n2.  SRM needs to gather training samples which are 100 accuracy curves for T-1 epochs.[[DAT-NEU,EXP-NEU], [EMP-NEU], [DIS], [MIN]] This is the big challenge of SRM because training different variations of a deep neural networks to T-1 epochs can be a very time consuming process.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MIN]] Therefore, SRM has huge preparing training dataset time complexity that is not mentioned in the paper.[[EXP-NEU], [EMP-NEU], [CRT], [MIN]] The other methods use only the first epochs of considered deep neural network to guess about its curve shape for epoch T.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] These methods are time consuming in prediction time.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] The authors compare only the prediction time of SRM with them which is really fast.[[MET-NEG], [EMP-NEG], [CRT], [MIN]] By the way still, SRM is interesting method if it can be trained once and then be used for different datasets without retraining. [[MET-POS], [EMP-POS], [APC], [MAJ]]Authors should show these results for SRM.[[RES-POS], [EMP-POS], [DIS], [MIN]] \n3. Discussing about the robustness of SRM for different depth is interesting and I suggest to prepare more results to show the robustness of SRM to violation of different hyperparameters.[[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]] \n4. There is no report of results on huge datasets like big Imagenet which takes a lot of time for deep training and we need automatic advance stopping algorithms to tune the hyper parameters of our model on it.[[DAT-NEG,RES-NEG], [SUB-NEG,EMP-NEG], [DFT,CRT], [MAJ]]\n5. In Table 2 and Figure 3 the results are reported with percentage of using the learning curve. [[TNF-NEU,RES-NEU], [EMP-NEU], [CRT], [MIN]]To be more informative they should be reported by number of epochs, in addition or not to percentage.[[MET-NEU], [null], [DIS], [GEN]]\n6. In section 4, the authors talk about estimating the model uncertainty in the stopping point and propose a way to estimate it.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] But we cannot find any experimental results that is related to the effectiveness of proposed method and considered assumptions.[[EXP-NEG,MET-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nThere are also some  typos.[[OAL-NEG], [CLA-NEG], [CRT], [MAJ]] In section 3.3 part Ablation Study on Features Sets, line 5, the sentence should be \u201cAp are more important than HP\u201d.\n"[[EXP-NEU,MET-NEU], [EMP-NEU], [SUG], [MIN]]