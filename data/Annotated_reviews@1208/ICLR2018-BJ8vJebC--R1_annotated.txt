"This paper investigates the impact of character-level noise on various flavours of neural machine translation.[[INT-NEU], [null], [SMY], [GEN]] It tests 4 different NMT systems with varying degrees and types of character awareness, including a novel meanChar system that uses averaged unigram character embeddings as word representations on the source side.[[PDI-NEU], [null], [SMY], [GEN]] The authors test these systems under a variety of noise conditions, including synthetic scrambling and keyboard replacements, as well as natural (human-made) errors found in other corpora and transplanted to the training and/or testing bitext via replacement tables.[[MET-NEU], [null], [SMY], [GEN]] They show that all NMT systems, whether BPE or character-based, degrade drastically in quality in the presence of both synthetic and natural noise, and that it is possible to train a system to be resistant to these types of noise by including them in the training data.[[PDI-NEU,DAT-NEU], [null], [SMY], [GEN]] Unfortunately, they are not able to show any types of synthetic noise helping address natural noise.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] However, they are able to show that a system trained on a mixture of error types is able to perform adequately on all types of noise.[[EXP-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nThis is a thorough exploration of a mostly under-studied problem.[[PDI-NEG], [EMP-NEG], [CRT], [MIN]] The paper is well-written and easy to follow.[[OAL-POS], [CLA-POS], [APC], [MAJ]] The authors do a good job of positioning their study with respect to related work on black-box adversarial techniques, but overall, by working on the topic of noisy input data at all, they are guaranteed novelty.[[RWK-POS,DAT-POS,MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]] The inclusion of so many character-based systems is very nice, but it is the inclusion of natural sources of noise that really makes the paper work.[[RES-POS], [EMP-POS], [APC], [MAJ]] Their transplanting of errors from other corpora is a good solution to the problem, and one likely to be built upon by others.[[DAT-POS,RES-POS], [EMP-POS], [APC], [MAJ]] In terms of negatives, it feels like this work is just starting to scratch the surface of noise in NMT.[[MET-NEG], [SUB-NEG], [CRT], [MIN]] The proposed meanChar architecture doesn\u2019t look like a particularly good approach to producing noise-resistant translation systems, and the alternative solution of training on data where noise has been introduced through replacement tables isn\u2019t extremely satisfying.[[DAT-POS,EXP-POS,MET-POS], [EMP-POS], [APC], [MAJ]] Furthermore, the use of these replacement tables means that even when the noise is natural, it\u2019s still kind of artificial.[[RES-NEU], [EMP-NEU], [DIS], [MIN]] Finally, this paper doesn\u2019t seem to be a perfect fit for ICLR, as it is mostly experimental with few technical contributions that are likely to be impactful; it feels like it might be more at home and have greater impact in a *ACL conference.[[EXP-NEG,FWK-NEG,OAL-NEG], [SUB-NEG,APR-NEG], [CRT], [MAJ]]\n\nRegarding the artificialness of their natural noise - obviously the only solution here is to find genuinely noisy parallel data, but even granting that such a resource does not yet exist, what is described here feels unnaturally artificial.[[RES-NEU], [EMP-NEU], [DIS], [MIN]] First of all, errors learned from the noisy data sources are constrained to exist within a word.[[RES-NEU], [EMP-NEU], [APC], [MIN]] This tilts the comparison in favour of architectures that retain word boundaries (such as the charCNN system here), while those systems may struggle with other sources of errors such as missing spaces between words.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Second, if I understand correctly, once an error is learned from the noisy data, it is applied uniformly and consistently throughout the training and/or test data.[[DAT-NEU,MET-NEU], [EMP-NEU], [DIS], [GEN]] This seems worse than estimating the frequency of the error and applying them stochastically (or trying to learn when an error is likely to occur).[[DAT-NEG,MET-NEG], [EMP-NEG], [CRT], [MIN]] I feel like these issues should at least be mentioned in the paper, so it is clear to the reader that there is work left to be done in evaluating the system on truly natural noise.\n\nAlso, it is somewhat jarring that only the charCNN approach is included in the experiments with noisy training data (Table 6).[[DAT-NEU,EXP-NEU], [EMP-NEU], [DIS], [GEN]] I realize that this is likely due to computational or time constraints, but it is worth providing some explanation in the text for why the experiments were conducted in this manner.[[EXP-NEU], [SUB-NEU], [SUG,DFT], [MIN]] On a related note, the line in the abstract stating that \u201c... a character convolutional neural network  is able to simultaneously learn representations robust to multiple kinds of noise\u201d implies that the other (non-charCNN) architectures could not learn these representations, when in reality, they simply weren\u2019t given the chance.[[ABS-NEG], [PNF-NEG], [CRT], [MIN]]\n\nSection 7.2 on the richness of natural noise is extremely interesting,[[CNT], [EMP-POS], [APC], [MAJ]] but maybe less so to an ICLR audience.[[OAL-NEG], [APR-NEG], [CRT], [MAJ]] From my perspective, it would be interesting to see that section expanded, or used as the basis for future work on improve architectures or training strategies.[[EXP-NEG,FWK-NEG], [SUB-NEG,IMP-NEG], [SUG], [MAJ]]\n\nI have only one small, specific suggestion: at the end of Section 3, consider deleting the last paragraph break, so there is one paragraph for each system (charCNN currently has two paragraphs).\n\n[edited for typos]"[[CNT], [CLA-NEG], [SUG], [MIN]]