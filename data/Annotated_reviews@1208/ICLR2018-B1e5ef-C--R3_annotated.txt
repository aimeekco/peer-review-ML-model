"My review reflects more from the compressive sensing perspective, instead that of deep learners.[[EXT-NEU], [null], [SMY], [GEN]]\n\nIn general, I find many of the observations in this paper interesting.[[RES-POS], [EMP-POS], [APC], [MAJ]] However, this paper is not strong enough as a theory paper; rather, the value lies perhaps in its fresh perspective.[[OAL-NEG], [EMP-NEG], [CRT], [MAJ]]\n\nThe paper studies text embeddings through the lens of compressive sensing theory.[[PDI-NEU], [null], [SMY], [GEN]] The authors proved that, for the proposed embedding scheme, certain LSTMs with random initialization are at least as good as the linear classifiers; the theorem is almost a direction application of the RIP of random Rademacher matrices.[[RWK-NEU,MET-NEU], [null], [SMY], [GEN]] Several simplifying assumptions are introduced, which rendered the implication of the main theorem vague,[[MET-NEG], [EMP-NEG], [CRT], [MIN]] but it can serve as a good start for the hardcore statistical learning-theoretical analysis to follow.[[ANA-POS,FWK-POS], [IMP-POS], [APC], [MAJ]]\n\nThe second contribution of the paper is the (empirical) observation that, in terms of sparse recovery of embedded words, the pretrained embeddings are better than random matrices, the latter being the main focus of compressive sensing theory.[[MET-POS], [EMP-POS], [APC], [MAJ]] Partial explanations are provided, again using results in compressive sensing theory.[[RES-NEU], [null], [DIS], [GEN]] In my personal opinion, the explanations are opaque and unsatisfactory.[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] An alternative route is suggested in my detailed review.[[EXT-NEU], [null], [SMY], [GEN]]\nFinally, extensive experiments are conducted and they are in accordance with the theory.[[EXP-POS], [null], [DIS], [MIN]]\n\nMy most criticism regarding this paper is the narrow scope on compressive sensing, and this really undermines the potential contribution in Section 5.[[MET-NEG,FWK-NEG], [IMP-NEG], [CRT], [MIN]]\n\nSpecifically, the authors considered only Basis Pursuit estimators for sparse recovery, and they used the RIP of design matrices as the main tool to argue what is explainable by compressive sensing and what is not.[[MET-NEG], [SUB-NEG], [CRT], [MIN]] This seems to be somewhat of a tunnel-visioning for me: There are a variety of estimators in sparse recovery problems, and there are much less restrictive conditions than RIP of the design matrices that guarantee perfect recovery.[[MET-NEG], [SUB-NEG], [CRT], [MIN]]\n\nIn particular, in Section 5, instead of invoking [Donoho&Tanner 2005], I believe that a more plausible approach is through [Chandrasekaran et al. 2012].[[RWK-NEG], [CMP-NEG], [SUG], [MIN]] There, a simple deterministic condition (the null space property) for successful recovery is proved.[[RWK-NEU], [null], [DIS], [GEN]] It would be of direct interest to check whether such condition holds for a pretrained embedding (say GloVe) given some BoWs.[[RWK-NEU], [null], [DIS], [GEN]] Furthermore, it is proved in the same paper that Restricted Strong Convexity (RSC) alone is enough to guarantee successful recovery; RIP is not required at all.[[RWK-NEG], [CMP-NEG], [DIS], [MIN]] While, as the authors argued in Section 5.2, it is easy to see that pretrained embeddings can never possess RIP, they do not rule out the possibility of RSC.[[RWK-NEU,MET-NEU], [CMP-NEU], [DIS], [MIN]]\n\nExactly the same comments above apply to many other common estimators (lasso, Dantzig selector, etc.) in compressive sensing which might be more tolerant to noise.[[RWK-NEU], [null], [DIS], [GEN]] \n\nSeveral minor comments:\n\n1. Please avoid the use of \u201cinformation theory\u201d, especially \u201cclassical information theory\u201d, in the current context.[[RWK-NEG], [EMP-NEG], [SUG], [GEN]]  These words should be reserved to studies of Channel Capacity/Source Coding `a la Shannon.[[RWK-NEG], [EMP-NEG], [SUG,CRT], [MIN]] I understand that in recent years people are expanding the realm of information theory, but as compressive sensing is a fascinating field that deserves its own name, there\u2019s no need to mention information theory here.[[FWK-NEG], [IMP-NEG], [SUG], [MIN]]\n\n2. In Theorem 4.1, please be specific about how the l2-regularization is chosen.[[MET-NEU], [null], [DIS], [MIN]]\n\n3. In Section 4.1, please briefly describe why you need to extend previous analysis to the Lipschitz case.[[RWK-NEG,ANA-NEG], [SUB-NEG], [DFT], [MIN]] I understood the necessity only through reading proofs.[[ANA-NEG], [null], [DIS], [MIN]]\n\n4. Can the authors briefly comment on the two assumptions in Section 4, especially the second one (on n- cooccurrence)? Is this practical?[[MET-NEU], [EMP-NEG], [QSN], [MIN]]\n\n5. Page 1, there is a typo in the sentence preceding [Radfors et al., 2017].[[RWK-NEG], [CLA-NEG], [CRT], [MIN]]\n\n6. Page 2, first paragraph of related work, the sentence \u201cOur method also closely related to ...\u201d is incomplete.[[RWK-NEG], [PNF-NEG], [CRT], [MIN]]\n\n7. Page 2, second paragraph of related work, \u201cPagliardini also introduceD a linear ...\u201d\n\n8.[[RWK-NEG], [PNF-NEG], [CRT], [MIN]] Page 9, conclusion, the beginning sentence of the second paragraph is erroneous.[[RWK-NEG], [CLA-NEG], [CRT], [MIN]]\n\n[1] Venkat Chandrasekaran, Benjamin Recht, Pablo A. Parrilo, Alan S. Willsky, \u201cThe Convex Geometry of Linear Inverse Problems\u201d, Foundations of Computational Mathematics, 2012."[[BIB-NEU], [null], [DIS], [GEN]]