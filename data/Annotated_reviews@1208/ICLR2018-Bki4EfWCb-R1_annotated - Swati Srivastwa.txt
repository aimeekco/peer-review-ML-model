"* EDIT: Increased score from 5 to 6 to reflect improvements made in the revision.[[OAL-NEU], [REC-NEU], [FBK], [MAJ]]\n\nThe authors break down the \"inference gap\" in VAEs (the slack in the variational lower bound) into two components: 1. the \"amortization gap\", measuring what part of the slack is due to amortizing inference using a neural net encoder, as compared to separate optimization per example.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] 2. the \"approximation gap\": the part of the slack due to using a restricted parametric form for the posterior approximation.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] They perform various experiments to analyze how these quantities depend on modeling decisions and data sets.[[DAT-NEU,EXP-NEU], [null], [SMY], [GEN]]\n\nBreaking down the inference gap into its components is an interesting idea and could potentially provide insights when analyzing VAE performance and for further improving VAEs.[[PDI-POS], [EMP-POS], [APC], [MAJ]] I enjoyed reading the paper,[[OAL-POS], [CNT], [APC], [MAJ]] but I think its contribution is on the small side for a conference paper.[[OAL-NEG], [APR-NEG], [CRT], [MAJ]] It would be a good workshop paper.[[OAL-NEU], [null], [DIS], [GEN]] The main limitation of the proposed method of analysis I think is that the two parts of the inference gap are not really separable: Because the VAE encoder is trained jointly with the decoder, the different limitations of the encoder and decoder all interact.[[MET-NEG,ANA-NEG], [EMP-NEG], [CRT], [MAJ]] E.g. one could imagine cases where jointly training the VAE encoder and decoder finds a local optimum where inference is perfect, but which is still much worse than the optimum that could be achieved if the encoder would have been more flexible.[[MET-NEG,ANA-NEG], [EMP-NEG], [CRT], [MAJ]] The authors do seem to realize this and they provide experiments examining this interaction.[[MET-NEG,ANA-NEG], [EMP-NEG], [CRT], [MAJ]] I think these experiments should be elaborated on.[[EXP-NEU,ANA-NEU], [SUB-NEU], [DIS], [MIN]] For example: What happens when the decoder is trained separately using more flexible inference (e.g. Hamiltonian MC) and the encoder is trained later?[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]] What happens when the encoder is optimized separately for each data point during training as well as testing?"[[EXP-NEU,MET-NEU], [EMP-NEU], [QSN], [MIN]]