"The paper tackles the problem of training predictive state recurrent neural networks (PSRNN), which \nuses large kernel ridge regression (KRR) problems as a subprimitive, and makes two main contributions:\n- the suggestion to use orthogonal random features (ORFs) in lieu of standard random fourier features (RFFs) to reduce the size of the KRR problems[[PDI-NEU], [null], [DIS], [GEN]]\n- a novel analysis of the risk of KRR using ORFs which shows that the risk of ORFs is no larger than that of using RFFs[[PDI-NEU,ANA-POS], [NOV-POS], [SMY,APC], [MAJ]]\n\nThe contribution to the practice of PSRNNs seems significant (to my non-expert eyes): when back-propagation through time is used, using ORFs to do the two-stage KRR training needed visibly outperforms using standard RFMs to do the KRR.[[EXP-POS,MET-POS], [CMP-POS,EMP-POS], [APC], [MAJ]] I would like the authors to have provided results on more than the current three datasets, as well as an explanation of how meaningful the MSEs are in each dataset (is a MSE of 0.2 meaningful for the Swimmer Dataset, for instance? the reader does not know apriori).[[DAT-NEU,EXP-NEU], [EMP-NEU], [SUG,QSN], [MAJ]] \n\nThe contribution in terms of the theory of using random features to perform kernel ridge regression is novel, and interesting.[[MET-POS], [NOV-POS,EMP-POS], [APC], [MAJ]] Specifically, the author argue that the moment-generating function for the pointwise kernel approximation error of ORF features grows slower than the moment-generating function for the pointwise kernel approximation error of RFM features, which implies that error bounds derived using the MGF of the RFM features will also hold for ORF features.[[EXP-NEU,MET-NEU], [null], [DIS], [GEN]] This is a weaker result than their claim that ORFs satisfy better error,[[RES-NEG], [EMP-NEG], [CRT], [MAJ]] but close enough to be of interest and certainly indicates that their method is principled.[[MET-POS,RES-NEU], [EMP-POS], [APC], [MAJ]] Unfortunately, the proof of this result is poorly written:\n- equation (20) takes a long time to parse --- more effort should be put into making this clear[[RES-NEG], [CLA-NEG], [CRT], [MAJ]]\n- give a reference for the expressions given for A(k,n) in 24 and 25\n- (27) and (28) should be explained in more detail.[[MET-NEG,ANA-NEG], [SUB-NEG], [CRt], [MAJ]]\nMy staying power was exhausted around equation 31.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] The proof should be broken up into several manageable lemmas instead of its current monolithic and taxing form. \n"[[MET-NEU], [EMP-NEU], [DIS], [MAJ]]