"The paper proposes an adaptation of existing Graph ConvNets and evaluates this formulation on a several existing benchmarks of the graph neural network community.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] In particular, a tree structured LSTM is taken and modified.[[MET-NEU], [null], [SMY], [GEN]]  The authors describe this as adapting it to general graphs, stacking, followed by adding edge gates and residuality.[[PDI-NEU,MET-NEU], [null], [SMY], [GEN]] \n\nMy biggest concern is novelty, as the modifications are minor.[[OAL-NEG], [NOV-NEG], [CRT], [MAJ]] In particular, the formulation can be seen in a different way.[[MET-NEU], [null], [DIS], [MIN]] As I see it, instead of adapting Tree LSTMs to arbitary graphs, it can be seen as taking the original formulation by Scarselli and replacing the RNN by a gated version, i.e. adding the known LSTM gates (input, output, forget gate).[[RWK-NEU,MET-NEU], [CMP-NEU], [SUG], [MIN]]  This is a minor modification.[[RWK-NEU,MET-NEU], [EMP-NEU], [SUG], [MIN]] Adding stacking and residuality are now standard operations in deep learning, and edge-gates have also already been introduced in the literature, as described in the paper.[[MET-NEU], [EMP-NEU], [DIS], [MIN]]\n\nA second concern is the presentation of the paper, which can be confusing at some points.[[OAL-NEG], [PNF-NEG], [CRT], [MIN]] A major example is the mathematical description of the methods.[[MET-NEG], [PNF-NEG], [CRT], [MIN]] When reading the description as given, one should actually infer that Graph ConvNets and Graph RNNs are the same thing, which can be seen by the fact that equations (1) and (6) are equivalent.[[MET-NEG,TNF-NEG], [PNF-NEG], [CRT], [MIN]] \n\nAnother example, after (2), the important point to raise is the difference to classical (sequential) RNNs, namely the fact that the dependence graph of the model is not a DAG anymore, which introduces cyclic dependencies.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \n\nGenerally, a clear introduction of the problem is also missing.[[INT-NEG,PDI-NEG], [PNF-NEG], [DFT,CRT], [MAJ]] What are the inputs, what are the outputs, what kind of problems should be solved?[[PDI-NEG], [PNF-NEG], [QSN], [MAJ]]  The update equations for the hidden states are given for all models, but how is the output calculated given the hidden states from variable numbers of nodes of an irregular graph?[[MET-NEU,RES-NEU], [EMP-NEU], [QSN], [MIN]] \n\nThe model has been evaluated on standard datasets with a performance, which seems to be on par, or a slight edge, which could probably be due to the newly introduced residuality.[[DAT-POS,MET-POS], [EMP-POS], [APC], [MAJ]] \n\nA couple of details :\n\n- the length of a graph is not defined.[[TNF-NEG], [PNF-NEG], [CRT], [MIN]]  The size of the set of nodes might be meant.[[TNF-NEG], [PNF-NEG], [SUG,CRT], [MIN]]\n\n- at the beginning of section 2.1 I do not understand the reference to word prediction and natural language processing.[[CNT], [PNF-NEG], [CRT], [MIN]] RNNs are not restricted to NLP and I think there is no need to introduce an application at this point.[[MET-NEG], [CNT], [SUG], [MIN]]\n\n- It is unclear what does the following sentence means: \"ConvNets are more pruned to deep networks than RNNs\"?[[MET-NEG], [PNF-NEG], [CRT], [MIN]]\n\n- What are \"heterogeneous graph domains\"?\n"[[TNF-NEG], [PNF-NEG], [CRT], [MIN]]