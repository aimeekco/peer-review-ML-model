"The main significance of this paper is to propose the task of generating the lead section of Wikipedia articles by viewing it as a multi-document summarization problem.[[INT-NEU,PDI-NEU], [IMP-NEU], [SMY], [MAJ]] Linked articles as well as the results of an external web search query are used as input documents, from which the Wikipedia lead section must be generated.[[PDI-NEU], [null], [SMY], [GEN]] Further preprocessing of the input articles is required, using simple heuristics to extract the most relevant sections to feed to a neural abstractive summarizer.[[MET-NEU], [null], [DIS], [MAJ]] A number of variants of attention mechanisms are compared, including the transofer-decoder, and a variant with memory-compressed attention in order to handle longer sequences.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] The outputs are evaluated by ROUGE-L and test perplexity.[[MET-NEU], [EMP-NEU], [SMY], [GEN]]  There is also a A-B testing setup by human evaluators to show that ROUGE-L rankings correspond to human preferences of systems, at least for large ROUGE differences.[[MET-NEU], [EMP-NEU], [SMY], [GEN]]\n\nThis paper is quite original and clearly written.[[OAL-POS], [CLA-POS,NOV-POS], [APC], [MAJ]] The main strength is in the task setup with the dataset and the proposed input sources for generating Wikipedia articles.[[DAT-NEU,MET-NEU,RES-NEU], [CNT], [DIS], [MAJ]] The main weakness is that I would have liked to see more analysis and comparisons in the evaluation.[[MET-NEG,ANA-NEG], [SUB-NEG,CMP-NEG], [DFT], [MAJ]]\n\nEvaluation:\nCurrently, only neural abstractive methods are compared.[[MET-NEG], [SUB-NEG,CMP-NEG], [DFT,CRT], [MIN]] I would have liked to see the ROUGE performance of some current unsupervised multi-document extractive summarization methods, as well as some simple multi-document selection algorithms such as SumBasic.[[EXP-NEU,MET-NEU], [SUB-NEU], [DIS], [MIN]] Do redundancy cues which work for multi-document news summarization still work for this task?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]\n\nExtractiveness analysis:\nI would also have liked to see more analysis of how extractive the Wikipedia articles actually are, as well as how extractive the system outputs are.[[ANA-NEG], [SUB-NEG], [DFT], [MIN]] Does higher extractiveness correspond to higher or lower system ROUGE scores? [[MET-NEU], [EMP-NEU], [QSN], [MIN]]This would help us understand the difficulty of the problem, and how much abstractive methods could be expected to help.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] \n\nA further analysis which would be nice to do (though I have less clear ideas how to do it), would be to have some way to figure out which article types or which section types are amenable to this setup, and which are not.[[MET-NEU,ANA-NEU], [SUB-NEU], [SUG,DIS], [MIN]] \n\nI have some concern that extraction could do very well if you happen to find a related article in another website which contains encyclopedia-like or definition-like entries (e.g., Baidu, Wiktionary) which is not caught by clone detection.[[BIB-NEU], [null], [SUG,DIS], [MIN]] In this case, the problem could become less interesting, as no real analysis is required to do well here.[[PDI-NEG,ANA-NEG], [SUB-NEG,EMP-NEG], [CRT], [MAJ]]\n\nOverall, I quite like this line of work,[[OAL-POS], [CNT], [APC], [MAJ]] but I think the paper would be a lot stronger and more convincing with some additional work.[[OAL-NEU], [SUB-NEU], [SUG,CRT], [MAJ]]\n\n----\nAfter reading the authors' response and the updated submission, I am satisfied that my concerns above have been adequately addressed in the new version of the paper.[[OAL-POS], [CNT], [APC], [MAJ]] This is a very nice contribution.\n"[[OAL-POS], [CNT], [APC], [MAJ]]