"The authors propose a new network architecture for RL that contains some relevant inductive biases about planning.[[INT-POS], [null], [SMY], [GEN]] This fits into the recent line of work on implicit planning where forms of models are learned to be useful for a prediction/planning task.[[RWK-NEU,MET-POS], [EMP-POS], [APC], [MAJ]] The proposed architecture performs something analogous to a full-width tree search using an abstract model (learned end-to-end).[[RWK-NEU,MET-NEU], [CMP-NEU], [SMY], [MAJ]] This is done by expanding all possible transitions to a fixed depth before performing a max backup on all expanded nodes.[[EXP-NEU,MET-NEU], [null], [SMY], [GEN]] The final backup value is the Q-value prediction for a given state, or can represent a policy through a softmax.[[MET-NEU], [null], [SMY], [GEN]]\n\nI thought the paper was clear and well-motivated.[[PDI-POS], [CLA-POS], [SMY], [MAJ]] The architecture (and various associated tricks like state vector normalization) are well-described for reproducibility.[[MET-POS], [EMP-POS], [APC], [MAJ]] \n\nExperimental results seem promising but I wasn\u2019t fully convinced of its conclusions.[[EXP-POS,RES-POS,ANA-NEU], [EMP-POS], [DFT], [MAJ]] In both domains, TreeQN and AtreeC are compared to a DQN architecture, but it wasn\u2019t clear to me that this is the right baseline.[[RWK-NEU], [CMP-NEU], [DFT], [MAJ]] Indeed TreeQN and AtreeC share the same conv stack in the encoder (I think?), but also have the extra capacity of the tree on top.[[MET-NEU], [EMP-NEU], [DIS], [MAJ]] Can the performance gain we see in the Push task as a function of tree depth be explained by the added network capacity?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] Same comment in Atari, but there it\u2019s not really obvious that the proposed architecture is helping.[[RWK-NEU,MET-NEU], [EMP-NEU], [DFT], [MAJ]] Baselines could include unsharing the weights in the tree, removing the max backup, having a regular MLP with similar capacity, etc.[[RWK-NEU,MET-NEU], [EMP-NEU], [SUG], [MAJ]]\n\nPage 5, the auxiliary loss on reward prediction seems appropriate, but it\u2019s not clear from the text and experiments whether it actually was necessary.[[EXP-NEU,ANA-POS], [EMP-NEG], [DFT], [MAJ]] Is it that makes interpretability of the model easier (like we see in Fig 5c)? Or does it actually lead to better performance?[[MET-NEU], [EMP-NEU], [QSN], [MIN]]  \n\nDespite some shortcomings in the result section, I believe this is good work and worth communicating as is."[[RES-NEG], [REC-POS], [FBK], [MAJ]]