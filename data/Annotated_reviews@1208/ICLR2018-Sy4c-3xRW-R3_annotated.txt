"The paper discusses dropping out the pre-softmax logits in an adaptive manner.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] This isn't a huge conceptual leap given previous work, for instance that of Ba and Frey 2013 or the sequence of papers by Gal and his coauthors on variational interprations of dropout.[[RWK-NEG], [CMP-NEG], [CRT], [MIN]] In the spirit of the latter series of papers on variational dropout there is a derivation of this algorithm using ideas from variational inference.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] The variational approximation is a bit odd in that it doesn't have any variational parameters, and indeed a further regulariser in equation (14) is needed to give the desired behaviour.[[MET-NEG, RES-NEG], [EMP-NEG], [CRT], [MIN]] A fairly small, but consistent improvement on the base model and other similar ideas is reported in Table 1.[[RES-POS,TNF-POS], [EMP-POS], [APC], [MAJ]] I would have liked to have seen results on ImageNet.[[DAT-NEU,RES-NEU], [SUB-NEU], [DIS], [MIN]] I don't find (the too small) Figure 2 to be compelling evidence that \"our dropmax effectively prevents\noverfiting by converging to much lower test loss\".[[RES-NEG,TNF-NEG], [PNF-NEG,EMP-NEG], [CRT], [MIN]] The test loss in question looks like a noisy version of the base test loss with a slightly lower mean.[[RES-NEG], [EMP-NEG], [CRT], [MIN]] There are grammatical errors throughout the paper at a higher rate than would normally be found in a successful submission at this stage.[[OAL-NEG], [CLA-NEG], [CRT], [MIN]] Figure 3 illustrates the idea nicely.[[TNF-POS], [PNF-POS], [APC], [MIN]] Which of the MNIST models from Table 1 was used?\n"[[DAT-NEU,TNF-NEU], [EMP-NEU], [QSN], [MIN]]