"The work investigates convergence guarantees of gradient-type policies for reinforcement learning and continuous control\nproblems, both in deterministic and randomized case, whiling coping with non-convexity of the objective.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] I found that the paper suffers many shortcomings that must be addressed:\n\n1) The writing and organization is quite cumbersome and should be improved.[[OAL-NEG], [CLA-NEG], [CRT], [MIN]]\n2) The authors state in the abstract (and elsewhere): \"... showing that (model free) policy gradient methods globally converge to the optimal solution ...\". This is misleading and NOT true.[[ABS-NEG], [CLA-NEG], [CRT], [MAJ]] The authors show the convergence of the objective but not of the iterates sequence.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] This should be rephrased elsewhere.[[MET-NEG], [CLA-NEG], [CRT], [MAJ]]\n3) An important literature on convergence of descent-type methods for semialgebraic objectives is available but not discussed."[[MET-NEG], [SUB-NEG], [CRT], [MIN]]