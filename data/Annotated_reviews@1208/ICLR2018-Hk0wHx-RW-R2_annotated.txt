"This paper presents a sparse latent representation learning algorithm based on an information theoretic objective formulated through meta-Gaussian information bottleneck and solved via variational auto-encoder stochastic optimization.[[INT-NEU,MET-NEU], [null], [SMY], [GEN]] The authors suggest Gaussianify the data using copula transformation and  further adopt a diagonal determinant approximation with justification of minimizing an upper bound of mutual information.[[PDI-NEU,EXP-NEU,ANA-NEU], [EMP-NEU], [SUG], [GEN]]  Experiments include both artificial data and real data.[[DAT-NEU,EXP-NEU], [null], [SMY], [GEN]] \n\nThe paper is unclear at some places and writing gets confusing.[[OAL-NEG], [CLA-NEG], [CRT], [MIN]] For example, it is unclear whether and when explicit or implicit transforms are used for x and y in the experiments, and the discussion at the end of Section 3.3 also sounds confusing.[[RWK-NEG,EXP-NEG], [CLA-NEG,EMP-NEG], [CRT], [MIN]] It would be more helpful if the author can make those points more clear and offer some guidance about the choices between explicit and implicit transform in practice.[[PDI-NEU,EXP-NEU], [CLA-NEU], [SUG,CRT], [GEN]] Moreover, what is the form of f_beta and how beta is optimized?[[PDI-NEU,EXP-NEU], [null], [QSN], [GEN]]  In the first equation on page 5, is tilde y involved?[[RWK-NEU], [null], [QSN], [GEN]] How to choose lambda?[[PDI-NEU], [null], [QSN], [GEN]]\n\nIf MI is invariant to monotone transformations and information curves are determined by MIs, why \u201ctransformations basically makes information curve arbitrary\u201d?[[PDI-NEU,EXP-NEU], [null], [QSN], [GEN]] Can you elaborate?[[RWK-NEU], [null], [QSN], [GEN]]  \n\nAlthough the experimental results demonstrate that the proposed approach with copula transformation yields higher information curves, more compact representation and better reconstruction quality, it would be more significant if the author can show whether these would necessarily lead to any improvements on other goals such as classification accuracy or robustness under adversarial attacks.[[RWK-NEU,EXP-NEU,RES-NEU,ANA-NEU], [IMP-NEU], [SUG], [GEN]] \n\nMinor comments: \n\n- What is the meaning of the dashed lines and the solid lines respectively in Figure 1? [[RWK-NEU,TNF-NEU], [null], [QSN], [GEN]]\n- Section 3.3 at the bottom of page 4: what is tilde t_j? and x in the second term?[[RWK-NEU], [null], [QSN], [GEN]] Is there a typo?[[RWK-NEU], [null], [QSN], [GEN]] \n- typo, find the \u201cmost orthogonal\u201d representation if the inputs -> of the inputs [[RWK-NEU], [null], [QSN], [GEN]]\n\nOverall, the main idea of this paper is interesting and well motivated and but the technical contribution seems incremental.[[PDI-POS,OAL-POS], [IMP-POS,EMP-NEU], [APC,FBK], [MAJ]] The paper suffers from lack of clarity at several places and the experimental results are convincing but not strong enough.[[EXP-NEG,RES-NEG,OAL-NEG], [CLA-NEG,EMP-NEG], [DFT], [MIN]] \n\n***************\nUpdates: \n***************\nThe authors have clarified some questions that I had and further demonstrated the benefits of copula transform with new experiments in the revised paper.[[EXT-NEU], [null], [SUG,DIS,FBK], [GEN]] The new results are quite informative and addressed some of the concerns raised by me and other reviewers.[[EXT-NEU], [null], [SUG,FBK], [GEN]] I have updated my score to 6 accordingly. [[EXT-NEU], [null], [FBK], [GEN]]\n\n\n"