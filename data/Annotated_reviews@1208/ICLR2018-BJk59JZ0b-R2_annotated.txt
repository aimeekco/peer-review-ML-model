"The paper introduces a modified actor-critic algorithm where a \u201cguide actor\u201d uses approximate second order methods to aid computation.[[INT-NEU,MET-NEU], [null], [SMY], [GEN]] The experimental results are similar to previously proposed methods.[[EXP-NEU,MET-NEU], [CMP-NEU], [DIS], [MIN]] \n\nThe paper is fairly well-written, provides proofs of detailed properties of the algorithm, and has decent experimental results.[[EXP-POS,MET-POS,RES-POS], [CLA-POS,EMP-POS], [APC], [MAJ]] However, the method is not properly motivated.[[MET-POS], [EMP-POS], [APC], [MAJ]] As far as I can tell, the paper never answers the questions: Why do we need a guide actor?[[MET-NEU], [null], [QSN], [MIN]] What problem does the guide actor solve?[[MET-NEU], [EMP-NEU], [QSN], [MIN]] \n\nThe paper argues that the guide actor allows to introduce second order methods, but (1) there are other ways of doing so and[[MET-NEU], [EMP-NEU], [DIS], [GEN]] (2) it\u2019s not clear why we should want to use second-order methods in reinforcement learning in the first place.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] Using second order methods is not an end in itself.[[MET-NEU], [EMP-NEU], [DIS], [GEN]] The experimental results show the authors have found a way to use second order methods without making performance *worse*.[[EXP-POS,RES-POS], [EMP-POS], [APC], [MAJ]] Given the high variability of deep RL, they have not convincingly shown it performs better.[[RES-POS], [EMP-POS], [APC], [MAJ]]\n\nThe paper does not discuss the computational cost of the method.[[ANA-NEG], [SUB-NEG], [DFT], [MIN]] How does it compare to other methods?[[MET-NEU], [CMP-NEU], [QSN], [MIN]] My worry is that the method is more complicated and slower than existing methods, without significantly improved performance.[[MET-NEG], [CMP-NEG], [CRT], [MAJ]]\n\nI recommend the authors take the time to make a much stronger conceptual and empirical case for their algorithm. \n"[[MET-NEU], [EMP-NEU], [SUG], [MIN]]