"The main idea of the paper is to improve off-policy policy gradient estimates using control variates based on multi-step rollouts, and reduce the variance of those control variates using the reparameterization trick.[[PDI-POS,EXP-NEU,MET-NEU], [EMP-NEU], [SMY,APC], [MAJ]]This is laid out primarily in Equations 1-5, and seems like a nice idea, although I must admit I had some trouble following the maths in Equation 5.[[PDI-POS], [EMP-POS], [APC], [MAJ]] They include results showing that their method has better sample efficiency than TRPO (which their method also uses under the hood to update value function parameters).[[MET-POS,RES-POS], [IMP-POS,EMP-POS], [APC], [MAJ]]\n\nMy main issue with this paper is that the empirical section is a bit weak, for instance only one run seems to be shown for both methods, there is no mention of hyper-parameter selection, and the measure used for generating Table 1 seems pretty arbitrary to me (how were those thresholds chosen?).[[RWK-NEU,EXP-NEU,MET-NEU,OAL-NEU], [EMP-NEG], [SUG,QSN], [GEN]] In addition, one thing I would have liked to get out of this paper is a better understanding of how much each component helps.[[OAL-POS], [CLA-POS,IMP-POS], [APC], [MAJ]] This could have been done via empirical work, for instance:\n- Explore the effect of the planning horizon, and implicitly compare to SVG(1), which as the authors point out is the same as their method with a horizon of 1.[[RWK-NEU], [EMP-NEU], [SMY], [GEN]]\n- Show the effect of the reparameterization trick on estimator variance.\n- Compare the bias and variance of TRPO estimates vs the proposed method.[[PDI-NEU,EXP-NEU,MET-NEU], [CMP-NEU,EMP-NEU], [SMY], [GEN]]"