"Authors proposed a new neural-network based machine translation method that generates the target sentence by generating multiple partial segments in the target sentence from different positions in the source information.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]] The model is based on the SWAN architecture which is previously proposed, and an additional \"local reordering\" layer to reshuffle source information to adjust those positions to the target sentence.[[RWK-NEU,PDI-NEU], [null], [SMY], [GEN]]\n\nUsing the SWAN architecture looks more reasonable than the conventional attention mechanism when the ground-truth word alignment is monotone.[[RWK-POS,MET-POS], [CMP-POS], [APC], [MAJ]] Also, the concept of local reordering mechanism looks well to improve the basic SWAN model to reconfigure it to the situation of machine translation tasks.[[MET-POS], [EMP-POS], [APC], [MAJ]]\n\nThe \"window size\" of the local reordering layer looks like the \"distortion limit\" used in traditional phrase-based statistical machine translation methods, and this hyperparameter may impose a similar issue with that of the distortion limit into the proposed model; small window sizes may drop information about long dependency.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] For example, verbs in German sentences sometimes move to the tail of the sentence and they introduce a dependency between some distant words in the sentence.[[MET-NEU], [EMP-NEU], [DIS], [MIN]] Since reordering windows restrict the context of each position to a limited number of neighbors, it may not capture distant information enough.[[EXP-NEG,MET-NEG], [EMP-NEU], [DIS], [MIN]] I expected that some observations about this point will be unveiled in the paper, but unfortunately, the paper described only a few BLEU scores with different window sizes which have not enough information about it.[[DAT-NEG,ANA-NEG], [SUB-NEG], [DFT,CRT], [MAJ]] It is useful for all followers of this paper to provide some observations about this point.[[ANA-NEG], [SUB-NEG], [SUG], [MAJ]]\nIn addition, it could be very meaningful to provide some experimental results on linguistically distant language pairs, such as Japanese and English, or simply reversing word orders in either source or target sentences (this might work to simulate the case of distant reordering).[[EXP-NEU,RES-NEU], [SUB-NEU,EMP-NEU], [SUG], [MAJ]]\n\nAuthors argued some differences between conventional attention mechanism and the local reordering mechanism, but it is somewhat unclear that which ones are the definite difference between those approaches.[[MET-NEG], [CMP-NEG], [CRT], [MAJ]]\n\nA super interesting and mysterious point of the proposed method is that it achieves better BLEU than conventional methods despite no any global language models (Table 1 row 8),[[MET-POS,RES-POS,TNF-POS], [EMP-POS], [APC], [MAJ]] and the language model options (Table 1 row 9 and footnote 4) may reduce the model accuracy as well as it works not so effectively.[[MET-NEG], [EMP-NEG], [DIS], [MIN]] This phenomenon definitely goes against the intuitions about developing most of the conventional machine translation models.[[FWK-NEG], [IMP-NEG], [DIS], [MIN]] Specifically, it is unclear how the model correctly treats word connections between segments without any global language model.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] Authors should pay attention to explain more detailed analysis about this point in the paper.[[ANA-NEU], [SUB-NEU], [SUG], [MIN]]\n\nEq. (1) is incorrect[[MET-NEG], [EMP-NEG], [CRT], [MIn]]. According to Fig. 2, the conditional probability in the product operator should be revised to p(a_t | x_{1:t}, a_{1:t-1}), and the independence approximation to remove a_{1:t-1} from the conditions should also be noted in the paper.[[MET-NEU,TNF-NEU], [SUB-NEU], [SUG], [MAJ]]\nNevertheless, the condition x_{1:t} could not be reduced because the source position is always conditioned by all previous positions through an RNN.\n\n"[[MET-NEU], [EMP-NEU], [DIS], [GEN]]