"The authors propose reducing the number of parameters learned by a deep network by setting up sparse connection weights in classification layers.[[INT-NEU,PDI-NEU], [null], [SMY], [MIN]] Numerical experiments show that such sparse networks can have similar performance to fully connected ones.[[MET-NEU], [null], [DIS], [MIN]] They introduce a concept of \u201cscatter\u201d that correlates with network performance.[[MET-NEU], [null], [DIS], [MIN]] Although  I found the results useful and potentially promising,[[RES-POS], [EMP-POS], [APC], [MAJ]] I did not find much insight in this paper.[[OAL-NEG], [CNT], [CRT], [MAJ]]\nIt was not clear to me why scatter (the way it is defined in the paper) would be a useful performance proxy anywhere but the first classification layer.[[MET-NEG], [EMP-NEG], [CRT], [MAJ]] Once the signals from different windows are intermixed, how do you even define the windows?[[MET-NEG], [EMP-NEG], [QSN], [MIN]]  \nMinor\nSecond line of Section 2.1: \u201clesser\u201d -> less or fewer\n"[[MET-NEU], [EMP-NEU], [CRT], [MIN]]