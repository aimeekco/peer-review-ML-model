"This paper presents Advantage-based Regret Minimization, somewhat similar to advantage actor-critic with REINFORCE.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]]\nThe main focus of the paper seems to be the motivation/justification of this algorithm with connection to the regret minimization literature (and without Markov assumptions).[[MET-NEU], [null], [SMY], [GEN]]\nThe claim that ARM is more robust to partially observable domains is supported by experiments where it outperforms DQN.[[EXP-POS], [EMP-POS], [APC], [MAJ]]\n\nThere are several things to like about this paper:\n- The authors do a good job of reviewing/referencing several papers in the field of \"regret minimization\" that would probably be of interest to the ICLR community + provide non-obvious connections / summaries of these perspectives.[[RWK-POS,BIB-POS], [CMP-POS,EMP-POS], [APC], [MAJ]]\n- The issue of partial observability is good to bring up, rather than simply relying on the MDP framework that is often taken as a given in \"deep reinforcement learning\".[[MET-POS], [EMP-POS], [APC], [MAJ]]\n- The experimental results show that ARM outperforms DQN on a suite of deep RL tasks.[[EXP-POS], [EMP-POS], [APC], [MAJ]]\n\nHowever, there are also some negatives:\n- Reviewing so much of the CFR-literature in a short paper means that it ends up feeling a little rushed and confused.[[RWK-NEG], [SUB-NEG], [CRT], [MAJ]]\n- The ultimate algorithm *seems* like it is really quite similar to other policy gradient methods such as A3C, TRPO etc.[[MET-NEG], [CMP-NEG], [CRT], [MIN]] At a high enough level, these algorithms can be written the same way... there are undoubtedly some key differences in how they behave, but it's not spelled out to the reader and I think the connections can be missed.[[MET-NEU], [EMP-NEU], [SUG], [MIN]]\n- The experiment/motivation I found most compelling was 4.1 (since it clearly matches the issue of partial observability)[[EXP-POS,RES-POS], [EMP-POS], [APC], [MAJ]] but we only see results compared to DQN ... it feels like you don't put a compelling case for the non-Markovian benefits of ARM vs other policy gradient methods.[[MET-NEG,RES-NEG], [EMP-NEG], [CRT], [MAJ]] Yes A3C and TRPO seem like they perform very poorly compared to ARM[[MET-NEG], [CMP-NEG], [CRT], [MAJ]]... but I'm left wondering how/why?[[EXP-NEU], [EMP-NEU], [QSN], [MAJ]]\n\nI feel like this paper is in a difficult position of trying to cover a lot of material/experiments in too short a paper.[[EXP-NEG], [SUB-NEG], [CRT], [MAJ]]\nA lot of the cited literature was also new to me, so it could be that I'm missing something about why this is so interesting.[[BIB-NEG], [null], [DIS], [MIN]]\nHowever, I came away from this paper quite uncertain about the real benefits/differences of ARM versus other similar policy gradient methods[[MET-NEG], [CMP-NEG], [CRT], [MIN]]... I also didn't feel the experimental evaluations drove a clear message except \"ARM did better than all other methods on these experiments\"[[EXP-NEG,MET-NEU], [CMP-NEG], [CRT], [MAJ]]... I'd want to understand how/why and whether we should expect this universally.[[EXT-NEU], [CNT], [DIS], [MIN]]\nThe focus on \"regret minimization perspectives\" didn't really get me too excited[[MET-NEG], [CMP-NEG], [CRT], [MAJ]]...\n\nOverall I would vote against acceptance for this version.\n"[[OAL-NEG], [REC-NEG], [FBK], [MAJ]]