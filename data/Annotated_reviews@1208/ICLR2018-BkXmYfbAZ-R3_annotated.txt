"- The paper proposes to learn a soft ordering over a set of layers for multitask learning (MTL) i.e.\n  at every step of the forward propagation, each task is free to choose its unique soft (`convex')\n  combination of the outputs from all available layers.[[INT-NEU], [null], [SMY], [GEN]] This idea is novel and interesting.[[PDI-POS], [NOV-POS], [APC], [MAJ]]\n- The learning of such soft combination is done jointly while learning the tasks and is not set\n  manually cf. setting permutations of a fixed number of layer per task.[[MET-NEU], [EMP-NEU], [SMY], [MAJ]]\n- The empirical evaluation is done on intuitively related, superficially unrelated, and a real world\n  task.[[DAT-NEU,EXP-NEU], [null], [SMY], [GEN]] The first three results are on small datasets/tasks, O(10) feature dimensions, and number of\n  tasks and O(1000) images; (i) distinguish two MNIST digits, (ii) 10 UCI tasks with feature sizes\n  4--30 and number of classes 2--10, (iii) 50 different character recognition on Omniglot dataset.[[DAT-NEU], [SUB-NEU], [SMY], [GEN]]\n  The last task is real world -- 40 attribute classification on the CelebA face dataset of 200K\n  images.[[DAT-NEU], [SUB-NEU], [SMY], [GEN]] While the first three tasks are smaller proof of concept, the last task could have been\n  more convincing if near state-of-the-art methods were used.[[RWK-NEU,EXP-NEU], [EMP-NEU], [SUG], [MAJ]] The authors use a Resnet-50 which is a\n  smaller and lesser performing model, they do mention that benefits are expected to be \n  complimentary to say larger model, but in general it becomes harder to improve strong models.[[MET-NEU], [EMP-NEU], [SMY], [MAJ]]\n  While this does not significantly dilute the message, it would have made it much more convincing\n  if results were given with stronger networks.[[MET-NEU,RES-NEU], [EMP-NEU], [SUG], [MAJ]]                      \n- The results are otherwise convincing and clear improvements are shown with the proposed method.[[MET-POS,RES-POS], [EMP-POS], [APC], [MAJ]]\n- The number of layers over which soft ordering was tested was fixed however. [[MET-NEU], [EMP-NEU], [SMY], [MAJ]]It would be\n  interesting to see what would the method learn if the number of layers was explicitly set to be\n  large and an identity layer was put as one of the option.[[MET-NEU], [EMP-NEU], [SUG], [MAJ]] In that case the soft ordering could\n  actually learn the optimal depth as well, repeating identity layer beyond the option number of\n  layers.[[MET-NEU], [EMP-NEU], [SUG], [MAJ]]                                                            \n                                                                     \nOverall, the paper presents a novel idea, which is well motivated and clearly presented.[[PDI-POS,MET-POS], [NOV-POS,PNF-POS], [APC], [MAJ]] The \nempirical validation, while being limited in some aspects, is largely convincing.[[EXP-POS], [EMP-POS], [APC], [MAJ]]"