"Summary:\nThe paper presents three different methods of training a low precision student network from a teacher network using knowledge distillation.[[INT-NEU,PDI-NEU], [null], [SMY], [GEN]]\nScheme A consists of training a high precision teacher jointly with a low precision student.[[MET-NEU], [EMP-NEU], [SMY], [GEN]] Scheme B is the traditional knowledge distillation method and Scheme C uses knowledge distillation for fine-tuning a low precision student which was pretrained in high precision mode.[[MET-NEU], [EMP-NEU], [SMY], [GEN]]\n\nReview:\nThe paper is well written. [[OAL-POS], [CLA-POS], [APC], [MAJ]]The experiments are clear and the three different schemes provide good analytical insights.[[EXP-POS,MET-POS,ANA-POS], [EMP-POS], [APC], [MAJ]]\nUsing scheme B  and C student model with low precision could achieve accuracy close to teacher while compressing the model.\n\n[[EXP-NEU,MET-NEU,RES-NEU], [EMP-NEU], [SUG], [MIN]]Comments:\nTensorflow citation is missing.\n[[BIB-NEG], [PNF-NEG], [DFT], [MIN]]Conclusion is short and a few directions for future research would have been useful."[[FWK-NEU], [IMP-NEU], [SUG], [MIN]]